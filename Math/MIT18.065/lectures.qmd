---
title: "MIT 18.065: Linear Algebra Applications"
author: "Chao Ma"
date: "2025-11-29"
---

## All Lectures

My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. This course explores how linear algebra powers modern applications in data analysis, signal processing, and machine learning.

---

::: {.content-grid}

::: {.content-card}
**[Lecture 1: The Column Space of A Contains All Vectors Ax](mit18065-lecture1-column-space.qmd)**
How column combinations define $C(A)$, why rank counts independent columns, and how CR factorization exposes the basis for column and row space. A column-by-row view of $AB$ shows every column of $AB$ lies in $C(A)$.
:::

::: {.content-card}
**[Lecture 2: Multiplying and Factoring Matrices](mit18065-lecture2-multiplying-factoring.qmd)**
From elimination and $LU$ to rank-1 updates, $QR$, spectral decomposition, diagonalization, and SVD. Ends with the four fundamental subspaces and a null space example.
:::

::: {.content-card}
**[Lecture 3: Orthonormal Columns](mit18065-lecture3-orthonormal-columns.qmd)**
Orthogonal matrices with $Q^\top Q = I$: rotation matrices that preserve length, reflection matrices (including Householder reflections $H = I - 2uu^\top$), Hadamard matrices built recursively for signal processing, and wavelets that capture multiscale structure. Eigenvectors of symmetric matrices are orthogonal—culminating in discrete Fourier transforms.
:::

::: {.content-card}
**[Lecture 4: Eigenvalues and Eigenvectors](mit18065-lecture4-eigenvalues-eigenvectors.qmd)**
Eigenvectors $Ax = \lambda x$ preserve direction under transformation. Key properties: $A^k x = \lambda^k x$, solving difference equations $v_{k+1} = Av_k$ and differential equations. Similar matrices $B = M^{-1}AM$ share eigenvalues. Symmetric matrices have real eigenvalues and orthogonal eigenvectors, enabling spectral decomposition $S = Q\Lambda Q^\top$.
:::

::: {.content-card}
**[Lecture 5: Positive Definite and Semidefinite Matrices](mit18065-lecture5-positive-definite.qmd)**
Positive definite matrices have all $\lambda_i > 0$ and energy $x^\top Sx > 0$ for all $x \ne 0$. Factorize as $S = AA^\top$ with full rank A. The quadratic form creates an energy bowl—convex and minimizable via gradient descent. This connects linear algebra to optimization: deep learning minimizes loss landscapes just as we minimize $x^\top Sx$. Positive semidefinite relaxes to $\lambda_i \ge 0$.
:::

::: {.content-card}
**[Lecture 8: Norms of Vectors and Matrices](mit18065-lecture8-norms.qmd)**
Understanding vector p-norms ($\|v\|_p$) and when they satisfy the triangle inequality (only for $p \geq 1$). The "$\frac{1}{2}$-norm" creates non-convex unit balls for strong sparsity. S-norms $\sqrt{v^T S v}$ generalize to ellipses. Matrix norms: spectral $\|A\|_2 = \sigma_1$, Frobenius $\|A\|_F = \sqrt{\sum \sigma_i^2}$, nuclear $\|A\|_N = \sum \sigma_i$—all expressed via singular values.
:::

::: {.content-card}
**[Lecture 9: Four Ways to Solve Least Squares Problems](mit18065-lecture9-least-squares.qmd)**
Four equivalent methods for solving $Ax = b$ when $A$ has no inverse: pseudo-inverse $\hat{x} = A^+ b$ using SVD, normal equations $A^T A \hat{x} = A^T b$, algebraic minimization of $\|Ax - b\|_2^2$, and geometric projection of $b$ onto $C(A)$. All methods converge to the same solution when $A^T A$ is invertible: $\hat{x} = (A^T A)^{-1}A^T b$.
:::

:::
