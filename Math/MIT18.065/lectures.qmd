---
title: "MIT 18.065: Linear Algebra Applications"
author: "Chao Ma"
date: "2025-11-29"
---

## All Lectures

My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. This course explores how linear algebra powers modern applications in data analysis, signal processing, and machine learning.

---

::: {.content-grid}

::: {.content-card}
**[Lecture 1: The Column Space of A Contains All Vectors Ax](mit18065-lecture1-column-space.qmd)**
How column combinations define $C(A)$, why rank counts independent columns, and how CR factorization exposes the basis for column and row space. A column-by-row view of $AB$ shows every column of $AB$ lies in $C(A)$.
:::

::: {.content-card}
**[Lecture 2: Multiplying and Factoring Matrices](mit18065-lecture2-multiplying-factoring.qmd)**
From elimination and $LU$ to rank-1 updates, $QR$, spectral decomposition, diagonalization, and SVD. Ends with the four fundamental subspaces and a null space example.
:::

::: {.content-card}
**[Lecture 3: Orthonormal Columns](mit18065-lecture3-orthonormal-columns.qmd)**
Orthogonal matrices with $Q^\top Q = I$: rotation matrices that preserve length, reflection matrices (including Householder reflections $H = I - 2uu^\top$), Hadamard matrices built recursively for signal processing, and wavelets that capture multiscale structure. Eigenvectors of symmetric matrices are orthogonal—culminating in discrete Fourier transforms.
:::

::: {.content-card}
**[Lecture 8: Norms of Vectors and Matrices](mit18065-lecture8-norms.qmd)**
Understanding vector p-norms ($\|v\|_p$) and when they satisfy the triangle inequality (only for $p \geq 1$). The "$\frac{1}{2}$-norm" creates non-convex unit balls for strong sparsity. S-norms $\sqrt{v^T S v}$ generalize to ellipses. Matrix norms: spectral $\|A\|_2 = \sigma_1$, Frobenius $\|A\|_F = \sqrt{\sum \sigma_i^2}$, nuclear $\|A\|_N = \sum \sigma_i$—all expressed via singular values.
:::

::: {.content-card}
**[Lecture 9: Four Ways to Solve Least Squares Problems](mit18065-lecture9-least-squares.qmd)**
Four equivalent methods for solving $Ax = b$ when $A$ has no inverse: pseudo-inverse $\hat{x} = A^+ b$ using SVD, normal equations $A^T A \hat{x} = A^T b$, algebraic minimization of $\|Ax - b\|_2^2$, and geometric projection of $b$ onto $C(A)$. All methods converge to the same solution when $A^T A$ is invertible: $\hat{x} = (A^T A)^{-1}A^T b$.
:::

:::
