---
title: "MIT 18.065: Linear Algebra Applications"
author: "Chao Ma"
date: "2025-11-29"
---

## All Lectures

My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. This course explores how linear algebra powers modern applications in data analysis, signal processing, and machine learning.

---

::: {.content-grid}

::: {.content-card}
**[Lecture 8: Norms of Vectors and Matrices](mit18065-lecture8-norms.qmd)**
Understanding vector p-norms ($\|v\|_p$) and when they satisfy the triangle inequality (only for $p \geq 1$). The "$\frac{1}{2}$-norm" creates non-convex unit balls for strong sparsity. S-norms $\sqrt{v^T S v}$ generalize to ellipses. Matrix norms: spectral $\|A\|_2 = \sigma_1$, Frobenius $\|A\|_F = \sqrt{\sum \sigma_i^2}$, nuclear $\|A\|_N = \sum \sigma_i$â€”all expressed via singular values.
:::

::: {.content-card}
**[Lecture 9: Four Ways to Solve Least Squares Problems](mit18065-lecture9-least-squares.qmd)**
Four equivalent methods for solving $Ax = b$ when $A$ has no inverse: pseudo-inverse $\hat{x} = A^+ b$ using SVD, normal equations $A^T A \hat{x} = A^T b$, algebraic minimization of $\|Ax - b\|_2^2$, and geometric projection of $b$ onto $C(A)$. All methods converge to the same solution when $A^T A$ is invertible: $\hat{x} = (A^T A)^{-1}A^T b$.
:::

:::
