{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Deep Learning Book 6.2: Likelihood-Based Loss Functions\"\n",
    "author: \"Chao Ma\"\n",
    "date: \"2025-09-25\"\n",
    "categories: [\"Deep Learning\", \"Loss Functions\", \"Maximum Likelihood\"]\n",
    "code-fold: true\n",
    "code-summary: \"Show code\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This recap of Deep Learning Chapter 6.2 reveals the fundamental connection between probabilistic assumptions and the loss functions we use to train neural networks.*\n",
    "\n",
    "ðŸ““ **For a deeper dive with additional exercises and analysis**, see the [complete notebook on GitHub](https://github.com/ickma2311/foundations/blob/main/deep_learning/chapter6/6.2/exercises.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hidden Connection: Why These Loss Functions?\n",
    "\n",
    "Ever wondered why we use mean squared error for regression, cross-entropy for classification, and other specific loss functions? The answer lies in **maximum likelihood estimation** - each common loss function corresponds to the negative log-likelihood of a specific probabilistic model.\n",
    "\n",
    "| **Probabilistic Model** | **Loss Function** | **Use Case** |\n",
    "|-------------------------|-------------------|---------------|\n",
    "| Gaussian likelihood | Mean Squared Error | Regression |\n",
    "| Bernoulli likelihood | Binary Cross-Entropy | Binary Classification |\n",
    "| Categorical likelihood | Softmax Cross-Entropy | Multiclass Classification |"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ðŸŽ¯ Exploring the Connection: Probabilistic Models â†’ Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection 1: Gaussian Likelihood â†’ Mean Squared Error\n",
    "\n",
    "**The Setup**: When we assume our targets have Gaussian noise around our predictions:\n",
    "\n",
    "$$p(y|x) = \\mathcal{N}(y; \\hat{y}, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\hat{y})^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "**The Derivation**: Taking negative log-likelihood:\n",
    "\n",
    "$$-\\log p(y|x) = \\frac{(y-\\hat{y})^2}{2\\sigma^2} + \\frac{1}{2}\\log(2\\pi\\sigma^2)$$\n",
    "\n",
    "**The Result**: Minimizing this is equivalent to minimizing MSE (the constant term doesn't affect optimization)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Gaussian Likelihood â†” MSE Connection\n",
      "=============================================\n",
      "ðŸ“ˆ Mean Squared Error:     1.450860\n",
      "ðŸ“Š Gaussian NLL:           71.159339\n",
      "   â”œâ”€ Quadratic term:      72.542985\n",
      "   â””â”€ Constant term:       -1.383647\n",
      "\n",
      "ðŸ”— Mathematical Connection:\n",
      "   Quadratic term = 50.0 Ã— MSE\n",
      "   72.542985 = 50.0 Ã— 1.450860\n",
      "\n",
      "âœ… Minimizing MSE â‰¡ Maximizing Gaussian likelihood\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Gaussian likelihood = MSE connection\n",
    "np.random.seed(0)\n",
    "x = np.linspace(-1, 1, 20)\n",
    "y_true = 2 * x + 1\n",
    "y = y_true + np.random.normal(0, 0.1, size=x.shape)  # Gaussian noise\n",
    "\n",
    "# Simple linear model predictions\n",
    "w, b = 1.0, 0.0\n",
    "y_pred = w * x + b\n",
    "\n",
    "# Compute MSE\n",
    "mse = np.mean((y - y_pred)**2)\n",
    "\n",
    "# Compute Gaussian negative log-likelihood\n",
    "sigma_squared = 0.1**2\n",
    "quadratic_term = 0.5 * np.mean((y - y_pred)**2) / sigma_squared\n",
    "const_term = 0.5 * np.log(2 * np.pi * sigma_squared)\n",
    "nll_gaussian = quadratic_term + const_term\n",
    "\n",
    "print(\"ðŸ“Š Gaussian Likelihood â†” MSE Connection\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"ðŸ“ˆ Mean Squared Error:     {mse:.6f}\")\n",
    "print(f\"ðŸ“Š Gaussian NLL:           {nll_gaussian:.6f}\")\n",
    "print(f\"   â”œâ”€ Quadratic term:      {quadratic_term:.6f}\")\n",
    "print(f\"   â””â”€ Constant term:       {const_term:.6f}\")\n",
    "\n",
    "scaling_factor = 1 / (2 * sigma_squared)\n",
    "print(f\"\\nðŸ”— Mathematical Connection:\")\n",
    "print(f\"   Quadratic term = {scaling_factor:.1f} Ã— MSE\")\n",
    "print(f\"   {quadratic_term:.6f} = {scaling_factor:.1f} Ã— {mse:.6f}\")\n",
    "print(f\"\\nâœ… Minimizing MSE â‰¡ Maximizing Gaussian likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection 2: Bernoulli Likelihood â†’ Binary Cross-Entropy\n",
    "\n",
    "**The Setup**: For binary classification, we assume Bernoulli-distributed targets:\n",
    "\n",
    "$$p(y|x) = \\sigma(z)^y (1-\\sigma(z))^{1-y}$$\n",
    "\n",
    "where $\\sigma(z) = \\frac{1}{1+e^{-z}}$ is the sigmoid function.\n",
    "\n",
    "**The Derivation**: Taking negative log-likelihood:\n",
    "\n",
    "$$-\\log p(y|x) = -y\\log\\sigma(z) - (1-y)\\log(1-\\sigma(z))$$\n",
    "\n",
    "**The Result**: This is exactly binary cross-entropy loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ² Bernoulli Likelihood â†” Binary Cross-Entropy\n",
      "==================================================\n",
      "Input Data:\n",
      "   Logits:        [-0.5 -0.8  0.   0.8  0.5]\n",
      "   Labels:        [0. 0. 1. 1. 1.]\n",
      "   Probabilities: [0.37754068 0.3100255  0.5        0.6899745  0.62245935]\n",
      "\n",
      "ðŸ“Š Loss Function Comparison:\n",
      "   Manual Bernoulli NLL:  0.476700\n",
      "   PyTorch BCE Loss:      0.476700\n",
      "\n",
      "ðŸ”— Verification:\n",
      "   Absolute difference:   0.0000000000\n",
      "\n",
      "âœ… Binary cross-entropy IS Bernoulli negative log-likelihood!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Bernoulli likelihood = Binary cross-entropy connection\n",
    "z = torch.tensor([-0.5, -0.8, 0.0, 0.8, 0.5])  # Model logits\n",
    "y = torch.tensor([0.0, 0.0, 1.0, 1.0, 1.0])     # Binary labels\n",
    "p = torch.sigmoid(z)  # Convert to probabilities\n",
    "\n",
    "print(\"ðŸŽ² Bernoulli Likelihood â†” Binary Cross-Entropy\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Input Data:\")\n",
    "print(f\"   Logits:        {z.numpy()}\")\n",
    "print(f\"   Labels:        {y.numpy()}\")\n",
    "print(f\"   Probabilities: {p.numpy()}\")\n",
    "\n",
    "# Manual Bernoulli NLL computation\n",
    "bernoulli_nll = torch.mean(-(y * torch.log(p) + (1 - y) * torch.log(1 - p)))\n",
    "\n",
    "# PyTorch binary cross-entropy\n",
    "bce_loss = F.binary_cross_entropy(p, y)\n",
    "\n",
    "print(f\"\\nðŸ“Š Loss Function Comparison:\")\n",
    "print(f\"   Manual Bernoulli NLL:  {bernoulli_nll:.6f}\")\n",
    "print(f\"   PyTorch BCE Loss:      {bce_loss:.6f}\")\n",
    "\n",
    "# Verify they're identical\n",
    "difference = torch.abs(bernoulli_nll - bce_loss)\n",
    "print(f\"\\nðŸ”— Verification:\")\n",
    "print(f\"   Absolute difference:   {difference:.10f}\")\n",
    "print(f\"\\nâœ… Binary cross-entropy IS Bernoulli negative log-likelihood!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection 3: Categorical Likelihood â†’ Softmax Cross-Entropy\n",
    "\n",
    "**The Setup**: For multiclass classification, we use the categorical distribution:\n",
    "\n",
    "$$p(y=i|x) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} = \\text{softmax}(z)_i$$\n",
    "\n",
    "**The Derivation**: Taking negative log-likelihood:\n",
    "\n",
    "$$-\\log p(y|x) = -\\log \\frac{e^{z_y}}{\\sum_j e^{z_j}} = -z_y + \\log\\sum_j e^{z_j}$$\n",
    "\n",
    "**The Result**: This is exactly softmax cross-entropy loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Categorical Likelihood â†” Softmax Cross-Entropy\n",
      "=======================================================\n",
      "Input Data:\n",
      "   Logits shape:    torch.Size([3, 3])\n",
      "   True classes:    [2 1 0]\n",
      "\n",
      "Softmax Probabilities:\n",
      "   Sample 1: [0.25462854 0.28140804 0.46396342] â†’ Class 2\n",
      "   Sample 2: [0.25462854 0.46396342 0.28140804] â†’ Class 1\n",
      "   Sample 3: [0.46396342 0.25462854 0.28140804] â†’ Class 0\n",
      "\n",
      "ðŸ“Š Loss Function Comparison:\n",
      "   Manual Categorical NLL: 0.767950\n",
      "   PyTorch Cross-Entropy:  0.767950\n",
      "\n",
      "ðŸ”— Verification:\n",
      "   Absolute difference:    0.0000000000\n",
      "\n",
      "âœ… Cross-entropy IS categorical negative log-likelihood!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Categorical likelihood = Softmax cross-entropy connection\n",
    "z = torch.tensor([[0.1, 0.2, 0.7],    # Sample 1: class 2 highest\n",
    "                  [0.1, 0.7, 0.2],    # Sample 2: class 1 highest  \n",
    "                  [0.7, 0.1, 0.2]])   # Sample 3: class 0 highest\n",
    "\n",
    "y = torch.tensor([2, 1, 0])           # True class indices\n",
    "\n",
    "print(\"ðŸŽ¯ Categorical Likelihood â†” Softmax Cross-Entropy\")\n",
    "print(\"=\" * 55)\n",
    "print(\"Input Data:\")\n",
    "print(f\"   Logits shape:    {z.shape}\")\n",
    "print(f\"   True classes:    {y.numpy()}\")\n",
    "\n",
    "# Convert to probabilities\n",
    "softmax_probs = F.softmax(z, dim=1)\n",
    "print(f\"\\nSoftmax Probabilities:\")\n",
    "for i, (logit_row, prob_row, true_class) in enumerate(zip(z, softmax_probs, y)):\n",
    "    print(f\"   Sample {i+1}: {prob_row.numpy()} â†’ Class {true_class}\")\n",
    "\n",
    "# Manual categorical NLL (using log-softmax for numerical stability)\n",
    "log_softmax = F.log_softmax(z, dim=1)\n",
    "categorical_nll = -torch.mean(log_softmax[range(len(y)), y])\n",
    "\n",
    "# PyTorch cross-entropy\n",
    "ce_loss = F.cross_entropy(z, y)\n",
    "\n",
    "print(f\"\\nðŸ“Š Loss Function Comparison:\")\n",
    "print(f\"   Manual Categorical NLL: {categorical_nll:.6f}\")\n",
    "print(f\"   PyTorch Cross-Entropy:  {ce_loss:.6f}\")\n",
    "\n",
    "# Verify they're identical\n",
    "difference = torch.abs(categorical_nll - ce_loss)\n",
    "print(f\"\\nðŸ”— Verification:\")\n",
    "print(f\"   Absolute difference:    {difference:.10f}\")\n",
    "print(f\"\\nâœ… Cross-entropy IS categorical negative log-likelihood!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Matters: BCE vs MSE for Classification\n",
    "\n",
    "Understanding the probabilistic foundation explains why binary cross-entropy works better than MSE for classification, even though both can theoretically solve binary problems.\n",
    "\n",
    "**Key Differences:**\n",
    "- **BCE gradient**: $\\sigma(z) - y$ (simple, well-behaved)\n",
    "- **MSE gradient**: $2(\\sigma(z) - y) \\times \\sigma(z) \\times (1 - \\sigma(z))$ (can vanish!)\n",
    "\n",
    "Let's see this in practice:"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Understanding the probabilistic foundation of loss functions reveals:\n",
    "\n",
    "1. **MSE = Gaussian NLL**: Mean squared error emerges from assuming Gaussian noise\n",
    "2. **BCE = Bernoulli NLL**: Binary cross-entropy is exactly Bernoulli negative log-likelihood  \n",
    "3. **Cross-entropy = Categorical NLL**: Softmax cross-entropy corresponds to categorical distributions\n",
    "4. **Better gradients**: Probabilistically-motivated loss functions provide better optimization dynamics\n",
    "\n",
    "This connection between probability theory and optimization is fundamental to understanding why certain loss functions work well for specific tasks.\n",
    "\n",
    "---\n",
    "\n",
    "*This mathematical foundation helps explain not just which loss function to use, but why it works so effectively for the given problem type.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
