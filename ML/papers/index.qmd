---
title: "Papers in Deep Learning"
author: "Chao Ma"
date: "2026-02-08"
---

Research paper reading notes focused on key ideas, math intuition, and practical takeaways.

---

::: {.content-grid}

::: {.content-card}
**[Transformer: Attention Is All You Need](transformer-attention-is-all-you-need.qmd)**
The Transformer removes recurrence and centers the model on scaled dot-product attention, enabling parallel training and strong long-range dependency modeling.
:::

::: {.content-card}
**[Attention: The Origin of Transformer](attention-origin-transformer.qmd)**
Introduce learnable alignment scores and a dynamic context vector, replacing the fixed encoder bottleneck in seq2seq models.
:::

::: {.content-card}
**[Batch Normalization: Accelerating Deep Network Training](batch-normalization.qmd)**
Normalize mini-batch activations with learnable scale and shift to stabilize training, improve conditioning, and speed convergence.
:::

::: {.content-card}
**[LoRA: Low-Rank Adaptation of Large Language Models](lora.qmd)**
Freeze the base model and learn a low-rank update $\Delta W=BA$ for selected layers, achieving strong performance with far fewer trainable parameters and easy deployment.
:::

:::
