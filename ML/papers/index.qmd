---
title: "Papers in Deep Learning"
author: "Chao Ma"
date: "2026-02-05"
---

Research paper reading notes focused on key ideas, math intuition, and practical takeaways.

---

::: {.content-grid}

::: {.content-card}
**[Batch Normalization: Accelerating Deep Network Training](batch-normalization.qmd)**
Normalize mini-batch activations with learnable scale and shift to stabilize training, improve conditioning, and speed convergence.
:::

::: {.content-card}
**[LoRA: Low-Rank Adaptation of Large Language Models](lora.qmd)**
Freeze the base model and learn a low-rank update $\Delta W=BA$ for selected layers, achieving strong performance with far fewer trainable parameters and easy deployment.
:::

:::
