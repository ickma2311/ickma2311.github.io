---
title: "Deep Learning Book"
author: "Chao Ma"
date: "2025-11-29"
---

## All Chapters

My notes and implementations while studying the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.

---

### Chapter 6: Deep Feedforward Networks

::: {.content-grid}

::: {.content-card}
**[Chapter 6.1: XOR Problem & ReLU Networks](xor-deep-learning.ipynb)**
How ReLU solves problems that linear models cannot handle.
:::

::: {.content-card}
**[Chapter 6.2: Likelihood-Based Loss Functions](likelihood-loss-functions.ipynb)**
The mathematical connection between probabilistic models and loss functions.
:::

::: {.content-card}
**[Chapter 6.3: Hidden Units and Activation Functions](activation-functions.qmd)**
Exploring activation functions and their impact on neural network learning.
:::

::: {.content-card}
**[Chapter 6.4: Architecture Design - Depth vs Width](architecture-design.qmd)**
How depth enables hierarchical feature reuse and exponential expressiveness.
:::

::: {.content-card}
**[Chapter 6.5: Back-Propagation and Other Differentiation Algorithms](backpropagation.qmd)**
The algorithm that makes training deep networks computationally feasible.
:::

:::

---

### Chapter 7: Regularization for Deep Learning

::: {.content-grid}

::: {.content-card}
**[Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature](hessian-prerequisites.ipynb)**
Essential second-order calculus concepts needed before Chapter 7.
:::

::: {.content-card}
**[Chapter 7.1.1: L2 Regularization](l2-regularization.qmd)**
How L2 regularization shrinks weights based on Hessian eigenvalues.
:::

::: {.content-card}
**[Chapter 7.1.2: L1 Regularization](l1-regularization.qmd)**
L1 regularization uses soft thresholding to create sparse solutions.
:::

::: {.content-card}
**[Chapter 7.2: Constrained Optimization View of Regularization](constrained-optimization-regularization.qmd)**
Regularization as constrained optimization with KKT conditions.
:::

::: {.content-card}
**[Chapter 7.3: Regularization and Under-Constrained Problems](regularization-underconstrained.qmd)**
Why regularization is mathematically necessary and ensures invertibility.
:::

::: {.content-card}
**[Chapter 7.4: Dataset Augmentation](dataset-augmentation.qmd)**
How transforming existing data improves generalization.
:::

::: {.content-card}
**[Chapter 7.5: Noise Robustness](noise-robustness.qmd)**
How adding Gaussian noise to weights is equivalent to penalizing large gradients.
:::

::: {.content-card}
**[Chapter 7.6: Semi-Supervised Learning](semi-supervised-learning.qmd)**
Leveraging unlabeled data to improve model performance when labeled data is scarce.
:::

::: {.content-card}
**[Chapter 7.7: Multi-Task Learning](multi-task-learning.qmd)**
Training a single model on multiple related tasks to improve generalization.
:::

::: {.content-card}
**[Chapter 7.8: Early Stopping](early-stopping.qmd)**
Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.
:::

::: {.content-card}
**[Chapter 7.9: Parameter Tying and Parameter Sharing](parameter-tying-sharing.qmd)**
Two strategies for reducing parameters: encouraging similarity vs. enforcing identity.
:::

::: {.content-card}
**[Chapter 7.10: Sparse Representations](representation-sparsity.qmd)**
Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.
:::

::: {.content-card}
**[Chapter 7.11: Bagging and Other Ensemble Methods](bagging-ensemble.qmd)**
How training multiple models on bootstrap samples and averaging their predictions reduces variance.
:::

::: {.content-card}
**[Chapter 7.12: Dropout](dropout.qmd)**
Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.
:::

::: {.content-card}
**[Chapter 7.13: Adversarial Training](adversarial-training.qmd)**
How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.
:::

::: {.content-card}
**[Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier](tangent-prop-manifold.qmd)**
Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.
:::

:::

---

### Chapter 8: Optimization for Training Deep Models

::: {.content-grid}

::: {.content-card}
**[Chapter 8.1: How Learning Differs from Pure Optimization](learning-vs-optimization.qmd)**
Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.
:::

::: {.content-card}
**[Chapter 8.2: Challenges in Deep Learning Optimization](optimization-challenges.qmd)**
Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.
:::

::: {.content-card}
**[Chapter 8.3: Basic Algorithms](basic-optimization-algorithms.qmd)**
SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.
:::

::: {.content-card}
**[Chapter 8.4: Parameter Initialization Strategies](parameter-initialization.qmd)**
Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.
:::

::: {.content-card}
**[Chapter 8.5: Algorithms with Adaptive Learning Rates](adaptive-learning-rates.qmd)**
From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.
:::

::: {.content-card}
**[Chapter 8.6: Second-Order Optimization Methods](second-order-methods.qmd)**
Newton's method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.
:::

::: {.content-card}
**[Chapter 8.7: Optimization Strategies and Meta-Algorithms](optimization-strategies.qmd)**
Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.
:::

:::

---

### Chapter 9: Convolutional Networks

::: {.content-grid}

::: {.content-card}
**[Chapter 9.1: Convolution Computation](convolution-computation.qmd)**
The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.
:::

::: {.content-card}
**[Chapter 9.2: Motivation for Convolutional Networks](cnn-motivation.qmd)**
Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m·n) to O(k·n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere—achieving 30,000× speedup over dense layers.
:::

::: {.content-card}
**[Chapter 9.3: Pooling](cnn-pooling.qmd)**
Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28×28 feature maps to 14×14 (4× fewer activations). Comparing three architectures—strided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.
:::

::: {.content-card}
**[Chapter 9.4: Convolution and Pooling as an Infinitely Strong Prior](cnn-infinitely-strong-prior.qmd)**
Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off—strong priors reduce sample complexity but only when assumptions match the data structure.
:::

::: {.content-card}
**[Chapter 9.5: Convolutional Functions](convolutional-functions.qmd)**
Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula $Z_{l,x,y} = \sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}$, stride for downsampling, three padding strategies (valid/same/full), and gradient computation—kernel gradients via correlation with input, input gradients via convolution with flipped kernel.
:::

::: {.content-card}
**[Chapter 9.6: Structured Outputs](cnn-structured-outputs.qmd)**
CNNs can generate high-dimensional structured objects through pixel-level predictions. Preserving spatial dimensions (no pooling, no stride > 1, SAME padding) enables full-resolution outputs. Recurrent convolution refines predictions iteratively: $U*X + H(t-1)*W = H(t)$, producing dense predictions for segmentation, depth estimation, and flow prediction.
:::

::: {.content-card}
**[Chapter 9.7: Data Types](cnn-data-types.qmd)**
CNNs can operate on different data types: 1D (audio, time series), 2D (images), and 3D (videos, CT scans) with varying channel counts. Unlike fully connected networks, convolutional kernels handle variable-sized inputs by sliding across spatial dimensions, producing outputs that scale accordingly—a unique flexibility for diverse domains.
:::

::: {.content-card}
**[Chapter 9.8: Efficient Convolution Algorithms](cnn-efficient-convolution.qmd)**
Separable convolution reduces computational cost from $O(HWk^2)$ to $O(HWk)$ by decomposing a 2D kernel into two 1D filters (vertical and horizontal). Parameter storage shrinks from $k^2$ to $2k$. This factorization enables faster, more memory-efficient models without sacrificing accuracy—foundational for architectures like MobileNet.
:::

::: {.content-card}
**[Chapter 9.9: Unsupervised or Semi-Supervised Feature Learning](cnn-unsupervised-learning.qmd)**
Before CNNs, computer vision relied on hand-crafted kernels (Sobel, Laplacian, Gaussian) and unsupervised methods (sparse coding, autoencoders, k-means). While these captured simple patterns, they couldn't match CNNs' hierarchical, end-to-end feature learning. Modern systems use CNNs to learn features from edges to semantic concepts—making hand-crafted filters largely obsolete.
:::

::: {.content-card}
**[Chapter 9.10: Neuroscientific Basis for Convolutional Networks](cnn-neuroscience.qmd)**
V1 simple cells detect oriented edges (modeled by Gabor filters), complex cells pool over simple cells for translation invariance (like CNN pooling). But CNNs lack key biological features: saccadic attention, multisensory integration, top-down feedback, and dynamic receptive fields. While CNNs excel at feed-forward recognition, biological vision is holistic, context-aware, and adaptive.
:::

:::

---

### Chapter 10: Sequence Modeling

::: {.content-grid}

::: {.content-card}
**[Chapter 10.1: Unfold Computation Graph](rnn-unfold-computation-graph.qmd)**
Unfolding computation graphs in RNNs enables parameter sharing across time steps. The same function with fixed parameters processes sequences of any length, compressing input history into fixed-size hidden states that retain only task-relevant information for predictions.
:::

::: {.content-card}
**[Chapter 10.2: Recurrent Neural Networks](rnn-recurrent-neural-networks.qmd)**
RNN architecture with hidden-to-hidden connections, teacher forcing for parallel training, back-propagation through time (BPTT), RNN as directed graphical models with O(τ) parameter efficiency, and context-based sequence-to-sequence models.
:::

::: {.content-card}
**[Chapter 10.3: Bidirectional RNN](rnn-bidirectional.qmd)**
Bidirectional RNNs process sequences in both forward and backward directions, allowing predictions to use information from the entire input sequence. Essential for tasks like speech recognition and handwriting recognition where future context matters.
:::

::: {.content-card}
**[Chapter 10.4: Encoder-Decoder Sequence-to-Sequence Architecture](rnn-encoder-decoder.qmd)**
The seq2seq architecture handles variable-length input and output sequences by compressing the input into a fixed context vector C, then decoding it step-by-step. This enables machine translation, summarization, and dialogue generation where input and output lengths differ.
:::

::: {.content-card}
**[Chapter 10.5: Deep Recurrent Networks](rnn-deep.qmd)**
Three architectural patterns for adding depth to RNNs: hierarchical hidden states (vertical stacking), deep transition RNNs (MLPs replace transformations), and deep transition with skip connections (residual paths for gradient flow).
:::

::: {.content-card}
**[Chapter 10.6: Recursive Neural Network](rnn-recursive.qmd)**
Recursive neural networks compute over tree structures rather than linear chains, applying shared composition functions at internal nodes to build hierarchical representations bottom-up. This reduces computation depth from O(τ) to O(log τ), but requires external tree structure specification.
:::

::: {.content-card}
**[Chapter 10.7: The Challenge of Long-Term Dependencies](rnn-long-term-dependency.qmd)**
The fundamental challenge of long-term dependencies in RNNs is training difficulty: gradients propagated across many time steps either vanish exponentially (common) or explode (rare but severe). Eigenvalue analysis shows how powers of the transition matrix govern this instability.
:::

::: {.content-card}
**[Chapter 10.8: Echo State Networks](rnn-echo-state.qmd)**
ESNs fix recurrent weights and train only output weights, viewing the network as a dynamical reservoir. Setting spectral radius near one enables long-term memory retention. Learning reduces to linear regression on hidden states, avoiding backpropagation through time—showing that carefully designed dynamics can capture temporal structure.
:::

::: {.content-card}
**[Chapter 10.9: Leaky Units and Multiple Time Scales](rnn-leaky-unit.qmd)**
Leaky units separate instantaneous state from long-term integration using $u^t = \alpha u^{t-1}+(1-\alpha)v^t$. Multiple time scale strategies include temporal skip connections (direct pathways across time steps) and removing short connections (forcing coarser time scales) to address long-term dependencies.
:::

::: {.content-card}
**[Chapter 10.10: LSTM and GRU](rnn-lstm-gru.qmd)**
LSTM uses learned gates (forget, input, output) to control information flow through explicit cell state paths, enabling adaptive long-term memory retention. GRU simplifies this design by merging forget and input into a single update gate, reducing parameters while maintaining the ability to capture long-range dependencies through gating mechanisms.
:::

::: {.content-card}
**[Chapter 10.11: Optimizing Long-Term Dependencies](rnn-optimize-long-term.qmd)**
Gradient clipping prevents training instability by rescaling gradients when their norm exceeds a threshold, protecting against sudden jumps across steep gradient cliffs. Regularizing information flow aims to maintain $\|\partial h^t/\partial h^{t-1}\| \approx 1$, though this is rarely used in practice due to computational cost.
:::

::: {.content-card}
**[Chapter 10.12: Explicit Memory](rnn-explicit-memory.qmd)**
Explicit memory separates storage from computation by introducing addressable memory outside network parameters. Classic architectures (Memory Networks, Neural Turing Machines) are computationally expensive but inspired modern mechanisms—attention, Transformers, and retrieval-augmented models are successful, scalable realizations of this idea.
:::

:::

---

### Chapter 11: Practical Methodology

::: {.content-grid}

::: {.content-card}
**[Chapter 11: Practical Methodology](practical-methodology.qmd)**
Define performance metrics aligned with application goals. Build baseline systems quickly using architecture patterns matched to data structure. Diagnose by comparing training and test error. Tune learning rate first, then other hyperparameters via random search. Debug systematically by isolating components, checking gradients, and monitoring numerical behavior.
:::

:::

---

### Chapter 12: Applications

::: {.content-grid}

::: {.content-card}
**[Chapter 12.1: Large-Scale Deep Learning](large-scale-deep-learning.qmd)**
Scaling deep learning requires specialized hardware (GPUs, TPUs, ASICs), distributed training strategies (data/model parallelism, asynchronous SGD), and efficiency optimizations (model compression, quantization, pruning). Dynamic computation enables conditional execution for computational efficiency. Specialized accelerators exploit reduced precision and massive parallelism for deployment on resource-constrained devices.
:::

::: {.content-card}
**[Chapter 12.2: Image Preprocessing and Normalization](image-preprocessing-normalization.qmd)**
Preprocessing images through normalization (scaling to [0,1] or [-1,1]) and data augmentation improves training stability and generalization. Global Contrast Normalization (GCN) removes global lighting variations by centering and L2-normalizing images. Local Contrast Normalization (LCN) enhances local structures by normalizing within spatial neighborhoods. Modern networks rely on batch normalization, but explicit contrast normalization remains valuable for challenging datasets.
:::

:::
