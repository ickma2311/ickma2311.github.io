[
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "",
    "section": "Project Overview",
    "text": "Project Overview\nThis is a Quarto-based technical blog hosted on GitHub Pages (ickma2311.github.io). The site covers machine learning, algorithms, and technical tutorials with a focus on mathematical foundations and practical implementations."
  },
  {
    "objectID": "CLAUDE.html#common-commands",
    "href": "CLAUDE.html#common-commands",
    "title": "",
    "section": "Common Commands",
    "text": "Common Commands\n\nDevelopment Workflow\n\nquarto render - Build the entire website (outputs to docs/ directory)\nquarto preview - Start local development server with live reload\nquarto render &lt;file.qmd&gt; - Render a specific document\nquarto check - Verify Quarto installation and project setup\n\n\n\nContent Management\n\nCreate new ML content in ML/ directory\nCreate new algorithm content in Algorithm/ directory\nUpdate navigation by editing _quarto.yml navbar section\nAdd new content to respective index.qmd files for discoverability"
  },
  {
    "objectID": "CLAUDE.html#project-structure",
    "href": "CLAUDE.html#project-structure",
    "title": "",
    "section": "Project Structure",
    "text": "Project Structure\n‚îú‚îÄ‚îÄ _quarto.yml          # Main configuration file\n‚îú‚îÄ‚îÄ docs/                # Generated output (GitHub Pages source)\n‚îú‚îÄ‚îÄ index.qmd            # Homepage\n‚îú‚îÄ‚îÄ about.qmd            # About page\n‚îú‚îÄ‚îÄ ML/                  # Machine Learning content\n‚îÇ   ‚îú‚îÄ‚îÄ index.qmd        # ML topics overview\n‚îÇ   ‚îú‚îÄ‚îÄ *.qmd            # ML articles\n‚îÇ   ‚îî‚îÄ‚îÄ *.ipynb          # Jupyter notebooks\n‚îú‚îÄ‚îÄ Algorithm/           # Algorithm content\n‚îÇ   ‚îú‚îÄ‚îÄ index.qmd        # Algorithm topics overview\n‚îÇ   ‚îî‚îÄ‚îÄ *.qmd            # Algorithm articles\n‚îú‚îÄ‚îÄ imgs/                # Image assets\n‚îú‚îÄ‚îÄ media/               # Media files\n‚îî‚îÄ‚îÄ styles.css           # Custom CSS styles"
  },
  {
    "objectID": "CLAUDE.html#content-organization",
    "href": "CLAUDE.html#content-organization",
    "title": "",
    "section": "Content Organization",
    "text": "Content Organization\nThe site uses a hierarchical navigation structure defined in _quarto.yml: - Two main sections: ‚ÄúML‚Äù and ‚ÄúAlgorithm‚Äù - Each section has an index page that serves as a directory - Content is categorized by topic (e.g., ‚ÄúNumPy Fundamentals‚Äù, ‚ÄúClustering Algorithms‚Äù)\n\nAdding New Content\n\nCreate the content file in the appropriate directory (ML/ or Math/ or Algorithm/)\nUpdate the corresponding index.qmd file to include the new content\nAdd navigation entry to _quarto.yml if it should appear in the navbar dropdown\nUse consistent frontmatter with title field\nSet publication date: Always use the current date from the system for the date field in frontmatter\n\nGet current date with: date +\"%Y-%m-%d\" (format: YYYY-MM-DD)\nExample: date: \"2025-10-26\"\n\nImportant: After adding new navbar items, run quarto render (full site render) to update the navbar on ALL existing pages. Individual file renders only update that specific page."
  },
  {
    "objectID": "CLAUDE.html#configuration-notes",
    "href": "CLAUDE.html#configuration-notes",
    "title": "",
    "section": "Configuration Notes",
    "text": "Configuration Notes\n\nOutput directory is set to docs/ for GitHub Pages compatibility\nTheme: Cosmo with custom branding\nAll pages include table of contents (toc: true)\nSite uses custom CSS from styles.css\nJupyter notebooks are supported alongside Quarto markdown"
  },
  {
    "objectID": "CLAUDE.html#github-pages-deployment",
    "href": "CLAUDE.html#github-pages-deployment",
    "title": "",
    "section": "GitHub Pages Deployment",
    "text": "GitHub Pages Deployment\nThe site is automatically deployed from the docs/ directory. After rendering, commit and push the docs/ folder to trigger GitHub Pages rebuild. - Author is Chao Ma - GitHub Pages URL: https://ickma2311.github.io/"
  },
  {
    "objectID": "CLAUDE.html#linkedin-post-guidelines",
    "href": "CLAUDE.html#linkedin-post-guidelines",
    "title": "",
    "section": "LinkedIn Post Guidelines",
    "text": "LinkedIn Post Guidelines\n\nEmoji Usage\nWhen drafting LinkedIn posts for blog content, use these emojis: - Deep Learning topics: ‚àá (delta/nabla symbol) - represents gradients and optimization - Linear Algebra topics: üìê (triangle/ruler) - represents geometric and matrix concepts\n\n\nWriting Process\n\nIdentify the key insight: Focus on the main conceptual connection or ‚Äúaha moment‚Äù from the blog post\nUse ‚Äúconnecting the dots‚Äù tone: Emphasize how concepts link together (e.g., ‚Äúhow linear algebra connects to machine learning‚Äù)\nStructure:\n\nStart with chapter reference if applicable (e.g., ‚ÄúDeep Learning Book (Chapter 7.3)‚Äù)\nPresent the problem (what breaks down?)\nPresent the solution (how is it fixed?)\nExplain the insight (why does this matter?)\nLink to full blog post\n\nKeep it concise: Aim for clarity over comprehensiveness\nSource attribution: Always mention the source book/material (e.g., ‚ÄúMy notes on Deep Learning (Ian Goodfellow) Chapter X.X‚Äù)\nInclude relevant hashtags: #MachineLearning #LinearAlgebra #DeepLearning"
  },
  {
    "objectID": "ML/kmeans.html",
    "href": "ML/kmeans.html",
    "title": "K-means Clustering",
    "section": "",
    "text": "Setup points and K\nwe will implement a KNN algorithm to cluster the points\n\n\nX=[[1,1],[2,2.1],[3,2.5],[6,7],[7,7.1],[9,7.5]]\nk=2\n\nmax_iter=3\n\n\n# Visualize the data\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter([x[0] for x in X],[x[1] for x in X])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pure python implementation of K-means clustering\ndef knn_iter(X,centroids):\n    # set up new clusters\n    new_clusters=[[] for _ in range(len(centroids))]\n    # k=len(centroids)\n    # assign each point to the nearest centroid\n    for x in X:\n        k,distance=0,(x[0]-centroids[0][0])**2+(x[1]-centroids[0][1])**2\n        for i,c in enumerate(centroids[1:],1):\n            if (x[0]-c[0])**2+(x[1]-c[1])**2&lt;distance:\n                k=i\n                distance=(x[0]-c[0])**2+(x[1]-c[1])**2\n        new_clusters[k].append(x)\n    \n    # calculate new centroids\n    new_centroids=[[\n        sum([x[0] for x in cluster])/len(cluster),\n        sum([x[1] for x in cluster])/len(cluster)\n    ] if cluster else centroids[i] for i,cluster in enumerate(new_clusters)]\n    return new_centroids\n\n\n\n\n\n\n\n\ndef iter_and_draw(X,k,max_iter):\n    centroids=X[:k]  # Randomly select 2 centroids\n    fig, axes = plt.subplots(max_iter//3+(1 if max_iter%3!=0 else 0),\n        3, figsize=(15, 10))\n    axes=axes.flatten()\n    for i in range(max_iter):\n        \n        # Plot points and centroids\n\n\n        # Assign each point to nearest centroid and plot with corresponding color\n        colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n        for j, x in enumerate(X):\n            # Find nearest centroid\n            min_dist = float('inf')\n            nearest_centroid = 0\n            for k, c in enumerate(centroids):\n                dist = (x[0]-c[0])**2 + (x[1]-c[1])**2\n                if dist &lt; min_dist:\n                    min_dist = dist\n                    nearest_centroid = k\n            # Plot point with color corresponding to its cluster\n            axes[i].scatter(x[0], x[1], c=colors[nearest_centroid % len(colors)], label=f'Cluster {nearest_centroid+1}' if j==0 else \"\")\n        axes[i].scatter([c[0] for c in centroids], [c[1] for c in centroids], c='red', marker='*', s=200, label='Centroids')\n        axes[i].set_title(f'Iteration {i}')\n        centroids = knn_iter(X, centroids)\n\n    plt.tight_layout()\n    plt.show()\n\niter_and_draw(X,k,max_iter)\n# print(centroids)\n\n\n\n\n\n\n\n\n\n# A 3 clusters example\n\nimport numpy as np\n\nX1=np.random.rand(20,2)+5 # Some points in the upper right corner\nX2=np.random.rand(20,2)+3 # Some points in the middle\nX3=np.random.rand(20,2) # Some points in the lower left corner\n\niter_and_draw(np.concatenate((X1,X2,X3)),3,5)\n\n\n\n\n\n\n\n\n\n\nA question?\n\nWhat to do if one cluster has no assigned points during iteration?\n\n\n\nFormula Derivation\nThe goal is to minimize the loss of inertia which is sum of the points to cluster centroids.\n\\[\nLoss= \\sum_{i=1}^n \\sum_{x \\in C_i} ||x-\\mu_i||^2\n\\]\nTo iter \\(\\mu\\) for each cluster, let us find the derivative of the following function. \\[\nf(\\mu)=\\sum_{i=1}^n ||x_i-\\mu||^2 =\n\\sum_{i=1}^n {x_i}^2+\\mu^2-2x_i\\mu\n\\]\nGiven a \\(\\nabla \\mu\\), \\[\nf(\\mu + \\nabla \\mu)=\\sum_{i=1}^n ||x_i+\\nabla \\mu -\\mu||^2 =\n\\sum_{i=1}^n  {x_i}^2+\\mu^2+{\\nabla \\mu}^2-2{x_i \\mu}-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\nf(\\mu + \\nabla \\mu)-f(\\mu)=\n\\sum_{i=1}^n {\\nabla \\mu}^2-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\n\\frac {f(\\mu + \\nabla \\mu)-f(\\mu)}{\\nabla \\mu}=\\sum_{i=1}^n {\\nabla \\mu} -2 \\mu +2{x_i} = 2\\sum_{i=1}^n x_i - 2n\\mu\n\\]\nNow we can see if \\(n\\mu = \\sum_{i=1}^n x_i\\), then the derivative is 0, this is why in each iteration, we need to set the center of the cluster as centroid."
  },
  {
    "objectID": "ML/noise-robustness.html",
    "href": "ML/noise-robustness.html",
    "title": "Noise Robustness: How Weight Perturbation Leads to Regularization",
    "section": "",
    "text": "Noise injection can be used as a regularization technique to improve model robustness and generalization. This section explores how adding random perturbations to weights leads to an effective regularization term."
  },
  {
    "objectID": "ML/noise-robustness.html#overview",
    "href": "ML/noise-robustness.html#overview",
    "title": "Noise Robustness: How Weight Perturbation Leads to Regularization",
    "section": "",
    "text": "Noise injection can be used as a regularization technique to improve model robustness and generalization. This section explores how adding random perturbations to weights leads to an effective regularization term."
  },
  {
    "objectID": "ML/noise-robustness.html#random-perturbation-on-weights",
    "href": "ML/noise-robustness.html#random-perturbation-on-weights",
    "title": "Noise Robustness: How Weight Perturbation Leads to Regularization",
    "section": "Random Perturbation on Weights",
    "text": "Random Perturbation on Weights\n\nOriginal Error Function\nThe standard mean squared error:\n\\[\nJ = \\mathbb{E}_{p(x,y)} \\left[ (\\hat{y}(x) - y)^2 \\right]\n\\]\n\n\nWeight Noise Model\nAdd Gaussian noise to the weights:\n\\[\n\\epsilon_W \\sim \\mathcal{N}(0, \\eta I)\n\\]\nThis is a normal distribution with:\n\nMean: \\(0\\)\nCovariance: \\(\\eta I\\) (where \\(\\eta\\) controls the noise magnitude)\n\n\n\nObjective Function with Noisy Weights\nLet \\(\\hat{y}_{\\epsilon_W}(x) = \\hat{y}_{W + \\epsilon_W}(x)\\) denote the model output with perturbed weights.\nThe new objective becomes:\n\\[\n\\tilde{J}_W = \\mathbb{E}_{p(x, y, \\epsilon_W)} \\left[ (\\hat{y}_{\\epsilon_W}(x) - y)^2 \\right]\n\\]\nFormula 7.31: This expectation is over the data distribution and the weight noise."
  },
  {
    "objectID": "ML/noise-robustness.html#deriving-the-regularization-term",
    "href": "ML/noise-robustness.html#deriving-the-regularization-term",
    "title": "Noise Robustness: How Weight Perturbation Leads to Regularization",
    "section": "Deriving the Regularization Term",
    "text": "Deriving the Regularization Term\n\nExpanding the Squared Error\n\\[\n\\tilde{J} = \\mathbb{E}_{p(x, y, \\epsilon_W)} \\left[ \\hat{y}_{\\epsilon_W}^2(x) - 2y \\hat{y}_{\\epsilon_W}(x) + y^2 \\right]\n\\]\nFormula 7.32\n\n\nTaylor Approximation\nWhen \\(\\eta\\) is small, we can approximate:\n\\[\n\\hat{y}_{W + \\epsilon_W}(x) \\approx \\hat{y}_W(x) + \\epsilon_W^T \\nabla_W \\hat{y}_W(x)\n\\]\n\n\n\n\n\n\nNoteInterpretation\n\n\n\nThe change in output is approximately the inner product of the weight noise \\(\\epsilon_W\\) and the gradient \\(\\nabla_W \\hat{y}_W(x)\\) ‚Äî i.e., the noise projected onto the gradient direction.\n\n\n\n\nSimplification\nLet:\n\n\\(a = \\hat{y}_W(x) - y\\) (prediction error)\n\\(b = \\epsilon_W^T \\nabla_W \\hat{y}_W(x)\\) (noise-induced perturbation)\n\nThen:\n\\[\n\\tilde{J} = \\mathbb{E}[a^2] + \\mathbb{E}[2ab] + \\mathbb{E}[b^2]\n\\]\nKey observations:\n\nCross-term vanishes: \\[\n\\mathbb{E}[ab] = 0\n\\] Because \\(\\epsilon_W\\) has zero mean and is independent of \\(a\\).\nNoise variance contributes regularization: \\[\n\\mathbb{E}[b^2] = \\mathbb{E}\\left[(\\epsilon_W^T \\nabla_W \\hat{y}_W(x))^2\\right]\n\\]\n\nSince \\(\\epsilon_W \\sim \\mathcal{N}(0, \\eta I)\\):\n\\[\n\\mathbb{E}[b^2] = \\eta ||\\nabla_W \\hat{y}_W(x)||^2\n\\]\n\n\n\n\n\n\nTipDerivation Detail\n\n\n\nFor a Gaussian random vector \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\\) and any vector \\(v\\):\n\\[\n\\mathbb{E}[(\\epsilon^T v)^2] = \\sigma^2 ||v||^2\n\\]"
  },
  {
    "objectID": "ML/noise-robustness.html#final-regularized-objective",
    "href": "ML/noise-robustness.html#final-regularized-objective",
    "title": "Noise Robustness: How Weight Perturbation Leads to Regularization",
    "section": "Final Regularized Objective",
    "text": "Final Regularized Objective\nCombining the terms:\n\\[\n\\tilde{J}(W; x, y) = J(W) + \\eta \\mathbb{E}_{p(x,y)} \\left[ ||\\nabla_W \\hat{y}_W(x)||^2 \\right]\n\\]\nInterpretation:\n\nFirst term: Original loss function\nSecond term: Regularization penalty proportional to the squared gradient norm\n\n\n\n\n\n\n\nImportantKey Insight\n\n\n\nAdding Gaussian noise to weights is equivalent to penalizing large gradients of the output with respect to the weights."
  },
  {
    "objectID": "ML/noise-robustness.html#geometric-interpretation",
    "href": "ML/noise-robustness.html#geometric-interpretation",
    "title": "Noise Robustness: How Weight Perturbation Leads to Regularization",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\nThe regularization term \\(||\\nabla_W \\hat{y}_W(x)||^2\\) measures how sensitive the output is to weight perturbations.\nWhat this encourages:\n\nFlat minima: Solutions where small weight changes don‚Äôt dramatically affect predictions\nRobust features: The model relies on stable patterns rather than fine-grained weight configurations\nGeneralization: Prevents overfitting to exact weight values\n\n\n\n\nRandom Perturbation Visualization"
  },
  {
    "objectID": "ML/noise-robustness.html#injecting-noise-at-the-output-targets",
    "href": "ML/noise-robustness.html#injecting-noise-at-the-output-targets",
    "title": "Noise Robustness: How Weight Perturbation Leads to Regularization",
    "section": "Injecting Noise at the Output Targets",
    "text": "Injecting Noise at the Output Targets\n\nLabel Smoothing\nInstead of using hard 0/1 targets, label smoothing softens the target distribution:\n\\[\ny'_k =\n\\begin{cases}\n1 - \\varepsilon, & \\text{if } k \\text{ is the correct class} \\\\\n\\varepsilon / (K - 1), & \\text{otherwise}\n\\end{cases}\n\\]\nwhere:\n\n\\(K\\) is the number of classes\n\\(\\varepsilon\\) is the smoothing parameter (typically 0.1)\n\nExample: For 3-class classification with correct class = 1 and \\(\\varepsilon = 0.1\\):\n\nOriginal: \\([0, 1, 0]\\)\nSmoothed: \\([0.05, 0.9, 0.05]\\)\n\n\n\nBenefits\n\nPrevents overconfidence: The model doesn‚Äôt push probabilities to exact 0 or 1\nImproves calibration: Predicted probabilities better reflect true uncertainty\nRegularization effect: Acts as implicit regularization on the output layer\n\n\n\n\n\n\n\nTipInterpretation\n\n\n\nLabel smoothing can be viewed as injecting small noise into the target distribution, making the model less overconfident and more robust.\n\n\n\nSource: Deep Learning Book, Chapter 7.5"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html",
    "href": "ML/likelihood-loss-functions.html",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.2 reveals the fundamental connection between probabilistic assumptions and the loss functions we use to train neural networks.\nüìì For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#the-hidden-connection-why-these-loss-functions",
    "href": "ML/likelihood-loss-functions.html#the-hidden-connection-why-these-loss-functions",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "The Hidden Connection: Why These Loss Functions?",
    "text": "The Hidden Connection: Why These Loss Functions?\nEver wondered why we use mean squared error for regression, cross-entropy for classification, and other specific loss functions? The answer lies in maximum likelihood estimation - each common loss function corresponds to the negative log-likelihood of a specific probabilistic model.\n\n\n\n\n\n\n\n\nProbabilistic Model\nLoss Function\nUse Case\n\n\n\n\nGaussian likelihood\nMean Squared Error\nRegression\n\n\nBernoulli likelihood\nBinary Cross-Entropy\nBinary Classification\n\n\nCategorical likelihood\nSoftmax Cross-Entropy\nMulticlass Classification"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#exploring-the-connection-probabilistic-models-loss-functions",
    "href": "ML/likelihood-loss-functions.html#exploring-the-connection-probabilistic-models-loss-functions",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "üéØ Exploring the Connection: Probabilistic Models ‚Üí Loss Functions",
    "text": "üéØ Exploring the Connection: Probabilistic Models ‚Üí Loss Functions\n\n\nShow code\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-1-gaussian-likelihood-mean-squared-error",
    "href": "ML/likelihood-loss-functions.html#connection-1-gaussian-likelihood-mean-squared-error",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 1: Gaussian Likelihood ‚Üí Mean Squared Error",
    "text": "Connection 1: Gaussian Likelihood ‚Üí Mean Squared Error\nThe Setup: When we assume our targets have Gaussian noise around our predictions:\n\\[p(y|x) = \\mathcal{N}(y; \\hat{y}, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\hat{y})^2}{2\\sigma^2}\\right)\\]\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = \\frac{(y-\\hat{y})^2}{2\\sigma^2} + \\frac{1}{2}\\log(2\\pi\\sigma^2)\\]\nThe Result: Minimizing this is equivalent to minimizing MSE (the constant term doesn‚Äôt affect optimization)!\n\n\nShow code\n# Demonstrate Gaussian likelihood = MSE connection\nnp.random.seed(0)\nx = np.linspace(-1, 1, 20)\ny_true = 2 * x + 1\ny = y_true + np.random.normal(0, 0.1, size=x.shape)  # Gaussian noise\n\n# Simple linear model predictions\nw, b = 1.0, 0.0\ny_pred = w * x + b\n\n# Compute MSE\nmse = np.mean((y - y_pred)**2)\n\n# Compute Gaussian negative log-likelihood\nsigma_squared = 0.1**2\nquadratic_term = 0.5 * np.mean((y - y_pred)**2) / sigma_squared\nconst_term = 0.5 * np.log(2 * np.pi * sigma_squared)\nnll_gaussian = quadratic_term + const_term\n\nprint(\"üìä Gaussian Likelihood ‚Üî MSE Connection\")\nprint(\"=\" * 45)\nprint(f\"üìà Mean Squared Error:     {mse:.6f}\")\nprint(f\"üìä Gaussian NLL:           {nll_gaussian:.6f}\")\nprint(f\"   ‚îú‚îÄ Quadratic term:      {quadratic_term:.6f}\")\nprint(f\"   ‚îî‚îÄ Constant term:       {const_term:.6f}\")\n\nscaling_factor = 1 / (2 * sigma_squared)\nprint(f\"\\nüîó Mathematical Connection:\")\nprint(f\"   Quadratic term = {scaling_factor:.1f} √ó MSE\")\nprint(f\"   {quadratic_term:.6f} = {scaling_factor:.1f} √ó {mse:.6f}\")\nprint(f\"\\n‚úÖ Minimizing MSE ‚â° Maximizing Gaussian likelihood\")\n\n\nüìä Gaussian Likelihood ‚Üî MSE Connection\n=============================================\nüìà Mean Squared Error:     1.450860\nüìä Gaussian NLL:           71.159339\n   ‚îú‚îÄ Quadratic term:      72.542985\n   ‚îî‚îÄ Constant term:       -1.383647\n\nüîó Mathematical Connection:\n   Quadratic term = 50.0 √ó MSE\n   72.542985 = 50.0 √ó 1.450860\n\n‚úÖ Minimizing MSE ‚â° Maximizing Gaussian likelihood"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-2-bernoulli-likelihood-binary-cross-entropy",
    "href": "ML/likelihood-loss-functions.html#connection-2-bernoulli-likelihood-binary-cross-entropy",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 2: Bernoulli Likelihood ‚Üí Binary Cross-Entropy",
    "text": "Connection 2: Bernoulli Likelihood ‚Üí Binary Cross-Entropy\nThe Setup: For binary classification, we assume Bernoulli-distributed targets:\n\\[p(y|x) = \\sigma(z)^y (1-\\sigma(z))^{1-y}\\]\nwhere \\(\\sigma(z) = \\frac{1}{1+e^{-z}}\\) is the sigmoid function.\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = -y\\log\\sigma(z) - (1-y)\\log(1-\\sigma(z))\\]\nThe Result: This is exactly binary cross-entropy loss!\n\n\nShow code\n# Demonstrate Bernoulli likelihood = Binary cross-entropy connection\nz = torch.tensor([-0.5, -0.8, 0.0, 0.8, 0.5])  # Model logits\ny = torch.tensor([0.0, 0.0, 1.0, 1.0, 1.0])     # Binary labels\np = torch.sigmoid(z)  # Convert to probabilities\n\nprint(\"üé≤ Bernoulli Likelihood ‚Üî Binary Cross-Entropy\")\nprint(\"=\" * 50)\nprint(\"Input Data:\")\nprint(f\"   Logits:        {z.numpy()}\")\nprint(f\"   Labels:        {y.numpy()}\")\nprint(f\"   Probabilities: {p.numpy()}\")\n\n# Manual Bernoulli NLL computation\nbernoulli_nll = torch.mean(-(y * torch.log(p) + (1 - y) * torch.log(1 - p)))\n\n# PyTorch binary cross-entropy\nbce_loss = F.binary_cross_entropy(p, y)\n\nprint(f\"\\nüìä Loss Function Comparison:\")\nprint(f\"   Manual Bernoulli NLL:  {bernoulli_nll:.6f}\")\nprint(f\"   PyTorch BCE Loss:      {bce_loss:.6f}\")\n\n# Verify they're identical\ndifference = torch.abs(bernoulli_nll - bce_loss)\nprint(f\"\\nüîó Verification:\")\nprint(f\"   Absolute difference:   {difference:.10f}\")\nprint(f\"\\n‚úÖ Binary cross-entropy IS Bernoulli negative log-likelihood!\")\n\n\nüé≤ Bernoulli Likelihood ‚Üî Binary Cross-Entropy\n==================================================\nInput Data:\n   Logits:        [-0.5 -0.8  0.   0.8  0.5]\n   Labels:        [0. 0. 1. 1. 1.]\n   Probabilities: [0.37754068 0.3100255  0.5        0.6899745  0.62245935]\n\nüìä Loss Function Comparison:\n   Manual Bernoulli NLL:  0.476700\n   PyTorch BCE Loss:      0.476700\n\nüîó Verification:\n   Absolute difference:   0.0000000000\n\n‚úÖ Binary cross-entropy IS Bernoulli negative log-likelihood!"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-3-categorical-likelihood-softmax-cross-entropy",
    "href": "ML/likelihood-loss-functions.html#connection-3-categorical-likelihood-softmax-cross-entropy",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 3: Categorical Likelihood ‚Üí Softmax Cross-Entropy",
    "text": "Connection 3: Categorical Likelihood ‚Üí Softmax Cross-Entropy\nThe Setup: For multiclass classification, we use the categorical distribution:\n\\[p(y=i|x) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} = \\text{softmax}(z)_i\\]\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = -\\log \\frac{e^{z_y}}{\\sum_j e^{z_j}} = -z_y + \\log\\sum_j e^{z_j}\\]\nThe Result: This is exactly softmax cross-entropy loss!\n\n\nShow code\n# Demonstrate Categorical likelihood = Softmax cross-entropy connection\nz = torch.tensor([[0.1, 0.2, 0.7],    # Sample 1: class 2 highest\n                  [0.1, 0.7, 0.2],    # Sample 2: class 1 highest  \n                  [0.7, 0.1, 0.2]])   # Sample 3: class 0 highest\n\ny = torch.tensor([2, 1, 0])           # True class indices\n\nprint(\"üéØ Categorical Likelihood ‚Üî Softmax Cross-Entropy\")\nprint(\"=\" * 55)\nprint(\"Input Data:\")\nprint(f\"   Logits shape:    {z.shape}\")\nprint(f\"   True classes:    {y.numpy()}\")\n\n# Convert to probabilities\nsoftmax_probs = F.softmax(z, dim=1)\nprint(f\"\\nSoftmax Probabilities:\")\nfor i, (logit_row, prob_row, true_class) in enumerate(zip(z, softmax_probs, y)):\n    print(f\"   Sample {i+1}: {prob_row.numpy()} ‚Üí Class {true_class}\")\n\n# Manual categorical NLL (using log-softmax for numerical stability)\nlog_softmax = F.log_softmax(z, dim=1)\ncategorical_nll = -torch.mean(log_softmax[range(len(y)), y])\n\n# PyTorch cross-entropy\nce_loss = F.cross_entropy(z, y)\n\nprint(f\"\\nüìä Loss Function Comparison:\")\nprint(f\"   Manual Categorical NLL: {categorical_nll:.6f}\")\nprint(f\"   PyTorch Cross-Entropy:  {ce_loss:.6f}\")\n\n# Verify they're identical\ndifference = torch.abs(categorical_nll - ce_loss)\nprint(f\"\\nüîó Verification:\")\nprint(f\"   Absolute difference:    {difference:.10f}\")\nprint(f\"\\n‚úÖ Cross-entropy IS categorical negative log-likelihood!\")\n\n\nüéØ Categorical Likelihood ‚Üî Softmax Cross-Entropy\n=======================================================\nInput Data:\n   Logits shape:    torch.Size([3, 3])\n   True classes:    [2 1 0]\n\nSoftmax Probabilities:\n   Sample 1: [0.25462854 0.28140804 0.46396342] ‚Üí Class 2\n   Sample 2: [0.25462854 0.46396342 0.28140804] ‚Üí Class 1\n   Sample 3: [0.46396342 0.25462854 0.28140804] ‚Üí Class 0\n\nüìä Loss Function Comparison:\n   Manual Categorical NLL: 0.767950\n   PyTorch Cross-Entropy:  0.767950\n\nüîó Verification:\n   Absolute difference:    0.0000000000\n\n‚úÖ Cross-entropy IS categorical negative log-likelihood!"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#why-this-matters-bce-vs-mse-for-classification",
    "href": "ML/likelihood-loss-functions.html#why-this-matters-bce-vs-mse-for-classification",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Why This Matters: BCE vs MSE for Classification",
    "text": "Why This Matters: BCE vs MSE for Classification\nUnderstanding the probabilistic foundation explains why binary cross-entropy works better than MSE for classification, even though both can theoretically solve binary problems.\nKey Differences: - BCE gradient: \\(\\sigma(z) - y\\) (simple, well-behaved) - MSE gradient: \\(2(\\sigma(z) - y) \\times \\sigma(z) \\times (1 - \\sigma(z))\\) (can vanish!)\nLet‚Äôs see this in practice:"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#key-takeaways",
    "href": "ML/likelihood-loss-functions.html#key-takeaways",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nUnderstanding the probabilistic foundation of loss functions reveals:\n\nMSE = Gaussian NLL: Mean squared error emerges from assuming Gaussian noise\nBCE = Bernoulli NLL: Binary cross-entropy is exactly Bernoulli negative log-likelihood\n\nCross-entropy = Categorical NLL: Softmax cross-entropy corresponds to categorical distributions\nBetter gradients: Probabilistically-motivated loss functions provide better optimization dynamics\n\nThis connection between probability theory and optimization is fundamental to understanding why certain loss functions work well for specific tasks.\n\nThis mathematical foundation helps explain not just which loss function to use, but why it works so effectively for the given problem type."
  },
  {
    "objectID": "ML/axis.html",
    "href": "ML/axis.html",
    "title": "Understanding Axis(dim) Operations Smartly",
    "section": "",
    "text": "import numpy as np\n\nx = np.array([[1, 2, 3], [4, 5, 6]])\nThe 2D array is: \\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\]\n\n\nprint(x.sum(axis=0))\nThe result is:\narray([5, 7, 9])\nWhen axis(dim) is 0, it means the operation is performed along 0 dimension. Items along 0 dimension are each sub-array. Then the result is just two vectors added together.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n4 & 5 & 6\n\\end{bmatrix}\n\\]\nx.sum(axis=1)==x[0]+x[1]\nOperations along axis 0 is just operate on all sub-arrays. For example,\nsum(x,axis=0) is just \\(\\vec{x[0]}+\\vec{x[1]}+...+\\vec{x[n]}\\)\n\n\n\nprint(x.sum(axis=1))\nThe result is:\narray([6, 15])\nWhen axis(dim) is 1, it means the operation is performed along 1 dimension.\n\\[\n\\begin{bmatrix}\n1+2+3 \\\\\n4+5+6\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "ML/axis.html#a-2d-example",
    "href": "ML/axis.html#a-2d-example",
    "title": "Understanding Axis(dim) Operations Smartly",
    "section": "",
    "text": "import numpy as np\n\nx = np.array([[1, 2, 3], [4, 5, 6]])\nThe 2D array is: \\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\]\n\n\nprint(x.sum(axis=0))\nThe result is:\narray([5, 7, 9])\nWhen axis(dim) is 0, it means the operation is performed along 0 dimension. Items along 0 dimension are each sub-array. Then the result is just two vectors added together.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n4 & 5 & 6\n\\end{bmatrix}\n\\]\nx.sum(axis=1)==x[0]+x[1]\nOperations along axis 0 is just operate on all sub-arrays. For example,\nsum(x,axis=0) is just \\(\\vec{x[0]}+\\vec{x[1]}+...+\\vec{x[n]}\\)\n\n\n\nprint(x.sum(axis=1))\nThe result is:\narray([6, 15])\nWhen axis(dim) is 1, it means the operation is performed along 1 dimension.\n\\[\n\\begin{bmatrix}\n1+2+3 \\\\\n4+5+6\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "ML/axis.html#a-3d-example",
    "href": "ML/axis.html#a-3d-example",
    "title": "Understanding Axis(dim) Operations Smartly",
    "section": "A 3D example",
    "text": "A 3D example\nx_3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nThe 3D array looks like:\n\\[\nX = \\left[\\begin{array}{c|c}\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix} &\n\\begin{bmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{bmatrix}\n\\end{array}\\right]\n\\]\n\nsum along axis 0\nprint(x_3d.sum(axis=0))\nThe result is:\narray([[8, 10, 12], [18, 20, 22]])\nThe result is the sum of two matrices.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n8 & 10 & 12 \\\\\n18 & 20 & 22\n\\end{bmatrix}\n\\]\nWhen axis(dim) is 0, given each element in this dimension is the matrix, so the sum is the sum of two matrices.\n\n\nsum along axis 1\nprint(x_3d.sum(axis=1))\nThe result is:\n\nWhen axis(dim) is 1, given each element in this dimension is the rows of the matrix, so the sum is the sum of all the rows in each matrix.\n\\[\n[\\begin{array}{c|c}\n\\begin{bmatrix}\n1 & 2 & 3\n\\end{bmatrix} +\n\\begin{bmatrix}\n4 & 5 & 6\n\\end{bmatrix} &\n\\begin{bmatrix}\n7 & 8 & 9\n\\end{bmatrix} +\n\\begin{bmatrix}\n10 & 11 & 12\n\\end{bmatrix}\n\\end{array}]\n\\]\nso the result is:\narray([[5,7,9], [17, 19, 21]])\n\n\nsum along axis 2\nWhen axis(dim) is 2, given each element in this dimension is the elements of the matrix, so the sum is the sum of the elements.\n\\[\n\\begin{array}{c|c}\n\\begin{bmatrix}\n1+2+3,4+5+6\n\\end{bmatrix} &\n\\begin{bmatrix}\n7+8+9,10+11+12\n\\end{bmatrix}\n\\end{array}\n\\]\nso the result is:\narray([[6, 15], [24, 33]])\nAso, when axis(dim) is -1, it means the operation is performed along the last dimension. So for 2d array, axis -1 is the same as axis 1."
  },
  {
    "objectID": "ML/axis.html#rules",
    "href": "ML/axis.html#rules",
    "title": "Understanding Axis(dim) Operations Smartly",
    "section": "Rules",
    "text": "Rules\n\nwhen operate on axis(dim) N, it means the operation is performed along the elements of dimension [0‚Ä¶N].\nfor 2d array, axis 0 is the sum of vectors, because each element of the array is a vector, computer sees m vectors at once for a (m,n) shape array.\nfor 3d array, axis 0 is the sum of matrices, because each element of the array is a matrix. Computer sees m matrices at once for a (m,n,p) shape array.\nfor 2d array, axis 1 is the sum of elements in each vector and merge back to (m,1) shape array. Computer sees 1 vector with n elements at once for m times.\nfor 3d array, axis 1 is the sum of all vectors in each matrix and merge back to (m,1,p) shape array. Computer sees n vectors at once for m times.\nfor 3d array, axis 2 is the sum of each vector in each matrix and merge back to (m,n,1) shape array. Computer sees p vectors for m*n times."
  },
  {
    "objectID": "ML/regularization-underconstrained.html",
    "href": "ML/regularization-underconstrained.html",
    "title": "Regularization and Under-Constrained Problems",
    "section": "",
    "text": "In linear regression, we often encounter scenarios where the standard solution breaks down:\nUnder-constrained scenarios:\n\n\\(m &lt; n\\) (fewer samples than dimensions)\nFeature-dependent columns (linearly dependent features)\n\nConsequence: Matrix \\(X^T X\\) is not invertible, preventing direct solution of linear regression.\nThis post explores why this happens and how regularization provides a mathematically sound solution."
  },
  {
    "objectID": "ML/regularization-underconstrained.html#problem-context",
    "href": "ML/regularization-underconstrained.html#problem-context",
    "title": "Regularization and Under-Constrained Problems",
    "section": "",
    "text": "In linear regression, we often encounter scenarios where the standard solution breaks down:\nUnder-constrained scenarios:\n\n\\(m &lt; n\\) (fewer samples than dimensions)\nFeature-dependent columns (linearly dependent features)\n\nConsequence: Matrix \\(X^T X\\) is not invertible, preventing direct solution of linear regression.\nThis post explores why this happens and how regularization provides a mathematically sound solution."
  },
  {
    "objectID": "ML/regularization-underconstrained.html#linear-regression-and-the-normal-equations",
    "href": "ML/regularization-underconstrained.html#linear-regression-and-the-normal-equations",
    "title": "Regularization and Under-Constrained Problems",
    "section": "Linear Regression and the Normal Equations",
    "text": "Linear Regression and the Normal Equations\n\nObjective Function\nIn linear regression, we want to minimize the squared error:\n\\[\n\\min_w ||Xw - y||^2\n\\]\nwhere: - \\(X\\) is the \\(m \\times n\\) data matrix (m samples, n features) - \\(w\\) is the \\(n \\times 1\\) weight vector - \\(y\\) is the \\(m \\times 1\\) target vector\n\n\nLoss Function\nThe squared error can be written as:\n\\[\nJ(w; X, y) = ||Xw - y||^2 = (Xw - y)^T (Xw - y)\n\\]"
  },
  {
    "objectID": "ML/regularization-underconstrained.html#derivation-of-the-normal-equations",
    "href": "ML/regularization-underconstrained.html#derivation-of-the-normal-equations",
    "title": "Regularization and Under-Constrained Problems",
    "section": "Derivation of the Normal Equations",
    "text": "Derivation of the Normal Equations\nLet‚Äôs derive the analytical solution step by step.\n\nStep 1: Expand the Loss\n\\[\nJ(w; X, y) = (Xw)^T Xw - (Xw)^T y - y^T Xw + y^T y\n\\]\n\n\nStep 2: Simplify Using Vector Dot Product Symmetry\nSince \\(Xw\\) and \\(y\\) are vectors, their dot product is commutative:\n\\[\n(Xw)^T y = y^T Xw \\quad \\text{(because } \\vec{v}_1 \\cdot \\vec{v}_2 = \\vec{v}_2 \\cdot \\vec{v}_1\\text{)}\n\\]\nTherefore:\n\\[\nJ(w; X, y) = w^T X^T Xw - 2y^T Xw + y^T y\n\\]\n\n\nStep 3: Compute the Gradient\nWe need \\(\\nabla_w J(w; X, y)\\). The gradient has three parts:\nPart 1: Quadratic Term \\(w^T X^T Xw\\)\nThis is a quadratic form. Using the matrix derivative rule \\(\\nabla_w (w^T A w) = 2Aw\\) for symmetric \\(A\\):\n\\[\n\\nabla_w (w^T X^T Xw) = 2X^T Xw\n\\]\n\n\n\n\n\n\nNoteDetailed Derivation\n\n\n\n\\[\n\\begin{aligned}\nw^T X^T Xw &= (Xw)^T (Xw) = ||Xw||^2 \\\\\n\\nabla_w ||Xw||^2 &= 2X^T Xw\n\\end{aligned}\n\\]\n\n\nPart 2: Linear Term \\(-2y^T Xw\\)\nThis is a linear term. Using the rule \\(\\nabla_w (a^T w) = a\\):\n\\[\n\\nabla_w (-2y^T Xw) = -2X^T y\n\\]\nExplanation of dimensions: - If \\(X\\) is \\(m \\times n\\) and \\(w\\) is \\(n \\times 1\\), then \\(Xw\\) is \\(m \\times 1\\) - \\(y\\) is \\(m \\times 1\\) - For the gradient with respect to \\(w\\) (which is \\(n \\times 1\\)), we need \\(X^T\\) (which is \\(n \\times m\\)) to match dimensions - The linear derivative formula \\(\\nabla_w (a^T w) = a\\) tells us the gradient is the transpose of the coefficient\nPart 3: Constant Term \\(y^T y\\)\nThis is constant with respect to \\(w\\), so its gradient is 0.\n\n\nStep 4: Complete Gradient\n\\[\n\\nabla_w J(w; X, y) = 2X^T Xw - 2X^T y\n\\]\n\n\nStep 5: Solve for Optimal \\(w\\)\nSet the gradient to zero:\n\\[\n\\begin{aligned}\n2X^T Xw - 2X^T y &= 0 \\\\\nX^T Xw &= X^T y \\\\\nw &= (X^T X)^{-1} X^T y \\quad \\text{(if } X^T X \\text{ is invertible)}\n\\end{aligned}\n\\]\nThis is the normal equation - the closed-form solution to linear regression."
  },
  {
    "objectID": "ML/regularization-underconstrained.html#the-problem-when-xt-x-is-not-invertible",
    "href": "ML/regularization-underconstrained.html#the-problem-when-xt-x-is-not-invertible",
    "title": "Regularization and Under-Constrained Problems",
    "section": "The Problem: When \\(X^T X\\) is Not Invertible",
    "text": "The Problem: When \\(X^T X\\) is Not Invertible\nThe normal equation requires \\((X^T X)^{-1}\\) to exist. However, \\(X^T X\\) is not invertible when:\n\nUnder-constrained: \\(m &lt; n\\) (fewer samples than features)\nRank deficient: Columns of \\(X\\) are linearly dependent\n\nIn these cases, we cannot compute \\((X^T X)^{-1}\\), and the standard solution fails."
  },
  {
    "objectID": "ML/regularization-underconstrained.html#solution-regularization",
    "href": "ML/regularization-underconstrained.html#solution-regularization",
    "title": "Regularization and Under-Constrained Problems",
    "section": "Solution: Regularization",
    "text": "Solution: Regularization\nThe key insight is to add a small positive constant \\(\\alpha\\) to the diagonal of \\(X^T X\\):\n\\[\nw = (X^T X + \\alpha I_n)^{-1} X^T y\n\\]\nwhere: - \\(\\alpha &gt; 0\\) is the regularization parameter - \\(I_n\\) is the \\(n \\times n\\) identity matrix\nEffect: \\(X^T X + \\alpha I_n\\) is always invertible for \\(\\alpha &gt; 0\\), even when \\(X^T X\\) is singular.\n\n\n\n\n\n\nTipWhy This Works\n\n\n\nAdding \\(\\alpha I_n\\) increases all eigenvalues of \\(X^T X\\) by \\(\\alpha\\). Since \\(\\alpha &gt; 0\\), no eigenvalue can be zero, ensuring invertibility.\n\n\n\nTwo Variants: Ridge vs Pseudo-Inverse\nThis approach gives us two related solutions depending on the value of \\(\\alpha\\):\n1. Ridge Regression (L2 Regularization): Use \\(\\alpha &gt; 0\\) in practice. This is the common approach where you choose a small positive regularization parameter.\n2. Moore-Penrose Pseudo-Inverse: The limiting case as \\(\\alpha \\to 0\\):\n\\[\nX^+ = \\lim_{\\alpha \\to 0} (X^T X + \\alpha I_n)^{-1} X^T\n\\]\nThis is the pseudo-inverse of \\(X\\), which exists even when \\(X^T X\\) is singular.\nProperties:\n\nWhen \\(X^T X\\) is invertible: \\(X^+ = (X^T X)^{-1} X^T\\) (standard inverse)\nWhen \\(X^T X\\) is singular: \\(X^+\\) provides the minimum-norm solution"
  },
  {
    "objectID": "ML/regularization-underconstrained.html#intuitive-derivation-from-scalar-to-matrix",
    "href": "ML/regularization-underconstrained.html#intuitive-derivation-from-scalar-to-matrix",
    "title": "Regularization and Under-Constrained Problems",
    "section": "Intuitive Derivation: From Scalar to Matrix",
    "text": "Intuitive Derivation: From Scalar to Matrix\nUnderstanding the matrix form becomes easier if we start from a simple scalar case.\n\nScalar Case\nFor a simple quadratic loss \\((ax - c)^2\\) where \\(a\\) is input, \\(x\\) is weight, and \\(c\\) is target:\nExpand:\n\\[\n\\text{Loss} = a^2 x^2 - 2acx + c^2\n\\]\nDerivative:\n\\[\n\\frac{\\partial \\text{Loss}}{\\partial x} = 2a^2 x - 2ac\n\\]\nSet to zero:\n\\[\n2a^2 x = 2ac \\quad \\Rightarrow \\quad x = \\frac{c}{a}\n\\]\n\n\nExtension to Matrix Form\nBy analogy, replace:\n\n\\(a^2 \\to X^T X\\) (scalar squared becomes matrix product)\n\\(x \\to w\\) (scalar weight becomes weight vector)\n\\(ac \\to X^T y\\) (scalar product becomes matrix-vector product)\n\nThis gives:\n\\[\n\\nabla_w J(w) = 2X^T Xw - 2X^T y\n\\]\n\n\n\n\n\n\nImportantKey Insight\n\n\n\nThe matrix derivative follows the same pattern as the scalar derivative, with appropriate transpose operations to maintain dimensional consistency."
  },
  {
    "objectID": "ML/regularization-underconstrained.html#summary",
    "href": "ML/regularization-underconstrained.html#summary",
    "title": "Regularization and Under-Constrained Problems",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\nScenario\n\\(X^T X\\) Status\nSolution Method\n\n\n\n\n\\(m \\geq n\\), full rank\nInvertible\n\\(w = (X^T X)^{-1} X^T y\\)\n\n\n\\(m &lt; n\\) (under-constrained)\nNot invertible\n\\(w = (X^T X + \\alpha I_n)^{-1} X^T y\\)\n\n\nRank deficient\nNot invertible\nUse pseudo-inverse \\(X^+\\)\n\n\n\nKey principle: Regularization \\(X^T X + \\alpha I_n\\) ensures invertibility by adding a small positive value to all eigenvalues of \\(X^T X\\), making the system solvable even in under-constrained scenarios."
  },
  {
    "objectID": "ML/regularization-underconstrained.html#related-posts",
    "href": "ML/regularization-underconstrained.html#related-posts",
    "title": "Regularization and Under-Constrained Problems",
    "section": "Related Posts",
    "text": "Related Posts\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots - Exploring how invertibility connects to fundamental linear algebra concepts\n\n\nSource: Deep Learning Book, Chapter 7.3"
  },
  {
    "objectID": "ML/l2-regularization.html",
    "href": "ML/l2-regularization.html",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "",
    "text": "My lecture notes\nL2 regularization (ridge regression) adds a penalty term to the loss function to prevent overfitting. This post walks through the math behind how L2 regularization affects the optimal weights, using eigenvalue decomposition to show that it shrinks weights differently in different directions based on the Hessian‚Äôs curvature."
  },
  {
    "objectID": "ML/l2-regularization.html#context",
    "href": "ML/l2-regularization.html#context",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "",
    "text": "My lecture notes\nL2 regularization (ridge regression) adds a penalty term to the loss function to prevent overfitting. This post walks through the math behind how L2 regularization affects the optimal weights, using eigenvalue decomposition to show that it shrinks weights differently in different directions based on the Hessian‚Äôs curvature."
  },
  {
    "objectID": "ML/l2-regularization.html#three-unproven-theorems",
    "href": "ML/l2-regularization.html#three-unproven-theorems",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Three Unproven Theorems",
    "text": "Three Unproven Theorems\nThese theorems are used directly here without proof. Note: Proofs will be provided in an additional blog post; here we just use them.\n\n\\(H = Q\\Lambda Q^T\\)\n\\([QAQ^T]^{-1} = (Q^T)^{-1}A^{-1}Q^{-1} = QA^{-1}Q^T\\)\n\nWhen Q is an orthogonal matrix, \\(Q^T = Q^{-1}\\)\n\nThe loss from w to w* is \\(\\frac{1}{2}(w-w^*)^TH(w-w^*)\\)\n\nThis is similar to the kinematic formula \\(s = \\frac{1}{2}at^2\\), with two key differences:\n\nHere the dimension is not time, but the displacement from w to w*\nHere the dimension is not the 0-dimensional t, but w which is a vector with dimensions, so we use H"
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.1",
    "href": "ML/l2-regularization.html#formula-7.1",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.1",
    "text": "Formula 7.1\n\\[\n\\tilde{J}(\\theta; x, y) = J(\\theta; x, y) + \\alpha \\Omega(\\theta)\n\\]\nTotal objective including regularization."
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.2",
    "href": "ML/l2-regularization.html#formula-7.2",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.2",
    "text": "Formula 7.2\nHere \\(w^Tw\\) represents the L2 norm of parameters, \\(\\alpha\\) represents the penalty coefficient, between 0-1, and relatively close to 0.\n\\[\n\\tilde{J}(w; \\theta; y) = \\frac{\\alpha}{2}w^Tw + J(w; X; y)\n\\]"
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.3",
    "href": "ML/l2-regularization.html#formula-7.3",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.3",
    "text": "Formula 7.3\n\\[\n\\nabla \\tilde{J}(w; X; y) = \\alpha w + \\nabla_w J(w; X; y)\n\\]\nTo understand this formula, the key is understanding the gradient of \\(w^Tw\\).\nIn the one-dimensional world, if we have \\(f(x) = x^2\\), which is equivalent to L2, then the gradient is \\(f'(x) = 2x\\).\nDerivation: \\[\n\\begin{align}\n(x + \\Delta x)^2 &= x^2 + 2x\\Delta x + \\Delta x^2 \\\\\n(x + \\Delta x)^2 - x^2 &= 2x\\Delta x + \\Delta x^2 \\\\\n\\frac{f(x) - f(\\Delta x)}{\\Delta x} &= 2x + \\Delta x\n\\end{align}\n\\]\nExtending to higher dimensions, \\(\\nabla(w^Tw) = 2w\\), so the gradient of \\(\\frac{\\alpha}{2}w^Tw\\) is \\(\\alpha w\\)."
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.4-7.5",
    "href": "ML/l2-regularization.html#formula-7.4-7.5",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.4 & 7.5",
    "text": "Formula 7.4 & 7.5\nThis is straightforward to understand: \\(\\epsilon\\) is the learning rate, or step size.\n\\[\nw \\leftarrow w - \\epsilon (\\alpha w + \\nabla J(w; X; y))\n\\]"
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.6",
    "href": "ML/l2-regularization.html#formula-7.6",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.6",
    "text": "Formula 7.6\nThis formula makes a quadratic approximation in the neighborhood of w* (the optimal solution) without introducing L2, yielding:\n\\[\n\\hat{J}(\\theta) = J(w^*) + \\frac{1}{2}(w - w^*)^T H(w - w^*)\n\\]\nThis can actually be seen as L2 weighted by the Hessian matrix. Because the curvature differs in each direction, we multiply the distance in each direction by the curvature in that direction to get a weighted squared distance.\nTo understand this, we need the Taylor expansion. For one dimension: \\[\nf(x) \\approx f(x^*) + f'(x)(x - x^*) + \\frac{1}{2}f''(x)(x - x^*)^2\n\\]\nFor second order: \\[\nJ(w) \\approx J(w^*) + (w - w^*)^\\top \\nabla J(w^*) + \\frac{1}{2}(w - w^*)^\\top H(w - w^*)\n\\]\nSince w* is the optimal solution, the gradient is 0, so we can directly remove \\((w - w^*)^\\top \\nabla J(w^*)\\).\nThis directly gives us formula 7.6."
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.7",
    "href": "ML/l2-regularization.html#formula-7.7",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.7",
    "text": "Formula 7.7\n\\[\n\\nabla \\hat{J}(w) = H(w - w^*)\n\\]\nFormula 7.7 is the derivative of formula 7.6. Since \\(J(w^*)\\) is a constant with gradient 0, we only need to find the gradient of \\(\\frac{1}{2}(w - w^*)^T H(w - w^*)\\), which is \\((w - w^*)\\) times H (can be derived from one dimension)."
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.87.97.10",
    "href": "ML/l2-regularization.html#formula-7.87.97.10",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.8/7.9/7.10",
    "text": "Formula 7.8/7.9/7.10\nFormula 7.8 combines 7.3 and 7.7: \\[\n\\alpha \\tilde{w} + H(\\tilde{w} - w^*) = 0\n\\]\nFrom 7.3, we already have: \\[\n\\nabla J(w; X; y) = \\alpha w + \\nabla_w J(w; X; y)\n\\]\nSubstituting into 7.7 gives us 7.8.\nFormula 7.9 is a transformation of 7.8: \\[\n\\begin{align}\n\\alpha \\tilde{w} + H\\tilde{w} &= Hw^* \\\\\n(H + \\alpha I)\\tilde{w} &= Hw^*\n\\end{align}\n\\]\nMultiplying both sides by the inverse of \\((H + \\alpha I)\\), the left side becomes just \\(\\tilde{w}\\): \\[\n\\tilde{w} = (H + \\alpha I)^{-1} Hw^*\n\\]\nWhen \\(\\alpha \\approx 0\\), \\(\\alpha I \\approx \\mathbf{0}\\), so \\(\\tilde{w} \\approx HH^{-1}w^* \\approx w^*\\)."
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.11-7.12-7.13",
    "href": "ML/l2-regularization.html#formula-7.11-7.12-7.13",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.11 & 7.12 & 7.13",
    "text": "Formula 7.11 & 7.12 & 7.13\nWe transform H to \\(H = Q\\Lambda Q^T\\), since it‚Äôs real symmetric, we can do this transformation.\nWe get formula 7.11: \\[\n\\tilde{w} = (Q\\Lambda Q^T + \\alpha I)^{-1} Q\\Lambda Q^T w^*\n\\]\nReplace I with \\(QQ^T\\): \\[\nQ\\Lambda Q^T + \\alpha QQ^T\n\\]\nThe common factors are Q and \\(Q^T\\), \\(\\Lambda\\) can be added with \\(\\alpha\\) because \\(\\alpha\\) is a constant multiplied by I.\nThis gives us formula 7.12: \\[\n\\tilde{w} = [Q(\\Lambda + \\alpha I)Q^T]^{-1} Q\\Lambda Q^T w^*\n\\]\nSubstituting \\([QAQ^T]^{-1} = QA^{-1}Q^T\\): \\[\n\\tilde{w} = Q(\\Lambda + \\alpha I)^{-1} \\Lambda Q^T w^*\n\\]\nTransform this formula to \\(\\frac{\\lambda_i}{\\lambda_i + \\alpha}\\):\n\n\\(\\Lambda = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_n), \\quad \\Lambda + \\alpha I = \\operatorname{diag}(\\lambda_1 + \\alpha, \\dots, \\lambda_n + \\alpha)\\)\n\\((\\Lambda + \\alpha I)^{-1} = \\operatorname{diag}\\!\\Big(\\tfrac{1}{\\lambda_1 + \\alpha}, \\dots, \\tfrac{1}{\\lambda_n + \\alpha}\\Big)\\)\n\\((\\Lambda + \\alpha I)^{-1}\\Lambda = \\operatorname{diag}\\!\\Big(\\tfrac{1}{\\lambda_i + \\alpha}\\Big) \\, \\operatorname{diag}(\\lambda_i) = \\operatorname{diag}\\!\\Big(\\tfrac{\\lambda_i}{\\lambda_i + \\alpha}\\Big)\\)\nSo: \\[\n\\tilde{w} = Q \\operatorname{diag}\\Big(\\frac{\\lambda_i}{\\lambda_i + \\alpha}\\Big) Q^T w^*\n\\]\n\nThis transformation tells us: in directions with large eigenvalues, w is preserved more; in directions with small eigenvalues, w is preserved less."
  },
  {
    "objectID": "ML/l2-regularization.html#contour-lines",
    "href": "ML/l2-regularization.html#contour-lines",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Contour Lines",
    "text": "Contour Lines\n\n\n\nL2 Regularization Contour Lines\n\n\nUnderstanding this figure:\n\nSolid lines are the true contour lines of the loss function. These ellipses represent contour curves under different loss scenarios. On each contour curve, each point means we need to move in the direction perpendicular to the tangent, otherwise there‚Äôs no change (gradient).\nDashed lines are the regularized loss. Because it‚Äôs a scalar, it affects all directions equally, so the contour lines are circles.\nWhen the first ellipse of the solid line intersects with a circle layer of the dashed line, that point achieves balance for both (gradients are opposite in direction, equal in magnitude).\nOur ellipse shows that change along the x-axis is larger, y-axis is smaller, proving that for equal loss, we need to move more along x, less along y.\nAt the intersection point, our regularization curve pulls up more along the x-axis, less along the y-axis, proving that the y-axis is preserved better, while x has relatively large adjustments.\n\n\nThe ellipse shows the sensitivity of the loss function in different directions. The circle shows uniform penalty of regularization in all directions. The point where they are tangent has balanced gradients and minimum loss, which is the optimal solution for ridge regression‚Äî shrinking more in unstable directions, less in stable directions.\n\n\nSource: Deep Learning Book, Chapter 7.1.1"
  },
  {
    "objectID": "ML/l1-regularization.html",
    "href": "ML/l1-regularization.html",
    "title": "Deep Learning Book Chapter 7.1.2: L1 Regularization",
    "section": "",
    "text": "My lecture notes\nL1 regularization penalizes the absolute values of weights, creating sparse solutions where many weights become exactly zero. This post derives the analytical solution and shows how soft thresholding leads to feature selection."
  },
  {
    "objectID": "ML/l1-regularization.html#context",
    "href": "ML/l1-regularization.html#context",
    "title": "Deep Learning Book Chapter 7.1.2: L1 Regularization",
    "section": "",
    "text": "My lecture notes\nL1 regularization penalizes the absolute values of weights, creating sparse solutions where many weights become exactly zero. This post derives the analytical solution and shows how soft thresholding leads to feature selection."
  },
  {
    "objectID": "ML/l1-regularization.html#definition",
    "href": "ML/l1-regularization.html#definition",
    "title": "Deep Learning Book Chapter 7.1.2: L1 Regularization",
    "section": "Definition",
    "text": "Definition\nFor L1 regularization, the penalty term is defined as:\nFormula 7.18: \\[\n\\Omega(\\theta) = ||w||_1 = \\sum_i |w_i|\n\\]\nUsing a regularization parameter \\(\\alpha\\), our total loss becomes:\nFormula 7.19: \\[\n\\tilde{J}(w;X,y) = \\alpha||w||_1 + J(w;X,y)\n\\]"
  },
  {
    "objectID": "ML/l1-regularization.html#gradient-calculation",
    "href": "ML/l1-regularization.html#gradient-calculation",
    "title": "Deep Learning Book Chapter 7.1.2: L1 Regularization",
    "section": "Gradient Calculation",
    "text": "Gradient Calculation\nFor the absolute value function \\(f = |w|\\), the derivative (subgradient) is: \\[\n\\frac{\\partial f}{\\partial w} = \\begin{cases}\n1, & w &gt; 0 \\\\\n-1, & w &lt; 0\n\\end{cases}\n\\]\nTherefore, the gradient of \\(\\alpha||w||_1\\) is \\(\\alpha \\text{sign}(w)\\), and we get:\nFormula 7.20: \\[\n\\nabla_w \\tilde{J}(w;X,y) = \\alpha \\text{sign}(w) + \\nabla_w J(w;X,y)\n\\]"
  },
  {
    "objectID": "ML/l1-regularization.html#analytical-solution",
    "href": "ML/l1-regularization.html#analytical-solution",
    "title": "Deep Learning Book Chapter 7.1.2: L1 Regularization",
    "section": "Analytical Solution",
    "text": "Analytical Solution\nFor the model, assuming the gradient of the unregularized loss can be approximated as:\nFormula 7.21: \\[\n\\nabla_w J(w;X,y) \\approx H(w - w^*)\n\\]\nwhere \\(w^*\\) is the optimal solution without regularization (where \\(\\nabla_w J(w^*;X,y) = 0\\)), and \\(H\\) is the Hessian matrix.\nUsing a quadratic approximation around \\(w^*\\), the regularized loss becomes:\nFormula 7.22: \\[\n\\hat{J}(w;X,y) = J(w^*;X,y) + \\sum_{i=1}^n \\left[\\frac{1}{2}H_{i,i}(w_i - w_i^*)^2 + \\alpha|w_i|\\right]\n\\]\nThe gradient with respect to \\(w_i\\) is: \\[\n\\frac{\\partial \\hat{J}}{\\partial w_i} = H_{i,i}(w_i - w_i^*) + \\alpha \\text{sign}(w_i)\n\\]\nSetting the gradient to zero: \\[\nH_{i,i}(w_i - w_i^*) + \\alpha \\text{sign}(w_i) = 0\n\\]\n\\[\nH_{i,i}w_i = H_{i,i}w_i^* - \\alpha \\text{sign}(w_i)\n\\]\nDividing both sides by \\(H_{i,i}\\): \\[\nw_i = w_i^* - \\frac{\\alpha}{H_{i,i}} \\text{sign}(w_i)\n\\]\nSince \\(\\alpha\\) and \\(H_{i,i}\\) are both positive:\n\nWhen \\(w_i^* &gt; 0\\): We expect \\(w_i &gt; 0\\), so \\(\\text{sign}(w_i) = +1\\): \\[\nw_i = w_i^* - \\frac{\\alpha}{H_{i,i}}\n\\]\nWhen \\(w_i^* &lt; 0\\): We expect \\(w_i &lt; 0\\), so \\(\\text{sign}(w_i) = -1\\): \\[\nw_i = w_i^* + \\frac{\\alpha}{H_{i,i}}\n\\]\nNote that for \\(w_i^* &lt; 0\\), we have \\(|w_i^*| = -w_i^*\\), so this can be written as: \\[\nw_i = -(|w_i^*| - \\frac{\\alpha}{H_{i,i}})\n\\]\n\nCombining both cases with the sign function:\nFormula 7.23: \\[\nw_i = \\text{sign}(w_i^*) \\max\\left(|w_i^*| - \\frac{\\alpha}{H_{i,i}}, 0\\right)\n\\]\nImportant note: The \\(\\max(\\cdot, 0)\\) prevents sign reversal. When \\(|w_i^*| &lt; \\frac{\\alpha}{H_{i,i}}\\), the regularization is strong enough to push \\(w_i\\) to exactly zero, rather than changing its sign. This is the soft thresholding operation."
  },
  {
    "objectID": "ML/l1-regularization.html#sparsity-property",
    "href": "ML/l1-regularization.html#sparsity-property",
    "title": "Deep Learning Book Chapter 7.1.2: L1 Regularization",
    "section": "Sparsity Property",
    "text": "Sparsity Property\nL1 regularization has a sparse solution property: it tends to push many weights to exactly zero, effectively performing feature selection. This is in contrast to L2 regularization, which shrinks weights toward zero but rarely sets them exactly to zero.\nThis sparsity arises from the soft thresholding effect shown in Formula 7.23, where weights smaller than the threshold \\(\\frac{\\alpha}{H_{i,i}}\\) are set to zero.\n\nSource: Deep Learning Book, Chapter 7.1.2"
  },
  {
    "objectID": "ML/backpropagation.html",
    "href": "ML/backpropagation.html",
    "title": "Deep Learning Book 6.5: Back-Propagation and Other Differentiation Algorithms",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.5 explores backpropagation‚Äîthe algorithm that makes training deep neural networks computationally feasible by efficiently computing gradients through the chain rule.\nüìì For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub.\nüöÄ Want to see backpropagation in action? Check out picograd‚Äîa minimal automatic differentiation engine I built (inspired by Andrej Karpathy‚Äôs micrograd) that implements reverse-mode autodiff from scratch. It demonstrates how computational graphs enable automatic gradient computation through backpropagation!"
  },
  {
    "objectID": "ML/backpropagation.html#the-gradient-problem-why-backpropagation",
    "href": "ML/backpropagation.html#the-gradient-problem-why-backpropagation",
    "title": "Deep Learning Book 6.5: Back-Propagation and Other Differentiation Algorithms",
    "section": "The Gradient Problem: Why Backpropagation?",
    "text": "The Gradient Problem: Why Backpropagation?\nTraining a neural network requires computing gradients of the loss function with respect to every parameter‚Äîpotentially millions of them. The naive approach of computing each gradient independently would be computationally intractable.\nBackpropagation solves this by exploiting the chain rule in a clever way: it reuses intermediate computations to calculate all gradients in a single backward pass through the network. This transforms an exponentially expensive problem into a linear one.\n\nQuick Reference: Understanding Backpropagation\nFor context on the mathematical foundations of backpropagation, see the Backpropagation summary.\nKey Concepts:\n\nChain Rule in Vector Form: For composite mapping \\(z = f(y), y = g(x)\\): \\[\\nabla_x z = \\left( \\frac{\\partial y}{\\partial x} \\right)^{\\top} \\nabla_y z\\]\nForward Pass: Cache all activations \\(h^{(l)} = f^{(l)}(W^{(l)} h^{(l-1)} + b^{(l)})\\)\nBackward Pass: Propagate gradients layer by layer: \\[\\nabla_{h^{(l-1)}} L = (W^{(l)})^{\\top} (\\nabla_{h^{(l)}} L \\odot f'^{(l)}(z^{(l)}))\\]\nParameter Gradients: \\[\\frac{\\partial L}{\\partial W^{(l)}} = (\\nabla_{h^{(l)}} L \\odot f'^{(l)}(z^{(l)})) (h^{(l-1)})^{\\top}\\]\n\n\n\n\n\n\n\n\n\nConcept\nDescription\nKey Insight\n\n\n\n\nComputational Graph\nDAG representing operations\nEnables reverse-mode automatic differentiation\n\n\nGradient Reuse\nShare intermediate computations\nReduces complexity from exponential to linear\n\n\nLocal Gradients\nEach operation computes local derivative\nChain rule combines them for global gradient"
  },
  {
    "objectID": "ML/backpropagation.html#exercise-1-understanding-jacobian-matrices",
    "href": "ML/backpropagation.html#exercise-1-understanding-jacobian-matrices",
    "title": "Deep Learning Book 6.5: Back-Propagation and Other Differentiation Algorithms",
    "section": "üî¨ Exercise 1: Understanding Jacobian Matrices",
    "text": "üî¨ Exercise 1: Understanding Jacobian Matrices\nHow do gradients flow through multiple layers? Let‚Äôs trace the chain rule through a 2-layer linear network.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\nprint(\"‚úì Setup complete\")\n\n\n‚úì Setup complete\n\n\n\nThe 2-Layer Network\nLayer 1 (Hidden layer): \\[\n\\begin{align}\nh_1 &= 2x_1 + x_2 \\\\\nh_2 &= x_1 + 3x_2 \\\\\nh_3 &= -x_1 + x_2\n\\end{align}\n\\]\nLayer 2 (Output layer): \\[\n\\begin{align}\ny_1 &= h_1 + 2h_2 - h_3 \\\\\ny_2 &= 3h_1 - h_2 + h_3\n\\end{align}\n\\]\n\n\nForward Pass Implementation\n\n\nShow code\ndef forward_pass(x: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute forward pass through the 2-layer network.\n\n    Args:\n        x: Input vector [x1, x2]\n\n    Returns:\n        h: Hidden layer output [h1, h2, h3]\n        y: Output layer [y1, y2]\n    \"\"\"\n    x1, x2 = x[0], x[1]\n\n    # Layer 1 (linear)\n    h1 = 2*x1 + x2\n    h2 = x1 + 3*x2\n    h3 = -x1 + x2\n    h = np.array([h1, h2, h3])\n\n    # Layer 2 (linear)\n    y1 = h1 + 2*h2 - h3\n    y2 = 3*h1 - h2 + h3\n    y = np.array([y1, y2])\n\n    return h, y\n\nprint(\"‚úì Forward pass defined\")\n\n\n‚úì Forward pass defined\n\n\n\n\nComputing Jacobian Matrices\nThe chain rule states: \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{h}} \\cdot \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{x}}\\)\n\n\nShow code\ndef compute_jacobian_h_x(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute ‚àÇh/‚àÇx analytically.\n\n    Args:\n        x: Input vector [x1, x2]\n\n    Returns:\n        Jacobian matrix (3x2)\n    \"\"\"\n    J = np.array([[2, 1], [1, 3], [-1, 1]])\n    return J\n\ndef compute_jacobian_y_h(x: np.ndarray, h: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute ‚àÇy/‚àÇh analytically.\n\n    Args:\n        x: Input vector [x1, x2]\n        h: Hidden layer [h1, h2, h3]\n\n    Returns:\n        Jacobian matrix (2x3)\n    \"\"\"\n    J = np.array([[1, 2, -1], [3, -1, 1]])\n    return J\n\ndef compute_jacobian_y_x(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute ‚àÇy/‚àÇx using chain rule.\n\n    Args:\n        x: Input vector [x1, x2]\n\n    Returns:\n        Jacobian matrix (2x2)\n    \"\"\"\n    return compute_jacobian_y_h(x, forward_pass(x)[0]) @ compute_jacobian_h_x(x)\n\nprint(\"‚úì Jacobian functions defined\")\n\n\n‚úì Jacobian functions defined\n\n\n\n\nVerification with Numerical Gradients\n\n\nShow code\ndef numerical_gradient(func, x, epsilon=1e-7):\n    \"\"\"\n    Compute numerical gradient using finite differences.\n    \"\"\"\n    grad = np.zeros((len(func(x)), len(x)))\n    for i in range(len(x)):\n        x_plus = x.copy()\n        x_minus = x.copy()\n        x_plus[i] += epsilon\n        x_minus[i] -= epsilon\n        grad[:, i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n    return grad\n\n# Test at point (1.0, 2.0)\nx_test = np.array([1.0, 2.0])\nh_test, y_test = forward_pass(x_test)\n\nprint(f\"Test point: x = {x_test}\")\nprint(f\"Hidden layer: h = {h_test}\")\nprint(f\"Output: y = {y_test}\")\nprint(\"\\n\" + \"=\"*50)\n\n# Compute analytical Jacobians\nJ_h_x_analytical = compute_jacobian_h_x(x_test)\nJ_y_h_analytical = compute_jacobian_y_h(x_test, h_test)\nJ_y_x_analytical = compute_jacobian_y_x(x_test)\n\n# Compute numerical Jacobians\nJ_h_x_numerical = numerical_gradient(lambda x: forward_pass(x)[0], x_test)\nJ_y_x_numerical = numerical_gradient(lambda x: forward_pass(x)[1], x_test)\n\nprint(\"\\n‚àÇh/‚àÇx (Analytical):\")\nprint(J_h_x_analytical)\nprint(\"\\n‚àÇh/‚àÇx (Numerical):\")\nprint(J_h_x_numerical)\nprint(f\"\\nDifference: {np.max(np.abs(J_h_x_analytical - J_h_x_numerical)):.2e}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"\\n‚àÇy/‚àÇh (Analytical):\")\nprint(J_y_h_analytical)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"\\n‚àÇy/‚àÇx (Analytical):\")\nprint(J_y_x_analytical)\nprint(\"\\n‚àÇy/‚àÇx (Numerical):\")\nprint(J_y_x_numerical)\nprint(f\"\\nDifference: {np.max(np.abs(J_y_x_analytical - J_y_x_numerical)):.2e}\")\n\n\nTest point: x = [1. 2.]\nHidden layer: h = [4. 7. 1.]\nOutput: y = [17.  6.]\n\n==================================================\n\n‚àÇh/‚àÇx (Analytical):\n[[ 2  1]\n [ 1  3]\n [-1  1]]\n\n‚àÇh/‚àÇx (Numerical):\n[[ 2.  1.]\n [ 1.  3.]\n [-1.  1.]]\n\nDifference: 2.80e-09\n\n==================================================\n\n‚àÇy/‚àÇh (Analytical):\n[[ 1  2 -1]\n [ 3 -1  1]]\n\n==================================================\n\n‚àÇy/‚àÇx (Analytical):\n[[5 6]\n [4 1]]\n\n‚àÇy/‚àÇx (Numerical):\n[[5.00000002 6.        ]\n [4.         0.99999999]]\n\nDifference: 2.29e-08"
  },
  {
    "objectID": "ML/backpropagation.html#exercise-2-gradient-descent-with-backpropagation",
    "href": "ML/backpropagation.html#exercise-2-gradient-descent-with-backpropagation",
    "title": "Deep Learning Book 6.5: Back-Propagation and Other Differentiation Algorithms",
    "section": "üî¨ Exercise 2: Gradient Descent with Backpropagation",
    "text": "üî¨ Exercise 2: Gradient Descent with Backpropagation\nLet‚Äôs implement a complete 3-layer network with backpropagation and watch how the parameters evolve during training.\n\nNetwork Architecture\nLayer 1: \\(h^{(1)}_i = \\tanh(w^{(1)}_{i1} x_1 + w^{(1)}_{i2} x_2 + b^{(1)}_i)\\)\nLayer 2: \\(h^{(2)}_i = \\tanh(w^{(2)}_{i1} a^{(1)}_1 + w^{(2)}_{i2} a^{(1)}_2 + b^{(2)}_i)\\)\nOutput: \\(\\hat{y} = w^{(3)}_1 a^{(2)}_1 + w^{(3)}_2 a^{(2)}_2 + b^{(3)}\\)\nLoss: \\(L = \\frac{1}{2}(\\hat{y} - y_{\\text{target}})^2\\)\n\n\nImplementation\n\n\nShow code\nclass ThreeLayerNetwork:\n    def __init__(self, seed=42):\n        \"\"\"\n        Initialize a 3-layer neural network with random weights.\n        \"\"\"\n        np.random.seed(seed)\n\n        # Layer 1: 2 inputs -&gt; 2 hidden units\n        self.W1 = np.random.randn(2, 2) * 0.5\n        self.b1 = np.random.randn(2) * 0.5\n\n        # Layer 2: 2 hidden -&gt; 2 hidden units\n        self.W2 = np.random.randn(2, 2) * 0.5\n        self.b2 = np.random.randn(2) * 0.5\n\n        # Layer 3: 2 hidden -&gt; 1 output\n        self.W3 = np.random.randn(2) * 0.5\n        self.b3 = np.random.randn() * 0.5\n\n        # For storing intermediate values during forward pass\n        self.cache = {}\n\n    def forward(self, x: np.ndarray) -&gt; float:\n        \"\"\"\n        Forward propagation through the network.\n\n        Args:\n            x: Input vector [x1, x2]\n\n        Returns:\n            y_hat: Predicted output (scalar)\n        \"\"\"\n        # Store input\n        self.cache['x'] = x\n\n        # Layer 1: h1 = W1 @ x + b1, a1 = tanh(h1)\n        self.cache['h1'] = self.W1 @ x + self.b1\n        self.cache['a1'] = np.tanh(self.cache['h1'])\n\n        # Layer 2: h2 = W2 @ a1 + b2, a2 = tanh(h2)\n        self.cache['h2'] = self.W2 @ self.cache['a1'] + self.b2\n        self.cache['a2'] = np.tanh(self.cache['h2'])\n\n        # Layer 3: y_hat = W3 @ a2 + b3 (linear output)\n        y_hat = self.W3 @ self.cache['a2'] + self.b3\n        self.cache['y_hat'] = y_hat\n\n        return y_hat\n\n    def backward(self, y_target: float, learning_rate: float) -&gt; dict:\n        \"\"\"\n        Backpropagation to compute gradients.\n\n        Args:\n            y_target: Target output (scalar)\n            learning_rate: Learning rate for gradient descent\n\n        Returns:\n            grads: Dictionary containing gradients for all parameters\n        \"\"\"\n        # Get cached values\n        x = self.cache['x']\n        a1 = self.cache['a1']\n        a2 = self.cache['a2']\n        y_hat = self.cache['y_hat']\n\n        # Output gradient: dL/dy_hat = y_hat - y_target\n        dL_dy = y_hat - y_target\n\n        # Layer 3 gradients\n        dL_dW3 = dL_dy * a2  # shape: (2,)\n        dL_db3 = dL_dy       # scalar\n        dL_da2 = dL_dy * self.W3  # shape: (2,)\n\n        # Layer 2 gradients\n        # tanh derivative: d(tanh(x))/dx = 1 - tanh(x)^2\n        dL_dh2 = dL_da2 * (1 - a2**2)  # shape: (2,)\n        dL_dW2 = np.outer(dL_dh2, a1)  # shape: (2, 2)\n        dL_db2 = dL_dh2                # shape: (2,)\n        dL_da1 = self.W2.T @ dL_dh2    # shape: (2,)\n\n        # Layer 1 gradients\n        dL_dh1 = dL_da1 * (1 - a1**2)  # shape: (2,)\n        dL_dW1 = np.outer(dL_dh1, x)   # shape: (2, 2)\n        dL_db1 = dL_dh1                # shape: (2,)\n\n        # Store gradients\n        grads = {\n            'dW3': dL_dW3,\n            'db3': dL_db3,\n            'dW2': dL_dW2,\n            'db2': dL_db2,\n            'dW1': dL_dW1,\n            'db1': dL_db1\n        }\n\n        # Update parameters\n        self.W3 -= learning_rate * dL_dW3\n        self.b3 -= learning_rate * dL_db3\n        self.W2 -= learning_rate * dL_dW2\n        self.b2 -= learning_rate * dL_db2\n        self.W1 -= learning_rate * dL_dW1\n        self.b1 -= learning_rate * dL_db1\n\n        return grads\n\n    def compute_loss(self, y_hat: float, y_target: float) -&gt; float:\n        \"\"\"\n        Compute MSE loss.\n        \"\"\"\n        return 0.5 * (y_hat - y_target)**2\n\n    def get_params(self) -&gt; dict:\n        \"\"\"\n        Get all parameters as a dictionary.\n        \"\"\"\n        return {\n            'W1': self.W1.copy(),\n            'b1': self.b1.copy(),\n            'W2': self.W2.copy(),\n            'b2': self.b2.copy(),\n            'W3': self.W3.copy(),\n            'b3': self.b3\n        }\n\nprint(\"‚úì Network class defined\")\n\n\n‚úì Network class defined\n\n\n\n\nTraining Loop\n\n\nShow code\n# Training configuration\nx_input = np.array([0.5, -0.3])\ny_target = 1.0\nlearning_rate = 0.01\nnum_iterations = 1000\n\n# Initialize network\nnetwork = ThreeLayerNetwork(seed=42)\n\n# Storage for logging\nloss_history = []\nparam_history = {\n    'W1': [],\n    'b1': [],\n    'W2': [],\n    'b2': [],\n    'W3': [],\n    'b3': []\n}\n\n# Training loop\nfor i in range(num_iterations):\n    # Forward pass\n    y_hat = network.forward(x_input)\n\n    # Compute loss\n    loss = network.compute_loss(y_hat, y_target)\n    loss_history.append(loss)\n\n    # Backward pass and parameter update\n    grads = network.backward(y_target, learning_rate)\n\n    # Log parameters\n    params = network.get_params()\n    param_history['W1'].append(params['W1'])\n    param_history['b1'].append(params['b1'])\n    param_history['W2'].append(params['W2'])\n    param_history['b2'].append(params['b2'])\n    param_history['W3'].append(params['W3'])\n    param_history['b3'].append(params['b3'])\n\n    # Print loss every 100 iterations\n    if (i + 1) % 100 == 0:\n        print(f\"Iteration {i+1:4d}: Loss = {loss:.6f}, y_hat = {y_hat:.6f}\")\n\nprint(\"\\nTraining completed!\")\n\n\nIteration  100: Loss = 0.019820, y_hat = 0.800902\nIteration  200: Loss = 0.000292, y_hat = 0.975849\nIteration  300: Loss = 0.000004, y_hat = 0.997066\nIteration  400: Loss = 0.000000, y_hat = 0.999644\nIteration  500: Loss = 0.000000, y_hat = 0.999957\nIteration  600: Loss = 0.000000, y_hat = 0.999995\nIteration  700: Loss = 0.000000, y_hat = 0.999999\nIteration  800: Loss = 0.000000, y_hat = 1.000000\nIteration  900: Loss = 0.000000, y_hat = 1.000000\nIteration 1000: Loss = 0.000000, y_hat = 1.000000\n\nTraining completed!\n\n\n\n\nVisualization: Loss Curve\n\n\nShow code\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(loss_history)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Training Loss over Time')\nplt.grid(True)\nplt.yscale('log')\n\nplt.subplot(1, 2, 2)\nplt.plot(loss_history[-500:])\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss (Last 500 iterations)')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nVisualization: Parameter Evolution\n\n\nShow code\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\n\n# W1\naxes[0, 0].plot(np.array(param_history['W1'])[:, 0, 0], label='W1[0,0]')\naxes[0, 0].plot(np.array(param_history['W1'])[:, 0, 1], label='W1[0,1]')\naxes[0, 0].plot(np.array(param_history['W1'])[:, 1, 0], label='W1[1,0]')\naxes[0, 0].plot(np.array(param_history['W1'])[:, 1, 1], label='W1[1,1]')\naxes[0, 0].set_title('Layer 1 Weights (W1)')\naxes[0, 0].legend()\naxes[0, 0].grid(True)\n\n# b1\naxes[0, 1].plot(np.array(param_history['b1'])[:, 0], label='b1[0]')\naxes[0, 1].plot(np.array(param_history['b1'])[:, 1], label='b1[1]')\naxes[0, 1].set_title('Layer 1 Biases (b1)')\naxes[0, 1].legend()\naxes[0, 1].grid(True)\n\n# W2\naxes[0, 2].plot(np.array(param_history['W2'])[:, 0, 0], label='W2[0,0]')\naxes[0, 2].plot(np.array(param_history['W2'])[:, 0, 1], label='W2[0,1]')\naxes[0, 2].plot(np.array(param_history['W2'])[:, 1, 0], label='W2[1,0]')\naxes[0, 2].plot(np.array(param_history['W2'])[:, 1, 1], label='W2[1,1]')\naxes[0, 2].set_title('Layer 2 Weights (W2)')\naxes[0, 2].legend()\naxes[0, 2].grid(True)\n\n# b2\naxes[1, 0].plot(np.array(param_history['b2'])[:, 0], label='b2[0]')\naxes[1, 0].plot(np.array(param_history['b2'])[:, 1], label='b2[1]')\naxes[1, 0].set_title('Layer 2 Biases (b2)')\naxes[1, 0].legend()\naxes[1, 0].grid(True)\n\n# W3\naxes[1, 1].plot(np.array(param_history['W3'])[:, 0], label='W3[0]')\naxes[1, 1].plot(np.array(param_history['W3'])[:, 1], label='W3[1]')\naxes[1, 1].set_title('Layer 3 Weights (W3)')\naxes[1, 1].legend()\naxes[1, 1].grid(True)\n\n# b3\naxes[1, 2].plot(param_history['b3'], label='b3')\naxes[1, 2].set_title('Layer 3 Bias (b3)')\naxes[1, 2].legend()\naxes[1, 2].grid(True)\n\nfor ax in axes.flat:\n    ax.set_xlabel('Iteration')\n    ax.set_ylabel('Parameter Value')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFinal Results\n\n\nShow code\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Initial Loss: {loss_history[0]:.6f}\")\nprint(f\"Final Loss: {loss_history[-1]:.6f}\")\nprint(f\"Loss Reduction: {(1 - loss_history[-1]/loss_history[0])*100:.2f}%\")\nprint(f\"\\nTarget: {y_target}\")\nfinal_prediction = network.forward(x_input)\nprint(f\"Final Prediction: {final_prediction:.6f}\")\nprint(f\"Prediction Error: {abs(final_prediction - y_target):.6f}\")\n\n\n\n============================================================\nTRAINING SUMMARY\n============================================================\nInitial Loss: 1.323064\nFinal Loss: 0.000000\nLoss Reduction: 100.00%\n\nTarget: 1.0\nFinal Prediction: 1.000000\nPrediction Error: 0.000000\n\n\n\nThis implementation demonstrates the power of backpropagation: efficiently computing gradients through the chain rule enables training deep neural networks that would otherwise be computationally intractable."
  },
  {
    "objectID": "ML/hessian-prerequisites.html",
    "href": "ML/hessian-prerequisites.html",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "",
    "text": "My notebook\nBefore diving into optimization algorithms for deep learning (Chapter 7), we need to understand second-order derivatives in multiple dimensions. The Hessian matrix is the key tool that generalizes the concept of curvature to high-dimensional spaces."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#context",
    "href": "ML/hessian-prerequisites.html#context",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "",
    "text": "My notebook\nBefore diving into optimization algorithms for deep learning (Chapter 7), we need to understand second-order derivatives in multiple dimensions. The Hessian matrix is the key tool that generalizes the concept of curvature to high-dimensional spaces."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#why-second-derivatives-matter",
    "href": "ML/hessian-prerequisites.html#why-second-derivatives-matter",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Why Second Derivatives Matter",
    "text": "Why Second Derivatives Matter\nIn one dimension, optimizing \\(f(x)\\) involves:\n\nFirst derivative \\(f'(x) = 0\\) ‚Üí Find critical points\n\nWhy set \\(f'(x) = 0\\)? At a minimum or maximum, the slope is flat (zero)\nThink of a hill: at the very top, you stop going up ‚Üí slope = 0\nAt the bottom of a valley, you stop going down ‚Üí slope = 0\nExample: For \\(f(x) = x^2\\), we have \\(f'(x) = 2x\\). Setting \\(f'(x) = 0\\) gives \\(x = 0\\) (the minimum)\n\nSecond derivative \\(f''(x)\\) ‚Üí Classify the critical point:\n\n\\(f''(x) &gt; 0\\) ‚Üí Local minimum (curves upward like a bowl)\n\\(f''(x) &lt; 0\\) ‚Üí Local maximum (curves downward like a dome)\n\\(f''(x) = 0\\) ‚Üí Inconclusive (could be an inflection point)\nWhy needed? Not all points where \\(f'(x) = 0\\) are minima! For \\(f(x) = x^3\\), we have \\(f'(0) = 0\\) but it‚Äôs neither a min nor max.\n\n\nThe challenge: How do we extend this to functions of many variables \\(f(x_1, x_2, \\ldots, x_n)\\)?\nThe answer: The Hessian matrix captures all second-order information.\n\nVisualizing Second Derivatives\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = np.linspace(-3, 3, 200)\n\n# Three functions with different second derivatives\nf1 = x**2           # f''(x) = 2 (positive, curves up)\nf2 = -x**2          # f''(x) = -2 (negative, curves down)\nf3 = x**3           # f''(x) = 6x (changes sign at x=0)\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\n\n# Function 1: f(x) = x¬≤\naxes[0, 0].plot(x, f1, 'b-', linewidth=2)\naxes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 0].set_title(\"f(x) = x¬≤\\nf''(x) = 2 &gt; 0\\n(Curves UP)\", fontsize=10)\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel('f(x)')\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].annotate('Minimum', xy=(0, 0), xytext=(0.5, 2),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Function 2: f(x) = -x¬≤\naxes[0, 1].plot(x, f2, 'r-', linewidth=2)\naxes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 1].set_title(\"f(x) = -x¬≤\\nf''(x) = -2 &lt; 0\\n(Curves DOWN)\", fontsize=10)\naxes[0, 1].set_xlabel('x')\naxes[0, 1].set_ylabel('f(x)')\naxes[0, 1].grid(True, alpha=0.3)\naxes[0, 1].annotate('Maximum', xy=(0, 0), xytext=(0.5, -2),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Function 3: f(x) = x¬≥\naxes[1, 0].plot(x, f3, 'g-', linewidth=2)\naxes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[1, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[1, 0].set_title(\"f(x) = x¬≥\\nf''(x) = 6x\\n(Changes sign)\", fontsize=10)\naxes[1, 0].set_xlabel('x')\naxes[1, 0].set_ylabel('f(x)')\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].annotate('Inflection point', xy=(0, 0), xytext=(1, -10),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Hide the unused subplot\naxes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nKey observations:\n\n\n\n\n\n\n\n\n\nSecond Derivative\nCurvature\nShape\nPoint Type\n\n\n\n\n\\(f''(x) &gt; 0\\)\nCurves upward\nBowl shape\nPotential minimum\n\n\n\\(f''(x) &lt; 0\\)\nCurves downward\nDome shape\nPotential maximum\n\n\n\\(f''(x) = 0\\) (at critical point)\nChanges sign\nFlat at that point\nInflection point\n\n\n\nNote on the third example: For \\(f(x) = x^3\\), we have \\(f''(x) = 6x\\). At the critical point \\(x = 0\\), \\(f''(0) = 0\\), which is inconclusive. The curvature changes sign: negative for \\(x &lt; 0\\) and positive for \\(x &gt; 0\\)."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#the-hessian-matrix",
    "href": "ML/hessian-prerequisites.html#the-hessian-matrix",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "The Hessian Matrix",
    "text": "The Hessian Matrix\n\nDefinition\nFor a scalar function \\(f(\\mathbf{x}) = f(x_1, x_2, \\ldots, x_n)\\), the Hessian matrix is the square matrix of all second-order partial derivatives:\n\\[\nH(f) =\n\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n\\]\n\n\nKey Properties\n\nSymmetric: If mixed partial derivatives are continuous, then \\(\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}\\), so \\(H = H^T\\).\nShape: Always \\(n \\times n\\) (determined by number of variables, not terms in the function)\nDescribes curvature in all directions simultaneously\nEigenvalue decomposition: Since the Hessian is symmetric, it can be decomposed as \\(H = Q\\Lambda Q^T\\) where \\(Q\\) contains orthonormal eigenvectors and \\(\\Lambda\\) is a diagonal matrix of eigenvalues\n\n\n\nSimple Example\nFor \\(f(x, y) = x^2 + 3y^2\\):\nStep 1: Compute first derivatives \\[\n\\frac{\\partial f}{\\partial x} = 2x, \\quad \\frac{\\partial f}{\\partial y} = 6y\n\\]\nStep 2: Compute second derivatives \\[\n\\frac{\\partial^2 f}{\\partial x^2} = 2, \\quad \\frac{\\partial^2 f}{\\partial y^2} = 6, \\quad \\frac{\\partial^2 f}{\\partial x \\partial y} = 0\n\\]\nStep 3: Build Hessian \\[\nH = \\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}\n\\]\nInterpretation: - Curvature along \\(x\\)-axis: 2 - Curvature along \\(y\\)-axis: 6 - No cross-dependency (off-diagonal = 0)\n\n\nExample with Cross Terms\nFor \\(f(x, y) = x^2 + xy + y^2\\):\n\\[\nH = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\n\\]\nThe off-diagonal term (1) indicates that \\(x\\) and \\(y\\) are coupled‚Äîchanging one affects the rate of change of the other."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#matrix-definiteness",
    "href": "ML/hessian-prerequisites.html#matrix-definiteness",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Matrix Definiteness",
    "text": "Matrix Definiteness\nFor a symmetric matrix \\(A\\), its definiteness is determined by the signs of its eigenvalues.\n\n\n\n\n\n\n\n\n\nType\nEigenvalues\nQuadratic Form \\(x^T A x\\)\nGeometric Shape\n\n\n\n\nPositive definite (PD)\nall \\(&gt; 0\\)\n\\(&gt; 0\\) for all \\(x \\neq 0\\)\nBowl (curves upward)\n\n\nNegative definite (ND)\nall \\(&lt; 0\\)\n\\(&lt; 0\\) for all \\(x \\neq 0\\)\nDome (curves downward)\n\n\nIndefinite\nsome \\(+\\), some \\(-\\)\ndepends on direction\nSaddle\n\n\nPositive semi-definite (PSD)\nall \\(\\geq 0\\)\n\\(\\geq 0\\) for all \\(x\\)\nFlat-bottom bowl\n\n\nNegative semi-definite (NSD)\nall \\(\\leq 0\\)\n\\(\\leq 0\\) for all \\(x\\)\nFlat-top dome\n\n\n\n\nQuick Test (2√ó2 case)\nFor \\(A = \\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}\\):\n\nPositive definite if \\(a &gt; 0\\) and \\(ac - b^2 &gt; 0\\)\nNegative definite if \\(a &lt; 0\\) and \\(ac - b^2 &gt; 0\\)\nIndefinite if \\(ac - b^2 &lt; 0\\)\n\n\n\nExamples\n\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}\\): eigenvalues = [2, 6] ‚Üí Positive definite\n\\(\\begin{bmatrix} -2 & 0 \\\\ 0 & -3 \\end{bmatrix}\\): eigenvalues = [-2, -3] ‚Üí Negative definite\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\\): eigenvalues = [2, -2] ‚Üí Indefinite\n\\(\\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix}\\): eigenvalues = [4, 0] ‚Üí Positive semi-definite"
  },
  {
    "objectID": "ML/hessian-prerequisites.html#interpreting-hessian-at-critical-points",
    "href": "ML/hessian-prerequisites.html#interpreting-hessian-at-critical-points",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Interpreting Hessian at Critical Points",
    "text": "Interpreting Hessian at Critical Points\nAt a critical point where \\(\\nabla f = 0\\), the Hessian determines the nature of the point:\n\n\n\n\n\n\n\n\n\nHessian Type\nEigenvalues\nSurface Shape\nPoint Type\n\n\n\n\nPositive definite\nall positive\nBowl (convex)\nLocal minimum\n\n\nNegative definite\nall negative\nDome (concave)\nLocal maximum\n\n\nIndefinite\nmixed signs\nSaddle\nNeither min nor max\n\n\nSemi-definite\nsome zero\nFlat in some directions\nInconclusive\n\n\n\n\nVisualization: Different Surface Types\n\n\nShow code\n# Create grid for plotting\nx_grid = np.linspace(-2, 2, 100)\ny_grid = np.linspace(-2, 2, 100)\nX, Y = np.meshgrid(x_grid, y_grid)\n\n# Define different functions with different Hessian types\ndef positive_definite(x, y):\n    \"\"\"Minimum: f = x¬≤ + y¬≤\"\"\"\n    return x**2 + y**2\n\ndef negative_definite(x, y):\n    \"\"\"Maximum: f = -x¬≤ - y¬≤\"\"\"\n    return -x**2 - y**2\n\ndef indefinite(x, y):\n    \"\"\"Saddle: f = x¬≤ - y¬≤\"\"\"\n    return x**2 - y**2\n\ndef semi_definite(x, y):\n    \"\"\"Flat direction: f = x¬≤\"\"\"\n    return x**2\n\n# Create 3D surface plots in 2x2 grid\nfig = plt.figure(figsize=(12, 10))\n\nfunctions = [\n    (positive_definite, \"Positive Definite\\n(Bowl - Minimum)\", \"Greens\"),\n    (negative_definite, \"Negative Definite\\n(Dome - Maximum)\", \"Reds\"),\n    (indefinite, \"Indefinite\\n(Saddle Point)\", \"RdBu\"),\n    (semi_definite, \"Semi-Definite\\n(Flat Direction)\", \"YlOrRd\")\n]\n\nfor idx, (func, title, cmap) in enumerate(functions, 1):\n    ax = fig.add_subplot(2, 2, idx, projection='3d')\n    Z = func(X, Y)\n\n    surf = ax.plot_surface(X, Y, Z, cmap=cmap, alpha=0.8,\n                           linewidth=0, antialiased=True)\n\n    ax.set_xlabel('x', fontsize=9)\n    ax.set_ylabel('y', fontsize=9)\n    ax.set_zlabel('f(x,y)', fontsize=9)\n    ax.set_title(title, fontsize=10)\n    ax.view_init(elev=25, azim=45)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nContour plots for better understanding:\n\n\nShow code\n# Contour plots in 2x2 grid\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\naxes = axes.flatten()\n\nfor idx, (func, title, cmap) in enumerate(functions):\n    ax = axes[idx]\n    Z = func(X, Y)\n\n    contour = ax.contour(X, Y, Z, levels=15, cmap=cmap)\n    ax.clabel(contour, inline=True, fontsize=7)\n\n    # Mark the critical point at origin\n    ax.plot(0, 0, 'r*', markersize=12, label='Critical point')\n\n    ax.set_xlabel('x', fontsize=9)\n    ax.set_ylabel('y', fontsize=9)\n    ax.set_title(title, fontsize=10)\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=8)\n    ax.set_aspect('equal')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ML/semi-supervised-learning.html",
    "href": "ML/semi-supervised-learning.html",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "",
    "text": "When labeled data is scarce, semi-supervised learning leverages both labeled and unlabeled data to improve model performance. This approach combines:\n\nGenerative modeling to learn data distribution \\(P(x)\\)\nSupervised classification to learn \\(P(y|x)\\)\nJoint optimization that balances both objectives"
  },
  {
    "objectID": "ML/semi-supervised-learning.html#overview",
    "href": "ML/semi-supervised-learning.html#overview",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "",
    "text": "When labeled data is scarce, semi-supervised learning leverages both labeled and unlabeled data to improve model performance. This approach combines:\n\nGenerative modeling to learn data distribution \\(P(x)\\)\nSupervised classification to learn \\(P(y|x)\\)\nJoint optimization that balances both objectives"
  },
  {
    "objectID": "ML/semi-supervised-learning.html#the-problem-limited-labeled-data",
    "href": "ML/semi-supervised-learning.html#the-problem-limited-labeled-data",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "1. The Problem: Limited Labeled Data",
    "text": "1. The Problem: Limited Labeled Data\nIn many real-world scenarios:\n\nLabeled data is expensive to obtain (requires human annotation)\nUnlabeled data is abundant and cheap\nModels trained only on limited labeled data tend to overfit\n\nSolution: Use unlabeled data to learn better representations and regularize the model."
  },
  {
    "objectID": "ML/semi-supervised-learning.html#two-learning-objectives",
    "href": "ML/semi-supervised-learning.html#two-learning-objectives",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "2. Two Learning Objectives",
    "text": "2. Two Learning Objectives\n\nGenerative Model (Unsupervised)\nObjective: Maximize the probability of generating correct inputs \\[\nP(x)\n\\]\nWhat this learns:\n\nThe underlying distribution of the data\nUseful representations of the input space\nStructure and patterns in unlabeled data\n\n\n\nClassification Model (Supervised)\nObjective: Maximize the probability of correct predictions given inputs \\[\nP(y|x)\n\\]\nWhat this learns:\n\nDecision boundaries between classes\nTask-specific features\nDirect mapping from inputs to labels"
  },
  {
    "objectID": "ML/semi-supervised-learning.html#joint-learning-objective",
    "href": "ML/semi-supervised-learning.html#joint-learning-objective",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "3. Joint Learning Objective",
    "text": "3. Joint Learning Objective\nCombined loss function: \\[\n\\mathcal{L} = -\\log P(y|x) - \\lambda \\log P(x)\n\\]\nwhere:\n\nFirst term: Supervised loss (classification accuracy)\nSecond term: Unsupervised loss (generative modeling)\n\\(\\lambda\\): Trade-off parameter controlling the balance\n\nInterpretation:\n\nThe model must simultaneously:\n\nPredict labels correctly (supervised term)\nModel the data distribution well (unsupervised term)\n\nThe unsupervised term acts as regularization, preventing overfitting to the small labeled set\n\n\n\n\nSemi-Supervised Learning"
  },
  {
    "objectID": "ML/semi-supervised-learning.html#why-this-works",
    "href": "ML/semi-supervised-learning.html#why-this-works",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "4. Why This Works",
    "text": "4. Why This Works\nKey insight: When the model learns how to represent \\(P(x)\\), it discovers where the data is dense. Decision boundaries should avoid cutting through high-density regions ‚Äî they should instead pass through low-density areas between clusters.\nGeometric interpretation:\n\nLearning \\(P(x)\\) reveals the natural clustering structure of the data\nClassification boundaries are encouraged to lie in low-density regions\nThis prevents the decision boundary from crossing through dense data manifolds\n\nBenefits:\n\nBetter representations: Unlabeled data reveals the structure of the input space\nCluster assumption: Decision boundaries naturally form between clusters, not through them\nRegularization: The generative term prevents the classifier from focusing only on labeled examples\nData efficiency: Can achieve high accuracy with significantly fewer labeled samples\n\nExample:\n\nWith only 10% labeled data, semi-supervised learning can match the performance of fully supervised learning with 100% labels"
  },
  {
    "objectID": "ML/semi-supervised-learning.html#real-world-applications",
    "href": "ML/semi-supervised-learning.html#real-world-applications",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "5. Real-World Applications",
    "text": "5. Real-World Applications\nNote: The following content is generated by ChatGPT.\n\n\n\n\n\n\n\n\n\n\n\nDomain\nTask / Problem\nUnlabeled Data Used\nMethod Family\nReal-World Benefit\nReference\n\n\n\n\nImage Recognition\nClassifying natural images (CIFAR-10, ImageNet-100)\nMillions of unlabeled web images\nConsistency Regularization (FixMatch, Mean Teacher)\n+15‚Äì25% accuracy with 10√ó fewer labeled samples\nSohn et al., FixMatch, 2020\n\n\nMedical Imaging\nTumor or lesion segmentation (MRI / CT)\nThousands of unlabeled scans\nGenerative / Consistency Hybrid (VAE, U-Net)\n~80% annotation cost reduction; works well with rare cases\nBai et al., MedIA, 2019\n\n\nSpeech Recognition\nAutomatic speech recognition (ASR)\nLarge amounts of raw audio\nRepresentation Learning (wav2vec 2.0)\nMatches full supervision using &lt;10% labeled data\nBaevski et al., wav2vec 2.0, 2020\n\n\nNatural Language Processing\nText classification, sentiment analysis\nBillions of unlabeled sentences\nSelf-Supervised Pretraining (BERT, RoBERTa)\nMassive improvement in downstream \\(P(y \\mid x)\\) tasks\nDevlin et al., BERT, 2018\n\n\nAutonomous Driving\nScene understanding, lane detection\nContinuous unlabeled video streams\nConsistency + Pseudo-Labeling\nRobust to lighting/weather; reduces manual labels\nFrench et al., 2020\n\n\nFinancial Fraud Detection\nDetecting anomalous transactions\nTransaction logs without labels\nGenerative Modeling (VAE / GAN)\nLearns normal patterns ‚Üí better anomaly detection\nXu et al., KDD, 2018\n\n\nRecommendation Systems\nPredicting user preferences\nUser‚Äìitem logs without explicit feedback\nRepresentation Learning (Autoencoder / Contrastive)\nImproves cold-start and leverages implicit signals\n‚Äî"
  },
  {
    "objectID": "ML/semi-supervised-learning.html#common-semi-supervised-learning-methods",
    "href": "ML/semi-supervised-learning.html#common-semi-supervised-learning-methods",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "6. Common Semi-Supervised Learning Methods",
    "text": "6. Common Semi-Supervised Learning Methods\nNote: The following content is generated by ChatGPT.\n\nConsistency Regularization\n\nIdea: Model should produce similar predictions for perturbed versions of the same input\nExamples: FixMatch, Mean Teacher, Virtual Adversarial Training\n\n\n\nPseudo-Labeling\n\nIdea: Use model‚Äôs confident predictions on unlabeled data as ‚Äúsoft labels‚Äù\nProcess: Train ‚Üí predict on unlabeled ‚Üí retrain with pseudo-labels\n\n\n\nGenerative Models\n\nIdea: Learn \\(P(x)\\) and \\(P(y|x)\\) jointly\nExamples: VAE, GAN-based approaches\n\n\n\nSelf-Supervised Pretraining\n\nIdea: Pretrain on unlabeled data with pretext tasks, then fine-tune on labeled data\nExamples: BERT (masked language modeling), wav2vec 2.0 (contrastive learning)\n\n\nSource: Deep Learning Book (Goodfellow et al.), Chapter 7.6"
  },
  {
    "objectID": "ML/architecture-design.html",
    "href": "ML/architecture-design.html",
    "title": "Deep Learning Book 6.4: Architecture Design - Depth vs Width",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.4 explores how network architecture‚Äîdepth versus width‚Äîfundamentally shapes what neural networks can learn and how efficiently they learn it.\nüìì For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "ML/architecture-design.html#the-architecture-question-deep-or-wide",
    "href": "ML/architecture-design.html#the-architecture-question-deep-or-wide",
    "title": "Deep Learning Book 6.4: Architecture Design - Depth vs Width",
    "section": "The Architecture Question: Deep or Wide?",
    "text": "The Architecture Question: Deep or Wide?\nWhen designing a neural network, one of the most fundamental decisions is choosing between depth (many layers) and width (many units per layer). Should you build a shallow network with many units, or a deep network with fewer units per layer?\nThe answer reveals something profound about how neural networks represent functions: deep networks can achieve exponentially greater expressiveness than shallow networks with the same number of parameters. This isn‚Äôt just theoretical‚Äîit has practical implications for model efficiency and performance.\n\nQuick Reference: Understanding Depth vs Width\nFor context on the fundamental concepts of network architecture, see the Architecture Design summary.\nKey insight: A deep ReLU network with \\(n\\) units per layer and depth \\(L\\) can create \\(\\mathcal{O}(n^L)\\) distinct linear regions in the input space. A shallow network would need exponentially many units (\\(\\mathcal{O}(n^L)\\) units in a single layer) to achieve the same expressiveness.\n\n\n\n\n\n\n\n\n\nArchitecture\nCharacteristic\nAdvantage\nChallenge\n\n\n\n\nDeep (many layers)\nHierarchical feature reuse\nExponential expressiveness with fewer parameters\nHarder to optimize (vanishing/exploding gradients)\n\n\nWide (many units per layer)\nIncreased capacity per layer\nEasier optimization\nParameter inefficient; requires exponentially more units"
  },
  {
    "objectID": "ML/architecture-design.html#experiment-shallow-vs-deep-network-comparison",
    "href": "ML/architecture-design.html#experiment-shallow-vs-deep-network-comparison",
    "title": "Deep Learning Book 6.4: Architecture Design - Depth vs Width",
    "section": "üî¨ Experiment: Shallow vs Deep Network Comparison",
    "text": "üî¨ Experiment: Shallow vs Deep Network Comparison\nLet‚Äôs explore whether depth provides an advantage in practice by comparing two networks: - Shallow Network: 1 hidden layer with 128 units - Deep Network: 3 hidden layers (16 ‚Üí 8 ‚Üí output)\nBoth networks are trained on the same regression task: \\(y = \\sin^2(x) + x^3\\)\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Configure plotting\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['axes.grid'] = True\nplt.rcParams['grid.alpha'] = 0.3\n\nprint(\"‚úì Setup complete\")\n\n\n‚úì Setup complete\n\n\n\nStep 1: Generate Training and Test Data\n\n\nShow code\n# Training data\nx_train = np.random.rand(200, 1)\ny_train = np.square(np.sin(x_train)) + np.power(x_train, 3)\n\n# Test data\nx_test = np.random.rand(100, 1)\ny_test = np.square(np.sin(x_test)) + np.power(x_test, 3)\n\n# Convert to PyTorch tensors\nx_train_tensor = torch.FloatTensor(x_train)\ny_train_tensor = torch.FloatTensor(y_train)\nx_test_tensor = torch.FloatTensor(x_test)\ny_test_tensor = torch.FloatTensor(y_test)\n\nprint(f\"Training samples: {len(x_train)}\")\nprint(f\"Test samples: {len(x_test)}\")\nprint(f\"Input range: [{x_train.min():.2f}, {x_train.max():.2f}]\")\nprint(f\"Target range: [{y_train.min():.2f}, {y_train.max():.2f}]\")\n\n\nTraining samples: 200\nTest samples: 100\nInput range: [0.01, 0.99]\nTarget range: [0.00, 1.66]\n\n\n\n\nStep 2: Define Model Architectures\n\n\nShow code\n# Shallow model: 1 hidden layer with 128 units\nshallow_model = nn.Sequential(\n    nn.Linear(1, 128),\n    nn.ReLU(),\n    nn.Linear(128, 1)\n)\n\n# Deep model: 3 hidden layers (16 ‚Üí 8 ‚Üí output)\ndeep_model = nn.Sequential(\n    nn.Linear(1, 16),\n    nn.ReLU(),\n    nn.Linear(16, 8),\n    nn.ReLU(),\n    nn.Linear(8, 1)\n)\n\nprint(\"‚úì Models created\")\nprint(f\"\\nShallow model architecture:\")\nprint(shallow_model)\nprint(f\"\\nDeep model architecture:\")\nprint(deep_model)\n\n\n‚úì Models created\n\nShallow model architecture:\nSequential(\n  (0): Linear(in_features=1, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=1, bias=True)\n)\n\nDeep model architecture:\nSequential(\n  (0): Linear(in_features=1, out_features=16, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=16, out_features=8, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=8, out_features=1, bias=True)\n)\n\n\n\n\nStep 3: Count Parameters\nHow many trainable parameters does each architecture use?\n\n\nShow code\ndef count_parameters(model):\n    \"\"\"Count total trainable parameters in a model\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nshallow_params = count_parameters(shallow_model)\ndeep_params = count_parameters(deep_model)\n\nprint(\"Parameter Counts:\")\nprint(\"-\" * 50)\nprint(f\"Shallow model (1 layer √ó 128 units): {shallow_params:,} parameters\")\nprint(f\"Deep model (3 layers):                {deep_params:,} parameters\")\nprint(\"-\" * 50)\nprint(f\"Ratio (shallow/deep): {shallow_params/deep_params:.2f}x\")\n\n# Visualize parameter counts\nfig, ax = plt.subplots(figsize=(8, 5))\nmodels = ['Shallow\\n(1√ó128)', 'Deep\\n(3 layers)']\nparams = [shallow_params, deep_params]\ncolors = ['#ff7f0e', '#1f77b4']\n\nbars = ax.bar(models, params, color=colors, alpha=0.7, edgecolor='black')\nax.set_ylabel('Number of Parameters', fontsize=12)\nax.set_title('Model Parameter Comparison', fontsize=14, fontweight='bold')\nax.grid(axis='y', alpha=0.3)\n\n# Add value labels on bars\nfor bar, param in zip(bars, params):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{param:,}',\n            ha='center', va='bottom', fontsize=11, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\nParameter Counts:\n--------------------------------------------------\nShallow model (1 layer √ó 128 units): 385 parameters\nDeep model (3 layers):                177 parameters\n--------------------------------------------------\nRatio (shallow/deep): 2.18x\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Train Both Models\n\n\nShow code\n# Training configuration\nn_epochs = 500\nlearning_rate = 0.01\nloss_fn = nn.MSELoss()\n\n# Track training history\nhistory = {\n    'Shallow': {'train_loss': [], 'test_loss': []},\n    'Deep': {'train_loss': [], 'test_loss': []}\n}\n\nmodels = {\n    'Shallow': shallow_model,\n    'Deep': deep_model\n}\n\n# Train each model\nfor name, model in models.items():\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    for epoch in range(n_epochs):\n        # Training\n        model.train()\n        y_pred = model(x_train_tensor)\n        loss = loss_fn(y_pred, y_train_tensor)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        history[name]['train_loss'].append(loss.item())\n\n        # Evaluation on test set\n        model.eval()\n        with torch.no_grad():\n            y_test_pred = model(x_test_tensor)\n            test_loss = loss_fn(y_test_pred, y_test_tensor).item()\n            history[name]['test_loss'].append(test_loss)\n\n    print(f\"‚úì {name} model trained\")\n\nprint(\"\\n‚úì Training complete\")\n\n\n‚úì Shallow model trained\n‚úì Deep model trained\n\n‚úì Training complete\n\n\n\n\nStep 5: Compare Model Performance\n\n\nShow code\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\ncolors = {'Shallow': '#ff7f0e', 'Deep': '#1f77b4'}\n\n# Plot training loss\nfor name, data in history.items():\n    axes[0].plot(data['train_loss'], label=name, linewidth=2, color=colors[name])\n\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Training Loss (MSE)', fontsize=12)\naxes[0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\naxes[0].set_yscale('log')\n\n# Plot test loss\nfor name, data in history.items():\n    axes[1].plot(data['test_loss'], label=name, linewidth=2, color=colors[name])\n\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Test Loss (MSE)', fontsize=12)\naxes[1].set_title('Test Loss Comparison', fontsize=14, fontweight='bold')\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\naxes[1].set_yscale('log')\n\nplt.tight_layout()\nplt.show()\n\n# Print final metrics\nprint(\"\\nFinal Performance (after {} epochs):\".format(n_epochs))\nprint(\"-\" * 70)\nprint(f\"{'Model':&lt;15} {'Parameters':&lt;15} {'Train Loss':&lt;15} {'Test Loss':&lt;15}\")\nprint(\"-\" * 70)\nfor name in models.keys():\n    params = count_parameters(models[name])\n    train_loss = history[name]['train_loss'][-1]\n    test_loss = history[name]['test_loss'][-1]\n    print(f\"{name:&lt;15} {params:&lt;15,} {train_loss:&lt;15.6f} {test_loss:&lt;15.6f}\")\n\n\n\n\n\n\n\n\n\n\nFinal Performance (after 500 epochs):\n----------------------------------------------------------------------\nModel           Parameters      Train Loss      Test Loss      \n----------------------------------------------------------------------\nShallow         385             0.000017        0.000022       \nDeep            177             0.000012        0.000013       \n\n\n\nThis experiment demonstrates the practical implications of depth versus width in neural network architecture design, showing how deeper networks can achieve competitive performance with fewer parameters."
  },
  {
    "objectID": "Math/index.html",
    "href": "Math/index.html",
    "title": "Math",
    "section": "",
    "text": "Mathematical foundations and explorations.\n\n\n\nThe Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots\n\n\n\n\n\nLecture 1: Geometry of Linear Equations\nLecture 2: Elimination with Matrices\nLecture 3: Matrix Multiplication and Inverse\nLecture 4: LU Decomposition\nLecture 5.1: Permutation Matrices\nLecture 5.2: Transpose\nLecture 5.3: Vector Spaces\nLecture 6: Column Space and Null Space\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nLecture 9: Independence, Basis, and Dimension\nLecture 10: Four Fundamental Subspaces\nLecture 11: Matrix Spaces, Rank-1, and Graphs\nLecture 12: Graphs, Networks, and Incidence Matrices\nLecture 13: Quiz 1 Review\nLecture 14: Orthogonal Vectors and Subspaces\nLecture 15: Projection onto Subspaces"
  },
  {
    "objectID": "Math/index.html#reflections-synthesis",
    "href": "Math/index.html#reflections-synthesis",
    "title": "Math",
    "section": "",
    "text": "The Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots"
  },
  {
    "objectID": "Math/index.html#mit-18.06sc-linear-algebra",
    "href": "Math/index.html#mit-18.06sc-linear-algebra",
    "title": "Math",
    "section": "",
    "text": "Lecture 1: Geometry of Linear Equations\nLecture 2: Elimination with Matrices\nLecture 3: Matrix Multiplication and Inverse\nLecture 4: LU Decomposition\nLecture 5.1: Permutation Matrices\nLecture 5.2: Transpose\nLecture 5.3: Vector Spaces\nLecture 6: Column Space and Null Space\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nLecture 9: Independence, Basis, and Dimension\nLecture 10: Four Fundamental Subspaces\nLecture 11: Matrix Spaces, Rank-1, and Graphs\nLecture 12: Graphs, Networks, and Incidence Matrices\nLecture 13: Quiz 1 Review\nLecture 14: Orthogonal Vectors and Subspaces\nLecture 15: Projection onto Subspaces"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture6-column-null-space.html",
    "href": "Math/MIT18.06/mit1806-lecture6-column-null-space.html",
    "title": "MIT 18.06SC Lecture 6: Column Space and Null Space",
    "section": "",
    "text": "My lecture notes\nColumn space and null space are two fundamental subspaces associated with any matrix. This lecture shows which vectors \\(b\\) make \\(Ax = b\\) solvable and which vectors \\(x\\) satisfy \\(Ax = 0\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#context",
    "href": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#context",
    "title": "MIT 18.06SC Lecture 6: Column Space and Null Space",
    "section": "",
    "text": "My lecture notes\nColumn space and null space are two fundamental subspaces associated with any matrix. This lecture shows which vectors \\(b\\) make \\(Ax = b\\) solvable and which vectors \\(x\\) satisfy \\(Ax = 0\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#subspace-properties",
    "href": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#subspace-properties",
    "title": "MIT 18.06SC Lecture 6: Column Space and Null Space",
    "section": "Subspace Properties",
    "text": "Subspace Properties\n\n\\(P \\cup L\\) is not a subspace (P and L are two subspaces)\n\\(P \\cap L\\) is a subspace"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#column-space",
    "href": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#column-space",
    "title": "MIT 18.06SC Lecture 6: Column Space and Null Space",
    "section": "Column Space",
    "text": "Column Space\n\nDefinition\nGiven a matrix A: \\[\nA = \\begin{bmatrix}\n1 & 1 & 2 \\\\\n2 & 1 & 3 \\\\\n3 & 1 & 4 \\\\\n4 & 1 & 5\n\\end{bmatrix}\n\\]\nThe column space of A consists of all possible linear combinations of the columns of A.\n\n\nKey Observation\nBecause the column space has 4 dimensions (rows) but only 3 columns (subspaces/lines), the entire space cannot be filled. There are many vectors \\(b\\) outside the column space.\nTherefore: We cannot say that for every \\(Ax = b\\), there is a solution.\n\n\nSolutions to \\(Ax = b\\)\n\nSpecial Case: \\(b = \\mathbf{0}\\)\n\n\\(b = [0, 0, 0, 0]\\) always has a solution\nThis is the origin point, and all subspaces pass through the origin\nThe solution is \\(x = [0, 0, 0]\\)\n\n\n\nWhich \\(b\\) Can Be Solved?\nGeneral Rule: \\(Ax = b\\) has a solution if and only if \\(b\\) is in the column space of A.\n\n\nExamples of Solvable \\(b\\):\n\n\\(b = [1, 2, 3, 4]\\) has solution \\(x = [1, 0, 0]\\)\n\nThis is the first column of A\n\n\\(b = [1, 1, 1, 1]\\) has solution \\(x = [0, 1, 0]\\)\n\nThis is the second column of A\n\nAny linear combination of columns has a solution\n\nIf \\(b = c_1 \\cdot \\text{col}_1 + c_2 \\cdot \\text{col}_2 + c_3 \\cdot \\text{col}_3\\)\nThen \\(x = [c_1, c_2, c_3]\\) is the solution"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#null-space",
    "href": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#null-space",
    "title": "MIT 18.06SC Lecture 6: Column Space and Null Space",
    "section": "Null Space",
    "text": "Null Space\n\nDefinition\nThe null space of A, denoted \\(N(A)\\), contains all vectors \\(x\\) that satisfy: \\[\nAx = \\mathbf{0}\n\\]\n\n\nProof: \\(N(A)\\) is a Subspace\nTo prove that the null space is a subspace, we must show it satisfies two properties:\n\n1. Closed Under Addition\nIf \\(v\\) and \\(w\\) are in \\(N(A)\\), then \\(v + w\\) is also in \\(N(A)\\).\nProof: \\[\n\\begin{align}\nAv &= \\mathbf{0} \\\\\nAw &= \\mathbf{0} \\\\\nA(v + w) &= Av + Aw = \\mathbf{0} + \\mathbf{0} = \\mathbf{0}\n\\end{align}\n\\]\nTherefore, \\(v + w \\in N(A)\\).\n\n\n2. Closed Under Scalar Multiplication\nIf \\(x\\) is in \\(N(A)\\) and \\(c\\) is any scalar, then \\(cx\\) is also in \\(N(A)\\).\nProof: \\[\n\\begin{align}\nAx &= \\mathbf{0} \\\\\nA(cx) &= c(Ax) = c \\cdot \\mathbf{0} = \\mathbf{0}\n\\end{align}\n\\]\nTherefore, \\(cx \\in N(A)\\).\n\n\n\nImportant Contrast: When \\(b \\neq \\mathbf{0}\\)\nThe solution set of \\(Ax = b\\) (when \\(b \\neq \\mathbf{0}\\)) is not a subspace.\n\nProof: Not Closed Under Scalar Multiplication\nGiven: \\[\nAx = b, \\quad b \\neq \\mathbf{0}\n\\]\nFor scalar \\(c \\neq 1\\): \\[\nA(cx) = c(Ax) = cb \\neq b\n\\]\nTherefore, if \\(x\\) is a solution, \\(cx\\) is not a solution (unless \\(c = 1\\)).\nConclusion: The solution set fails the scalar multiplication property, so it is not a subspace."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#summary",
    "title": "MIT 18.06SC Lecture 6: Column Space and Null Space",
    "section": "Summary",
    "text": "Summary\nColumn Space: - Column space = all possible linear combinations of columns - \\(Ax = b\\) is solvable \\(\\Leftrightarrow\\) \\(b\\) is in the column space - Not every \\(b \\in \\mathbb{R}^4\\) is in the column space of this particular A\nNull Space: - Null space \\(N(A) = \\{x : Ax = \\mathbf{0}\\}\\) is a subspace - Solution set of \\(Ax = b\\) (when \\(b \\neq \\mathbf{0}\\)) is not a subspace - The null space always contains the zero vector - The null space is closed under addition and scalar multiplication\n\nSource: MIT 18.06SC Linear Algebra, Lecture 6"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#overview",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#overview",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "Overview",
    "text": "Overview\nThis lecture connects linear algebra to graph theory and electrical networks:\n\nIncidence matrices: Representing graphs with matrices\nFour fundamental subspaces: Applied to graphs (loops and potential differences)\nOhm‚Äôs law: Relating currents to potential differences\nKirchhoff‚Äôs laws: Current conservation and voltage laws\nEuler‚Äôs formula: Relationship between nodes, edges, and loops"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#graph-representation-with-incidence-matrix",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#graph-representation-with-incidence-matrix",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "1. Graph Representation with Incidence Matrix",
    "text": "1. Graph Representation with Incidence Matrix\n\nExample Graph\n\n\n\nGraph with 4 nodes and 5 edges\n\n\nGraph structure:\n\n4 nodes (vertices)\n5 edges\n\n\n\nIncidence Matrix\n\\[\nA = \\begin{bmatrix}\n-1 & 1 & 0 & 0 \\\\\n0 & -1 & 1 & 0 \\\\\n-1 & 0 & 1 & 0 \\\\\n0 & 0 & -1 & 1 \\\\\n-1 & 0 & 0 & 1\n\\end{bmatrix}\n\\]\nDimensions: \\(A\\) is \\(5 \\times 4\\) (m √ó n matrix)\n\n\\(m = 5\\): number of edges (rows)\n\\(n = 4\\): number of nodes (columns)\n\nStructure:\n\nEach row represents an edge\nEach edge has exactly one \\(-1\\) (starting node) and one \\(+1\\) (ending node)\nThe rest are zeros\n\nExample: First row \\([-1, 1, 0, 0]\\) represents an edge from node 1 to node 2."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#null-space-of-a-constant-potentials",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#null-space-of-a-constant-potentials",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "2. Null Space of \\(A\\): Constant Potentials",
    "text": "2. Null Space of \\(A\\): Constant Potentials\n\nFinding \\(N(A)\\)\nTo find \\(N(A)\\), we solve \\(Ax = \\mathbf{0}\\):\n\\[\n\\begin{bmatrix}\n-1 & 1 & 0 & 0 \\\\\n0 & -1 & 1 & 0 \\\\\n-1 & 0 & 1 & 0 \\\\\n0 & 0 & -1 & 1 \\\\\n-1 & 0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4\n\\end{bmatrix}=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n\\]\nThis gives us the system:\n\nRow 1: \\(-x_1 + x_2 = 0\\) ‚Üí \\(x_1 = x_2\\)\nRow 2: \\(-x_2 + x_3 = 0\\) ‚Üí \\(x_2 = x_3\\)\nRow 4: \\(-x_3 + x_4 = 0\\) ‚Üí \\(x_3 = x_4\\)\n\nFrom these equations: \\(x_1 = x_2 = x_3 = x_4\\)\nSolution: \\[\nN(A) = c \\begin{bmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n1\n\\end{bmatrix}\n\\]\nDimension: \\(\\dim(N(A)) = 1\\)\nInterpretation:\n\nThe null space represents constant potentials across all nodes\nIf all nodes have the same potential, there is no voltage difference across any edge\nThis corresponds to setting all nodes to the same ‚Äúground level‚Äù\n\nRank calculation: \\[\n\\text{rank}(A) = n - \\dim(N(A)) = 4 - 1 = 3\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#left-null-space-of-a-loops",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#left-null-space-of-a-loops",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "3. Left Null Space of \\(A\\): Loops",
    "text": "3. Left Null Space of \\(A\\): Loops\n\nTranspose Matrix\n\\[\nA^T = \\begin{bmatrix}\n-1 & 0 & -1 & 0 & -1 \\\\\n1 & -1 & 0 & 0 & 0 \\\\\n0 & 1 & 1 & -1 & 0 \\\\\n0 & 0 & 0 & 1 & 1\n\\end{bmatrix}\n\\]\nDimensions: \\(4 \\times 5\\) (n √ó m matrix)\nStructure:\n\nEach column represents an edge\nEach row represents a node\nRow \\(i\\) shows: which edges flow out of node \\(i\\) (-1) and which flow in (+1)\n\nNode balance:\n\nNode 1: 3 edges out (edges 1, 3, 5)\nNode 2: 1 edge in (edge 1), 1 edge out (edge 2)\nNode 3: 2 edges in (edges 2, 3), 1 edge out (edge 4)\nNode 4: 2 edges in (edges 4, 5)\n\nKey property: Each column sums to \\(-1 + 1 = 0\\).\n\n\nFinding \\(N(A^T)\\)\nDimension:\n\\[\n\\dim(N(A^T)) = m - r = 5 - 3 = 2\n\\]\nSystem of equations: \\(A^T y = \\mathbf{0}\\) gives:\n\\[\n\\begin{aligned}\ny_1 + y_3 + y_5 &= 0 \\\\\ny_1 - y_2 &= 0 \\\\\ny_2 + y_3 - y_4 &= 0 \\\\\ny_4 + y_5 &= 0\n\\end{aligned}\n\\]\nSolution process:\n\nSet \\(y_1 = 1\\), then \\(y_2 = 1\\) (from equation 2)\nSet \\(y_3 = -1\\) to satisfy equation 1 (with \\(y_5 = 0\\))\nThis gives the first loop: edges 1, 2, 3\n\nFor the second loop:\n\nSet \\(y_4 = 1\\), then \\(y_5 = -1\\) (from equation 4)\nSet \\(y_1 = y_2 = y_3 = 0\\)\nThis gives the second loop: edges 4, 5\n\nBasis for \\(N(A^T)\\):\n\\[\nN(A^T) = c_1 \\begin{bmatrix}\n1 \\\\\n1 \\\\\n-1 \\\\\n0 \\\\\n0\n\\end{bmatrix} + c_2 \\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n1 \\\\\n-1\n\\end{bmatrix}\n\\]\nInterpretation:\n\nLoop 1: edges 1 ‚Üí 2 ‚Üí 3 form a cycle\nLoop 2: edges 4 ‚Üí 5 form a cycle\nThe \\(-1\\) entries indicate edges traversed in the opposite direction"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#currents-and-potentials",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#currents-and-potentials",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "4. Currents and Potentials",
    "text": "4. Currents and Potentials\n\nDefinitions\nPotential (\\(u_i\\)):\n\nAssociated with nodes\nRepresents ‚Äúvoltage‚Äù or ‚Äúenergy level‚Äù at each node\n\nCurrent (\\(x_i\\)):\n\nAssociated with edges\nRepresents flow of charge or material along edges"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#ohms-law",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#ohms-law",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "5. Ohm‚Äôs Law",
    "text": "5. Ohm‚Äôs Law\nStatement: The current on an edge is proportional to the potential drop across that edge.\nMathematical form:\n\\[\nx_{ij} = u_i - u_j\n\\]\nwhere:\n\n\\(x_{ij}\\): current on edge from node \\(i\\) to node \\(j\\)\n\\(u_i\\), \\(u_j\\): potentials at nodes \\(i\\) and \\(j\\)\n\nMatrix form:\n\\[\nx = A u\n\\]\nInterpretation:\n\n\\(Au\\) maps node potentials to edge currents\nEach current is the difference in potential between the edge‚Äôs endpoints"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#kirchhoffs-current-law",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#kirchhoffs-current-law",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "6. Kirchhoff‚Äôs Current Law",
    "text": "6. Kirchhoff‚Äôs Current Law\nStatement: The total current flowing into a node equals the total current flowing out.\nMathematical form:\n\\[\nA^T y = \\mathbf{0}\n\\]\nwhere \\(y\\) is the vector of edge currents.\nPhysical interpretation: ‚ÄúIn equals out‚Äù ‚Äî charge conservation at each node.\nConnection to left null space: Kirchhoff‚Äôs current law defines exactly the left null space of \\(A\\), which represents the valid current patterns (loops) in the graph."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#graph-properties",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#graph-properties",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "7. Graph Properties",
    "text": "7. Graph Properties\n\nTree\nDefinition: A graph without loops.\nProperties:\n\nA tree on \\(n\\) nodes has exactly \\(n - 1\\) edges\n\\(\\text{rank}(A) = n - 1\\) for a tree\n\\(\\dim(N(A^T)) = 0\\) (no loops)\n\n\n\nLoops\nNumber of loops:\n\\[\n\\text{number of loops} = \\dim(N(A^T)) = m - r\n\\]\nwhere:\n\n\\(m\\) = number of edges\n\\(r\\) = rank of \\(A\\) = (number of nodes - 1)\n\nFor our example:\n\nNumber of loops = \\(5 - 3 = 2\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#eulers-formula",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#eulers-formula",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "8. Euler‚Äôs Formula",
    "text": "8. Euler‚Äôs Formula\nFormula:\n\\[\n(\\text{nodes}) - (\\text{edges}) + (\\text{loops}) = 1\n\\]\nDerivation using rank-nullity:\n\\[\n\\begin{aligned}\n\\text{loops} &= m - r \\\\\n&= m - (n - 1) \\\\\n&= m - n + 1\n\\end{aligned}\n\\]\nRearranging:\n\\[\nn - m + (\\text{loops}) = 1\n\\]\nFor our example:\n\\[\n4 - 5 + 2 = 1 \\quad \\checkmark\n\\]\nInterpretation: This fundamental relationship connects graph topology to linear algebra through the rank-nullity theorem.\n\nKey concepts:\n\nIncidence matrix \\(A\\): Rows = edges, columns = nodes; entries are \\(-1\\), \\(0\\), \\(+1\\)\n\\(N(A)\\): Constant potentials (dimension = 1)\n\\(N(A^T)\\): Loops in the graph (dimension = number of loops)\nRank: \\(\\text{rank}(A) = n - 1\\) (number of nodes - 1)\nOhm‚Äôs law: \\(x = A^T u\\) (currents from potentials)\nKirchhoff‚Äôs law: \\(A^T y = \\mathbf{0}\\) (current conservation)\nEuler‚Äôs formula: (nodes) - (edges) + (loops) = 1\n\nPhysical applications:\n\nElectrical networks and circuits\nFlow networks and transportation\nCommunication networks\nStructural engineering (trusses)\n\n\nSource: MIT 18.06SC Linear Algebra, Lecture 12"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#dimension-of-subspaces",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#dimension-of-subspaces",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "1. Dimension of Subspaces",
    "text": "1. Dimension of Subspaces\nProblem: Suppose \\(u, v, w\\) are non-zero vectors in \\(\\mathbb{R}^7\\). They span a subspace of \\(\\mathbb{R}^7\\). What are the possible dimensions?\nAnswer: 1, 2, or 3\nExplanation:\n\nMinimum dimension: 1 (if all vectors are scalar multiples of each other)\nMaximum dimension: 3 (if all three vectors are linearly independent)\nMiddle case: 2 (if exactly two are independent)\nCannot be 0 (all vectors are non-zero)\nCannot exceed 3 (only three vectors available)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#null-space-dimensions",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#null-space-dimensions",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "2. Null Space Dimensions",
    "text": "2. Null Space Dimensions\nProblem: We have \\(A\\), a \\(5 \\times 3\\) matrix with rank \\(r = 3\\). What is the null space?\nAnswer: \\(N(A) = \\{\\mathbf{0}\\}\\) (just the zero vector)\nCalculation:\n\\[\n\\dim(N(A)) = n - r = 3 - 3 = 0\n\\]\nInterpretation: Since the rank equals the number of columns, all columns are independent. The only solution to \\(Ax = \\mathbf{0}\\) is \\(x = \\mathbf{0}\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#echelon-forms-with-block-matrices",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#echelon-forms-with-block-matrices",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "3. Echelon Forms with Block Matrices",
    "text": "3. Echelon Forms with Block Matrices\n\nProblem 3a: Matrix B\nGiven:\n\\[\nB = \\begin{bmatrix}\nu \\\\\n2u\n\\end{bmatrix}\n\\]\nwhere \\(u\\) is a row vector.\nEchelon form:\n\\[\n\\begin{bmatrix}\nu \\\\\n\\mathbf{0}\n\\end{bmatrix}\n\\]\nExplanation: Row 2 is a multiple of row 1, so elimination makes row 2 become zero.\n\n\n\nProblem 3b: Matrix C\nGiven:\n\\[\nC = \\begin{bmatrix}\nu & u \\\\\nu & 0\n\\end{bmatrix}\n\\]\nwhere \\(u\\) is a \\(5 \\times 3\\) matrix with rank 3.\nElimination steps:\n\\[\n\\begin{bmatrix}\nu & u \\\\\nu & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\nu & u \\\\\n0 & -u\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\nu & 0 \\\\\n0 & -u\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\nu & 0 \\\\\n0 & u\n\\end{bmatrix}\n\\]\nStep-by-step:\n\nSubtract row 1 from row 2: eliminates bottom-left block\nSubtract column 1 from column 2: eliminates top-right block\nMultiply row 2 by -1: normalize\n\nRank of C:\n\n\\(u\\) is \\(5 \\times 3\\) with rank 3\n\\(C\\) is \\(10 \\times 6\\) (two \\(5 \\times 3\\) blocks side by side)\nBoth \\(u\\) blocks contribute full rank\nRank of C = 6\n\nDimension of left null space:\n\\[\n\\dim(N(C^T)) = m - r = 10 - 6 = 4\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#complete-solution-to-linear-systems",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#complete-solution-to-linear-systems",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "4. Complete Solution to Linear Systems",
    "text": "4. Complete Solution to Linear Systems\nProblem: Given\n\\[\nAx = \\begin{bmatrix}\n2 \\\\\n4 \\\\\n2\n\\end{bmatrix}, \\quad\nx = \\begin{bmatrix}\n2 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n+ c\\begin{bmatrix}\n1 \\\\\n1 \\\\\n0\n\\end{bmatrix}\n+ d\\begin{bmatrix}\n0 \\\\\n0 \\\\\n1\n\\end{bmatrix}\n\\]\n\nPart a: Dimension of Row Space\nAnswer: 1\nReasoning:\n\nThe null space has dimension 2 (two free variables: \\(c\\) and \\(d\\))\n\\(\\dim(N(A)) = n - r = 2\\)\nSince \\(n = 3\\) (three columns), \\(r = 3 - 2 = 1\\)\nRow space dimension = rank = 1\n\n\n\n\nPart b: Find Matrix A\nStrategy: Use the particular solution and null space vectors.\nColumn 1:\n\\[\nA\\begin{bmatrix}\n2 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n= \\begin{bmatrix}\n2 \\\\\n4 \\\\\n2\n\\end{bmatrix}\n\\implies \\text{Column 1} = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n1\n\\end{bmatrix}\n\\]\nColumn 3:\n\\[\nA\\begin{bmatrix}\n0 \\\\\n0 \\\\\n1\n\\end{bmatrix}\n= \\mathbf{0}\n\\implies \\text{Column 3} = \\begin{bmatrix}\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n\\]\nColumn 2:\n\\[\nA\\begin{bmatrix}\n1 \\\\\n1 \\\\\n0\n\\end{bmatrix}\n= \\mathbf{0}\n\\implies \\text{Column 1} + \\text{Column 2} = \\mathbf{0}\n\\]\nTherefore, Column 2 = \\(-\\)Column 1:\n\\[\n\\text{Column 2} = \\begin{bmatrix}\n-1 \\\\\n-2 \\\\\n-1\n\\end{bmatrix}\n\\]\nMatrix A:\n\\[\nA = \\begin{bmatrix}\n1 & -1 & 0 \\\\\n2 & -2 & 0 \\\\\n1 & -1 & 0\n\\end{bmatrix}\n\\]\n\n\n\nPart c: Which b Can Be Solved?\nAnswer: \\(b\\) must be a multiple of \\(\\begin{bmatrix}1 \\\\ 2 \\\\ 1\\end{bmatrix}\\)\nReasoning:\n\nColumn space of \\(A\\) is spanned by \\(\\begin{bmatrix}1 \\\\ 2 \\\\ 1\\end{bmatrix}\\)\nAll columns are multiples of this vector\n\\(Ax = b\\) has a solution only if \\(b \\in C(A)\\)\n\nTherefore: \\(b = c\\begin{bmatrix}1 \\\\ 2 \\\\ 1\\end{bmatrix}\\) for some scalar \\(c\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#properties-of-null-spaces",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#properties-of-null-spaces",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "5. Properties of Null Spaces",
    "text": "5. Properties of Null Spaces\nProblem: If the null space is \\(\\{\\mathbf{0}\\}\\) and \\(A\\) is square, what is the null space of \\(A^T\\)?\nAnswer: \\(N(A^T) = \\{\\mathbf{0}\\}\\)\nProof:\n\n\\(A\\) is \\(n \\times n\\) (square)\n\\(N(A) = \\{\\mathbf{0}\\}\\) means \\(\\dim(N(A)) = 0\\)\nTherefore: \\(\\text{rank}(A) = n - 0 = n\\)\nSince \\(A\\) is square with full rank, \\(A\\) is invertible\nFor \\(A^T\\): \\(\\dim(N(A^T)) = m - r = n - n = 0\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#subspaces-of-matrix-spaces",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#subspaces-of-matrix-spaces",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "6. Subspaces of Matrix Spaces",
    "text": "6. Subspaces of Matrix Spaces\nProblem: Consider the space of all \\(5 \\times 5\\) matrices (dimension 25). Do the invertible \\(5 \\times 5\\) matrices form a subspace?\nAnswer: No\nReason: Not closed under addition.\nCounterexample:\n\\[\nA = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}, \\quad\nB = \\begin{bmatrix}\n-1 & 0 \\\\\n0 & -1\n\\end{bmatrix}\n\\]\nBoth \\(A\\) and \\(B\\) are invertible, but:\n\\[\nA + B = \\begin{bmatrix}\n0 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\\]\nis not invertible."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#nilpotent-matrices",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#nilpotent-matrices",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "7. Nilpotent Matrices",
    "text": "7. Nilpotent Matrices\nProblem: If \\(B^2 = 0\\), must \\(B = \\mathbf{0}\\)?\nAnswer: False\nCounterexample:\n\\[\nB = \\begin{bmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n\\]\nVerification:\n\\[\nB^2 = \\begin{bmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n= \\begin{bmatrix}\n0 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\\]\nExplanation: Rows times columns can all be zero even when the matrix itself is non-zero."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#solvability-of-square-systems",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#solvability-of-square-systems",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "8. Solvability of Square Systems",
    "text": "8. Solvability of Square Systems\nProblem: If an \\(n \\times n\\) matrix has rank \\(n\\), does \\(Ax = b\\) always have a solution?\nAnswer: Yes\nProof:\n\nRank \\(n\\) for an \\(n \\times n\\) matrix means \\(A\\) is invertible\nTherefore: \\(x = A^{-1}b\\) always exists\nEvery \\(b \\in \\mathbb{R}^n\\) is in the column space"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#null-space-from-matrix-factorization",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#null-space-from-matrix-factorization",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "9. Null Space from Matrix Factorization",
    "text": "9. Null Space from Matrix Factorization\nProblem: Given\n\\[\nB = \\begin{bmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n1 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 & -1 & 2 \\\\\n0 & 1 & 1 & -1 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nWithout multiplication, find the basis for \\(N(B)\\).\n\nAnalysis\nDimensions:\n\nLeft matrix: \\(3 \\times 3\\), full rank (invertible)\nRight matrix: \\(3 \\times 4\\), rank 2\n\\(B\\): \\(3 \\times 4\\)\n\nKey insight: \\(\\operatorname{rank}(AB) \\leq \\min(\\operatorname{rank}(A), \\operatorname{rank}(B))\\)\nTherefore: \\(\\operatorname{rank}(B) = 2\\)\nNull space dimension:\n\\[\n\\dim(N(B)) = n - r = 4 - 2 = 2\n\\]\nFinding basis: Since the left matrix is invertible, \\(N(B) = N(\\text{right matrix})\\)\nFrom the right matrix in RREF:\n\nPivot columns: 1, 2\nFree variables: columns 3, 4\n\nBasis for N(B):\n\\[\nc_1\\begin{bmatrix}\n1 \\\\\n-1 \\\\\n1 \\\\\n0\n\\end{bmatrix}\n+ c_2\\begin{bmatrix}\n-2 \\\\\n1 \\\\\n0 \\\\\n1\n\\end{bmatrix}\n\\]\nVerification:\n\nColumn 3 = \\(1 \\cdot\\)Column 1 + \\((-1) \\cdot\\)Column 2\nColumn 4 = \\((-2) \\cdot\\)Column 1 + \\(1 \\cdot\\)Column 2\n\n\n\n\nSolving Bx = b\nProblem: Solve \\(Bx = \\begin{bmatrix}1 \\\\ 0 \\\\ 1\\end{bmatrix}\\)\nAnalysis:\n\\[\nB_{\\text{col1}} = [1, 0, 1]^T \\\\\nB_{\\text{col2}} = [1, 1, 0]^T \\\\\nB_{\\text{col3}} = [0, 1, -1]^T \\\\\nB_{\\text{col4}} = [1, -1, 2]^T\n\\]\nParticular solution: \\(x_p = \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix}\\)\nComplete solution:\n\\[\nx = \\begin{bmatrix}\n1 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n+ c_1\\begin{bmatrix}\n1 \\\\\n-1 \\\\\n1 \\\\\n0\n\\end{bmatrix}\n+ c_2\\begin{bmatrix}\n-2 \\\\\n1 \\\\\n0 \\\\\n1\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#conceptual-questions",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#conceptual-questions",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "10. Conceptual Questions",
    "text": "10. Conceptual Questions\n\nQuestion 1: Do A and -A Share the Same Four Fundamental Subspaces?\nAnswer: Yes\nProof:\n\n\\(C(A) = C(-A)\\) (columns are just scaled by -1)\n\\(N(A) = N(-A)\\) (if \\(Ax = \\mathbf{0}\\), then \\((-A)x = \\mathbf{0}\\))\n\\(C(A^T) = C((-A)^T)\\) (row space argument)\n\\(N(A^T) = N((-A)^T)\\) (left null space argument)\n\n\n\n\nQuestion 2: If m = n, Are Row Space and Column Space the Same?\nAnswer: No, not in general\nWhen true: Only when \\(A\\) is symmetric (\\(A = A^T\\))\nCounterexample:\n\\[\nA = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\\]\n\nColumn space: span of \\(\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\\)\nRow space: span of \\(\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\\)\n\n(In this case they happen to be the same, but that‚Äôs coincidental)\nBetter counterexample:\n\\[\nA = \\begin{bmatrix}\n1 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n\\]\n\nColumn space: \\(\\{c\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\\}\\) (in \\(\\mathbb{R}^2\\))\nRow space: \\(\\{c\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}\\}\\) (in \\(\\mathbb{R}^2\\))\n\nDifferent vectors!\n\n\n\nQuestion 3: If A and B Have the Same Four Subspaces, Is A a Multiple of B?\nAnswer: No\nCounterexample: If \\(A\\) and \\(B\\) are both full rank \\(n \\times n\\) matrices:\n\nThey both have column space = \\(\\mathbb{R}^n\\)\nThey both have null space = \\(\\{\\mathbf{0}\\}\\)\nThey both have row space = \\(\\mathbb{R}^n\\)\nThey both have left null space = \\(\\{\\mathbf{0}\\}\\)\n\nBut \\(A\\) and \\(B\\) can be completely different matrices (e.g., different invertible matrices).\n\n\n\nQuestion 4: If I Change Two Rows of A, Which Subspaces Stay the Same?\nAnswer:\n\n\\(N(A)\\) (null space)\n\\(C(A^T)\\) (row space)\n\nExplanation:\n\nRow operations preserve the null space\nRow operations preserve the row space (just produce different linear combinations)\nColumn space and left null space will generally change\n\n\n\n\nQuestion 5: Can Vector [1, 2, 3] Be in Both Null Space and Row Space?\nAnswer: No (assuming non-zero matrix)\nReason:\n\nIf \\(v\\) is in the null space: \\(Av = \\mathbf{0}\\)\nIf \\(v\\) is also a row of \\(A\\): then \\(Av\\) includes the dot product \\(v \\cdot v = \\|v\\|^2 &gt; 0\\)\nThis is a contradiction\n\nKey insight: Null space and row space are orthogonal complements in \\(\\mathbb{R}^n\\).\n\nSource: MIT 18.06SC Linear Algebra, Lecture 13"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture2-elimination.html",
    "href": "Math/MIT18.06/mit1806-lecture2-elimination.html",
    "title": "MIT 18.06SC Lecture 2: Elimination with Matrices",
    "section": "",
    "text": "This recap of MIT 18.06SC Lecture 2 explores Gaussian elimination‚Äîthe systematic algorithm that transforms any linear system into an easily solvable upper triangular form.\nüìì For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture2-elimination.html#from-linear-systems-to-upper-triangular-form",
    "href": "Math/MIT18.06/mit1806-lecture2-elimination.html#from-linear-systems-to-upper-triangular-form",
    "title": "MIT 18.06SC Lecture 2: Elimination with Matrices",
    "section": "From Linear Systems to Upper Triangular Form",
    "text": "From Linear Systems to Upper Triangular Form\nHow do we actually solve a system of linear equations like this?\n\\[\\begin{cases}\n2u + v + w = 5 \\\\\n4u - 6v = -2 \\\\\n-2u + 7v + 2w = 9\n\\end{cases}\\]\nThe answer is Gaussian elimination‚Äîa systematic process that transforms the coefficient matrix into an upper triangular form, where solutions can be read off directly through back substitution. This fundamental algorithm underpins much of numerical linear algebra and is essential for understanding how computers solve linear systems.\n\nQuick Reference: Understanding Elimination\nFor context on the mathematical foundations of Gaussian elimination, see the Lecture 2 summary.\nThe Two-Step Process:\n\nForward Elimination: Transform \\(A\\) into upper triangular \\(U\\) using row operations\n\nFor each pivot position, eliminate all entries below it\nRow operation: \\(\\text{Row}_i \\leftarrow \\text{Row}_i - m_{ij} \\cdot \\text{Row}_j\\) where \\(m_{ij} = a_{ij}/\\text{pivot}\\)\n\nBack Substitution: Solve \\(Ux = c\\) from bottom to top\n\nStart with the last equation (only one unknown)\nWork upward, substituting known values\n\n\nKey insight: Each elimination step can be represented as multiplication by an elimination matrix \\(E_{ij}\\), connecting row operations to matrix multiplication."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture2-elimination.html#exercise-implement-gaussian-elimination-from-scratch",
    "href": "Math/MIT18.06/mit1806-lecture2-elimination.html#exercise-implement-gaussian-elimination-from-scratch",
    "title": "MIT 18.06SC Lecture 2: Elimination with Matrices",
    "section": "üî¨ Exercise: Implement Gaussian Elimination from Scratch",
    "text": "üî¨ Exercise: Implement Gaussian Elimination from Scratch\nLet‚Äôs implement the complete Gaussian elimination algorithm and verify it works correctly.\n\n\nShow code\nimport numpy as np\nfrom IPython.display import display, Markdown\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\ndef matrix_to_latex(matrix, precision=2):\n    \"\"\"Convert numpy matrix to LaTeX format.\"\"\"\n    if len(matrix.shape) == 1:\n        # Vector\n        elements = \" \\\\\\\\ \".join([f\"{x:.{precision}f}\" for x in matrix])\n        return f\"$$\\\\begin{{bmatrix}}{elements}\\\\end{{bmatrix}}$$\"\n    else:\n        # Matrix\n        rows = []\n        for row in matrix:\n            row_str = \" & \".join([f\"{x:.{precision}f}\" for x in row])\n            rows.append(row_str)\n        matrix_str = \" \\\\\\\\ \".join(rows)\n        return f\"$$\\\\begin{{bmatrix}}{matrix_str}\\\\end{{bmatrix}}$$\"\n\nprint(\"‚úì Setup complete\")\n\n\n‚úì Setup complete\n\n\n\nAlgorithm Overview\nThe algorithm consists of two phases:\n\nForward Elimination: Transform \\(A\\) into upper triangular form \\(U\\)\n\nFor each column \\(j\\) from 0 to \\(n-1\\):\n\nPivot = \\(A[j, j]\\)\nFor each row \\(i\\) below pivot (\\(i &gt; j\\)):\n\nCompute multiplier: \\(m_{ij} = A[i, j] / \\text{pivot}\\)\nRow operation: \\(A[i, :] = A[i, :] - m_{ij} \\cdot A[j, :]\\)\nUpdate RHS: \\(b[i] = b[i] - m_{ij} \\cdot b[j]\\)\n\n\n\nBack Substitution: Solve \\(Ux = c\\) from bottom to top\n\nStart from last row: \\(x[n-1] = c[n-1] / U[n-1, n-1]\\)\nFor each row \\(i\\) from \\(n-2\\) down to \\(0\\):\n\n\\(x[i] = (c[i] - \\sum_{j=i+1}^{n-1} U[i,j] \\cdot x[j]) / U[i,i]\\)\n\n\n\n\n\nImplementation\n\n\nShow code\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using Gaussian elimination.\n\n    Parameters:\n    -----------\n    A : numpy.ndarray, shape (n, n)\n        Coefficient matrix\n    b : numpy.ndarray, shape (n,)\n        Right-hand side vector\n\n    Returns:\n    --------\n    x : numpy.ndarray, shape (n,)\n        Solution vector\n    \"\"\"\n    A_copy = A.copy()\n    b_copy = b.copy()\n\n    rows, cols = A.shape\n    assert rows == cols, \"Matrix must be square\"\n\n    for i in range(rows - 1):\n        multiplier = A_copy[i + 1:, i] / A_copy[i, i]\n        for index, m in enumerate(multiplier):\n            A_copy[i + 1 + index] = A_copy[i + 1 + index] - m * A_copy[i]\n            assert abs(A_copy[i + 1 + index, i]) &lt; 1e-10, \"Upper triangular matrix expected\"\n            b_copy[i + 1 + index] = b_copy[i + 1 + index] - m * b_copy[i]\n\n    U = A_copy\n    print(\"Upper triangular matrix U:\")\n    display(Markdown(matrix_to_latex(U)))\n\n    # Now our matrix is upper triangular\n    # Solve for x from bottom to top\n    x = np.zeros(rows)\n    for i in range(rows - 1, -1, -1):\n        x[i] = (b_copy[i] - np.dot(A_copy[i][i + 1:], x[i + 1:])) / A_copy[i][i]\n\n    return x\n\nprint(\"‚úì Function defined\")\n\n\n‚úì Function defined\n\n\n\n\nTest on Lecture Example\nTest on the system from the lecture:\n\\[\\begin{cases}\n2u + v + w = 5 \\\\\n4u - 6v = -2 \\\\\n-2u + 7v + 2w = 9\n\\end{cases}\\]\nExpected solution: \\(u = 1, v = 1, w = 2\\)\n\n\nShow code\n# Define the system from lecture\nA = np.array([\n    [2, 1, 1],\n    [4, -6, 0],\n    [-2, 7, 2]\n], dtype=float)\n\nb = np.array([5, -2, 9], dtype=float)\n\nprint(\"System to solve:\")\nprint(\"A =\")\nprint(A)\nprint(\"\\nb =\")\nprint(b)\n\n# Solve using our implementation\nx_our = gaussian_elimination(A, b)\n\n# Solve using NumPy\nx_numpy = np.linalg.solve(A, b)\n\n# Compare results\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Results:\")\nprint(\"=\" * 60)\nprint(f\"Our implementation:      x = {x_our}\")\nprint(f\"NumPy (np.linalg.solve): x = {x_numpy}\")\nprint(\"=\" * 60)\n\n# Verify solution\nresidual_our = np.linalg.norm(A @ x_our - b)\nresidual_numpy = np.linalg.norm(A @ x_numpy - b)\n\nprint(f\"\\nResidual (our):   ||Ax - b|| = {residual_our:.2e}\")\nprint(f\"Residual (NumPy): ||Ax - b|| = {residual_numpy:.2e}\")\n\n# Check difference\ndifference = np.linalg.norm(x_our - x_numpy)\nprint(f\"\\nDifference: ||x_our - x_numpy|| = {difference:.2e}\")\n\nif difference &lt; 1e-10:\n    print(\"\\n‚úÖ Solutions match! Implementation is correct.\")\nelse:\n    print(\"\\n‚ùå Solutions differ. Check implementation.\")\n\n\nSystem to solve:\nA =\n[[ 2.  1.  1.]\n [ 4. -6.  0.]\n [-2.  7.  2.]]\n\nb =\n[ 5. -2.  9.]\nUpper triangular matrix U:\n\n\n\\[\\begin{bmatrix}2.00 & 1.00 & 1.00 \\\\ 0.00 & -8.00 & -2.00 \\\\ 0.00 & 0.00 & 1.00\\end{bmatrix}\\]\n\n\n\n============================================================\nResults:\n============================================================\nOur implementation:      x = [1. 1. 2.]\nNumPy (np.linalg.solve): x = [1. 1. 2.]\n============================================================\n\nResidual (our):   ||Ax - b|| = 0.00e+00\nResidual (NumPy): ||Ax - b|| = 0.00e+00\n\nDifference: ||x_our - x_numpy|| = 0.00e+00\n\n‚úÖ Solutions match! Implementation is correct.\n\n\n\n\nVisualize Upper Triangular Matrix\nLet‚Äôs test on a larger 10√ó10 system to see the upper triangular structure more clearly.\n\n\nShow code\n# Create a 10x10 random system\nnp.random.seed(123)\nA_large = np.random.randn(10, 10)\nb_large = np.random.randn(10)\n\nprint(\"Original 10√ó10 matrix A:\")\ndisplay(Markdown(matrix_to_latex(A_large)))\n\nprint(\"\\n\" + \"=\" * 70)\n\n# Solve using our implementation\nx_large = gaussian_elimination(A_large, b_large)\n\nprint(\"=\" * 70)\nprint(\"\\n‚úì Upper triangular structure achieved!\")\n\n\nOriginal 10√ó10 matrix A:\n\n\n\\[\\begin{bmatrix}-1.09 & 1.00 & 0.28 & -1.51 & -0.58 & 1.65 & -2.43 & -0.43 & 1.27 & -0.87 \\\\ -0.68 & -0.09 & 1.49 & -0.64 & -0.44 & -0.43 & 2.21 & 2.19 & 1.00 & 0.39 \\\\ 0.74 & 1.49 & -0.94 & 1.18 & -1.25 & -0.64 & 0.91 & -1.43 & -0.14 & -0.86 \\\\ -0.26 & -2.80 & -1.77 & -0.70 & 0.93 & -0.17 & 0.00 & 0.69 & -0.88 & 0.28 \\\\ -0.81 & -1.73 & -0.39 & 0.57 & 0.34 & -0.01 & 2.39 & 0.41 & 0.98 & 2.24 \\\\ -1.29 & -1.04 & 1.74 & -0.80 & 0.03 & 1.07 & 0.89 & 1.75 & 1.50 & 1.07 \\\\ -0.77 & 0.79 & 0.31 & -1.33 & 1.42 & 0.81 & 0.05 & -0.23 & -1.20 & 0.20 \\\\ 0.47 & -0.83 & 1.16 & -1.10 & -2.12 & 1.04 & -0.40 & -0.13 & -0.84 & -1.61 \\\\ 1.26 & -0.69 & 1.66 & 0.81 & -0.31 & -1.09 & -0.73 & -1.21 & 2.09 & 0.16 \\\\ 1.15 & -1.27 & 0.18 & 1.18 & -0.34 & 1.03 & -1.08 & -1.36 & 0.38 & -0.38\\end{bmatrix}\\]\n\n\n\n======================================================================\nUpper triangular matrix U:\n\n\n\\[\\begin{bmatrix}-1.09 & 1.00 & 0.28 & -1.51 & -0.58 & 1.65 & -2.43 & -0.43 & 1.27 & -0.87 \\\\ -0.00 & -0.72 & 1.31 & 0.30 & -0.08 & -1.47 & 3.72 & 2.46 & 0.21 & 0.93 \\\\ -0.00 & 0.00 & 3.22 & 1.07 & -1.89 & -3.94 & 10.50 & 5.69 & 1.36 & 1.35 \\\\ -0.00 & 0.00 & 0.00 & 0.82 & -2.93 & -3.41 & 8.91 & 3.46 & 1.04 & -0.34 \\\\ 0.00 & 0.00 & 0.00 & 0.00 & 6.41 & 7.26 & -17.36 & -8.55 & -1.51 & 2.79 \\\\ 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 1.15 & -1.79 & -0.91 & -0.09 & -0.48 \\\\ 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 3.20 & 0.39 & -1.78 & -0.72 \\\\ -0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -2.27 & 0.17 & 2.74 \\\\ 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 1.73 & -2.90 \\\\ -0.00 & -0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 3.24\\end{bmatrix}\\]\n\n\n======================================================================\n\n‚úì Upper triangular structure achieved!\n\n\n\nThis implementation demonstrates the fundamental algorithm for solving linear systems, connecting the geometric view from Lecture 1 with the algebraic machinery of row operations and matrix elimination."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#overview",
    "href": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#overview",
    "title": "MIT 18.06SC Lecture 14: Orthogonal Vectors and Subspaces",
    "section": "Overview",
    "text": "Overview\nThis lecture introduces orthogonality ‚Äî one of the most important geometric concepts in linear algebra:\n\nOrthogonal vectors (perpendicular direction)\nOrthogonal subspaces (every vector in one is perpendicular to every vector in the other)\nThe fundamental orthogonal relationships: row space ‚ä• null space, column space ‚ä• left null space\nPreview of least squares and the normal equations\n\nReference: Lecture 10: Four Fundamental Subspaces"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#orthogonal-vectors",
    "href": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#orthogonal-vectors",
    "title": "MIT 18.06SC Lecture 14: Orthogonal Vectors and Subspaces",
    "section": "Orthogonal Vectors",
    "text": "Orthogonal Vectors\nDefinition: In \\(n\\)-dimensional space, two vectors are orthogonal if the angle between them is \\(90¬∞\\).\nMathematical condition:\n\\[\nx^T y = 0\n\\]\nInterpretation: The dot product (inner product) of orthogonal vectors equals zero.\n\nPythagorean Theorem in Vector Spaces\n\n\n\nPythagorean Theorem\n\n\nClassical form:\n\\[\n\\|a\\|^2 + \\|b\\|^2 = \\|c\\|^2\n\\]\nVector space form: For orthogonal vectors \\(x\\) and \\(y\\) (where \\(x^T y = 0\\)):\n\\[\n\\|x\\|^2 + \\|y\\|^2 = \\|x + y\\|^2\n\\]\nProof:\n\\[\n\\begin{aligned}\n\\|x + y\\|^2 &= (x + y)^T(x + y) \\\\\n&= x^T x + x^T y + y^T x + y^T y \\\\\n&= \\|x\\|^2 + 2(x^T y) + \\|y\\|^2 \\\\\n&= \\|x\\|^2 + \\|y\\|^2\n\\end{aligned}\n\\]\nThe cross terms vanish because \\(x^T y = 0\\) (orthogonality condition).\n\n\n\n\n\n\nNoteVector Norm Squared\n\n\n\n\\[\n\\|x\\|^2 = x^T \\cdot x = \\sum_{i=1}^n x_i^2\n\\]\nThe squared length (magnitude) of a vector equals the sum of squares of its components."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#orthogonal-subspaces",
    "href": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#orthogonal-subspaces",
    "title": "MIT 18.06SC Lecture 14: Orthogonal Vectors and Subspaces",
    "section": "Orthogonal Subspaces",
    "text": "Orthogonal Subspaces\nDefinition: Subspace \\(S\\) is orthogonal to subspace \\(T\\) if every vector in \\(S\\) is orthogonal to every vector in \\(T\\).\nMathematical statement:\n\\[\nS \\perp T \\iff \\text{for all } s \\in S \\text{ and } t \\in T, \\quad s^T t = 0\n\\]\n\nExample: Are Wall and Floor Orthogonal Subspaces?\n\n\n\nFloor and Wall\n\n\nQuestion: In 3D space, is the wall (a 2D subspace) orthogonal to the floor (another 2D subspace)?\nAnswer: No, for two reasons:\nReason 1 (Intersection): - Their intersection is a line, not just the origin - Vectors along this line are in both subspaces - A vector cannot be orthogonal to itself (unless it‚Äôs zero) - Therefore, not all vectors in one are orthogonal to all vectors in the other\nReason 2 (Dimension): - Wall has dimension 2 - Floor has dimension 2 - The whole space has dimension 3 - For orthogonal subspaces: \\(\\dim(S) + \\dim(T) \\leq \\dim(\\text{space})\\) - Here: \\(2 + 2 = 4 &gt; 3\\) (impossible!)\n\n\n\n\n\n\nImportantKey Insight\n\n\n\nOrthogonal subspaces can only intersect at the origin."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#row-space-and-null-space",
    "href": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#row-space-and-null-space",
    "title": "MIT 18.06SC Lecture 14: Orthogonal Vectors and Subspaces",
    "section": "Row Space and Null Space",
    "text": "Row Space and Null Space\n\n\n\nSubspace Orthogonality\n\n\nTheorem: The null space \\(N(A)\\) contains all vectors perpendicular to the row space \\(C(A^T)\\).\nProof: For any \\(x \\in N(A)\\) and any row \\(r_i\\) of \\(A\\):\n\\[\nAx = \\mathbf{0} \\implies r_i^T x = 0 \\text{ for all rows } r_i\n\\]\nTherefore \\(x\\) is orthogonal to every row, hence orthogonal to the entire row space.\nDirect sum decomposition:\n\\[\n\\mathbb{R}^n = C(A^T) \\oplus N(A)\n\\]\nInterpretation: - Every vector in \\(\\mathbb{R}^n\\) can be uniquely decomposed into a row space component and a null space component - These two subspaces are orthogonal complements - \\(\\dim(C(A^T)) + \\dim(N(A)) = n\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#column-space-and-left-null-space",
    "href": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#column-space-and-left-null-space",
    "title": "MIT 18.06SC Lecture 14: Orthogonal Vectors and Subspaces",
    "section": "Column Space and Left Null Space",
    "text": "Column Space and Left Null Space\nTheorem: The left null space \\(N(A^T)\\) contains all vectors perpendicular to the column space \\(C(A)\\).\nProof: For any \\(y \\in N(A^T)\\):\n\\[\nA^T y = \\mathbf{0} \\implies y^T A = \\mathbf{0}^T\n\\]\nThis means \\(y\\) is orthogonal to every column of \\(A\\).\nDirect sum decomposition:\n\\[\n\\mathbb{R}^m = C(A) \\oplus N(A^T)\n\\]\nInterpretation: - Every vector in \\(\\mathbb{R}^m\\) can be uniquely decomposed into a column space component and a left null space component - These two subspaces are orthogonal complements - \\(\\dim(C(A)) + \\dim(N(A^T)) = m\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#orthogonality-and-the-least-squares-problem",
    "href": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#orthogonality-and-the-least-squares-problem",
    "title": "MIT 18.06SC Lecture 14: Orthogonal Vectors and Subspaces",
    "section": "Orthogonality and the Least Squares Problem",
    "text": "Orthogonality and the Least Squares Problem\n\nWhen There‚Äôs No Exact Solution\nProblem: For \\(Ax = b\\) where \\(m &gt; n\\), there‚Äôs typically no exact solution.\nSolution: Find the best approximate solution \\(\\hat{x}\\) that minimizes \\(\\|Ax - b\\|^2\\).\n\n\nNormal Equations\nApproximate solution:\n\\[\nA^T A\\hat{x} = A^T b\n\\]\nNote: The derivation of \\(\\hat{x}\\) and why this works will be introduced in Lecture 15.\n\n\nProperties of \\(A^T A\\)\nNull space relationship:\n\\[\nN(A^T A) = N(A)\n\\]\nProof: - If \\(Ax = \\mathbf{0}\\), then \\(A^T Ax = \\mathbf{0}\\) - Conversely, if \\(A^T Ax = \\mathbf{0}\\), then \\(x^T A^T Ax = \\|Ax\\|^2 = 0 \\implies Ax = \\mathbf{0}\\)\nRank relationship:\n\\[\n\\operatorname{rank}(A^T A) = \\operatorname{rank}(A)\n\\]\nInvertibility: \\(A^T A\\) is invertible when \\(\\operatorname{rank}(A) = n\\) (full column rank).\nWhen full column rank:\n\\[\n\\hat{x} = (A^T A)^{-1} A^T b\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#summary-of-orthogonal-complements",
    "href": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#summary-of-orthogonal-complements",
    "title": "MIT 18.06SC Lecture 14: Orthogonal Vectors and Subspaces",
    "section": "Summary of Orthogonal Complements",
    "text": "Summary of Orthogonal Complements\n\n\n\nSpace in \\(\\mathbb{R}^n\\)\nOrthogonal Complement\nTotal Dimension\n\n\n\n\nRow space \\(C(A^T)\\)\nNull space \\(N(A)\\)\n\\(r + (n-r) = n\\)\n\n\n\n\n\n\nSpace in \\(\\mathbb{R}^m\\)\nOrthogonal Complement\nTotal Dimension\n\n\n\n\nColumn space \\(C(A)\\)\nLeft null space \\(N(A^T)\\)\n\\(r + (m-r) = m\\)\n\n\n\nKey relationships:\n\n\\(C(A^T) \\perp N(A)\\) and they span \\(\\mathbb{R}^n\\)\n\\(C(A) \\perp N(A^T)\\) and they span \\(\\mathbb{R}^m\\)\nDimensions add up to the ambient space dimension\nOrthogonal subspaces intersect only at the origin\n\n\nSource: MIT 18.06SC Linear Algebra, Lecture 14"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-2-transpose.html",
    "href": "Math/MIT18.06/mit1806-lecture5-2-transpose.html",
    "title": "MIT 18.06SC Lecture 5.2: Transpose",
    "section": "",
    "text": "My lecture notes\nThe transpose operation switches rows to columns. This post covers the transpose portion of Lecture 5."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-2-transpose.html#context",
    "href": "Math/MIT18.06/mit1806-lecture5-2-transpose.html#context",
    "title": "MIT 18.06SC Lecture 5.2: Transpose",
    "section": "",
    "text": "My lecture notes\nThe transpose operation switches rows to columns. This post covers the transpose portion of Lecture 5."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-2-transpose.html#definition",
    "href": "Math/MIT18.06/mit1806-lecture5-2-transpose.html#definition",
    "title": "MIT 18.06SC Lecture 5.2: Transpose",
    "section": "Definition",
    "text": "Definition\nTranspose is the operation that switches rows to columns - each \\(A_{i,j}\\) becomes \\(A_{j,i}\\)\nFor a matrix \\(A\\), the transpose \\(A^T\\) satisfies: \\[\n(A^T)_{i,j} = A_{j,i}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-2-transpose.html#symmetric-matrices",
    "href": "Math/MIT18.06/mit1806-lecture5-2-transpose.html#symmetric-matrices",
    "title": "MIT 18.06SC Lecture 5.2: Transpose",
    "section": "Symmetric Matrices",
    "text": "Symmetric Matrices\nA symmetric matrix satisfies \\(A = A^T\\)\n\nProperty: \\(RR^T\\) is Symmetric\nFor any matrix \\(R\\), the product \\(RR^T\\) is always symmetric.\nProof: \\[\n(RR^T)^T = (R^T)^T R^T = RR^T\n\\]\nTherefore, \\(RR^T\\) is symmetric.\n\nSource: MIT 18.06SC Linear Algebra, Lecture 5"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#overview",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#overview",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Overview",
    "text": "Overview\nThis lecture explores new vector spaces beyond \\(\\mathbb{R}^n\\):\n\nMatrix spaces as vector spaces with their own subspaces\nRank-1 matrices and their fundamental structure\nDimension formulas for intersections and sums of subspaces\nApplications to differential equations and constraints"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#matrix-subspaces",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#matrix-subspaces",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Matrix Subspaces",
    "text": "Matrix Subspaces\n\n\n\n\n\n\nNoteBackground: Vector Spaces and Subspaces\n\n\n\nFor a review of vector space fundamentals (what makes something a subspace, closure under addition and scalar multiplication), see Lecture 5.3: Vector Spaces. Matrix subspaces follow the same rules‚Äîthey just happen to be spaces of matrices rather than vectors in \\(\\mathbb{R}^n\\).\n\n\n\nSpace of All 3√ó3 Matrices\nNotation: \\(M\\) = all 3√ó3 matrices\nDimension: \\(\\dim(M) = 9\\)\nEach entry is a free variable, so a 3√ó3 matrix space has dimension \\(3 \\times 3 = 9\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#important-matrix-subspaces",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#important-matrix-subspaces",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Important Matrix Subspaces",
    "text": "Important Matrix Subspaces\n\nSymmetric Matrices: \\(S\\)\nDefinition: A matrix \\(A\\) is symmetric if \\(A = A^T\\)\nProperties:\n\nSum of two symmetric matrices is symmetric (closed under addition)\nScalar multiple of a symmetric matrix is symmetric (closed under scalar multiplication)\nTherefore \\(S\\) is a subspace\n\nDimension: \\(\\dim(S) = 6\\)\nFree variables (independent entries):\n\n\\((1,1)\\) - first diagonal entry\n\\((2,2)\\) - second diagonal entry\n\\((3,3)\\) - third diagonal entry\n\\((1,2) = (2,1)\\) - upper/lower symmetric pair\n\\((1,3) = (3,1)\\) - upper/lower symmetric pair\n\\((2,3) = (3,2)\\) - upper/lower symmetric pair\n\nExample: \\[\n\\begin{bmatrix}\na & b & c \\\\\nb & d & e \\\\\nc & e & f\n\\end{bmatrix}\n\\] has 6 free parameters: \\(a, b, c, d, e, f\\).\n\n\nUpper Triangular Matrices: \\(U\\)\nDefinition: A matrix \\(A\\) is upper triangular if all entries below the diagonal are zero\nDimension: \\(\\dim(U) = 6\\)\nFree variables:\n\n3 diagonal entries: \\((1,1), (2,2), (3,3)\\)\n3 above-diagonal entries: \\((1,2), (1,3), (2,3)\\)\n\nExample: \\[\n\\begin{bmatrix}\na & b & c \\\\\n0 & d & e \\\\\n0 & 0 & f\n\\end{bmatrix}\n\\]\n\n\nDiagonal Matrices: \\(D = S \\cap U\\)\nDefinition: The intersection of symmetric and upper triangular matrices\nDimension: \\(\\dim(D) = 3\\)\nFree variables:\n\n\\((1,1)\\)\n\\((2,2)\\)\n\\((3,3)\\)\n\nExample: \\[\n\\begin{bmatrix}\na & 0 & 0 \\\\\n0 & b & 0 \\\\\n0 & 0 & c\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nTipKey Insight\n\n\n\nA matrix that is both symmetric and upper triangular must be diagonal."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#dimension-formula-for-subspaces",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#dimension-formula-for-subspaces",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Dimension Formula for Subspaces",
    "text": "Dimension Formula for Subspaces\n\nSum of Subspaces: \\(S + U\\)\nDefinition: \\(S + U\\) contains all matrices that can be written as the sum of a symmetric matrix and an upper triangular matrix.\nFor 3√ó3 matrices: \\[\nS + U = M\n\\]\nEvery 3√ó3 matrix can be decomposed into symmetric and upper triangular parts.\n\n\nDimension Formula\nGeneral formula: \\[\n\\dim(S) + \\dim(U) = \\dim(S \\cap U) + \\dim(S + U)\n\\]\nFor our example: \\[\n6 + 6 = 3 + 9\n\\]\n\n\n\n\n\n\nNoteInclusion-Exclusion Analogy\n\n\n\nThis is analogous to the inclusion-exclusion principle:\n\nThe sum of dimensions counts the intersection twice\nSo we must subtract it once to get the dimension of the union/sum"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#example-differential-equations-as-vector-spaces",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#example-differential-equations-as-vector-spaces",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Example: Differential Equations as Vector Spaces",
    "text": "Example: Differential Equations as Vector Spaces\n\nSecond-Order Homogeneous Differential Equation\nEquation: \\[\n\\frac{d^2y}{dx^2} + y = 0\n\\]\nor equivalently: \\[\ny'' + y = 0\n\\]\nSolutions: The solution space is spanned by: \\[\ny = c_1 \\cos(x) + c_2 \\sin(x)\n\\]\nDimension: 2 (two free parameters: \\(c_1\\) and \\(c_2\\))\n\n\n\n\n\n\nImportantKey Principle\n\n\n\nThe set of all solutions to a linear homogeneous differential equation forms a vector space. The dimension equals the order of the differential equation."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#rank-1-matrices",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#rank-1-matrices",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Rank-1 Matrices",
    "text": "Rank-1 Matrices\n\nStructure of Rank-1 Matrices\nDefinition: A matrix has rank 1 if it can be written as the outer product of a column vector and a row vector.\nGeneral form: \\[\nA = u v^T\n\\]\nwhere:\n\n\\(u\\) is a column vector (in \\(\\mathbb{R}^m\\))\n\\(v^T\\) is a row vector (in \\(\\mathbb{R}^n\\))\n\n\n\nExample\n\\[\nA = \\begin{bmatrix}\n1 & 4 & 5 \\\\\n2 & 8 & 10\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 4 & 5\n\\end{bmatrix}\n\\]\nVerification: \\[\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 4 & 5\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 \\cdot 1 & 1 \\cdot 4 & 1 \\cdot 5 \\\\\n2 \\cdot 1 & 2 \\cdot 4 & 2 \\cdot 5\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 4 & 5 \\\\\n2 & 8 & 10\n\\end{bmatrix}\n\\]\nKey observation: All rows are multiples of each other (row 2 = 2 √ó row 1).\n\n\nRank-1 Matrices Do Not Form a Subspace\nClaim: The set of all rank-1 matrices is not a subspace.\nReason: Rank-1 matrices are not closed under addition.\nCounterexample: \\[\nA_1 = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}, \\quad\nA_2 = \\begin{bmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]\nBoth have rank 1, but: \\[\nA_1 + A_2 = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]\nhas rank 2.\n\n\n\n\n\n\nWarningGeneral Principle\n\n\n\nIf \\(A\\) has rank \\(n\\) and \\(B\\) has rank \\(m\\), then \\(A + B\\) can have rank anywhere from \\(|n - m|\\) to \\(n + m\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#null-space-example",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#null-space-example",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Null Space Example",
    "text": "Null Space Example\n\nProblem Setup\nGiven: \\(S\\) is the set of all vectors \\(v\\) in \\(\\mathbb{R}^4\\) such that: \\[\nv_1 + v_2 + v_3 + v_4 = 0\n\\]\nQuestion: What is the dimension of \\(S\\)?\n\n\nSolution Using Null Space\nDefine the matrix: \\[\nA = \\begin{bmatrix}\n1 & 1 & 1 & 1\n\\end{bmatrix}\n\\]\nThen \\(S\\) is the null space of \\(A\\): \\[\nS = N(A) = \\left\\{\nv = \\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\nv_3 \\\\\nv_4\n\\end{bmatrix}\n: Av = 0\n\\right\\}\n\\]\nDimension calculation:\n\n\\(\\text{rank}(A) = 1\\) (one independent row)\n\\(n = 4\\) (number of columns)\nBy the rank-nullity theorem: \\[\n\\dim(N(A)) = n - r = 4 - 1 = 3\n\\]\n\nInterpretation: Three degrees of freedom. If we choose any values for \\(v_1, v_2, v_3\\), then \\(v_4\\) is determined by \\(v_4 = -(v_1 + v_2 + v_3)\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#summary",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Summary",
    "text": "Summary\nKey concepts:\n\nMatrix spaces are vector spaces with dimension equal to the number of independent entries\nSymmetric matrices (3√ó3): dimension 6\nUpper triangular matrices (3√ó3): dimension 6\nDiagonal matrices (3√ó3): dimension 3\nDimension formula: \\(\\dim(S) + \\dim(U) = \\dim(S \\cap U) + \\dim(S + U)\\)\nRank-1 matrices: \\(A = uv^T\\) (outer product structure)\nRank-1 matrices are not a subspace: sum of two rank-1 matrices can have rank 2\n\n\nSource: MIT 18.06SC Linear Algebra, Lecture 11"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nPermutation matrices reorder rows and columns using a simple structure of 0s and 1s. This post covers the permutation matrices portion of Lecture 5."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html#context",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html#context",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nPermutation matrices reorder rows and columns using a simple structure of 0s and 1s. This post covers the permutation matrices portion of Lecture 5."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html#what-is-a-permutation-matrix",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html#what-is-a-permutation-matrix",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "What is a Permutation Matrix?",
    "text": "What is a Permutation Matrix?\nA matrix is a permutation matrix if: - Each row has exactly one 1, all other entries are 0 - Each column has exactly one 1, all other entries are 0\n\nExample: Row Swapping\n\n\nShow code\nimport numpy as np\nfrom IPython.display import display, Markdown\n\ndef matrix_to_latex(mat, name=\"\"):\n    \"\"\"Convert numpy matrix to LaTeX bmatrix format\"\"\"\n    rows = []\n    for row in mat:\n        rows.append(\" & \".join(map(str, row)))\n    latex = r\"\\begin{bmatrix}\" + \" \\\\\\\\ \".join(rows) + r\"\\end{bmatrix}\"\n    if name:\n        latex = f\"{name} = {latex}\"\n    return f\"$${latex}$$\"\n\n# Create a random 3√ó4 matrix\nA = np.random.randint(0, 10, (3, 4))\ndisplay(Markdown(\"**Matrix A:**\"))\ndisplay(Markdown(matrix_to_latex(A)))\n\n# Create a permutation matrix that swaps rows 0 and 1\nP = np.zeros((3, 3), dtype=int)\nP[0, 1] = 1\nP[1, 0] = 1\nP[2, 2] = 1\ndisplay(Markdown(\"**Permutation matrix P:**\"))\ndisplay(Markdown(matrix_to_latex(P)))\n\n# Apply permutation: PA swaps rows 0 and 1 of A\ndisplay(Markdown(\"**P @ A (rows 0 and 1 swapped):**\"))\ndisplay(Markdown(matrix_to_latex(P @ A)))\n\n\nMatrix A:\n\n\n\\[\\begin{bmatrix}8 & 9 & 4 & 0 \\\\ 4 & 9 & 8 & 6 \\\\ 5 & 2 & 5 & 0\\end{bmatrix}\\]\n\n\nPermutation matrix P:\n\n\n\\[\\begin{bmatrix}0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\n\n\nP @ A (rows 0 and 1 swapped):\n\n\n\\[\\begin{bmatrix}4 & 9 & 8 & 6 \\\\ 8 & 9 & 4 & 0 \\\\ 5 & 2 & 5 & 0\\end{bmatrix}\\]\n\n\nKey insight: Left multiplication by \\(P\\) reorders the rows of \\(A\\) according to the pattern encoded in \\(P\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html#counting-permutation-matrices",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html#counting-permutation-matrices",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Counting Permutation Matrices",
    "text": "Counting Permutation Matrices\nAn \\(n \\times n\\) matrix has exactly \\(n!\\) permutation matrices.\nExamples: - \\(n=3\\): \\(3! = 6\\) permutation matrices - \\(n=4\\): \\(4! = 24\\) permutation matrices - \\(n=5\\): \\(5! = 120\\) permutation matrices\n\nGenerating All Permutations\n\n\nShow code\nfrom itertools import permutations\n\n# Generate all permutations for n=3\nn = 3\nperms = list(permutations(range(n)))\ndisplay(Markdown(f\"There are **{len(perms)}** permutation matrices for n={n}\"))\ndisplay(Markdown(\"\"))\n\n# Display all 6 permutation matrices\nfor i, perm in enumerate(perms):\n    display(Markdown(f\"**Permutation {i+1}:** {perm}\"))\n    P = np.zeros((n, n), dtype=int)\n    P[np.arange(n), perm] = 1\n    display(Markdown(matrix_to_latex(P)))\n    display(Markdown(\"\"))\n\n\nThere are 6 permutation matrices for n=3\n\n\n\n\n\nPermutation 1: (0, 1, 2)\n\n\n\\[\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 2: (0, 2, 1)\n\n\n\\[\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 3: (1, 0, 2)\n\n\n\\[\\begin{bmatrix}0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 4: (1, 2, 0)\n\n\n\\[\\begin{bmatrix}0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 5: (2, 0, 1)\n\n\n\\[\\begin{bmatrix}0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 6: (2, 1, 0)\n\n\n\\[\\begin{bmatrix}0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0\\end{bmatrix}\\]\n\n\n\n\n\n\n\nNotable Permutation Matrices for \\(n=3\\)\nIdentity (no permutation): \\[\nI = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\\]\nSingle swap (rows 0 and 1): \\[\nP = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\\]\nCyclic permutation: \\[\nP = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html#key-properties-of-permutation-matrices",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html#key-properties-of-permutation-matrices",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Key Properties of Permutation Matrices",
    "text": "Key Properties of Permutation Matrices\nPermutation matrices satisfy three properties:\n\n\\(P^{-1} = P^T\\) (inverse equals transpose)\n\\(PP^T = P^TP = I\\) (orthogonal matrix)\n\\(\\det(P) = \\pm 1\\) (determinant is \\(\\pm 1\\))"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html#proof-1-p-1-pt",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html#proof-1-p-1-pt",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Proof 1: \\(P^{-1} = P^T\\)",
    "text": "Proof 1: \\(P^{-1} = P^T\\)\n\nStrategy\nSince \\(PP^{-1} = I\\) by definition of inverse, we only need to prove that \\(PP^T = I\\). Then we can conclude \\(P^{-1} = P^T\\).\n\n\nProof of \\(PP^T = I\\)\n\n\nShow code\n# Example permutation matrix (swaps rows 0 and 1)\nP = np.array([[0, 1, 0],\n              [1, 0, 0],\n              [0, 0, 1]])\n\n# Compute transpose\nP_T = P.T\n\n# Verify PP^T = I\nresult = P @ P_T\ndisplay(Markdown(\"**P @ P^T =**\"))\ndisplay(Markdown(matrix_to_latex(result)))\n\n\nP @ P^T =\n\n\n\\[\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\n\n\nIntuition:\nThe \\(j\\)-th column of \\(P^T\\) is the \\(j\\)-th row of \\(P\\).\nWhen computing \\((PP^T)_{ij}\\) (the \\((i,j)\\) entry): - \\((PP^T)_{ij} = \\text{(row } i \\text{ of } P) \\cdot \\text{(column } j \\text{ of } P^T\\text{)}\\) - \\(= \\text{(row } i \\text{ of } P) \\cdot \\text{(row } j \\text{ of } P\\text{)}\\)\nTwo cases: - When \\(i = j\\): Dot product of a row with itself = 1 (each row has exactly one 1) - When \\(i \\neq j\\): Dot product of different rows = 0 (the 1s are in different positions)\nTherefore, \\(PP^T = I\\).\n\n\nVerification by Row\n\n\nShow code\n# Verify by computing each row of P times P^T\nfor i in range(3):\n    result_row = P[i] @ P_T\n    display(Markdown(f\"**Row {i} of P times P^T:** $[{' \\\\ '.join(map(str, result_row))}]$\"))\ndisplay(Markdown(\"\"))\ndisplay(Markdown(\"Each row gives one row of the identity matrix!\"))\n\n\nRow 0 of P times P^T: \\([1 \\ 0 \\ 0]\\)\n\n\nRow 1 of P times P^T: \\([0 \\ 1 \\ 0]\\)\n\n\nRow 2 of P times P^T: \\([0 \\ 0 \\ 1]\\)\n\n\n\n\n\nEach row gives one row of the identity matrix!\n\n\n\n\nConclusion: \\(P^{-1} = P^T\\)\nProof:\n\\[\n\\begin{align}\nPP^T &= I \\quad \\text{(proved above)} \\\\\nPP^{-1} &= I \\quad \\text{(definition of inverse)} \\\\\n\\therefore P^{-1} &= P^T\n\\end{align}\n\\]\nPractical implications: - Computing \\(P^{-1}\\) is as simple as transposing \\(P\\) - Transposition is \\(O(n^2)\\), much faster than general matrix inversion \\(O(n^3)\\) - If \\(PA\\) permutes rows, then \\(P^T(PA) = A\\) undoes the permutation"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html#proof-2-detp-pm-1",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html#proof-2-detp-pm-1",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Proof 2: \\(\\det(P) = \\pm 1\\)",
    "text": "Proof 2: \\(\\det(P) = \\pm 1\\)\n\nProperties of Determinants\nProperties used: 1. \\(\\det(AB) = \\det(A)\\det(B)\\) (product property) 2. \\(\\det(A^T) = \\det(A)\\) (transpose property)\n\n\nProof\n\\[\n\\begin{align}\n\\det(PP^T) &= \\det(I) = 1 \\\\\n\\det(PP^T) &= \\det(P)\\det(P^T) \\quad \\text{(product property)} \\\\\n&= \\det(P)\\det(P) \\quad \\text{(transpose property)} \\\\\n&= [\\det(P)]^2 \\\\\n\\therefore [\\det(P)]^2 &= 1 \\\\\n\\det(P) &= \\pm 1\n\\end{align}\n\\]\n\n\nInterpretation: Even vs.¬†Odd Permutations\n\n\\(\\det(P) = +1\\): Even permutation (even number of row swaps)\n\\(\\det(P) = -1\\): Odd permutation (odd number of row swaps)\n\nExamples:\n\n\nShow code\n# Identity matrix (0 swaps)\nI = np.eye(3, dtype=int)\ndisplay(Markdown(f\"$\\\\det(I) = {np.linalg.det(I):.0f}$ (even: 0 swaps)\"))\n\n# Single swap (1 swap)\nP_swap = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\ndisplay(Markdown(f\"$\\\\det(P_{{swap}}) = {np.linalg.det(P_swap):.0f}$ (odd: 1 swap)\"))\n\n# Cyclic permutation (2 swaps)\nP_cycle = np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\ndisplay(Markdown(f\"$\\\\det(P_{{cycle}}) = {np.linalg.det(P_cycle):.0f}$ (even: 2 swaps)\"))\n\n\n\\(\\det(I) = 1\\) (even: 0 swaps)\n\n\n\\(\\det(P_{swap}) = -1\\) (odd: 1 swap)\n\n\n\\(\\det(P_{cycle}) = 1\\) (even: 2 swaps)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html#summary",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Summary",
    "text": "Summary\nPermutation matrices:\n\nStructure: One 1 per row and column, rest are 0s\nCount: \\(n!\\) permutation matrices for \\(n \\times n\\) matrices\nInverse: \\(P^{-1} = P^T\\) (orthogonal property)\nDeterminant: \\(\\det(P) = \\pm 1\\) (sign indicates even/odd permutation)\nApplications: Critical for LU decomposition, numerical stability, and efficient computation\n\n\nSource: MIT 18.06SC Linear Algebra, Lecture 5"
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html",
    "href": "Math/reflections/mit1806-invertibility-connections.html",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "",
    "text": "In linear algebra, concepts like invertibility, null space, linear independence, rank, and pivots often seem like separate topics. However, they are deeply interconnected - different views of the same mathematical reality. This post synthesizes these fundamental concepts and reveals how they all tell the same story about whether a linear transformation preserves information."
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html#introduction",
    "href": "Math/reflections/mit1806-invertibility-connections.html#introduction",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "",
    "text": "In linear algebra, concepts like invertibility, null space, linear independence, rank, and pivots often seem like separate topics. However, they are deeply interconnected - different views of the same mathematical reality. This post synthesizes these fundamental concepts and reveals how they all tell the same story about whether a linear transformation preserves information."
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html#invertibility-the-foundation",
    "href": "Math/reflections/mit1806-invertibility-connections.html#invertibility-the-foundation",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "Invertibility: The Foundation",
    "text": "Invertibility: The Foundation\nReference: Lecture 3: Matrix Multiplication and Inverse\n\nDefinition\nA square matrix \\(A\\) is invertible if there exists a matrix \\(A^{-1}\\) such that:\n\\[\nAA^{-1} = A^{-1}A = I_n\n\\]\n\n\nProperties of Invertible Matrices\nAn invertible matrix \\(A\\) must satisfy:\n\nSquare: \\(m = n\\) (same number of rows and columns)\nFull rank: \\(\\text{rank}(A) = n\\)\nAll pivot variables: \\(r = n\\) pivot positions\nNo free variables: \\(n - r = 0\\) free variables\nIndependent rows: All rows are linearly independent\n\n\n\nSolving Systems with Invertible Matrices\nWhen \\(A\\) is invertible, solving \\(Ax = b\\) becomes straightforward:\n\\[\n\\begin{aligned}\nAx &= b \\\\\nA^{-1}Ax &= A^{-1}b \\\\\nx &= A^{-1}b\n\\end{aligned}\n\\]\nThe solution is unique and exists for every \\(b\\)."
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html#null-space-a-direct-test-for-invertibility",
    "href": "Math/reflections/mit1806-invertibility-connections.html#null-space-a-direct-test-for-invertibility",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "Null Space: A Direct Test for Invertibility",
    "text": "Null Space: A Direct Test for Invertibility\nReference: Lecture 7: Solving Ax=0\n\nThe Connection\nThe null space \\(N(A)\\) provides a direct test for invertibility:\n\nIf \\(N(A) = \\{\\mathbf{0}\\}\\): Matrix is invertible\nIf \\(N(A) \\neq \\{\\mathbf{0}\\}\\): Matrix is NOT invertible\n\n\n\nWhy Non-Trivial Null Space Prevents Invertibility\nProof by contradiction:\nSuppose \\(N(A) \\neq \\{\\mathbf{0}\\}\\) and \\(A^{-1}\\) exists. Let \\(v_1 \\in N(A)\\) with \\(v_1 \\neq \\mathbf{0}\\).\nThen:\n\\[\n\\begin{aligned}\nAv_1 &= \\mathbf{0} \\quad \\text{(by definition of null space)} \\\\\nA^{-1}(Av_1) &= A^{-1}\\mathbf{0} \\quad \\text{(multiply both sides by } A^{-1}\\text{)} \\\\\n(A^{-1}A)v_1 &= \\mathbf{0} \\\\\nI_n v_1 &= \\mathbf{0} \\\\\nv_1 &= \\mathbf{0}\n\\end{aligned}\n\\]\nContradiction! We assumed \\(v_1 \\neq \\mathbf{0}\\), but our logic forces \\(v_1 = \\mathbf{0}\\).\nTherefore, \\(A^{-1}\\) cannot exist when \\(N(A)\\) contains non-zero vectors.\n\n\n\n\n\n\nTipKey Insight\n\n\n\nA non-trivial null space means information is lost in the transformation \\(A\\), making it impossible to uniquely reverse."
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html#linear-independence-the-geometric-view",
    "href": "Math/reflections/mit1806-invertibility-connections.html#linear-independence-the-geometric-view",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "Linear Independence: The Geometric View",
    "text": "Linear Independence: The Geometric View\nReference: Lecture 8: Solving Ax=b\n\nConnection to Invertibility\nIf the rows (or columns) of a matrix are not all linearly independent, the matrix cannot be invertible.\nFor a square matrix:\n\nIndependent rows/columns ‚ü∫ \\(\\text{rank}(A) = n\\)\nDependent rows/columns ‚ü∫ \\(\\text{rank}(A) &lt; n\\)\n\n\n\nWhy Dependence Prevents Invertibility\nWhen rows are dependent:\n\nRank: \\(r &lt; n\\) (not full rank)\nPivots: Only \\(r &lt; n\\) pivot positions\nFree variables: \\(n - r &gt; 0\\) free variables exist\n\n\nTwo Perspectives on Dependence\n1. Algebraic Perspective\nWhen free variables exist:\n\\[\nR_{\\text{rref}} = [I_r \\mid F]\n\\]\nwhere \\(F\\) is the \\((n-r)\\)-dimensional free variable matrix.\nThe null space \\(N(A)\\) has dimension \\(n - r &gt; 0\\), containing infinitely many vectors. By our earlier proof, this means \\(A\\) is not invertible.\n2. Geometric Perspective\nIf rows are dependent, the transformation \\(A\\) collapses the \\(n\\)-dimensional space down to an \\(r\\)-dimensional subspace (where \\(r &lt; n\\)).\n\nInformation loss: The transformation maps multiple distinct inputs to the same output\nCannot invert: We cannot uniquely recover the original \\(n\\)-dimensional vector from its \\(r\\)-dimensional image\nMissing dimensions: The \\((n-r)\\) dimensions of information are permanently lost\n\n\n\n\n\n\n\nNoteExample: Dimensional Collapse\n\n\n\nA \\(3 \\times 3\\) matrix with rank 2 maps all of \\(\\mathbb{R}^3\\) onto a 2-dimensional plane. Infinitely many points in 3D space map to each point on the plane. There‚Äôs no way to uniquely invert this mapping."
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html#the-big-picture-everything-is-connected",
    "href": "Math/reflections/mit1806-invertibility-connections.html#the-big-picture-everything-is-connected",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "The Big Picture: Everything is Connected",
    "text": "The Big Picture: Everything is Connected\nAll these concepts are different views of the same mathematical reality:\n\n\n\n\n\n\n\n\nPerspective\nInvertible (\\(A^{-1}\\) exists)\nNot Invertible (\\(A^{-1}\\) doesn‚Äôt exist)\n\n\n\n\nNull Space\n\\(N(A) = \\{\\mathbf{0}\\}\\)\n\\(N(A) \\neq \\{\\mathbf{0}\\}\\)\n\n\nRank\n\\(\\text{rank}(A) = n\\) (full rank)\n\\(\\text{rank}(A) &lt; n\\) (rank deficient)\n\n\nPivots\n\\(n\\) pivots (all columns)\n\\(r &lt; n\\) pivots\n\n\nFree Variables\n0 free variables\n\\(n - r &gt; 0\\) free variables\n\n\nIndependence\nRows/columns independent\nRows/columns dependent\n\n\nDimension\n\\(\\dim(N(A)) = 0\\)\n\\(\\dim(N(A)) = n - r &gt; 0\\)\n\n\nSolutions to \\(Ax = 0\\)\nOnly \\(x = \\mathbf{0}\\)\nInfinitely many solutions\n\n\nDeterminant\n\\(\\det(A) \\neq 0\\)\n\\(\\det(A) = 0\\)"
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html#fundamental-theorem-of-invertible-matrices",
    "href": "Math/reflections/mit1806-invertibility-connections.html#fundamental-theorem-of-invertible-matrices",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "Fundamental Theorem of Invertible Matrices",
    "text": "Fundamental Theorem of Invertible Matrices\nFor a square \\(n \\times n\\) matrix \\(A\\), the following are equivalent (all true or all false):\n\n\\(A\\) is invertible\n\\(A^{-1}\\) exists\n\\(\\text{rank}(A) = n\\)\n\\(N(A) = \\{\\mathbf{0}\\}\\)\nColumns of \\(A\\) are linearly independent\nRows of \\(A\\) are linearly independent\n\\(\\det(A) \\neq 0\\)\n\\(Ax = 0\\) has only the trivial solution\n\\(Ax = b\\) has a unique solution for every \\(b\\)\n\\(A\\) has \\(n\\) pivot positions\n\n\n\n\n\n\n\nImportantThe Core Insight\n\n\n\nAll these conditions are testing whether the linear transformation preserves information. If any information is lost (through dimensional collapse, non-trivial null space, or linear dependence), the transformation cannot be inverted."
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html#related-posts",
    "href": "Math/reflections/mit1806-invertibility-connections.html#related-posts",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "Related Posts",
    "text": "Related Posts\n\nLecture 3: Matrix Multiplication and Inverse\nLecture 7: Solving Ax=0 - Finding the Null Space\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nLecture 9: Independence, Basis, and Dimension"
  },
  {
    "objectID": "Algorithm/dp_regex.html",
    "href": "Algorithm/dp_regex.html",
    "title": "DP: Regular Expression Matching",
    "section": "",
    "text": "Dynamic programming is a technique for solving problems by breaking them down into smaller sub-problems and solving each subproblem only once."
  },
  {
    "objectID": "Algorithm/dp_regex.html#example-of-regular-expression-matching",
    "href": "Algorithm/dp_regex.html#example-of-regular-expression-matching",
    "title": "DP: Regular Expression Matching",
    "section": "Example of Regular Expression Matching",
    "text": "Example of Regular Expression Matching\nA problem from Leetcode 10:\nYou are given a string s and a pattern p, implement regular expression matching with support for ‚Äò.‚Äô and ‚Äô*‚Äô where:\n‚Äò.‚Äô Matches any single character. ‚Äô*‚Äô Matches zero or more of the preceding element. The matching should cover the entire input string (not partial).\ns = \"abcabc\"    \np1 = \".*c\"    \np2 = \".*d\""
  },
  {
    "objectID": "Algorithm/dp_regex.html#dp-table",
    "href": "Algorithm/dp_regex.html#dp-table",
    "title": "DP: Regular Expression Matching",
    "section": "1. DP Table",
    "text": "1. DP Table\nLook at the following case.\n\nCase 1\np1 is valid if we have a table like this:\nwe can see that the last cell is T, so p1 is valid.\n\n\n\n\n\n.\n*\nc\n\n\n\n\n\nT\nF\nT\nF\n\n\na\nF\nT\nT\nF\n\n\nb\nF\nF\nT\nF\n\n\nc\nF\nF\nT\nT\n\n\na\nF\nF\nT\nF\n\n\nb\nF\nF\nT\nF\n\n\nc\nF\nF\nT\nT\n\n\n\nThe table is the match result of s[0:i] and p[0:j],\nso the last cell is the match result of s[0:6](the entire string) and p[0:3](the entire pattern). If the result is T, then the entire string matches the entire pattern.\n\n\nHow does each cell is calculated?\n\nthe last cell, p[:3] matches s[:6], also p[:2] matches s[:5]\n\nit is now a dp problem, the cell‚Äôs value is the match result of p[:i] and s[:j] and the match result of p[:i-1] and s[:j-1],meaning both should be T.\n\n\n\nCase 2\nNow look at an invalid case:\np2 is invalid because .* can match abcab but d cannot match c\n\n\n\n\n\n.\n*\nd\n\n\n\n\n\nT\nF\nT\nF\n\n\na\nF\nT\nT\nF\n\n\nb\nF\nF\nT\nF\n\n\nc\nF\nF\nT\nF\n\n\na\nF\nF\nT\nF\n\n\nb\nF\nF\nT\nF\n\n\nc\nF\nF\nT\nF\n\n\n\nLook at the last cell, p[:3] matches s[:6], but p[2] does not match s[5], so the last cell is F."
  },
  {
    "objectID": "Algorithm/dp_regex.html#formula-derivation",
    "href": "Algorithm/dp_regex.html#formula-derivation",
    "title": "DP: Regular Expression Matching",
    "section": "2. Formula Derivation",
    "text": "2. Formula Derivation\n\nTwo rules\n\nwe can compare single character of the string s[i] with 1 or 2 characters of the pattern p[j],p[j-2]....,\nwe can query the previous results from the DP table dp[i-1][j-1], dp[i][j-2], dp[i-1][j].\n\n\n\nThe flow\nThe diagramm below shows how can we calculate the match result of s[0...i] and p[0...j].\n\n\n\nalt\n\n\nNow the formula seems to be: \\[\n\\text{dp}[i][j] =\n\\begin{cases}\n\\text{true} & \\text{if } p[i] \\neq '*'  \\land s[i] \\text{ matches } p[j] \\land \\text{dp}[i-1][j-1] = \\text{true} \\\\\n\\text{true} & \\text{if } p[i] = '*'  \\land dp[i][j-2] = \\text{true} \\\\\n\\text{true} & \\text{if } p[i] = '*'  \\land s[i] \\text{ matches } p[j-1] \\land \\text{dp}[i-1][j-2] = \\text{true} \\\\\n\\text{true} & \\text{if } p[i] = '*'  \\land s[i] \\text{ matches } p[j-1] \\land \\text{dp}[i-1][j] = \\text{true} \\\\\n\\text{false} & \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "Algorithm/dp_regex.html#code-example",
    "href": "Algorithm/dp_regex.html#code-example",
    "title": "DP: Regular Expression Matching",
    "section": "3. Code Example",
    "text": "3. Code Example\nPlease not that in the code, when we retrieve character from the string or pattern, we need to use s[i-1] and p[j-1] instead of s[i] and p[j] as the index of the string and pattern is 0-based.\nfrom collections import defaultdict\nclass Solution:\n    def isMatch(self,s, p):\n        m, n = len(s), len(p)\n        dp = [[False] * (n + 1) for _ in range(m + 1)]\n     # DP is a table with m+1 rows and n+1 columns\n     # we retrieve dp[i][k], i is the index of s, k is the index of p\n        dp[0][0] = True\n        for j in range(2,n+1):\n            if p[j-1]=='*':\n                dp[0][j]=dp[0][j-2]\n            \n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if p[j-1] == '*':\n                    dp[i][j] = dp[i][j-2] # zero occurrence\n                    if s[i-1]==p[j-2] or p[j-2]=='.':\n                        dp[i][j]|=dp[i-1][j] or dp[i-1][j-2] # one or more occurrence\n                else:\n                    dp[i][j] = dp[i-1][j-1] and (s[i-1] == p[j-1] or p[j-1] == '.')\n        return dp[m][n]"
  },
  {
    "objectID": "index-backup.html",
    "href": "index-backup.html",
    "title": "ickma.dev",
    "section": "",
    "text": "My learning notes and thoughts on math and machine learning.\nCurrently reading the Deep Learning book.\n\n\n\n\nHow ReLU solves problems that linear models cannot handle.\n\n\n\nThe mathematical connection between probabilistic models and loss functions.\n\n\n\nExploring activation functions and their impact on neural network learning.\n\n\n\nHow depth enables hierarchical feature reuse and exponential expressiveness with fewer parameters.\n\n\n\nThe algorithm that makes training deep networks computationally feasible through efficient gradient computation.\n\n\n\nEssential second-order calculus concepts needed before Chapter 7 on optimization algorithms.\n\n\n\nHow L2 regularization shrinks weights based on Hessian eigenvalues, preserving important directions while penalizing less sensitive ones.\n\n\n\nL1 regularization uses soft thresholding to push small weights to exactly zero, creating sparse solutions that perform feature selection.\n\n\n\nRegularization as constrained optimization: penalty form vs Lagrangian with KKT conditions and min-max dual training.\n\n\n\nWhy regularization is mathematically necessary when solving under-constrained linear systems, and how it ensures invertibility.\n\n\n\nHow transforming existing data can improve generalization and combat overfitting when training data is limited.\n\n\n\nMathematical derivation showing how adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\n\n\n\n\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation in linear transformations.\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations\nTwo powerful perspectives that reveal the hidden beauty of linear systems: row picture vs column picture.\nLecture 2: Elimination with Matrices\nThe systematic algorithm that transforms linear systems into upper triangular form for easy solution.\nLecture 3: Matrix Multiplication and Inverse\nFive different perspectives on matrix multiplication, from element-wise computation to rank-1 decomposition, plus understanding when matrices can‚Äôt be inverted.\nLecture 4: LU Decomposition\nFactoring matrices into Lower √ó Upper triangular form: the foundation of efficient numerical linear algebra and solving multiple systems with the same matrix.\nLecture 5.1: Permutation Matrices\nPermutation matrices reorder rows and columns using a simple structure of 0s and 1s.\nLecture 5.2: Transpose\nThe transpose operation switches rows to columns, creating symmetric matrices.\nLecture 5.3: Vector Spaces\nVector spaces and subspaces: closed under addition and scalar multiplication.\nLecture 6: Column Space and Null Space\nColumn space determines which \\(b\\) make \\(Ax = b\\) solvable. Null space contains all solutions to \\(Ax = 0\\).\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nSystematic algorithm to find null space using pivot/free variables and RREF. Dimension of null space is n-r.\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nComplete solution is particular solution plus null space. Four cases based on rank: exactly determined (unique), overdetermined (0 or 1), underdetermined (infinite), and rank deficient (0 or infinite).\nLecture 9: Independence, Basis, and Dimension Linear independence prevents redundancy, basis is minimal spanning set, dimension measures space size. Rank-nullity theorem: dim(C(A)) + dim(N(A)) = n.\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix: column space, null space, row space, and left null space.\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas for subspace intersections and sums, and differential equations as vector spaces.\n\n\n\n\n\n\n\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\n\n\nDP Regex"
  },
  {
    "objectID": "index-backup.html#deep-learning-book",
    "href": "index-backup.html#deep-learning-book",
    "title": "ickma.dev",
    "section": "",
    "text": "How ReLU solves problems that linear models cannot handle.\n\n\n\nThe mathematical connection between probabilistic models and loss functions.\n\n\n\nExploring activation functions and their impact on neural network learning.\n\n\n\nHow depth enables hierarchical feature reuse and exponential expressiveness with fewer parameters.\n\n\n\nThe algorithm that makes training deep networks computationally feasible through efficient gradient computation.\n\n\n\nEssential second-order calculus concepts needed before Chapter 7 on optimization algorithms.\n\n\n\nHow L2 regularization shrinks weights based on Hessian eigenvalues, preserving important directions while penalizing less sensitive ones.\n\n\n\nL1 regularization uses soft thresholding to push small weights to exactly zero, creating sparse solutions that perform feature selection.\n\n\n\nRegularization as constrained optimization: penalty form vs Lagrangian with KKT conditions and min-max dual training.\n\n\n\nWhy regularization is mathematically necessary when solving under-constrained linear systems, and how it ensures invertibility.\n\n\n\nHow transforming existing data can improve generalization and combat overfitting when training data is limited.\n\n\n\nMathematical derivation showing how adding Gaussian noise to weights is equivalent to penalizing large gradients."
  },
  {
    "objectID": "index-backup.html#mathematics",
    "href": "index-backup.html#mathematics",
    "title": "ickma.dev",
    "section": "",
    "text": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation in linear transformations.\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations\nTwo powerful perspectives that reveal the hidden beauty of linear systems: row picture vs column picture.\nLecture 2: Elimination with Matrices\nThe systematic algorithm that transforms linear systems into upper triangular form for easy solution.\nLecture 3: Matrix Multiplication and Inverse\nFive different perspectives on matrix multiplication, from element-wise computation to rank-1 decomposition, plus understanding when matrices can‚Äôt be inverted.\nLecture 4: LU Decomposition\nFactoring matrices into Lower √ó Upper triangular form: the foundation of efficient numerical linear algebra and solving multiple systems with the same matrix.\nLecture 5.1: Permutation Matrices\nPermutation matrices reorder rows and columns using a simple structure of 0s and 1s.\nLecture 5.2: Transpose\nThe transpose operation switches rows to columns, creating symmetric matrices.\nLecture 5.3: Vector Spaces\nVector spaces and subspaces: closed under addition and scalar multiplication.\nLecture 6: Column Space and Null Space\nColumn space determines which \\(b\\) make \\(Ax = b\\) solvable. Null space contains all solutions to \\(Ax = 0\\).\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nSystematic algorithm to find null space using pivot/free variables and RREF. Dimension of null space is n-r.\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nComplete solution is particular solution plus null space. Four cases based on rank: exactly determined (unique), overdetermined (0 or 1), underdetermined (infinite), and rank deficient (0 or infinite).\nLecture 9: Independence, Basis, and Dimension Linear independence prevents redundancy, basis is minimal spanning set, dimension measures space size. Rank-nullity theorem: dim(C(A)) + dim(N(A)) = n.\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix: column space, null space, row space, and left null space.\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas for subspace intersections and sums, and differential equations as vector spaces."
  },
  {
    "objectID": "index-backup.html#more",
    "href": "index-backup.html#more",
    "title": "ickma.dev",
    "section": "",
    "text": "K-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\n\n\nDP Regex"
  },
  {
    "objectID": "Algorithm/index.html",
    "href": "Algorithm/index.html",
    "title": "Algorithm Topics",
    "section": "",
    "text": "DP: Regular Expression Matching"
  },
  {
    "objectID": "Algorithm/index.html#dynamic-programming",
    "href": "Algorithm/index.html#dynamic-programming",
    "title": "Algorithm Topics",
    "section": "",
    "text": "DP: Regular Expression Matching"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ickma.dev",
    "section": "",
    "text": "A growing collection of structured study notes and visual explanations ‚Äî written for clarity, reproducibility, and long-term memory."
  },
  {
    "objectID": "index.html#deep-learning-book",
    "href": "index.html#deep-learning-book",
    "title": "ickma.dev",
    "section": "Deep Learning Book",
    "text": "Deep Learning Book\n\n\nChapter 6.1: XOR Problem & ReLU Networks How ReLU solves problems that linear models cannot handle.\n\n\nChapter 6.2: Likelihood-Based Loss Functions The mathematical connection between probabilistic models and loss functions.\n\n\nChapter 6.3: Hidden Units and Activation Functions Exploring activation functions and their impact on neural network learning.\n\n\nChapter 6.4: Architecture Design - Depth vs Width How depth enables hierarchical feature reuse and exponential expressiveness.\n\n\nChapter 6.5: Back-Propagation and Other Differentiation Algorithms The algorithm that makes training deep networks computationally feasible.\n\n\nChapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature Essential second-order calculus concepts needed before Chapter 7.\n\n\nChapter 7.1.1: L2 Regularization How L2 regularization shrinks weights based on Hessian eigenvalues.\n\n\nChapter 7.1.2: L1 Regularization L1 regularization uses soft thresholding to create sparse solutions.\n\n\nChapter 7.2: Constrained Optimization View of Regularization Regularization as constrained optimization with KKT conditions.\n\n\nChapter 7.3: Regularization and Under-Constrained Problems Why regularization is mathematically necessary and ensures invertibility.\n\n\nChapter 7.4: Dataset Augmentation How transforming existing data improves generalization.\n\n\nChapter 7.5: Noise Robustness How adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\nChapter 7.6: Semi-Supervised Learning Leveraging unlabeled data to improve model performance when labeled data is scarce.\n\n\nChapter 7.7: Multi-Task Learning Training a single model on multiple related tasks to improve generalization.\n\n\nChapter 7.8: Early Stopping Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.\n\n\nChapter 7.9: Parameter Tying and Parameter Sharing Two strategies for reducing parameters: encouraging similarity vs.¬†enforcing identity.\n\n\nChapter 7.10: Sparse Representations Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.\n\n\nChapter 7.11: Bagging and Other Ensemble Methods How training multiple models on bootstrap samples and averaging their predictions reduces variance."
  },
  {
    "objectID": "index.html#mathematics",
    "href": "index.html#mathematics",
    "title": "ickma.dev",
    "section": "Mathematics",
    "text": "Mathematics\n\nMy journey through MIT‚Äôs Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.\n\n\nReflections & Synthesis\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation.\n\n\n\nMIT 18.06SC Linear Algebra\n\n\nLecture 1: The Geometry of Linear Equations Two powerful perspectives: row picture vs column picture.\n\n\nLecture 2: Elimination with Matrices The systematic algorithm that transforms linear systems into upper triangular form.\n\n\nLecture 3: Matrix Multiplication and Inverse Five different perspectives on matrix multiplication.\n\n\nLecture 4: LU Decomposition Factoring matrices into Lower √ó Upper triangular form.\n\n\nLecture 5.1: Permutation Matrices Permutation matrices reorder rows and columns.\n\n\nLecture 5.2: Transpose The transpose operation switches rows to columns.\n\n\nLecture 5.3: Vector Spaces Vector spaces and subspaces: closed under addition and scalar multiplication.\n\n\nLecture 6: Column Space and Null Space Column space determines which \\(b\\) make \\(Ax = b\\) solvable.\n\n\nLecture 7: Solving Ax=0 Systematic algorithm to find null space using pivot/free variables and RREF.\n\n\nLecture 8: Solving Ax=b Complete solution is particular solution plus null space.\n\n\nLecture 9: Independence, Basis, and Dimension Linear independence, basis, and dimension. Rank-nullity theorem.\n\n\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix.\n\n\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.\n\n\nLecture 12: Graphs, Networks, and Incidence Matrices Applying linear algebra to graph theory: incidence matrices, Kirchhoff‚Äôs laws, and Euler‚Äôs formula.\n\n\nLecture 13: Quiz 1 Review Practice problems reviewing key concepts: subspaces, rank, null spaces, and the four fundamental subspaces.\n\n\nLecture 14: Orthogonal Vectors and Subspaces Orthogonal vectors, orthogonal subspaces, and the fundamental relationships between the four subspaces.\n\n\nLecture 15: Projection onto Subspaces Projecting onto lines and subspaces, the geometry of least squares, and deriving normal equations."
  },
  {
    "objectID": "index.html#more-topics",
    "href": "index.html#more-topics",
    "title": "ickma.dev",
    "section": "More Topics",
    "text": "More Topics\n\n\nMachine Learning\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\nAlgorithms\n\nDP Regex"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strang‚Äôs fourth lecture introduces one of the most important matrix factorizations: LU decomposition, which factors any invertible matrix \\(A\\) into the product of a Lower triangular matrix and an Upper triangular matrix. This factorization is the foundation of efficient numerical linear algebra."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#context",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#context",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strang‚Äôs fourth lecture introduces one of the most important matrix factorizations: LU decomposition, which factors any invertible matrix \\(A\\) into the product of a Lower triangular matrix and an Upper triangular matrix. This factorization is the foundation of efficient numerical linear algebra."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#what-is-lu-decomposition",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#what-is-lu-decomposition",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "What is LU Decomposition?",
    "text": "What is LU Decomposition?\nGoal: Factor any invertible matrix \\(A\\) as the product of: - \\(L\\) = Lower triangular matrix (with 1‚Äôs on diagonal) - \\(U\\) = Upper triangular matrix (the result of elimination)\n\\[\nA = LU\n\\]\n\nWhy is this useful?\n\nEfficient solving: \\(Ax = b\\) becomes two simpler triangular solves:\nStep 1 - Forward substitution: Solve \\(Lc = b\\) for \\(c\\)\nStep 2 - Back substitution: Solve \\(Ux = c\\) for \\(x\\)\nHow this works:\nSince \\(A = LU\\), we have \\(Ax = LUx = b\\). Let \\(Ux = c\\), then: \\[\nLUx = Lc = b\n\\]\nForward substitution (solving \\(Lc = b\\)):\nSince \\(L\\) is lower triangular with 1‚Äôs on the diagonal, we can solve for \\(c\\) step by step: \\[\n\\begin{aligned}\nc_1 &= b_1 \\\\\nc_2 &= b_2 - m_{21}c_1 \\\\\nc_3 &= b_3 - m_{31}c_1 - m_{32}c_2 \\\\\n&\\vdots\n\\end{aligned}\n\\]\nEach \\(c_i\\) depends only on previously computed values, so we solve forward from \\(c_1\\) to \\(c_n\\).\nBack substitution (solving \\(Ux = c\\)):\nSince \\(U\\) is upper triangular, we solve backward from \\(x_n\\) to \\(x_1\\): \\[\n\\begin{aligned}\nx_n &= \\frac{c_n}{u_{nn}} \\\\\nx_{n-1} &= \\frac{c_{n-1} - u_{n-1,n}x_n}{u_{n-1,n-1}} \\\\\n&\\vdots\n\\end{aligned}\n\\]\nResult: We‚Äôve solved \\(Ax = b\\) without ever explicitly computing \\(A^{-1}\\)!\nReusable factorization: When \\(A\\) is fixed but \\(b\\) changes, we can reuse \\(L\\) and \\(U\\)\n\nFactorization: \\(O(n^3)\\) operations (done once)\nEach solve: \\(O(n^2)\\) operations\nHuge savings for multiple right-hand sides!\n\nFoundation of numerical computing: Used in MATLAB, NumPy, and all scientific computing libraries"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#how-elimination-creates-u",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#how-elimination-creates-u",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "How Elimination Creates U",
    "text": "How Elimination Creates U\n\nThe Elimination Process\nStarting with \\(A\\), we apply elimination matrices \\(E_{21}, E_{31}, E_{32}, \\ldots\\) to get upper triangular \\(U\\):\n\\[\nE_{32} E_{31} E_{21} A = U\n\\]\nExample (3√ó3 case):\n\\[\nA = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\n\\]\nStep 1: Eliminate below first pivot (rows 2 and 3)\n\\[\nE_{21} = \\begin{bmatrix} 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad\nE_{31} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix}\n\\]\nStep 2: Eliminate below second pivot (row 3)\n\\[\nE_{32} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -1 & 1 \\end{bmatrix}\n\\]\n\n\nStructure of Elimination Matrices\nAn elimination matrix \\(E_{ij}\\) eliminates the entry at position \\((i,j)\\) by subtracting a multiple of row \\(j\\) from row \\(i\\).\nGeneral form: \\[\nE_{ij} = I - m_{ij} \\mathbf{e}_i \\mathbf{e}_j^T\n\\]\nwhere: - \\(m_{ij}\\) = multiplier = \\(\\frac{A_{ij}}{\\text{pivot at } (j,j)}\\) - \\(\\mathbf{e}_i\\) = \\(i\\)-th standard basis vector - The \\((i,j)\\) entry of \\(E_{ij}\\) is \\(-m_{ij}\\)\nKey properties: 1. Lower triangular (operates below diagonal) 2. Determinant = 1 (doesn‚Äôt change volume) 3. Easy to invert: \\(E_{ij}^{-1} = I + m_{ij} \\mathbf{e}_i \\mathbf{e}_j^T\\) (just flip the sign!)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#inverting-to-get-l-the-key-insight",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#inverting-to-get-l-the-key-insight",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Inverting to Get L: The Key Insight",
    "text": "Inverting to Get L: The Key Insight\nFrom elimination, we have:\n\\[\nE_{32} E_{31} E_{21} A = U\n\\]\nMultiply both sides by the inverses (in reverse order):\n\\[\nA = E_{21}^{-1} E_{31}^{-1} E_{32}^{-1} U = LU\n\\]\nwhere: \\[\nL = E_{21}^{-1} E_{31}^{-1} E_{32}^{-1}\n\\]\n\nThe Beautiful Result\nWhen elimination matrices are multiplied in the right order, their inverses combine beautifully:\n\\[\nL = \\begin{bmatrix}\n1 & 0 & 0 & \\cdots \\\\\nm_{21} & 1 & 0 & \\cdots \\\\\nm_{31} & m_{32} & 1 & \\cdots \\\\\n\\vdots & \\vdots & \\ddots & \\ddots\n\\end{bmatrix}\n\\]\nThe multipliers \\(m_{ij}\\) (used during elimination) directly fill in the entries of \\(L\\) below the diagonal!\nNo extra computation needed ‚Äî just save the multipliers as you eliminate."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#computational-complexity",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#computational-complexity",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Computational Complexity",
    "text": "Computational Complexity\n\nOperation Counts\nFor an \\(n \\times n\\) matrix:\n\n\n\nStep\nOperations\nOrder\n\n\n\n\nElimination (find U)\n\\(\\frac{n^3}{3} + O(n^2)\\)\n\\(O(n^3)\\)\n\n\nForward substitution \\((Lc = b)\\)\n\\(\\frac{n^2}{2}\\)\n\\(O(n^2)\\)\n\n\nBack substitution \\((Ux = c)\\)\n\\(\\frac{n^2}{2}\\)\n\\(O(n^2)\\)\n\n\n\n\n\nWhy \\(\\frac{n^3}{3}\\)?\nAt step \\(k\\), we update an \\((n-k) \\times (n-k)\\) submatrix:\n\\[\n\\text{Total operations} = \\sum_{k=1}^{n-1} (n-k)^2 \\approx \\int_0^n x^2 \\, dx = \\frac{n^3}{3}\n\\]\n\n\nWhen is LU Worth It?\nSingle solve: \\(Ax = b\\) costs \\(O(n^3)\\) either way\nMultiple solves: If solving \\(Ax = b_1, Ax = b_2, \\ldots, Ax = b_m\\): - Without LU: \\(m \\times O(n^3)\\) - With LU: \\(O(n^3)\\) (once) + \\(m \\times O(n^2)\\) ‚úÖ\nHuge savings when \\(A\\) is fixed but \\(b\\) changes!"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#hands-on-exercises",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#hands-on-exercises",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Hands-On Exercises",
    "text": "Hands-On Exercises\nLet‚Äôs practice LU decomposition with concrete examples.\n\n\nShow code\nimport numpy as np\n\nprint(\"‚úì Libraries imported successfully\")\n\n\n‚úì Libraries imported successfully\n\n\n\nExercise 1: Manual LU Decomposition (2√ó2)\nCompute the LU decomposition of \\(A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}\\) by hand.\nSteps: 1. Perform elimination to get \\(U\\) 2. Record the multiplier \\(m_{21}\\) to build \\(L\\) 3. Verify \\(A = LU\\)\n\n\nShow code\nfrom IPython.display import display, Markdown, Latex\n\n# Original matrix\nA = np.array([[2, 3],\n              [4, 7]])\n\ndisplay(Markdown(\"**Original matrix A:**\"))\ndisplay(Latex(r\"$$A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}$$\"))\n\n# Compute multiplier m21\nm21 = 4/2  # row2[0] / row1[0]\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 1: Compute multiplier**\"))\ndisplay(Latex(f\"$$m_{{21}} = \\\\frac{{4}}{{2}} = {m21}$$\"))\n\n# Build L matrix\nL = np.array([[1, 0],\n              [m21, 1]])\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 2: Build L matrix**\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 \\\\ m_{21} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}$$\"))\n\n# Build U matrix (result after elimination)\n# After: row2 = row2 - m21*row1\n# [2, 3]        [2, 3]\n# [4, 7]  --&gt;   [0, 1]  (because 7 - 2*3 = 1)\nU = np.array([[2, 3],\n              [0, 1]])\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 3: Build U matrix (after elimination)**\"))\ndisplay(Markdown(\"Row 2 ‚Üí Row 2 - 2 √ó Row 1\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Verification: $A = LU$**\"))\ndisplay(Latex(r\"$$LU = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix} = A \\quad \\checkmark$$\"))\n\n\nOriginal matrix A:\n\n\n\\[A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}\\]\n\n\n\n\n\nStep 1: Compute multiplier\n\n\n\\[m_{21} = \\frac{4}{2} = 2.0\\]\n\n\n\n\n\nStep 2: Build L matrix\n\n\n\\[L = \\begin{bmatrix} 1 & 0 \\\\ m_{21} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nStep 3: Build U matrix (after elimination)\n\n\nRow 2 ‚Üí Row 2 - 2 √ó Row 1\n\n\n\\[U = \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(A = LU\\)\n\n\n\\[LU = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix} = A \\quad \\checkmark\\]\n\n\nKey observation: The multiplier \\(m_{21} = 2\\) goes directly into position \\((2,1)\\) of \\(L\\)!\n\n\nExercise 2: LU Decomposition (3√ó3)\nPerform LU decomposition on:\n\\[\nA = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\n\\]\nGoal: Find \\(L\\) and \\(U\\) such that \\(A = LU\\)\n\n\nShow code\nfrom IPython.display import display, Markdown, Latex\n\nA = np.array([[2, 1, 1],\n              [4, -6, 0],\n              [-2, 7, 2]])\n\ndisplay(Markdown(\"**Original matrix A:**\"))\ndisplay(Latex(r\"$$A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Step 1: Eliminate column 1\"))\n\n# Calculate multipliers for column 1\nm21 = A[1, 0] / A[0, 0]  # 4/2 = 2\nm31 = A[2, 0] / A[0, 0]  # -2/2 = -1\n\ndisplay(Markdown(\"**Multipliers:**\"))\ndisplay(Latex(f\"$$m_{{21}} = \\\\frac{{4}}{{2}} = {m21}, \\\\quad m_{{31}} = \\\\frac{{-2}}{{2}} = {m31}$$\"))\n\n# Create A1 after first elimination\nA1 = A.copy().astype(float)\nA1[1] = A1[1] - m21 * A1[0]  # row2 - 2*row1\nA1[2] = A1[2] - m31 * A1[0]  # row3 - (-1)*row1\n\ndisplay(Markdown(\"**After eliminating column 1:**\"))\ndisplay(Markdown(\"- Row 2 ‚Üí Row 2 - 2 √ó Row 1\"))\ndisplay(Markdown(\"- Row 3 ‚Üí Row 3 - (-1) √ó Row 1\"))\ndisplay(Latex(r\"$$A^{(1)} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 8 & 3 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Step 2: Eliminate column 2\"))\n\n# Calculate multiplier for column 2\nm32 = A1[2, 1] / A1[1, 1]  # 8/(-8) = -1\n\ndisplay(Markdown(\"**Multiplier:**\"))\ndisplay(Latex(f\"$$m_{{32}} = \\\\frac{{8}}{{-8}} = {m32}$$\"))\n\n# Create U (final upper triangular)\nU = A1.copy()\nU[2] = U[2] - m32 * U[1]  # row3 - (-1)*row2\n\ndisplay(Markdown(\"**After eliminating column 2:**\"))\ndisplay(Markdown(\"- Row 3 ‚Üí Row 3 - (-1) √ó Row 2\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Build L from multipliers\"))\n\n# Build L from multipliers\nL = np.array([[1, 0, 0],\n              [m21, 1, 0],\n              [m31, m32, 1]])\n\ndisplay(Markdown(\"The multipliers directly fill in $L$:\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 & 0 \\\\ m_{21} & 1 & 0 \\\\ m_{31} & m_{32} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Verification: $A = LU$\"))\n\ndisplay(Latex(r\"$$LU = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix} = A \\quad \\checkmark$$\"))\n\n\nOriginal matrix A:\n\n\n\\[A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\\]\n\n\n\n\n\nStep 1: Eliminate column 1\n\n\nMultipliers:\n\n\n\\[m_{21} = \\frac{4}{2} = 2.0, \\quad m_{31} = \\frac{-2}{2} = -1.0\\]\n\n\nAfter eliminating column 1:\n\n\n\nRow 2 ‚Üí Row 2 - 2 √ó Row 1\n\n\n\n\nRow 3 ‚Üí Row 3 - (-1) √ó Row 1\n\n\n\n\\[A^{(1)} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 8 & 3 \\end{bmatrix}\\]\n\n\n\n\n\nStep 2: Eliminate column 2\n\n\nMultiplier:\n\n\n\\[m_{32} = \\frac{8}{-8} = -1.0\\]\n\n\nAfter eliminating column 2:\n\n\n\nRow 3 ‚Üí Row 3 - (-1) √ó Row 2\n\n\n\n\\[U = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nBuild L from multipliers\n\n\nThe multipliers directly fill in \\(L\\):\n\n\n\\[L = \\begin{bmatrix} 1 & 0 & 0 \\\\ m_{21} & 1 & 0 \\\\ m_{31} & m_{32} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(A = LU\\)\n\n\n\\[LU = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix} = A \\quad \\checkmark\\]\n\n\nKey observation: All three multipliers \\((m_{21}, m_{31}, m_{32})\\) go directly into their corresponding positions in \\(L\\):\n\\[\nL = \\begin{bmatrix}\n1 & 0 & 0 \\\\\nm_{21} & 1 & 0 \\\\\nm_{31} & m_{32} & 1\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n-1 & -1 & 1\n\\end{bmatrix}\n\\]\n\nNote: In practice, numerical libraries like SciPy provide scipy.linalg.lu() which computes LU decomposition efficiently and includes automatic row permutation (pivoting) for numerical stability.\n\n\nShow code\nfrom scipy.linalg import lu\nfrom IPython.display import display, Markdown, Latex\n\nA = np.array([[2, 1, 1],\n              [4, -6, 0],\n              [-2, 7, 2]], dtype=float)\n\n# SciPy returns P, L, U where PA = LU (P is permutation matrix)\nP, L_scipy, U_scipy = lu(A)\n\ndisplay(Markdown(\"**SciPy's LU decomposition:**\"))\ndisplay(Markdown(\"SciPy returns $P$, $L$, $U$ where $PA = LU$ ($P$ is a permutation matrix)\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Permutation matrix P:**\"))\ndisplay(Latex(r\"$$P = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\ndisplay(Markdown(\"(This swaps rows 1 and 2 for numerical stability)\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Lower triangular L:**\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0.5 & 1 & 0 \\\\ -0.5 & 1 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Upper triangular U:**\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 4 & -6 & 0 \\\\ 0 & 4 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Verification: $PA = LU$**\"))\n\n# Note: If P = I (identity), then our manual L and U should match\nif np.allclose(P, np.eye(3)):\n    display(Markdown(\"‚úì No row swaps needed! Our manual $L$ and $U$ match SciPy.\"))\nelse:\n    display(Markdown(\"‚ö† **Row swaps were performed** (pivot strategy for numerical stability).\"))\n    display(Markdown(\"SciPy chose the largest pivot to minimize rounding errors.\"))\n    display(Markdown(\"Our manual decomposition is valid but uses a different pivot order.\"))\n\n\nSciPy‚Äôs LU decomposition:\n\n\nSciPy returns \\(P\\), \\(L\\), \\(U\\) where \\(PA = LU\\) (\\(P\\) is a permutation matrix)\n\n\n\n\n\nPermutation matrix P:\n\n\n\\[P = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n(This swaps rows 1 and 2 for numerical stability)\n\n\n\n\n\nLower triangular L:\n\n\n\\[L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0.5 & 1 & 0 \\\\ -0.5 & 1 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nUpper triangular U:\n\n\n\\[U = \\begin{bmatrix} 4 & -6 & 0 \\\\ 0 & 4 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(PA = LU\\)\n\n\n‚ö† Row swaps were performed (pivot strategy for numerical stability).\n\n\nSciPy chose the largest pivot to minimize rounding errors.\n\n\nOur manual decomposition is valid but uses a different pivot order."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html",
    "href": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html",
    "title": "MIT 18.06SC Lecture 8: Solving Ax=b - Complete Solution to Linear Systems",
    "section": "",
    "text": "My lecture notes\nThis lecture presents the complete solution structure for \\(Ax = b\\) and classifies all linear systems into four cases based on rank: exactly determined (unique solution), overdetermined (0 or 1 solution), underdetermined (infinite solutions), and rank deficient (0 or infinite solutions)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#context",
    "href": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#context",
    "title": "MIT 18.06SC Lecture 8: Solving Ax=b - Complete Solution to Linear Systems",
    "section": "",
    "text": "My lecture notes\nThis lecture presents the complete solution structure for \\(Ax = b\\) and classifies all linear systems into four cases based on rank: exactly determined (unique solution), overdetermined (0 or 1 solution), underdetermined (infinite solutions), and rank deficient (0 or infinite solutions)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#complete-solution-structure",
    "href": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#complete-solution-structure",
    "title": "MIT 18.06SC Lecture 8: Solving Ax=b - Complete Solution to Linear Systems",
    "section": "Complete Solution Structure",
    "text": "Complete Solution Structure\nThe general solution to \\(Ax = b\\) (when it exists) has the form: \\[\nx = x_p + x_n\n\\]\nwhere: - \\(x_p\\) is a particular solution: any specific solution satisfying \\(Ax_p = b\\) - \\(x_n\\) is any vector from the null space: \\(Ax_n = 0\\)\n\nWhy This Works\n\\[\n\\begin{aligned}\nAx_p &= b \\\\\nAx_n &= 0 \\\\\n\\hline\nA(x_p + x_n) &= Ax_p + Ax_n = b + 0 = b\n\\end{aligned}\n\\]\nKey insight: The complete solution is the particular solution plus the entire null space."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#finding-a-particular-solution",
    "href": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#finding-a-particular-solution",
    "title": "MIT 18.06SC Lecture 8: Solving Ax=b - Complete Solution to Linear Systems",
    "section": "Finding a Particular Solution",
    "text": "Finding a Particular Solution\n\nAlgorithm\n\nRow reduce \\([A \\mid b]\\) to reduced row echelon form\nSet all free variables to 0\nSolve for the pivot variables from the reduced equations\nThis gives \\(x_p\\)\n\nIf row reduction produces a row like \\([0 \\, 0 \\, \\cdots \\, 0 \\mid c]\\) where \\(c \\neq 0\\), then no solution exists."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#rank-and-solution-existence",
    "href": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#rank-and-solution-existence",
    "title": "MIT 18.06SC Lecture 8: Solving Ax=b - Complete Solution to Linear Systems",
    "section": "Rank and Solution Existence",
    "text": "Rank and Solution Existence\nFor an \\(m \\times n\\) matrix \\(A\\) with rank \\(r\\):\n\\[\nr \\leq \\min(m, n)\n\\]\nThis is because: - \\(r \\leq m\\) (rank cannot exceed number of rows) - \\(r \\leq n\\) (rank cannot exceed number of columns)\n\nSolvability Condition\nThe system \\(Ax = b\\) has a solution if and only if \\(b\\) is in the column space of \\(A\\): \\[\nb \\in C(A)\n\\]\nEquivalently, the system is solvable when \\(\\text{rank}(A) = \\text{rank}([A \\mid b])\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#four-cases-based-on-rank",
    "href": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#four-cases-based-on-rank",
    "title": "MIT 18.06SC Lecture 8: Solving Ax=b - Complete Solution to Linear Systems",
    "section": "Four Cases Based on Rank",
    "text": "Four Cases Based on Rank\n\nCase 1: Exactly Determined (Square, Full Rank)\nConditions: \\(r = n = m\\) (square matrix with full rank)\nProperties: - Matrix is invertible - RREF: \\(R = I_n\\) (identity matrix) - No free variables - Null space: \\(N(A) = \\{\\vec{0}\\}\\) (only zero vector)\nSolutions: - Unique solution for every \\(b\\): \\(x = A^{-1}b\\) - Number of solutions = 1 (always)\nExample: \\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad r = 2 = m = n\n\\]\n\n\nCase 2: Overdetermined (Tall, Full Column Rank)\nConditions: \\(m &gt; n\\) and \\(r = n\\) (more equations than unknowns)\nProperties: - Matrix is tall and skinny - RREF: \\(R = \\begin{bmatrix} I_n \\\\ 0 \\end{bmatrix}\\) (identity on top, zeros below) - No free variables (\\(n - r = 0\\)) - Null space: \\(N(A) = \\{\\vec{0}\\}\\) (only zero vector)\nSolutions: - 0 or 1 solution (depends on whether \\(b \\in C(A)\\)) - If solution exists, it is unique (no free variables) - Most \\(b\\) vectors have no solution (overconstrained system)\nExample: \\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}, \\quad r = 2, \\, m = 3, \\, n = 2\n\\]\nInterpretation: More constraints than degrees of freedom; typically no exact solution (leads to least squares in practice).\n\n\nCase 3: Underdetermined (Wide, Full Row Rank)\nConditions: \\(m &lt; n\\) and \\(r = m\\) (fewer equations than unknowns)\nProperties: - Matrix is short and wide - RREF: \\(R = [I_m \\mid F]\\) (identity on left, free columns on right) - Has free variables (\\(n - r = n - m &gt; 0\\)) - Non-trivial null space: \\(\\dim(N(A)) = n - m\\)\nSolutions: - Infinitely many solutions for every \\(b\\) (when \\(r = m\\)) - Solution exists because \\(C(A) = \\mathbb{R}^m\\) (full row rank) - Complete solution: \\(x = x_p + x_n\\) where \\(x_n\\) ranges over \\((n-m)\\)-dimensional null space\nExample: \\[\nA = \\begin{bmatrix} 1 & 2 & 3 & 4 \\\\ 0 & 0 & 1 & 2 \\end{bmatrix}, \\quad r = 2, \\, m = 2, \\, n = 4\n\\]\nInterpretation: More degrees of freedom than constraints; infinitely many ways to satisfy the equations.\n\n\nCase 4: Rank Deficient (Neither Full Column nor Full Row Rank)\nConditions: \\(r &lt; m\\) and \\(r &lt; n\\) (rank deficient)\nProperties: - Has free variables: \\(n - r &gt; 0\\) - Column space is proper subspace: \\(C(A) \\subsetneq \\mathbb{R}^m\\)\nSolutions: - 0 or infinitely many solutions - If \\(b \\in C(A)\\): infinitely many solutions (due to non-trivial null space) - If \\(b \\notin C(A)\\): no solution\nExample: \\[\nA = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\\\ 0 & 0 & 0 \\end{bmatrix}, \\quad r = 1, \\, m = 3, \\, n = 3\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#summary-table",
    "href": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#summary-table",
    "title": "MIT 18.06SC Lecture 8: Solving Ax=b - Complete Solution to Linear Systems",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\n\n\nCase\nRank Condition\nMatrix Shape\n# Free Vars\n# Solutions\n\n\n\n\nExactly determined\n\\(r = n = m\\)\nSquare, invertible\n0\n1 (always)\n\n\nOverdetermined\n\\(r = n &lt; m\\)\nTall, full column rank\n0\n0 or 1\n\n\nUnderdetermined\n\\(r = m &lt; n\\)\nWide, full row rank\n\\(n - m\\)\n\\(\\infty\\) (always)\n\n\nRank deficient\n\\(r &lt; \\min(m,n)\\)\nGeneral\n\\(n - r &gt; 0\\)\n0 or \\(\\infty\\)\n\n\n\nKey principle: - Number of free variables = \\(n - r\\) - If \\(r = n\\): 0 or 1 solution (unique if exists) - If \\(r &lt; n\\): 0 or \\(\\infty\\) solutions (null space is non-trivial)\n\nSource: MIT 18.06SC Linear Algebra, Lecture 8"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "This is for MIT 18.06SC Lecture 1, covering how to understand linear systems from two perspectives: geometry (row picture) and algebra (column picture)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#the-example-system",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#the-example-system",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "The Example System",
    "text": "The Example System\nLet‚Äôs work with this concrete example:\n\\[\\begin{align}\nx + 2y &= 5 \\\\\n3x + 4y &= 6\n\\end{align}\\]\nIn matrix form: \\[\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nWe can interpret this system in two completely different ways."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#row-picture-geometry",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#row-picture-geometry",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Row Picture (Geometry)",
    "text": "Row Picture (Geometry)\nIn the row picture, each equation represents a geometric object: - In 2D: each equation is a line - In 3D: each equation is a plane\n- In higher dimensions: each equation is a hyperplane\nThe solution is where all these objects intersect.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the equations in the form y = mx + c\n# Line 1: x + 2y = 5  =&gt;  y = -1/2*x + 5/2\n# Line 2: 3x + 4y = 6  =&gt;  y = -3/4*x + 3/2\nx = np.linspace(-10, 10, 100)\ny1 = -1/2 * x + 5/2\ny2 = -3/4 * x + 3/2\n\n# Solve for intersection point\nA = np.array([[1, 2], [3, 4]])\nb = np.array([5, 6])\nsolution = np.linalg.solve(A, b)\n\n# Plot both lines and intersection\nplt.figure(figsize=(8, 6))\nplt.plot(x, y1, 'b-', label='Line 1: x + 2y = 5', linewidth=2)\nplt.plot(x, y2, 'r-', label='Line 2: 3x + 4y = 6', linewidth=2)\nplt.scatter(solution[0], solution[1], color='green', s=100, zorder=5, \n           label=f'Solution: ({solution[0]:.1f}, {solution[1]:.1f})', edgecolor='white', linewidth=2)\n\nplt.xlim(-8, 8)\nplt.ylim(-1, 8)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Row Picture: Where Lines Meet')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Solution: x = {solution[0]:.3f}, y = {solution[1]:.3f}\")\nprint(f\"Verification: {A @ solution} equals {b}\")\n\n\n\n\n\n\n\n\n\nSolution: x = -4.000, y = 4.500\nVerification: [5. 6.] equals [5 6]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#column-picture-algebra",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#column-picture-algebra",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Column Picture (Algebra)",
    "text": "Column Picture (Algebra)\nThe column picture reframes the same system as a question about vector combinations:\n\\[x \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} + y \\begin{bmatrix}2 \\\\ 4\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nInstead of asking ‚Äúwhere do lines intersect?‚Äù, we ask: ‚ÄúCan we combine these vectors to reach our target?‚Äù\n\n\nCode\n# Define column vectors and target vector\na1 = np.array([1, 3])\na2 = np.array([2, 4])\nb = np.array([5, 6])\n\n# Solve for coefficients\nA = np.column_stack([a1, a2])\nsolution = np.linalg.solve(A, b)\nx, y = solution[0], solution[1]\n\nprint(f\"Question: Can we write b as a linear combination of a‚ÇÅ and a‚ÇÇ?\")\nprint(f\"Answer: {x:.3f} √ó a‚ÇÅ + {y:.3f} √ó a‚ÇÇ = b\")\nprint(f\"Verification: {x*a1} + {y*a2} = {x*a1 + y*a2}\")\n\n# Visualize the vector construction\nplt.figure(figsize=(8, 6))\n\n# Step 1: Draw x*a1 (scaled version)\nplt.arrow(0, 0, x*a1[0], x*a1[1], head_width=0.2, head_length=0.2, \n         fc='blue', ec='blue', linewidth=3,\n         label=f'{x:.2f} √ó a‚ÇÅ')\n\n# Step 2: Draw y*a2 starting from the tip of x*a1\nplt.arrow(x*a1[0], x*a1[1], y*a2[0], y*a2[1], head_width=0.2, head_length=0.2, \n         fc='green', ec='green', linewidth=3,\n         label=f'{y:.2f} √ó a‚ÇÇ')\n\n# Show final result vector b\nplt.arrow(0, 0, b[0], b[1], head_width=0.25, head_length=0.25, \n         fc='red', ec='red', linewidth=4, alpha=0.8,\n         label=f'b = [{b[0]}, {b[1]}]')\n\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.xlim(-1, 6)\nplt.ylim(-12, 7)\nplt.xlabel('x-component')\nplt.ylabel('y-component')\nplt.title('Column Picture: Vector Combination')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\n\n\nQuestion: Can we write b as a linear combination of a‚ÇÅ and a‚ÇÇ?\nAnswer: -4.000 √ó a‚ÇÅ + 4.500 √ó a‚ÇÇ = b\nVerification: [ -4. -12.] + [ 9. 18.] = [5. 6.]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Three Types of Linear Systems",
    "text": "Three Types of Linear Systems\nLinear systems can have three possible outcomes:\n\nUnique solution - Lines intersect at one point\nNo solution - Lines are parallel (don‚Äôt intersect)\nInfinitely many solutions - Lines are the same (overlap completely)\n\n\n\nCode\n# Case (a): Unique solution - non-parallel vectors\nprint(\"üéØ Case (a) - Unique Solution:\")\nA_a = np.array([[1, 2], [3, 4]])\nb_a = np.array([5, 6])\nsolution_a = np.linalg.solve(A_a, b_a)\ndet_a = np.linalg.det(A_a)\nprint(f\"   Solution: {solution_a}\")\nprint(f\"   Matrix determinant: {det_a:.3f} ‚â† 0 ‚Üí linearly independent columns\")\nprint(f\"   Column space: ENTIRE 2D plane (any point reachable)\")\n\n# Case (b): No solution - parallel vectors, b not in span\nprint(f\"\\n‚ùå Case (b) - No Solution:\")\nA_b = np.array([[1, 2], [2, 4]])  # Columns are parallel\nb_b = np.array([5, 6])            # b not in span\ndet_b = np.linalg.det(A_b)\nprint(f\"   Matrix determinant: {det_b:.3f} = 0 ‚Üí linearly dependent columns\")\nprint(f\"   Column space: 1D line only (most points unreachable)\")\nprint(f\"   Target b = {b_b} is NOT on the line ‚Üí No solution exists\")\n\n# Case (c): Infinitely many solutions - parallel vectors, b in span\nprint(f\"\\n‚ôæÔ∏è  Case (c) - Infinitely Many Solutions:\")\nA_c = np.array([[1, 2], [2, 4]])  # Same parallel columns\nb_c = np.array([3, 6])            # b = 3 * [1, 2], so b is in span\ndet_c = np.linalg.det(A_c)\nprint(f\"   Matrix determinant: {det_c:.3f} = 0 ‚Üí linearly dependent columns\")\nprint(f\"   Column space: 1D line only\")\nprint(f\"   Target b = {b_c} IS on the line ‚Üí Infinite solutions exist\")\n\n# Find one particular solution using pseudoinverse\nsolution_c = np.linalg.pinv(A_c) @ b_c\nprint(f\"   One particular solution: {solution_c}\")\nprint(f\"   Other solutions: {solution_c} + t√ó[2, -1] for any real number t\")\n\n\nüéØ Case (a) - Unique Solution:\n   Solution: [-4.   4.5]\n   Matrix determinant: -2.000 ‚â† 0 ‚Üí linearly independent columns\n   Column space: ENTIRE 2D plane (any point reachable)\n\n‚ùå Case (b) - No Solution:\n   Matrix determinant: 0.000 = 0 ‚Üí linearly dependent columns\n   Column space: 1D line only (most points unreachable)\n   Target b = [5 6] is NOT on the line ‚Üí No solution exists\n\n‚ôæÔ∏è  Case (c) - Infinitely Many Solutions:\n   Matrix determinant: 0.000 = 0 ‚Üí linearly dependent columns\n   Column space: 1D line only\n   Target b = [3 6] IS on the line ‚Üí Infinite solutions exist\n   One particular solution: [0.6 1.2]\n   Other solutions: [0.6 1.2] + t√ó[2, -1] for any real number t\n\n\n\n\nCode\n# Visualize all three cases\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Case (a): Unique solution\nax = axes[0]\nax.fill_between([-1, 6], [-1, -1], [7, 7], color='lightblue', alpha=0.2, \n                label='Column space = ENTIRE plane')\n\n# Draw vectors\nax.arrow(0, 0, A_a[0,0], A_a[1,0], head_width=0.15, head_length=0.15,\n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,3]')\nax.arrow(0, 0, A_a[0,1], A_a[1,1], head_width=0.15, head_length=0.15,\n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4]')\nax.arrow(0, 0, b_a[0], b_a[1], head_width=0.2, head_length=0.2,\n         fc='red', ec='red', linewidth=3, label='b = [5,6]')\n\nax.set_title('Unique Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (b): No solution\nax = axes[1]\nt = np.linspace(-2, 5, 100)\nspan_x, span_y = t * A_b[0,0], t * A_b[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6, \n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_b[0,0], A_b[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,2]')\nax.arrow(0, 0, A_b[0,1], A_b[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4] = 2√óa‚ÇÅ')\nax.arrow(0, 0, b_b[0], b_b[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [5,6] (off line)')\n\nax.set_title('No Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (c): Infinitely many solutions\nax = axes[2]\nt = np.linspace(-1, 4, 100)\nspan_x, span_y = t * A_c[0,0], t * A_c[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6,\n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_c[0,0], A_c[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,2]')\nax.arrow(0, 0, A_c[0,1], A_c[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4] = 2√óa‚ÇÅ')\nax.arrow(0, 0, b_c[0], b_c[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [3,6] (on line)')\n\nax.set_title('Infinite Solutions')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key insight: Solution depends on whether target vector b lies in the column space\")\n\n\n\n\n\n\n\n\n\nKey insight: Solution depends on whether target vector b lies in the column space\n\n\n\nThis covers the core geometric foundations from MIT 18.06SC Lecture 1: understanding linear systems through both row and column perspectives."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html",
    "href": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html",
    "title": "MIT 18.06SC Lecture 7: Solving Ax=0 - Pivot Variables and Special Solutions",
    "section": "",
    "text": "My lecture notes\nThis lecture develops a systematic algorithm to find all solutions to \\(Ax = 0\\) using pivot variables, free variables, and special solutions. The null space structure is revealed through reduced row echelon form."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#context",
    "href": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#context",
    "title": "MIT 18.06SC Lecture 7: Solving Ax=0 - Pivot Variables and Special Solutions",
    "section": "",
    "text": "My lecture notes\nThis lecture develops a systematic algorithm to find all solutions to \\(Ax = 0\\) using pivot variables, free variables, and special solutions. The null space structure is revealed through reduced row echelon form."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#homogeneous-linear-system",
    "href": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#homogeneous-linear-system",
    "title": "MIT 18.06SC Lecture 7: Solving Ax=0 - Pivot Variables and Special Solutions",
    "section": "Homogeneous Linear System",
    "text": "Homogeneous Linear System\nWe consider the homogeneous linear system: \\[\nAx = 0\n\\]\nwhere \\(A\\) is an \\(m \\times n\\) matrix with rank \\(r\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#pivot-variables-and-free-variables",
    "href": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#pivot-variables-and-free-variables",
    "title": "MIT 18.06SC Lecture 7: Solving Ax=0 - Pivot Variables and Special Solutions",
    "section": "Pivot Variables and Free Variables",
    "text": "Pivot Variables and Free Variables\nAfter performing row reduction on \\(A\\), we identify:\n\nPivot variables: Variables corresponding to columns with pivot positions (leading entries in row echelon form)\nFree variables: Variables corresponding to columns without pivot positions\nNumber of free variables: \\(n - r\\) (total variables minus rank)\n\n\nRelationship to Solutions\n\nIf rank \\(r = n\\): No free variables\n\nOnly the trivial solution \\(x = \\vec{0}\\) exists\nThe null space contains only the zero vector\n\nIf rank \\(r &lt; n\\): There are \\(n - r\\) free variables\n\nInfinitely many non-trivial solutions exist\nThe null space is a \\((n-r)\\)-dimensional subspace\n\n\nKey insight: Free variables are necessary for non-trivial solutions. Each free variable adds one dimension to the null space."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#special-solutions",
    "href": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#special-solutions",
    "title": "MIT 18.06SC Lecture 7: Solving Ax=0 - Pivot Variables and Special Solutions",
    "section": "Special Solutions",
    "text": "Special Solutions\nSpecial solutions form a basis for the null space \\(N(A)\\). To find them:\n\nAlgorithm\n\nIdentify the free variables (there are \\(n - r\\) of them)\nFor each free variable:\n\nSet that free variable to 1\nSet all other free variables to 0\nSolve for the pivot variables\nThis gives one special solution\n\n\nThe null space is the span of all special solutions.\n\n\nExample\nConsider the row echelon form matrix: \\[\nR = \\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n0 & 0 & 2 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nFrom this, we have: - Pivot columns: 1 and 3 (variables \\(x_1\\) and \\(x_3\\)) - Free columns: 2 and 4 (variables \\(x_2\\) and \\(x_4\\)) - Equations: \\(x_1 + 2x_2 + 3x_3 + 4x_4 = 0\\) and \\(2x_3 + x_4 = 0\\)\nSpecial solution 1: Set \\(x_2 = 1\\), \\(x_4 = 0\\) - From equation 2: \\(2x_3 = 0 \\Rightarrow x_3 = 0\\) - From equation 1: \\(x_1 + 2(1) + 3(0) + 4(0) = 0 \\Rightarrow x_1 = -2\\) - Special solution: \\(\\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\\)\nSpecial solution 2: Set \\(x_2 = 0\\), \\(x_4 = 1\\) - From equation 2: \\(2x_3 + 1 = 0 \\Rightarrow x_3 = -\\frac{1}{2}\\) - From equation 1: \\(x_1 + 2(0) + 3(-\\frac{1}{2}) + 4(1) = 0 \\Rightarrow x_1 = -\\frac{5}{2}\\) - Special solution: \\(\\begin{bmatrix} -\\frac{5}{2} \\\\ 0 \\\\ -\\frac{1}{2} \\\\ 1 \\end{bmatrix}\\)\nComplete null space: \\[\nN(A) = c_1 \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + c_2 \\begin{bmatrix} -\\frac{5}{2} \\\\ 0 \\\\ -\\frac{1}{2} \\\\ 1 \\end{bmatrix}\n\\]\nfor any scalars \\(c_1, c_2 \\in \\mathbb{R}\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#rref-reduced-row-echelon-form",
    "href": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#rref-reduced-row-echelon-form",
    "title": "MIT 18.06SC Lecture 7: Solving Ax=0 - Pivot Variables and Special Solutions",
    "section": "RREF (Reduced Row Echelon Form)",
    "text": "RREF (Reduced Row Echelon Form)\nThe reduced row echelon form provides the clearest view of the null space structure.\n\nProperties of RREF\n\nEach pivot column contains exactly one 1 (the pivot) and all other entries are 0\nThe pivot is the only non-zero entry in its row among pivot columns\nMakes it easy to read off special solutions directly\n\n\n\nBlock Structure\nFor a matrix with \\(r\\) pivot columns and \\(n-r\\) free columns, the RREF has the form: \\[\nR_{\\text{rref}} = \\begin{bmatrix}\nI_r & F\n\\end{bmatrix}\n\\]\nwhere: - \\(I_r\\) is the \\(r \\times r\\) identity matrix (pivot columns) - \\(F\\) is an \\(r \\times (n-r)\\) matrix (free variable coefficients)\n\n\nExample\nFor our example, RREF would be: \\[\nR_{\\text{rref}} = \\begin{bmatrix}\n1 & 2 & 0 & \\frac{5}{2} \\\\\n0 & 0 & 1 & \\frac{1}{2} \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n2 & \\frac{5}{2} \\\\\n0 & \\frac{1}{2}\n\\end{bmatrix}\n\\]\nHere \\(I_r = I_2\\) and \\(F = \\begin{bmatrix} 2 & \\frac{5}{2} \\\\ 0 & \\frac{1}{2} \\end{bmatrix}\\).\n\n\nDirect Formula for Null Space\nFrom the RREF block structure \\([I_r \\mid F]\\), the null space matrix (whose columns are special solutions) is: \\[\nN(A) = \\begin{bmatrix}\n-F \\\\\nI_{n-r}\n\\end{bmatrix}\n\\]\nThis \\(n \\times (n-r)\\) matrix contains all special solutions as its columns.\n\n\nDimensions Summary\n\n\n\nMatrix\nShape\nDescription\n\n\n\n\n\\(A\\)\n\\(m \\times n\\)\nOriginal matrix\n\n\n\\(I_r\\)\n\\(r \\times r\\)\nIdentity block in RREF\n\n\n\\(F\\)\n\\(r \\times (n-r)\\)\nFree variable coefficients\n\n\n\\(N(A)\\)\n\\(n \\times (n-r)\\)\nNull space basis matrix\n\n\n\nThe null space \\(N(A)\\) is an \\((n-r)\\)-dimensional subspace of \\(\\mathbb{R}^n\\).\n\nSource: MIT 18.06SC Linear Algebra, Lecture 7"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#projection-of-a-vector-onto-a-line",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#projection-of-a-vector-onto-a-line",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "1. Projection of a Vector onto a Line",
    "text": "1. Projection of a Vector onto a Line\n\n\n\nProjection onto a Line\n\n\nSetup: Project vector \\(b\\) onto vector \\(a\\).\nKey equations:\n\\[\n\\begin{aligned}\np &= xa \\\\\na^T(b - xa) &= 0 \\\\\na^T b &= xa^T a \\\\\nx &= \\frac{a^T b}{a^T a}\n\\end{aligned}\n\\]\nComponents: - \\(p\\): Projection of \\(b\\) onto \\(a\\) - \\(e\\): Error vector (\\(e = b - p\\)) - \\(e \\perp a\\) (error is perpendicular to \\(a\\)) - \\(p + e = b\\) - \\(p = b - e\\)\nProjection formula:\n\\[\np = a\\frac{a^T b}{a^T a}\n\\]\nProperties: - If \\(b\\) is doubled, then \\(p\\) is doubled - If \\(a\\) is doubled, then \\(p\\) does not change at all"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#projection-matrix",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#projection-matrix",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "2. Projection Matrix",
    "text": "2. Projection Matrix\nMatrix form:\n\\[\np = Pb\n\\]\nwhere\n\\[\nP = \\frac{aa^T}{a^T a}\n\\]\nAnalysis: - \\(aa^T\\) is \\((n \\times 1)(1 \\times n) = n \\times n\\) - \\(a^T a\\) is a scalar - \\(P\\) is \\(n \\times n\\) - Column space of \\(P\\) is the line through \\(a\\) - Rank is 1 - \\(P\\) is symmetric: \\(P^T = P\\) - \\(P^2 = P\\) (if I project twice, the result is the same)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#why-projection",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#why-projection",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "3. Why Projection?",
    "text": "3. Why Projection?\nProblem: The equation \\(Ax = b\\) may have 0 solutions when \\(m &gt; n\\).\nSolution: Solve \\(A\\hat{x} = p\\) instead, where \\(p\\) is the projection of \\(b\\) onto the column space.\n\n\n\nProjection in Higher Dimensions\n\n\nSetup: - \\(e = b - p\\) is perpendicular to the plane (spanned by \\(a_1\\) and \\(a_2\\)) - \\(A = \\begin{bmatrix}| & | \\\\ a_1 & a_2 \\\\ | & |\\end{bmatrix}\\) - \\(p = \\hat{x}_1 a_1 + \\hat{x}_2 a_2 = A\\hat{x}\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#finding-hatx",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#finding-hatx",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "4. Finding \\(\\hat{x}\\)",
    "text": "4. Finding \\(\\hat{x}\\)\nKey idea: \\(b - A\\hat{x} \\perp C(A)\\) (error is perpendicular to column space)\nDerivation: Error \\(e\\) is perpendicular to \\(a_1\\) and \\(a_2\\):\n\\[\n\\begin{aligned}\na_1^T(b - A\\hat{x}) &= 0 \\text{ and } a_2^T(b - A\\hat{x}) = 0 \\\\\n\\begin{bmatrix}a_1^T \\\\ a_2^T\\end{bmatrix}(b - A\\hat{x}) &= \\begin{bmatrix}0 \\\\ 0\\end{bmatrix} \\\\\nA^T(b - A\\hat{x}) &= \\mathbf{0}\n\\end{aligned}\n\\]\nKey relationships: - \\(e = b - A\\hat{x}\\) is in the left null space - \\(e \\in N(A^T)\\) - \\(e \\perp C(A)\\) (error is perpendicular to column space of \\(A\\))\nNormal equations:\n\\[\nA^T b = A^T A\\hat{x}\n\\]\nSolution:\n\\[\n\\hat{x} = (A^T A)^{-1} A^T b\n\\]\nProjection:\n\\[\np = A\\hat{x} = A(A^T A)^{-1} A^T b\n\\]\nProjection matrix:\n\\[\nP = A(A^T A)^{-1} A^T\n\\]\nSpecial case: If \\(A\\) is invertible, then \\(P = I\\) (identity matrix).\nProperties in high dimensions: - \\(P^T = P\\) (symmetric) - \\(P^2 = P\\) (idempotent)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#least-squares-application",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#least-squares-application",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "5. Least Squares Application",
    "text": "5. Least Squares Application\n\n\n\nLeast Squares\n\n\nProblem: Fit a line \\(y = c + dt\\) to data points \\((1,1)\\), \\((2,2)\\), and \\((3,3)\\).\nSetup:\n\\[\nA = \\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3\n\\end{bmatrix}, \\quad\nb = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}, \\quad\nx = \\begin{bmatrix}\nc \\\\\nd\n\\end{bmatrix}\n\\]\nEquation system: \\(Ax = b\\) - Row 1: \\(c + 1 \\cdot d = 1\\) - Row 2: \\(c + 2 \\cdot d = 2\\) - Row 3: \\(c + 3 \\cdot d = 3\\)\nIssue: Cannot find exact solution for \\(x\\) because: - \\(A\\) is not invertible (not square: \\(3 \\times 2\\)) - System is overdetermined (3 equations, 2 unknowns)\nSolution: Use projection to find \\(\\hat{x}\\) that minimizes \\(\\|Ax - b\\|^2\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#geometric-interpretation",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#geometric-interpretation",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "6. Geometric Interpretation",
    "text": "6. Geometric Interpretation\n3 Lines in 3D: If we have 3 lines in 3-dimensional space, then \\(A\\) will be square.\nIf \\(A\\) is invertible (full rank): - \\(Ax = b\\) has an exact solution - The solution geometrically means: find the plane spanned by column 1 and column 2, then find where column 3 crosses it\n\nSource: MIT 18.06SC Linear Algebra, Lecture 15"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#overview",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#overview",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Overview",
    "text": "Overview\nFor any \\(m \\times n\\) matrix \\(A\\), there are four fundamental subspaces that completely characterize its structure:\n\n\n\n\n\n\n\n\n\n\nSubspace\nNotation\nLives in\nDimension\nSolves\n\n\n\n\nColumn Space\n\\(C(A)\\)\n\\(\\mathbb{R}^m\\)\n\\(r\\)\n\\(Ax = b\\)\n\n\nNull Space\n\\(N(A)\\)\n\\(\\mathbb{R}^n\\)\n\\(n - r\\)\n\\(Ax = \\mathbf{0}\\)\n\n\nRow Space\n\\(C(A^T)\\)\n\\(\\mathbb{R}^n\\)\n\\(r\\)\n\\(A^T y = c\\)\n\n\nLeft Null Space\n\\(N(A^T)\\)\n\\(\\mathbb{R}^m\\)\n\\(m - r\\)\n\\(A^T y = \\mathbf{0}\\)\n\n\n\nwhere \\(r = \\text{rank}(A)\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#column-space-ca",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#column-space-ca",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Column Space: \\(C(A)\\)",
    "text": "Column Space: \\(C(A)\\)\nDefinition: The span of all columns of \\(A\\).\nProperties:\n\nLives in \\(\\mathbb{R}^m\\) (same dimension as the rows)\nDimension = \\(\\text{rank}(A) = r\\)\nRepresents all possible outputs of \\(Ax\\)\n\nLinear combination interpretation:\n\\[\nx_1 A_{:,1} + x_2 A_{:,2} + \\cdots + x_n A_{:,n} = b\n\\]\nKey question: For which \\(b\\) does \\(Ax = b\\) have a solution?\nAnswer: When \\(b \\in C(A)\\) (when \\(b\\) is in the column space)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#null-space-na",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#null-space-na",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Null Space: \\(N(A)\\)",
    "text": "Null Space: \\(N(A)\\)\nDefinition: All vectors \\(x\\) such that \\(Ax = \\mathbf{0}\\).\nProperties:\n\nLives in \\(\\mathbb{R}^n\\) (same dimension as the columns)\nDimension = \\(n - r\\) (number of free variables)\nContains all solutions to the homogeneous system \\(Ax = \\mathbf{0}\\)\n\nInterpretation: Vectors that get ‚Äúkilled‚Äù by the matrix \\(A\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#row-space-cat",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#row-space-cat",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Row Space: \\(C(A^T)\\)",
    "text": "Row Space: \\(C(A^T)\\)\nDefinition: The span of all rows of \\(A\\), equivalently the column space of \\(A^T\\).\nProperties:\n\nLives in \\(\\mathbb{R}^n\\) (same dimension as the columns)\nDimension = \\(\\text{rank}(A) = r\\) (same as column space)\nRepresents all possible outputs of \\(A^T y\\)\n\n\n\n\n\n\n\nTipKey Insight\n\n\n\nRow rank equals column rank (both equal \\(r\\))."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#left-null-space-nat",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#left-null-space-nat",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Left Null Space: \\(N(A^T)\\)",
    "text": "Left Null Space: \\(N(A^T)\\)\nDefinition: All vectors \\(y\\) such that \\(A^T y = \\mathbf{0}\\).\nProperties:\n\nLives in \\(\\mathbb{R}^m\\) (same dimension as the rows)\nDimension = \\(m - r\\)\nAlso called the left null space of \\(A\\)\n\nWhy ‚Äúleft null space‚Äù?\nStarting from \\(A^T y = \\mathbf{0}\\):\n\\[\n\\begin{aligned}\nA^T y &= \\mathbf{0} \\\\\n(A^T y)^T &= \\mathbf{0}^T = [\\mathbf{0}] \\\\\ny^T A &= [\\mathbf{0}]\n\\end{aligned}\n\\]\nSince \\(y^T\\) is \\(1 \\times m\\) and appears on the left of \\(A\\), this is called the left null space."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#rref-and-the-four-subspaces",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#rref-and-the-four-subspaces",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "RREF and the Four Subspaces",
    "text": "RREF and the Four Subspaces\n\nExample Matrix\n\\[\nA = \\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n5 & 6 & 7 & 8 \\\\\n2 & 4 & 6 & 8\n\\end{bmatrix}\n\\]\n\n\nRow Echelon Form (REF)\n\\[\n\\text{REF} = \\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n0 & -4 & -8 & -12 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\n\n\nReduced Row Echelon Form (RREF)\nStep 1: Normalize pivot rows\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n0 & 1 & 2 & 3 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nStep 2: Eliminate above pivots\n\\[\n\\text{RREF} = \\begin{bmatrix}\n1 & 0 & -1 & -2 \\\\\n0 & 1 & 2 & 3 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\n\n\nAnalysis\n\nRank: \\(r = 2\\) (two pivot columns)\nFree variables: \\(n - r = 4 - 2 = 2\\) (columns 3 and 4)\nDimension of \\(N(A)\\): \\(n - r = 2\\)\nDimension of \\(C(A)\\): \\(r = 2\\)\nDimension of \\(C(A^T)\\): \\(r = 2\\)\nDimension of \\(N(A^T)\\): \\(m - r = 3 - 2 = 1\\)\n\n\n\n\n\n\n\nNoteKey Observation\n\n\n\nRREF is the unique simplest form of a matrix obtained through row operations that preserve the solution space."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#visualization",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#visualization",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Visualization",
    "text": "Visualization\n\n\n\nFour Fundamental Subspaces"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#dimension-relationships",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#dimension-relationships",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Dimension Relationships",
    "text": "Dimension Relationships\nFor an \\(m \\times n\\) matrix \\(A\\) with rank \\(r\\):\n\nIn \\(\\mathbb{R}^n\\):\n\n\\(\\dim(C(A^T)) + \\dim(N(A)) = r + (n - r) = n\\)\nRow space and null space partition \\(\\mathbb{R}^n\\)\n\n\n\nIn \\(\\mathbb{R}^m\\):\n\n\\(\\dim(C(A)) + \\dim(N(A^T)) = r + (m - r) = m\\)\nColumn space and left null space partition \\(\\mathbb{R}^m\\)\n\n\n\n\n\n\n\nImportantKey Insight\n\n\n\nThe four subspaces come in complementary pairs that completely partition their ambient spaces."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#summary",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Summary",
    "text": "Summary\nThe four fundamental subspaces:\n\nColumn space \\(C(A)\\): where outputs \\(Ax\\) live\nNull space \\(N(A)\\): inputs that map to zero\nRow space \\(C(A^T)\\): perpendicular complement to null space in \\(\\mathbb{R}^n\\)\nLeft null space \\(N(A^T)\\): perpendicular complement to column space in \\(\\mathbb{R}^m\\)\n\nDimension formula:\n\n\\(\\dim(C(A)) = \\dim(C(A^T)) = r\\)\n\\(\\dim(N(A)) = n - r\\)\n\\(\\dim(N(A^T)) = m - r\\)\n\nTotal check: \\(r + (n-r) = n\\) and \\(r + (m-r) = m\\)\n\nSource: MIT 18.06SC Linear Algebra, Lecture 10"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture3-multiplication.html",
    "href": "Math/MIT18.06/mit1806-lecture3-multiplication.html",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strang‚Äôs third lecture reveals a profound insight: matrix multiplication isn‚Äôt just one operation‚Äîit‚Äôs five different perspectives on the same computation, each revealing different structural properties. Then we explore matrix inverses and why some matrices fundamentally cannot be inverted."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture3-multiplication.html#context",
    "href": "Math/MIT18.06/mit1806-lecture3-multiplication.html#context",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strang‚Äôs third lecture reveals a profound insight: matrix multiplication isn‚Äôt just one operation‚Äîit‚Äôs five different perspectives on the same computation, each revealing different structural properties. Then we explore matrix inverses and why some matrices fundamentally cannot be inverted."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture3-multiplication.html#the-five-ways-of-matrix-multiplication",
    "href": "Math/MIT18.06/mit1806-lecture3-multiplication.html#the-five-ways-of-matrix-multiplication",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "The Five Ways of Matrix Multiplication",
    "text": "The Five Ways of Matrix Multiplication\nGiven \\(C = AB\\) where \\(A\\) is \\(m \\times n\\) and \\(B\\) is \\(n \\times p\\):\n\n1. Element-by-Element View (Row √ó Column)\nThe standard definition: each entry \\(C_{ij}\\) is the dot product of row \\(i\\) of \\(A\\) with column \\(j\\) of \\(B\\):\n\\[\nC_{ij} = \\sum_{k=1}^n A_{ik} B_{kj}\n\\]\nExample: \\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n=\n\\begin{bmatrix} 1 \\cdot 5+2 \\cdot 7 & 1 \\cdot 6+2 \\cdot 8 \\\\ 3 \\cdot 5+4 \\cdot 7 & 3 \\cdot 6+4 \\cdot 8 \\end{bmatrix}\n=\n\\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n\\]\n\n\n2. Column View\nKey insight: Each column of \\(C\\) is a linear combination of the columns of \\(A\\).\n\\[\nC = A \\begin{bmatrix} | & | & & | \\\\ b_1 & b_2 & \\cdots & b_p \\\\ | & | & & | \\end{bmatrix}\n= \\begin{bmatrix} | & | & & | \\\\ Ab_1 & Ab_2 & \\cdots & Ab_p \\\\ | & | & & | \\end{bmatrix}\n\\]\nInterpretation: Multiply \\(A\\) by each column of \\(B\\) to get each column of \\(C\\).\nThis perspective is crucial for understanding: - The column space of \\(C\\) lies in the column space of \\(A\\) - Matrix-vector multiplication \\(Ax\\) as a linear combination of \\(A\\)‚Äôs columns\n\n\n3. Row View\nKey insight: Each row of \\(C\\) is a linear combination of the rows of \\(B\\).\n\\[\nC = \\begin{bmatrix} ‚Äî & a_1 & ‚Äî \\\\ ‚Äî & a_2 & ‚Äî \\\\ & \\vdots & \\\\ ‚Äî & a_m & ‚Äî \\end{bmatrix} B\n= \\begin{bmatrix} ‚Äî & a_1B & ‚Äî \\\\ ‚Äî & a_2B & ‚Äî \\\\ & \\vdots & \\\\ ‚Äî & a_mB & ‚Äî \\end{bmatrix}\n\\]\nInterpretation: Each row of \\(A\\) multiplies the entire matrix \\(B\\) to produce a row of \\(C\\).\nThis shows that the row space of \\(C\\) lies in the row space of \\(B\\).\n\n\n4. Column √ó Row View (Sum of Rank-1 Matrices)\nThe most powerful perspective:\n\\[\nAB = \\sum_{k=1}^n (\\text{column } k \\text{ of } A) \\times (\\text{row } k \\text{ of } B)\n\\]\nEach term is a rank-1 matrix (outer product of a column vector with a row vector).\nExample: \\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n=\n\\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\\begin{bmatrix} 5 & 6 \\end{bmatrix}\n+\n\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\n\\begin{bmatrix} 7 & 8 \\end{bmatrix}\n\\]\n\\[\n=\n\\begin{bmatrix} 5 & 6 \\\\ 15 & 18 \\end{bmatrix}\n+\n\\begin{bmatrix} 14 & 16 \\\\ 28 & 32 \\end{bmatrix}\n=\n\\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n\\]\nWhy this matters: - Each rank-1 matrix has all rows as multiples of each other - Matrix multiplication is a sum of simple, structured pieces - Foundation for understanding matrix rank and decompositions (SVD, eigendecomposition)\n\n\n5. Block Multiplication\nMatrices can be partitioned into blocks and multiplied block-wise:\n\\[\n\\begin{bmatrix} A_1 & A_2 \\\\ A_3 & A_4 \\end{bmatrix}\n\\begin{bmatrix} B_1 & B_2 \\\\ B_3 & B_4 \\end{bmatrix}\n=\n\\begin{bmatrix} A_1B_1 + A_2B_3 & A_1B_2 + A_2B_4 \\\\ A_3B_1 + A_4B_3 & A_3B_2 + A_4B_4 \\end{bmatrix}\n\\]\nCondition: Block dimensions must be compatible for multiplication.\nApplications: - Efficient computation for sparse or structured matrices - Recursive algorithms for large matrices - Parallel computing strategies"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture3-multiplication.html#matrix-inverse",
    "href": "Math/MIT18.06/mit1806-lecture3-multiplication.html#matrix-inverse",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "Matrix Inverse",
    "text": "Matrix Inverse\n\nDefinition\nFor a square matrix \\(A\\), if there exists a matrix \\(A^{-1}\\) such that:\n\\[\nA^{-1}A = I \\quad \\text{and} \\quad AA^{-1} = I\n\\]\nthen \\(A\\) is invertible (or non-singular).\n\n\nWhen Does an Inverse NOT Exist?\nA matrix \\(A\\) has no inverse if any of these equivalent conditions hold:\n\nZero determinant: \\(\\det(A) = 0\\)\nDependent columns: Some column is a linear combination of others\nNon-trivial null space: \\(Ax = 0\\) for some non-zero \\(x\\)\n\n\n\nWhy Singular Matrices Have No Inverse: A Proof\nConsider the singular matrix:\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix}\n\\]\nObservation: The second column is 2√ó the first (columns are dependent).\nWe can find \\(x = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} \\neq 0\\) such that:\n\\[\nAx = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n\\]\nProof by contradiction:\nSuppose \\(A^{-1}\\) exists. Then:\n\nWe have \\(Ax = 0\\) where \\(x \\neq 0\\)\nMultiply both sides by \\(A^{-1}\\): \\(A^{-1}(Ax) = A^{-1} \\cdot 0\\)\nLeft side simplifies: \\(A^{-1}(Ax) = (A^{-1}A)x = Ix = x\\)\nRight side: \\(A^{-1} \\cdot 0 = 0\\)\nTherefore: \\(x = 0\\)\n\nContradiction! We started with \\(x = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} \\neq 0\\), but our logic says \\(x = 0\\).\nThis proves \\(A^{-1}\\) cannot exist. The fundamental issue: if \\(A\\) maps different inputs to the same output (like both \\(x\\) and \\(0\\) to \\(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)), we cannot uniquely reverse the operation.\n\n\nComputing the Inverse\nFor 2√ó2 matrices:\n\\[\nA = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n\\quad \\Rightarrow \\quad\nA^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\\]\nNote: \\(ad - bc\\) is the determinant. If \\(\\det(A) = 0\\), no inverse exists.\nFor larger matrices, use Gauss-Jordan elimination:\n\\[\n[A | I] \\xrightarrow{\\text{row operations}} [I | A^{-1}]\n\\]\nStart with \\(A\\) augmented with the identity matrix, then use row operations to transform \\(A\\) into \\(I\\). The same operations transform \\(I\\) into \\(A^{-1}\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture3-multiplication.html#key-takeaways",
    "href": "Math/MIT18.06/mit1806-lecture3-multiplication.html#key-takeaways",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nMatrix multiplication has five perspectives:\n\nElement-by-element (computational)\nColumn view (output columns from input columns)\nRow view (output rows from input rows)\nSum of rank-1 matrices (most insightful)\nBlock multiplication (practical for large matrices)\n\nMatrix inverse exists if and only if:\n\nDeterminant is non-zero\nColumns are independent\n\\(Ax = 0\\) only when \\(x = 0\\)\n\nWhy this matters:\n\nSolving \\(Ax = b\\): if \\(A^{-1}\\) exists, then \\(x = A^{-1}b\\)\nHowever, Gaussian elimination is usually more efficient than computing \\(A^{-1}\\)\nUnderstanding when inverses don‚Äôt exist reveals the structure of linear transformations\n\n\nThe rank-1 decomposition (view 4) is particularly powerful‚Äîit‚Äôs the foundation for understanding matrix rank, the Four Fundamental Subspaces, and advanced topics like Singular Value Decomposition."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture3-multiplication.html#exercises",
    "href": "Math/MIT18.06/mit1806-lecture3-multiplication.html#exercises",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "Exercises",
    "text": "Exercises\n\nExample 1: Matrix Multiplication (All Five Perspectives)\n\n\nShow code\nimport numpy as np\nfrom IPython.display import display, Markdown, Latex\n\ndef matrix_to_latex(M, name=\"\"):\n    \"\"\"Convert numpy matrix to LaTeX bmatrix format\"\"\"\n    if len(M.shape) == 1:\n        M = M.reshape(-1, 1)\n    rows = [\" & \".join([f\"{x:.0f}\" if x == int(x) else f\"{x:.2f}\" for x in row]) for row in M]\n    latex_str = r\"\\begin{bmatrix}\" + r\" \\\\ \".join(rows) + r\"\\end{bmatrix}\"\n    if name:\n        latex_str = f\"{name} = \" + latex_str\n    return latex_str\n\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\nC = A @ B\n\ndisplay(Markdown(\"**Given matrices:**\"))\ndisplay(Latex(f\"$${matrix_to_latex(A, 'A')}$$\"))\ndisplay(Latex(f\"$${matrix_to_latex(B, 'B')}$$\"))\n\n\nGiven matrices:\n\n\n\\[A = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}\\]\n\n\n\\[B = \\begin{bmatrix}5 & 6 \\\\ 7 & 8\\end{bmatrix}\\]\n\n\nMethod 1: Element-by-element\n\n\nShow code\ndisplay(Latex(f\"$${matrix_to_latex(C, 'C = AB')}$$\"))\ndisplay(Latex(f\"$$C_{{11}} = (1)(5) + (2)(7) = {C[0,0]}$$\"))\ndisplay(Latex(f\"$$C_{{12}} = (1)(6) + (2)(8) = {C[0,1]}$$\"))\ndisplay(Latex(f\"$$C_{{21}} = (3)(5) + (4)(7) = {C[1,0]}$$\"))\ndisplay(Latex(f\"$$C_{{22}} = (3)(6) + (4)(8) = {C[1,1]}$$\"))\n\n\n\\[C = AB = \\begin{bmatrix}19 & 22 \\\\ 43 & 50\\end{bmatrix}\\]\n\n\n\\[C_{11} = (1)(5) + (2)(7) = 19\\]\n\n\n\\[C_{12} = (1)(6) + (2)(8) = 22\\]\n\n\n\\[C_{21} = (3)(5) + (4)(7) = 43\\]\n\n\n\\[C_{22} = (3)(6) + (4)(8) = 50\\]\n\n\nMethod 2: Column view\n\n\nShow code\ncol1 = A @ B[:, 0]\ncol2 = A @ B[:, 1]\n\ndisplay(Markdown(\"Column 1 of C is a linear combination of A's columns:\"))\ndisplay(Latex(f\"$$A \\\\begin{{bmatrix}} 5 \\\\\\\\ 7 \\\\end{{bmatrix}} = {matrix_to_latex(col1)}$$\"))\n\ndisplay(Markdown(\"Column 2 of C is a linear combination of A's columns:\"))\ndisplay(Latex(f\"$$A \\\\begin{{bmatrix}} 6 \\\\\\\\ 8 \\\\end{{bmatrix}} = {matrix_to_latex(col2)}$$\"))\n\n\nColumn 1 of C is a linear combination of A‚Äôs columns:\n\n\n\\[A \\begin{bmatrix} 5 \\\\ 7 \\end{bmatrix} = \\begin{bmatrix}19 \\\\ 43\\end{bmatrix}\\]\n\n\nColumn 2 of C is a linear combination of A‚Äôs columns:\n\n\n\\[A \\begin{bmatrix} 6 \\\\ 8 \\end{bmatrix} = \\begin{bmatrix}22 \\\\ 50\\end{bmatrix}\\]\n\n\nMethod 3: Row view\n\n\nShow code\nrow1 = A[0, :] @ B\nrow2 = A[1, :] @ B\n\ndisplay(Markdown(\"Row 1 of C is a linear combination of B's rows:\"))\ndisplay(Latex(f\"$$\\\\begin{{bmatrix}} 1 & 2 \\\\end{{bmatrix}} B = {matrix_to_latex(row1.reshape(1, -1))}$$\"))\n\ndisplay(Markdown(\"Row 2 of C is a linear combination of B's rows:\"))\ndisplay(Latex(f\"$$\\\\begin{{bmatrix}} 3 & 4 \\\\end{{bmatrix}} B = {matrix_to_latex(row2.reshape(1, -1))}$$\"))\n\n\nRow 1 of C is a linear combination of B‚Äôs rows:\n\n\n\\[\\begin{bmatrix} 1 & 2 \\end{bmatrix} B = \\begin{bmatrix}19 & 22\\end{bmatrix}\\]\n\n\nRow 2 of C is a linear combination of B‚Äôs rows:\n\n\n\\[\\begin{bmatrix} 3 & 4 \\end{bmatrix} B = \\begin{bmatrix}43 & 50\\end{bmatrix}\\]\n\n\nMethod 4: Sum of rank-1 matrices (most powerful!)\n\n\nShow code\nrank1_1 = np.outer(A[:, 0], B[0, :])\nrank1_2 = np.outer(A[:, 1], B[1, :])\n\ndisplay(Markdown(\"**Rank-1 term 1:**\"))\ndisplay(Latex(f\"$$\\\\begin{{bmatrix}} 1 \\\\\\\\ 3 \\\\end{{bmatrix}} \\\\begin{{bmatrix}} 5 & 6 \\\\end{{bmatrix}} = {matrix_to_latex(rank1_1)}$$\"))\n\ndisplay(Markdown(\"**Rank-1 term 2:**\"))\ndisplay(Latex(f\"$$\\\\begin{{bmatrix}} 2 \\\\\\\\ 4 \\\\end{{bmatrix}} \\\\begin{{bmatrix}} 7 & 8 \\\\end{{bmatrix}} = {matrix_to_latex(rank1_2)}$$\"))\n\ndisplay(Markdown(\"**Sum of rank-1 matrices:**\"))\ndisplay(Latex(f\"$${matrix_to_latex(rank1_1)} + {matrix_to_latex(rank1_2)} = {matrix_to_latex(rank1_1 + rank1_2)}$$\"))\n\ndisplay(Markdown(\"‚úì **All methods give the same result!**\"))\n\n\nRank-1 term 1:\n\n\n\\[\\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} \\begin{bmatrix} 5 & 6 \\end{bmatrix} = \\begin{bmatrix}5 & 6 \\\\ 15 & 18\\end{bmatrix}\\]\n\n\nRank-1 term 2:\n\n\n\\[\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} \\begin{bmatrix} 7 & 8 \\end{bmatrix} = \\begin{bmatrix}14 & 16 \\\\ 28 & 32\\end{bmatrix}\\]\n\n\nSum of rank-1 matrices:\n\n\n\\[\\begin{bmatrix}5 & 6 \\\\ 15 & 18\\end{bmatrix} + \\begin{bmatrix}14 & 16 \\\\ 28 & 32\\end{bmatrix} = \\begin{bmatrix}19 & 22 \\\\ 43 & 50\\end{bmatrix}\\]\n\n\n‚úì All methods give the same result!\n\n\n\n\nExample 2: Matrix Inverse\nInvertible matrix:\n\n\nShow code\nA_inv = np.array([[2, 1], [5, 3]])\nA_inv_inverse = np.linalg.inv(A_inv)\ndet_A = np.linalg.det(A_inv)\n\ndisplay(Latex(f\"$${matrix_to_latex(A_inv, 'A')}$$\"))\ndisplay(Latex(f\"$$\\\\det(A) = {det_A:.0f} \\\\neq 0 \\\\quad \\\\checkmark \\\\text{{ Invertible!}}$$\"))\ndisplay(Latex(f\"$${matrix_to_latex(A_inv_inverse, 'A^{-1}')}$$\"))\n\ndisplay(Markdown(\"**Verification:**\"))\nidentity = A_inv @ A_inv_inverse\ndisplay(Latex(f\"$$A A^{{-1}} = {matrix_to_latex(np.round(identity, 10), '')} = I$$\"))\n\n\n\\[A = \\begin{bmatrix}2 & 1 \\\\ 5 & 3\\end{bmatrix}\\]\n\n\n\\[\\det(A) = 1 \\neq 0 \\quad \\checkmark \\text{ Invertible!}\\]\n\n\n\\[A^{-1} = \\begin{bmatrix}3.00 & -1.00 \\\\ -5.00 & 2.00\\end{bmatrix}\\]\n\n\nVerification:\n\n\n\\[A A^{-1} = \\begin{bmatrix}1 & 0 \\\\ -0 & 1\\end{bmatrix} = I\\]\n\n\nSingular matrix (no inverse):\n\n\nShow code\nA_singular = np.array([[1, 2], [2, 4]])\ndet_singular = np.linalg.det(A_singular)\n\ndisplay(Latex(f\"$${matrix_to_latex(A_singular, 'A')}$$\"))\ndisplay(Latex(f\"$$\\\\det(A) = 0 \\\\quad \\\\times \\\\text{{ Singular!}}$$\"))\n\ndisplay(Markdown(\"**Observation:** Column 2 = 2 √ó Column 1 (linearly dependent)\"))\ndisplay(Latex(f\"$${matrix_to_latex(A_singular[:, 1])} = 2 \\\\times {matrix_to_latex(A_singular[:, 0])}$$\"))\n\nx = np.array([2, -1])\nAx = A_singular @ x\n\ndisplay(Markdown(f\"**Non-zero vector satisfying $Ax = 0$:**\"))\ndisplay(Latex(f\"$$A {matrix_to_latex(x, 'x')} = {matrix_to_latex(Ax)} = 0$$\"))\ndisplay(Markdown(\"‚üπ **No inverse exists** (proven by contradiction earlier)\"))\n\n\n\\[A = \\begin{bmatrix}1 & 2 \\\\ 2 & 4\\end{bmatrix}\\]\n\n\n\\[\\det(A) = 0 \\quad \\times \\text{ Singular!}\\]\n\n\nObservation: Column 2 = 2 √ó Column 1 (linearly dependent)\n\n\n\\[\\begin{bmatrix}2 \\\\ 4\\end{bmatrix} = 2 \\times \\begin{bmatrix}1 \\\\ 2\\end{bmatrix}\\]\n\n\nNon-zero vector satisfying \\(Ax = 0\\):\n\n\n\\[A x = \\begin{bmatrix}2 \\\\ -1\\end{bmatrix} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix} = 0\\]\n\n\n‚üπ No inverse exists (proven by contradiction earlier)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-3-spaces.html",
    "href": "Math/MIT18.06/mit1806-lecture5-3-spaces.html",
    "title": "MIT 18.06SC Lecture 5.3: Vector Spaces",
    "section": "",
    "text": "My lecture notes\nVector spaces and subspaces are fundamental structures in linear algebra. This post covers the vector spaces portion of Lecture 5."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#context",
    "href": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#context",
    "title": "MIT 18.06SC Lecture 5.3: Vector Spaces",
    "section": "",
    "text": "My lecture notes\nVector spaces and subspaces are fundamental structures in linear algebra. This post covers the vector spaces portion of Lecture 5."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#definition",
    "href": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#definition",
    "title": "MIT 18.06SC Lecture 5.3: Vector Spaces",
    "section": "Definition",
    "text": "Definition\nThe space \\(\\mathbb{R}^n\\) has exactly n dimensions.\nExample: \\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\] is a matrix in \\(\\mathbb{R}^{2 \\times 2}\\) (2√ó2 real matrices)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#subspaces",
    "href": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#subspaces",
    "title": "MIT 18.06SC Lecture 5.3: Vector Spaces",
    "section": "Subspaces",
    "text": "Subspaces\nA subspace is a subset of \\(\\mathbb{R}^n\\) that is closed under addition and scalar multiplication.\n\nExamples of Subspaces\nIn \\(\\mathbb{R}^2\\): - A line through the origin creates a subspace\nIn \\(\\mathbb{R}^3\\): - A line through the origin creates a line subspace - A plane through the origin creates a plane subspace\nIn \\(\\mathbb{R}^4\\): - Think: How can we create subspaces in \\(\\mathbb{R}^4\\)?"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#properties-of-subspaces",
    "href": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#properties-of-subspaces",
    "title": "MIT 18.06SC Lecture 5.3: Vector Spaces",
    "section": "Properties of Subspaces",
    "text": "Properties of Subspaces\nA subspace must satisfy two properties:\n\n1. Closed Under Addition\nIf vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) are in the subspace, then \\(\\vec{v} + \\vec{w}\\) is also in the subspace.\nExample: For \\(\\mathbb{R}^2\\) line \\(y=2x\\): - \\([1,2]\\) and \\([2,4]\\) are in the line - \\([1,2] + [2,4] = [3,6]\\) is still in the line ‚úì\n\n\n2. Closed Under Scalar Multiplication\nIf vector \\(\\vec{v}\\) is in the subspace and \\(c\\) is any scalar, then \\(c\\vec{v}\\) is also in the subspace.\nExample: For \\(\\mathbb{R}^2\\) line \\(y=2x\\): - \\([1,2]\\) is in the line - \\(1.5 \\times [1,2] = [1.5, 3]\\) is still in the line ‚úì\n\nSource: MIT 18.06SC Linear Algebra, Lecture 5"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html",
    "href": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html",
    "title": "MIT 18.06SC Lecture 9: Independence, Basis, and Dimension",
    "section": "",
    "text": "My lecture notes\nThis lecture develops three fundamental concepts: linear independence (no redundancy), basis (minimal spanning set), and dimension (fundamental measure of vector space size). These concepts unify our understanding of vector spaces through the rank-nullity theorem."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#context",
    "href": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#context",
    "title": "MIT 18.06SC Lecture 9: Independence, Basis, and Dimension",
    "section": "",
    "text": "My lecture notes\nThis lecture develops three fundamental concepts: linear independence (no redundancy), basis (minimal spanning set), and dimension (fundamental measure of vector space size). These concepts unify our understanding of vector spaces through the rank-nullity theorem."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#linear-independence",
    "href": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#linear-independence",
    "title": "MIT 18.06SC Lecture 9: Independence, Basis, and Dimension",
    "section": "Linear Independence",
    "text": "Linear Independence\n\nDefinition\nVectors \\(x_1, x_2, \\ldots, x_n\\) are linearly independent if the only linear combination that produces the zero vector is the trivial combination (all coefficients zero):\n\\[\nc_1x_1 + c_2x_2 + \\cdots + c_nx_n = 0 \\quad \\Rightarrow \\quad c_1 = c_2 = \\cdots = c_n = 0\n\\]\nEquivalently, vectors are dependent if there exists a non-trivial combination (some \\(c_i \\neq 0\\)) that gives zero:\n\\[\nc_1x_1 + c_2x_2 + \\cdots + c_nx_n = 0 \\quad \\text{with some } c_i \\neq 0\n\\]\n\n\nWhen Are Vectors Dependent?\nCase 1: Zero vector present\nIf one of the vectors is \\(\\vec{0}\\), the vectors are automatically dependent: \\[\nr \\cdot \\vec{0} + 0 \\cdot x_1 + \\cdots + 0 \\cdot x_n = 0 \\quad \\text{(non-trivial combination)}\n\\]\nCase 2: Vectors in the same direction (collinear)\nIf any two vectors are scalar multiples of each other, the set is dependent.\n\n\n\nCollinear vectors\n\n\nExample: Vectors \\(v_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) and \\(v_2 = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\\) are dependent because: \\[\n2v_1 - v_2 = 2\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n\\]\nGeneral case: If \\(v_2 = cv_1\\) for some scalar \\(c\\), then: \\[\ncv_1 - v_2 = 0 \\quad \\text{(non-trivial combination)}\n\\]\nCase 3: More vectors than dimensions (\\(n &gt; m\\))\nIf we have \\(n\\) vectors in \\(\\mathbb{R}^m\\) with \\(n &gt; m\\), and the first \\(m\\) vectors are linearly independent (not collinear/coplanar), then all \\(n\\) vectors must be dependent.\nExample in \\(\\mathbb{R}^2\\): Consider three vectors \\(x_1, x_2, x_3\\) in the plane where \\(x_1\\) and \\(x_2\\) are not collinear.\n\n\n\nThree vectors in plane\n\n\n\nSince \\(x_1\\) and \\(x_2\\) are linearly independent, their combinations \\(c_1x_1 + c_2x_2\\) span the entire plane\nTherefore, \\(x_3\\) can be expressed as some combination of \\(x_1\\) and \\(x_2\\)\nThis means we can find \\(c_1, c_2\\) such that \\(c_1x_1 + c_2x_2 = x_3\\)\nRearranging: \\(c_1x_1 + c_2x_2 - x_3 = 0\\) (a non-trivial combination equals zero)\nThus \\(x_1, x_2, x_3\\) are dependent\n\n\n\nNull Space Interpretation\nTo test independence of vectors \\(v_1, v_2, \\ldots, v_n\\), form the matrix: \\[\nA = [v_1 \\mid v_2 \\mid \\cdots \\mid v_n]\n\\]\nThe equation \\(c_1v_1 + c_2v_2 + \\cdots + c_nv_n = 0\\) becomes \\(Ax = 0\\) where \\(x = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}\\).\nVectors are independent if and only if: - \\(N(A) = \\{\\vec{0}\\}\\) (null space contains only zero vector) - \\(\\text{rank}(A) = n\\) (full column rank) - No free variables\nVectors are dependent if and only if: - \\(N(A)\\) contains non-zero vectors - \\(\\text{rank}(A) &lt; n\\) (rank deficient) - Has free variables (\\(n - r &gt; 0\\))\nExample: For 3 vectors in \\(\\mathbb{R}^2\\) (underdetermined system with \\(m = 2 &lt; n = 3\\)): - We have \\(r = 2 = m &lt; n = 3\\) - Number of free variables: \\(n - r = 3 - 2 = 1\\) - \\(N(A)\\) has infinitely many solutions - Therefore, the vectors are dependent"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#spanning-sets",
    "href": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#spanning-sets",
    "title": "MIT 18.06SC Lecture 9: Independence, Basis, and Dimension",
    "section": "Spanning Sets",
    "text": "Spanning Sets\n\nDefinition\nVectors \\(v_1, v_2, \\ldots, v_l\\) span a space \\(S\\) if every vector in \\(S\\) can be written as a linear combination of \\(v_1, \\ldots, v_l\\):\n\\[\nS = \\text{span}(v_1, \\ldots, v_l) = \\{c_1v_1 + c_2v_2 + \\cdots + c_lv_l \\mid c_i \\in \\mathbb{R}\\}\n\\]\nInterpretation: The span is the set of all possible linear combinations of the vectors.\n\n\n\nSpan illustration"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#basis",
    "href": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#basis",
    "title": "MIT 18.06SC Lecture 9: Independence, Basis, and Dimension",
    "section": "Basis",
    "text": "Basis\n\nDefinition\nA basis for a vector space \\(S\\) is a sequence of vectors \\(v_1, v_2, \\ldots, v_n\\) that satisfies two properties:\n\nIndependence: The vectors are linearly independent\n\nEnsures no redundancy (can‚Äôt remove any vector)\nRank = number of vectors\n\nSpanning: The vectors span the space \\(S\\)\n\nEvery vector in \\(S\\) can be expressed as a combination\nRank = dimension of space\n\n\nKey insight: A basis is a minimal spanning set (independent) and a maximal independent set (spanning).\n\n\nStandard Basis for \\(\\mathbb{R}^n\\)\nFor the space \\(\\mathbb{R}^n\\), the standard basis consists of \\(n\\) vectors.\nExample in \\(\\mathbb{R}^3\\): \\[\n\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad\n\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\n\\]\nThese are the columns of the identity matrix \\(I_3\\).\n\n\nProperties of Bases\n\nEvery vector has a unique representation: If \\(\\{v_1, \\ldots, v_n\\}\\) is a basis for \\(S\\), then every \\(x \\in S\\) can be written uniquely as: \\[\nx = c_1v_1 + c_2v_2 + \\cdots + c_nv_n\n\\]\nAll bases have the same size: Any two bases for the same space have the same number of vectors (this number is the dimension)\nBasis matrix is invertible: If we form a matrix \\(B = [v_1 \\mid \\cdots \\mid v_n]\\) where \\(\\{v_1, \\ldots, v_n\\}\\) is a basis for \\(\\mathbb{R}^n\\), then \\(B\\) is square (\\(n \\times n\\)) and invertible"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#dimension",
    "href": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#dimension",
    "title": "MIT 18.06SC Lecture 9: Independence, Basis, and Dimension",
    "section": "Dimension",
    "text": "Dimension\n\nDefinition\nThe dimension of a vector space \\(S\\) is the number of vectors in any basis for \\(S\\).\nNotation: \\(\\dim(S)\\)\n\n\nDimension Formulas\nFor an \\(m \\times n\\) matrix \\(A\\) with rank \\(r\\):\n\nDimension of column space: \\[\n\\dim(C(A)) = r\n\\] The pivot columns form a basis for \\(C(A)\\).\nDimension of null space (also called nullity): \\[\n\\dim(N(A)) = n - r\n\\] The number of special solutions equals the number of free variables.\nRank-Nullity Theorem: \\[\n\\dim(C(A)) + \\dim(N(A)) = n\n\\] or equivalently: \\[\nr + (n - r) = n\n\\]\n\n\n\nExamples\nExample 1: Matrix with full column rank \\[\nA = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\\\ 3 & 2 \\end{bmatrix}, \\quad r = 2, \\, m = 3, \\, n = 2\n\\]\n\n\\(\\dim(C(A)) = 2\\) (columns are independent, they form a basis)\n\\(\\dim(N(A)) = n - r = 2 - 2 = 0\\) (only zero vector in null space)\n\nExample 2: Rank-deficient matrix \\[\nA = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{bmatrix}, \\quad r = 1, \\, m = 2, \\, n = 3\n\\]\n\n\\(\\dim(C(A)) = 1\\) (only one independent column)\n\\(\\dim(N(A)) = n - r = 3 - 1 = 2\\) (2-dimensional null space)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#summary",
    "title": "MIT 18.06SC Lecture 9: Independence, Basis, and Dimension",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\nConcept\nDefinition\nTest\n\n\n\n\nIndependence\nNo non-trivial combination gives zero\n\\(N(A) = \\{\\vec{0}\\}\\) or \\(r = n\\)\n\n\nSpanning\nAll vectors in space are combinations\n\\(C(A) = S\\) or \\(r = \\dim(S)\\)\n\n\nBasis\nIndependent + Spanning\n\\(r = n = \\dim(S)\\)\n\n\nDimension\nNumber of vectors in basis\n\\(\\dim(C(A)) = r\\), \\(\\dim(N(A)) = n - r\\)\n\n\n\nKey relationships: - Independence prevents redundancy (no vector is a combination of others) - Spanning ensures completeness (every vector in the space is reachable) - Basis achieves both: minimal spanning set = maximal independent set - Dimension is the fundamental measure of ‚Äúsize‚Äù of a vector space\n\nSource: MIT 18.06SC Linear Algebra, Lecture 9"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I‚Äôm Chao Ma (aka ickma), a passionate developer and researcher focused on machine learning, algorithms, and problem-solving.\n\n\nI enjoy exploring the intersection of mathematics and computer science, with a particular interest in:\n\nü§ñ Machine Learning & AI - From fundamental concepts to practical implementations\nüßÆ Algorithms & Data Structures - Solving complex problems efficiently\n\nüìä Data Science - Extracting insights from data using Python and NumPy\nüíª Software Development - Building robust, scalable solutions\n\n\n\n\nThis site serves as my digital notebook where I share:\n\nAlgorithm explanations with visual examples and code implementations\nProblem-solving approaches for coding challenges and mathematical concepts\nTechnical insights from my learning journey\n\nI believe in learning by doing and explaining concepts clearly with code examples, visualizations, and mathematical foundations.\n\n\n\nMy content covers:\n\nAlgorithms - Dynamic programming, optimization, data structures, and algorithmic problem solving\nMathematics - Linear algebra, calculus, statistics, and mathematical foundations for CS\nReinforcement Learning & Deep Learning - Neural networks, policy optimization, and AI agents\nParallel Computation - Distributed systems, GPU computing, and performance optimization\n\n\n\n\nI‚Äôm always excited to discuss technology, collaborate on projects, or help fellow learners!\n\nüìß Email: ickma2311@gmail.com\nüíª GitHub: @ickma2311\nüê¶ Twitter: @ickma2311\n\nFeel free to reach out if you have questions about any of my posts, want to collaborate, or just want to chat about machine learning and algorithms!\n\nThis blog is built with Quarto and hosted on GitHub Pages. All code examples are available in my repositories."
  },
  {
    "objectID": "about.html#what-i-do",
    "href": "about.html#what-i-do",
    "title": "About",
    "section": "",
    "text": "I enjoy exploring the intersection of mathematics and computer science, with a particular interest in:\n\nü§ñ Machine Learning & AI - From fundamental concepts to practical implementations\nüßÆ Algorithms & Data Structures - Solving complex problems efficiently\n\nüìä Data Science - Extracting insights from data using Python and NumPy\nüíª Software Development - Building robust, scalable solutions"
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "This site serves as my digital notebook where I share:\n\nAlgorithm explanations with visual examples and code implementations\nProblem-solving approaches for coding challenges and mathematical concepts\nTechnical insights from my learning journey\n\nI believe in learning by doing and explaining concepts clearly with code examples, visualizations, and mathematical foundations."
  },
  {
    "objectID": "about.html#technical-focus",
    "href": "about.html#technical-focus",
    "title": "About",
    "section": "",
    "text": "My content covers:\n\nAlgorithms - Dynamic programming, optimization, data structures, and algorithmic problem solving\nMathematics - Linear algebra, calculus, statistics, and mathematical foundations for CS\nReinforcement Learning & Deep Learning - Neural networks, policy optimization, and AI agents\nParallel Computation - Distributed systems, GPU computing, and performance optimization"
  },
  {
    "objectID": "about.html#connect-with-me",
    "href": "about.html#connect-with-me",
    "title": "About",
    "section": "",
    "text": "I‚Äôm always excited to discuss technology, collaborate on projects, or help fellow learners!\n\nüìß Email: ickma2311@gmail.com\nüíª GitHub: @ickma2311\nüê¶ Twitter: @ickma2311\n\nFeel free to reach out if you have questions about any of my posts, want to collaborate, or just want to chat about machine learning and algorithms!\n\nThis blog is built with Quarto and hosted on GitHub Pages. All code examples are available in my repositories."
  },
  {
    "objectID": "ML/activation-functions.html",
    "href": "ML/activation-functions.html",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "",
    "text": "This exploration of Deep Learning Chapter 6.3 reveals how activation functions shape the behavior of hidden units in neural networks - and why choosing the right one matters.\nüìì For the complete implementation with additional exercises, see the notebook on GitHub.\nüìö For theoretical background and summary, see the chapter summary."
  },
  {
    "objectID": "ML/activation-functions.html#why-activation-functions-matter",
    "href": "ML/activation-functions.html#why-activation-functions-matter",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "Why Activation Functions Matter",
    "text": "Why Activation Functions Matter\nLinear transformations alone can only represent linear relationships. No matter how many layers you stack, \\(W_3(W_2(W_1x))\\) is still just a linear function. Activation functions introduce the non-linearity that makes deep learning powerful.\nBut which activation function should you use? The answer depends on understanding their mathematical properties and how they affect gradient flow during training.\n\n\n\n\n\n\n\n\n\nActivation\nOutput Range\nKey Property\nBest For\n\n\n\n\nReLU\n\\([0, \\infty)\\)\nZero for negatives\nHidden layers (default choice)\n\n\nSigmoid\n\\((0, 1)\\)\nSquashing, smooth\nBinary classification output\n\n\nTanh\n\\((-1, 1)\\)\nZero-centered\nHidden layers (when centering helps)"
  },
  {
    "objectID": "ML/activation-functions.html#exploring-activation-functions-shape-and-derivatives",
    "href": "ML/activation-functions.html#exploring-activation-functions-shape-and-derivatives",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "üéØ Exploring Activation Functions: Shape and Derivatives",
    "text": "üéØ Exploring Activation Functions: Shape and Derivatives\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Configure plotting\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['axes.grid'] = True\nplt.rcParams['grid.alpha'] = 0.3\n\n\nThe behavior of an activation function is determined by two things: 1. Its shape - how it transforms inputs 2. Its derivative - how gradients flow backward during training\n\nDefine Activation Functions\n\n\nShow code\ndef relu(x):\n    return np.clip(x, 0, np.inf)\n\ndef relu_derivative(x):\n    return np.where(x &gt; 0, 1, 0)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    return 1 - np.tanh(x)**2\n\n\n\n\nPlot Functions and Derivatives\n\n\nShow code\nx = np.linspace(-5, 5, 1000)\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 8))\nfig.suptitle('Common Activation Functions and Their Derivatives', fontsize=16)\n\n# ReLU\naxes[0, 0].plot(x, relu(x), linewidth=2, color='blue')\naxes[0, 0].set_title('ReLU', fontsize=12)\naxes[0, 0].set_ylabel('f(x)', fontsize=11)\naxes[1, 0].plot(x, relu_derivative(x), linewidth=2, color='blue')\naxes[1, 0].set_title('ReLU Derivative', fontsize=12)\naxes[1, 0].set_ylabel(\"f'(x)\", fontsize=11)\naxes[1, 0].set_xlabel('x', fontsize=11)\n\n# Sigmoid\naxes[0, 1].plot(x, sigmoid(x), linewidth=2, color='red')\naxes[0, 1].set_title('Sigmoid', fontsize=12)\naxes[1, 1].plot(x, sigmoid_derivative(x), linewidth=2, color='red')\naxes[1, 1].set_title('Sigmoid Derivative', fontsize=12)\naxes[1, 1].set_xlabel('x', fontsize=11)\n\n# Tanh\naxes[0, 2].plot(x, tanh(x), linewidth=2, color='green')\naxes[0, 2].set_title('Tanh', fontsize=12)\naxes[1, 2].plot(x, tanh_derivative(x), linewidth=2, color='green')\naxes[1, 2].set_title('Tanh Derivative', fontsize=12)\naxes[1, 2].set_xlabel('x', fontsize=11)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nKey observations:\n\nReLU: \\(f(x) = \\max(0, x)\\) - Zero for negative inputs, identity for positive. Derivative is 0 or 1 (simple!).\nSigmoid: \\(f(x) = \\frac{1}{1+e^{-x}}\\) - Squashes inputs to \\((0, 1)\\). Derivative peaks at 0, vanishes at extremes (gradient vanishing problem).\nTanh: \\(f(x) = \\tanh(x)\\) - Similar to sigmoid but outputs in \\((-1, 1)\\). Zero-centered with stronger gradients than sigmoid."
  },
  {
    "objectID": "ML/activation-functions.html#the-dead-relu-problem-when-neurons-stop-learning",
    "href": "ML/activation-functions.html#the-dead-relu-problem-when-neurons-stop-learning",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "The Dead ReLU Problem: When Neurons Stop Learning",
    "text": "The Dead ReLU Problem: When Neurons Stop Learning\nReLU‚Äôs simplicity is its strength, but also its weakness. A ReLU neuron can ‚Äúdie‚Äù - permanently outputting zero and never learning again.\nWhy does this happen?\nWhen a neuron‚Äôs pre-activation values are consistently negative (due to poor initialization, high learning rate, or bad gradients), ReLU outputs zero. Since the derivative is also zero for negative inputs, no gradient flows backward. The neuron is stuck forever.\n\n\nShow code\n# Generate input data\nx = torch.randn(1000, 10)  # 1000 samples, 10 features\nlinear = nn.Linear(10, 5)   # 5 hidden units\n\n# Set bias to large negative values to \"kill\" neurons\nwith torch.no_grad():\n    linear.bias.fill_(-10.0)\n\n# Forward pass\npre_activation = linear(x)\npost_activation = torch.relu(pre_activation)\n\n# Calculate statistics\ndead_percentage = (post_activation == 0).float().mean() * 100\nprint(f\"Percentage of dead neurons: {dead_percentage:.2f}%\\n\")\n\n# Display table showing ReLU input vs output\nprint(\"ReLU Input vs Output (first 10 samples, neuron 0):\")\nprint(\"-\" * 50)\nprint(f\"{'Sample':&lt;10} {'Pre-Activation':&lt;20} {'Post-Activation':&lt;20}\")\nprint(\"-\" * 50)\n\nfor i in range(10):\n    pre_val = pre_activation[i, 0].item()\n    post_val = post_activation[i, 0].item()\n    print(f\"{i:&lt;10} {pre_val:&lt;20.4f} {post_val:&lt;20.4f}\")\n\nprint(\"\\nObservation: All negative inputs become 0 after ReLU ‚Üí Dead neuron!\")\n\n\nPercentage of dead neurons: 100.00%\n\nReLU Input vs Output (first 10 samples, neuron 0):\n--------------------------------------------------\nSample     Pre-Activation       Post-Activation     \n--------------------------------------------------\n0          -9.7837              0.0000              \n1          -10.0322             0.0000              \n2          -10.4466             0.0000              \n3          -10.3243             0.0000              \n4          -10.5448             0.0000              \n5          -9.7712              0.0000              \n6          -10.8104             0.0000              \n7          -11.3418             0.0000              \n8          -9.8559              0.0000              \n9          -8.6873              0.0000              \n\nObservation: All negative inputs become 0 after ReLU ‚Üí Dead neuron!\n\n\nWith a large negative bias, every input becomes negative after the linear transformation. ReLU zeros them all out. The gradient is zero everywhere. The neuron never updates. It‚Äôs dead."
  },
  {
    "objectID": "ML/activation-functions.html#experiment-do-different-activations-make-a-difference",
    "href": "ML/activation-functions.html#experiment-do-different-activations-make-a-difference",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "Experiment: Do Different Activations Make a Difference?",
    "text": "Experiment: Do Different Activations Make a Difference?\nTheory is nice, but let‚Äôs see activation functions in action. We‚Äôll train three identical networks with different activations on a simple regression task: \\(y = \\sin(x) + x^2 + 1\\).\n\nGenerate Data\n\n\nShow code\n# Training data\nx_train = np.random.rand(200, 1)\ny_train = np.sin(x_train) + np.power(x_train, 2) + 1\n\n# Test data\nx_test = np.random.rand(50, 1)\ny_test = np.sin(x_test) + np.power(x_test, 2) + 1\n\n# Convert to PyTorch tensors\nx_train_tensor = torch.FloatTensor(x_train)\ny_train_tensor = torch.FloatTensor(y_train)\nx_test_tensor = torch.FloatTensor(x_test)\ny_test_tensor = torch.FloatTensor(y_test)\n\n\n\n\nCreate and Train Models\n\n\nShow code\ndef create_regression_model(activation_fn):\n    \"\"\"Create a 2-layer network with specified activation\"\"\"\n    return nn.Sequential(\n        nn.Linear(1, 20),\n        activation_fn,\n        nn.Linear(20, 1)\n    )\n\n# Create 3 models with different activations\nmodels = {\n    'ReLU': create_regression_model(nn.ReLU()),\n    'Sigmoid': create_regression_model(nn.Sigmoid()),\n    'Tanh': create_regression_model(nn.Tanh())\n}\n\n# Training configuration\nn_epochs = 100\nlearning_rate = 0.01\nloss_fn = nn.MSELoss()\n\n# Track metrics\nloss_history = {name: [] for name in models.keys()}\ntest_mse_history = {name: [] for name in models.keys()}\n\n# Train each model\nfor name, model in models.items():\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    for epoch in range(n_epochs):\n        # Training\n        model.train()\n        y_pred = model(x_train_tensor)\n        loss = loss_fn(y_pred, y_train_tensor)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loss_history[name].append(loss.item())\n\n        # Evaluation on test set\n        model.eval()\n        with torch.no_grad():\n            y_test_pred = model(x_test_tensor)\n            test_mse = loss_fn(y_test_pred, y_test_tensor).item()\n            test_mse_history[name].append(test_mse)\n\n\n\n\nCompare Learning Curves\n\n\nShow code\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\ncolors = {'ReLU': 'blue', 'Sigmoid': 'red', 'Tanh': 'green'}\n\n# Plot training loss\nfor name, losses in loss_history.items():\n    axes[0].plot(losses, label=name, linewidth=2, color=colors[name])\n\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Training Loss (MSE)', fontsize=12)\naxes[0].set_title('Training Loss Over Time', fontsize=14)\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\naxes[0].set_yscale('log')\n\n# Plot test MSE\nfor name, test_mse in test_mse_history.items():\n    axes[1].plot(test_mse, label=name, linewidth=2, color=colors[name])\n\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Test Loss (MSE)', fontsize=12)\naxes[1].set_title('Test Loss Over Time', fontsize=14)\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\naxes[1].set_yscale('log')\n\nplt.tight_layout()\nplt.show()\n\n# Print final metrics\nprint(\"\\nFinal Metrics after {} epochs:\".format(n_epochs))\nprint(\"-\" * 60)\nprint(f\"{'Activation':&lt;15} {'Train Loss':&lt;15} {'Test Loss':&lt;15}\")\nprint(\"-\" * 60)\nfor name in models.keys():\n    train_loss = loss_history[name][-1]\n    test_loss = test_mse_history[name][-1]\n    print(f\"{name:&lt;15} {train_loss:&lt;15.6f} {test_loss:&lt;15.6f}\")\n\n\n\n\n\n\n\n\n\n\nFinal Metrics after 100 epochs:\n------------------------------------------------------------\nActivation      Train Loss      Test Loss      \n------------------------------------------------------------\nReLU            0.007420        0.008211       \nSigmoid         0.227441        0.247947       \nTanh            0.035384        0.038743"
  },
  {
    "objectID": "ML/bagging-ensemble.html",
    "href": "ML/bagging-ensemble.html",
    "title": "Chapter 7.11: Bagging and Other Ensemble Methods",
    "section": "",
    "text": "Definition: Also known as model averaging, bagging trains several different models and combines their outputs through averaging or voting.\nKey idea: Train multiple models on slightly different datasets, then aggregate their predictions to reduce variance."
  },
  {
    "objectID": "ML/bagging-ensemble.html#bagging-bootstrap-aggregating",
    "href": "ML/bagging-ensemble.html#bagging-bootstrap-aggregating",
    "title": "Chapter 7.11: Bagging and Other Ensemble Methods",
    "section": "",
    "text": "Definition: Also known as model averaging, bagging trains several different models and combines their outputs through averaging or voting.\nKey idea: Train multiple models on slightly different datasets, then aggregate their predictions to reduce variance."
  },
  {
    "objectID": "ML/bagging-ensemble.html#mathematical-analysis-of-ensemble-error",
    "href": "ML/bagging-ensemble.html#mathematical-analysis-of-ensemble-error",
    "title": "Chapter 7.11: Bagging and Other Ensemble Methods",
    "section": "2. Mathematical Analysis of Ensemble Error",
    "text": "2. Mathematical Analysis of Ensemble Error\n\nSetup\nConsider an ensemble of \\(k\\) models with prediction errors:\n\nError of model \\(i\\): \\(\\epsilon_i\\)\nVariance: \\(\\mathbb{E}[\\epsilon_i^2] = v\\)\nCovariance: \\(\\mathbb{E}[\\epsilon_i \\epsilon_j] = c\\) (for \\(i \\neq j\\))\nNumber of models: \\(k\\)\n\n\n\nExpected Squared Error of Ensemble\nEquation 7.50 - Ensemble error analysis:\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\left(\\frac{1}{k} \\sum_i \\epsilon_i\\right)^2\\right] &= \\frac{1}{k^2}\\mathbb{E}\\left[\\sum_i \\left(\\epsilon_i^2 + \\sum_{i \\neq j} \\epsilon_i \\epsilon_j \\right) \\right] \\\\\n&= \\frac{1}{k}v + \\frac{k-1}{k}c\n\\end{aligned}\n\\]\nDerivation: - Expand the squared sum: \\((\\sum_i \\epsilon_i)^2 = \\sum_i \\epsilon_i^2 + \\sum_{i \\neq j} \\epsilon_i \\epsilon_j\\) - Take expectation: \\(\\mathbb{E}[\\epsilon_i^2] = v\\) and \\(\\mathbb{E}[\\epsilon_i \\epsilon_j] = c\\) - There are \\(k\\) terms with \\(\\epsilon_i^2\\) and \\(k(k-1)\\) terms with \\(\\epsilon_i \\epsilon_j\\) - Divide by \\(k^2\\) from the average\n\n\n\nAnalysis of Different Cases\nEquation 7.51 - Case 1: Perfectly correlated errors (\\(c = v\\))\n\\[\n\\frac{1}{k}v + \\frac{k-1}{k}v = \\frac{1}{k}v + v - \\frac{1}{k}v = v\n\\]\nInterpretation: The expected error equals \\(v\\), meaning bagging provides no benefit. Models make the same mistakes.\n\nCase 2: Uncorrelated errors (\\(c = 0\\))\n\\[\n\\frac{1}{k}v + \\frac{k-1}{k} \\cdot 0 = \\frac{1}{k}v\n\\]\nInterpretation: Expected error is \\(\\frac{1}{k}v\\). As \\(k\\) increases, error approaches zero. This is the ideal case for bagging.\n\nCase 3: Partially correlated errors (\\(0 &lt; c &lt; v\\))\n\\[\n\\frac{1}{k}v &lt; \\text{Expected Error} &lt; v\n\\]\nInterpretation: Error is between \\(\\frac{1}{k}v\\) and \\(v\\). Bagging provides some benefit, but not as much as the uncorrelated case.\n\nCase 4: Perfectly anti-correlated errors (\\(c &gt; v\\))\n\\[\n\\text{Not possible in practice}\n\\]\nReason: Covariance cannot exceed variance for prediction errors."
  },
  {
    "objectID": "ML/bagging-ensemble.html#training-approach",
    "href": "ML/bagging-ensemble.html#training-approach",
    "title": "Chapter 7.11: Bagging and Other Ensemble Methods",
    "section": "3. Training Approach",
    "text": "3. Training Approach\n\nStep 1: Bootstrap Sampling\nProcess: - From the original dataset, draw multiple new datasets with replacement - Each bootstrap dataset has the same size as the original - Contains some duplicated and some omitted examples - On average, each bootstrap dataset includes about two-thirds of the unique samples\n\n\n\nStep 2: Train Multiple Models\nProcess: - Train one model (ensemble member) on each bootstrap dataset - Because training data differ slightly, each model learns slightly different patterns - Each model makes different errors on different regions of input space\nEffect: Creates diversity among models, reducing correlation between errors.\n\n\n\nStep 3: Average Predictions\nInference: - All models predict on the same input - Final output is the average (regression) or majority vote (classification) of all predictions\nAggregation formulas:\nRegression:\n\\[\n\\hat{y} = \\frac{1}{k} \\sum_{i=1}^k f_i(x)\n\\]\nClassification:\n\\[\n\\hat{y} = \\operatorname{mode}\\{f_1(x), f_2(x), \\ldots, f_k(x)\\}\n\\]"
  },
  {
    "objectID": "ML/bagging-ensemble.html#example-digit-recognition",
    "href": "ML/bagging-ensemble.html#example-digit-recognition",
    "title": "Chapter 7.11: Bagging and Other Ensemble Methods",
    "section": "4. Example: Digit Recognition",
    "text": "4. Example: Digit Recognition\nOriginal dataset: Contains digits \\(\\{8, 6, 9\\}\\)\nBootstrap samples: - Bootstrap Sample 1 ‚Üí \\(\\{9, 6, 8\\}\\) - Bootstrap Sample 2 ‚Üí \\(\\{9, 9, 8\\}\\)\nTraining: - Each dataset trains a model to recognize the digit ‚Äú8‚Äù - Samples differ (one has more 9‚Äôs, another lacks 6‚Äôs) - Models learn different decision boundaries\nKey insight: - Individually, each model may be unreliable - Their average output is much more stable - Errors tend to cancel out through averaging\n\nSource: Deep Learning Book, Chapter 7.11"
  },
  {
    "objectID": "ML/multi-task-learning.html",
    "href": "ML/multi-task-learning.html",
    "title": "Chapter 7.7: Multi-Task Learning",
    "section": "",
    "text": "Multi-task learning trains a single model to perform multiple related tasks simultaneously by sharing representations across tasks. This approach:\n\nImproves generalization by learning shared features\nReduces overfitting through implicit regularization\nEnables knowledge transfer between related tasks"
  },
  {
    "objectID": "ML/multi-task-learning.html#overview",
    "href": "ML/multi-task-learning.html#overview",
    "title": "Chapter 7.7: Multi-Task Learning",
    "section": "",
    "text": "Multi-task learning trains a single model to perform multiple related tasks simultaneously by sharing representations across tasks. This approach:\n\nImproves generalization by learning shared features\nReduces overfitting through implicit regularization\nEnables knowledge transfer between related tasks"
  },
  {
    "objectID": "ML/multi-task-learning.html#concept",
    "href": "ML/multi-task-learning.html#concept",
    "title": "Chapter 7.7: Multi-Task Learning",
    "section": "1. Concept",
    "text": "1. Concept\n\nModel Architecture\nMulti-task learning uses a shared representation with task-specific outputs:\nTraining: \\[\n\\begin{aligned}\nh &= f(x; \\theta_{\\text{shared}}) \\\\\n\\hat{y}^{(t)} &= g(h; \\theta_t)\n\\end{aligned}\n\\]\nwhere:\n\n\\(h\\): Shared representation (common features learned across all tasks)\n\\(f(x; \\theta_{\\text{shared}})\\): Shared layers (e.g., CNN encoder, Transformer)\n\\(g(h; \\theta_t)\\): Task-specific head for task \\(t\\)\n\\(\\theta_{\\text{shared}}\\): Shared parameters\n\\(\\theta_t\\): Task-specific parameters\n\n\n\nLoss Function\nCombined loss: \\[\n\\mathcal{L} = \\sum_t \\lambda_t \\mathcal{L}_t(g(f(x; \\theta_{\\text{shared}}); \\theta_t))\n\\]\nwhere:\n\n\\(\\mathcal{L}_t\\): Loss function for task \\(t\\)\n\\(\\lambda_t\\): Weight for task \\(t\\) (controls importance)\nSum is over all tasks\n\nInterpretation: The model minimizes a weighted combination of task-specific losses, forcing the shared representation to be useful for all tasks.\n\n\n\nMulti-Task Learning Architecture"
  },
  {
    "objectID": "ML/multi-task-learning.html#benefit",
    "href": "ML/multi-task-learning.html#benefit",
    "title": "Chapter 7.7: Multi-Task Learning",
    "section": "2. Benefit",
    "text": "2. Benefit\nMulti-task learning improves generalization ability and reduces generalization error.\nWhy this works:\n\nShared representations: Common features learned from multiple tasks are more robust and general\nImplicit regularization: Training on multiple tasks prevents overfitting to any single task\nData efficiency: Each task benefits from the data of other tasks\nInductive bias: The model is encouraged to learn features that are useful across tasks\n\nExample:\n\nTraining on both face recognition and age estimation helps the model learn better facial features than training on either task alone\nThe shared features capture general facial characteristics useful for both tasks"
  },
  {
    "objectID": "ML/multi-task-learning.html#limitation",
    "href": "ML/multi-task-learning.html#limitation",
    "title": "Chapter 7.7: Multi-Task Learning",
    "section": "3. Limitation",
    "text": "3. Limitation\nMulti-task learning works only when the assumption that the tasks are related statistically holds.\nWhen it fails:\n\nUnrelated tasks: If tasks are not related, sharing representations can hurt performance\nNegative transfer: A poorly performing task can degrade the shared representation\nTask interference: Conflicting objectives can prevent convergence\n\nExamples of unrelated tasks:\n\nFace recognition + financial fraud detection (no shared structure)\nMedical diagnosis + game playing (different domains entirely)\n\nKey principle: Tasks should share some underlying structure or statistical properties for multi-task learning to be beneficial."
  },
  {
    "objectID": "ML/multi-task-learning.html#real-world-cases",
    "href": "ML/multi-task-learning.html#real-world-cases",
    "title": "Chapter 7.7: Multi-Task Learning",
    "section": "4. Real-World Cases",
    "text": "4. Real-World Cases\nNote: The following table is generated by ChatGPT.\n\n\n\n\n\n\n\n\n\n\nDomain\nTasks Learned Together\nShared Representation / Model\nPractical Benefit\nExample / Source\n\n\n\n\nFace Analysis\nFace recognition ¬∑ Age estimation ¬∑ Gender / Emotion classification\nShared CNN backbone (e.g., ResNet) with multiple output heads\nImproves accuracy and robustness by using shared facial features\nZhang et al., MTL-CNN for Face Analysis, CVPR 2014\n\n\nAutonomous Driving\nObject detection ¬∑ Lane segmentation ¬∑ Depth estimation\nShared encoder in perception network\nEnables one network to handle multiple perception tasks ‚Üí reduced compute & latency\nUber ATG, MultiNet, CVPR 2017\n\n\nMedical Imaging\nTumor segmentation ¬∑ Disease classification\nShared U-Net encoder with task-specific decoders\nCombines fine-grained segmentation and diagnosis ‚Üí less labeled data needed\nLiu et al., MT-UNet, MICCAI 2019\n\n\nSpeech Processing\nPhoneme recognition ¬∑ Speaker ID ¬∑ Emotion detection\nShared acoustic encoder (e.g., wav2vec backbone)\nImproves noise robustness and transfer learning across tasks\nBaevski et al., wav2vec 2.0, 2020\n\n\nNatural Language Processing\nPOS tagging ¬∑ NER ¬∑ Parsing ¬∑ Sentiment analysis\nShared Transformer encoder (e.g., BERT) with task-specific heads\nLearns richer linguistic features; boosts low-data tasks\nCollobert et al., Unified NLP with MTL, 2008; Devlin et al., BERT, 2019\n\n\nSearch & Recommendation\nClick prediction ¬∑ Conversion rate ¬∑ Dwell-time estimation\nShared user-embedding network\nCaptures user intent across tasks ‚Üí higher CTR and ranking precision\nGoogle Ads / YouTube Recommender Systems\n\n\nFinancial Risk Modeling\nCredit default ¬∑ Fraud detection ¬∑ Customer churn\nShared behavior-feature extractor\nReduces training cost, improves detection of rare events\nAnt Financial Research Team, 2020\n\n\nRobotics / Reinforcement Learning\nNavigation ¬∑ Object manipulation ¬∑ Balance control\nShared policy network or shared latent state\nLearns transferable motor skills across tasks\nDeepMind IMPALA (2018), Gato (2022)\n\n\n\n\nSource: Deep Learning Book (Goodfellow et al.), Chapter 7.7"
  },
  {
    "objectID": "ML/constrained-optimization-regularization.html",
    "href": "ML/constrained-optimization-regularization.html",
    "title": "Deep Learning Book Chapter 7.2: Constrained Optimization View of Regularization",
    "section": "",
    "text": "My lecture notes\nRegularization can be viewed as either adding a penalty term or enforcing a constraint. This post shows the equivalence between these two perspectives using Lagrange multipliers and dual optimization."
  },
  {
    "objectID": "ML/constrained-optimization-regularization.html#context",
    "href": "ML/constrained-optimization-regularization.html#context",
    "title": "Deep Learning Book Chapter 7.2: Constrained Optimization View of Regularization",
    "section": "",
    "text": "My lecture notes\nRegularization can be viewed as either adding a penalty term or enforcing a constraint. This post shows the equivalence between these two perspectives using Lagrange multipliers and dual optimization."
  },
  {
    "objectID": "ML/constrained-optimization-regularization.html#from-penalty-to-constraint",
    "href": "ML/constrained-optimization-regularization.html#from-penalty-to-constraint",
    "title": "Deep Learning Book Chapter 7.2: Constrained Optimization View of Regularization",
    "section": "From Penalty to Constraint",
    "text": "From Penalty to Constraint\nRegularization can be viewed from two equivalent perspectives:\n\nPenalty Form (Unconstrained)\nFormula 7.25: \\[\n\\tilde{J}(\\theta; X, y) = J(\\theta; X, y) + \\alpha \\Omega(\\theta)\n\\]\nThis adds a penalty term \\(\\alpha \\Omega(\\theta)\\) to the original loss, where \\(\\alpha\\) controls the strength of regularization.\n\n\nConstraint Form (Lagrangian)\nWe can equivalently express regularization as a constrained optimization problem requiring \\(\\Omega(\\theta) \\leq k\\).\nFormula 7.26 (Lagrangian): \\[\n\\mathcal{L}(\\theta, \\alpha; X, y) = J(\\theta; X, y) + \\alpha(\\Omega(\\theta) - k)\n\\]\nwhere \\(\\alpha \\geq 0\\) is the Lagrange multiplier.\nFormula 7.27 (Optimization Problem): \\[\n\\theta^* = \\arg\\min_{\\theta} \\max_{\\alpha \\geq 0} \\mathcal{L}(\\theta, \\alpha)\n\\]\nThis is equivalent to the constrained optimization: \\[\n\\theta^* = \\arg\\min_{\\theta} J(\\theta; X, y) \\quad \\text{subject to} \\quad \\Omega(\\theta) \\leq k\n\\]\nInterpretation: The solution corresponds to finding parameters \\(\\theta\\) that minimize the loss while satisfying the constraint. When \\(\\alpha\\) is large, it strongly enforces the constraint \\(\\Omega(\\theta) \\leq k\\)."
  },
  {
    "objectID": "ML/constrained-optimization-regularization.html#lagrange-multiplier-method",
    "href": "ML/constrained-optimization-regularization.html#lagrange-multiplier-method",
    "title": "Deep Learning Book Chapter 7.2: Constrained Optimization View of Regularization",
    "section": "Lagrange Multiplier Method",
    "text": "Lagrange Multiplier Method\nThe Lagrangian formulation transforms the problem into a min-max optimization: \\[\n\\min_{\\theta} \\max_{\\alpha \\geq 0} \\mathcal{L}(\\theta, \\alpha)\n\\]\n\nDual Direction Training\nInstead of a single gradient descent on \\(\\min \\mathcal{L}(\\theta)\\), we now have two opposing optimization directions:\n\\[\n\\begin{aligned}\n\\theta &\\downarrow \\quad \\text{(minimize w.r.t. } \\theta\\text{)} \\\\\n\\alpha &\\uparrow \\quad \\text{(maximize w.r.t. } \\alpha\\text{)}\n\\end{aligned}\n\\]\n\n\nTraining Dynamics\nThe training process balances two forces:\n\nWhen \\(\\Omega(\\theta) &gt; k\\) (constraint violated):\n\n\\(\\alpha\\) increases to penalize the violation\nLarger \\(\\alpha\\) pushes \\(\\theta\\) toward smaller norms\nThis enforces the constraint\n\nWhen \\(\\Omega(\\theta) &lt; k\\) (constraint satisfied):\n\n\\(\\alpha\\) decreases toward 0\nThe constraint is not active\nOptimization focuses on minimizing \\(J(\\theta)\\)\n\n\n\n\nSaddle Point Solution\nThe training converges to a saddle point satisfying the KKT (Karush-Kuhn-Tucker) conditions:\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\mathcal{L}(\\theta, \\alpha) &= 0 \\quad \\text{(stationarity)} \\\\\n\\alpha(\\Omega(\\theta) - k) &= 0 \\quad \\text{(complementary slackness)} \\\\\n\\alpha &\\geq 0 \\quad \\text{(dual feasibility)}\n\\end{aligned}\n\\]\nThe complementary slackness condition means: - Either \\(\\alpha = 0\\) (constraint inactive, \\(\\Omega(\\theta) &lt; k\\)) - Or \\(\\Omega(\\theta) = k\\) (constraint active, \\(\\alpha &gt; 0\\))"
  },
  {
    "objectID": "ML/constrained-optimization-regularization.html#penalty-vs-projection-methods",
    "href": "ML/constrained-optimization-regularization.html#penalty-vs-projection-methods",
    "title": "Deep Learning Book Chapter 7.2: Constrained Optimization View of Regularization",
    "section": "Penalty vs Projection Methods",
    "text": "Penalty vs Projection Methods\n\nWeight Norm Penalties (Soft Constraint)\nWhen using weight norm penalties such as L¬π or L¬≤ regularization: - The penalty term \\(\\alpha \\Omega(\\theta)\\) provides a ‚Äúsoft‚Äù constraint - The optimal solution may be locally optimal for the regularized objective \\(\\tilde{J}(\\theta)\\) - Even if increasing weights could reduce the original loss \\(J(\\theta)\\), the penalty prevents this - The regularization strength \\(\\alpha\\) determines how strictly the constraint is enforced\n\n\nExplicit Constraints (Hard Constraint)\nIn contrast, explicit constraint or projection methods enforce \\(\\Omega(\\theta) \\leq k\\) directly: - Provide a hard boundary on the weight norm - After each gradient step, project \\(\\theta\\) back onto the constraint set if needed - Often lead to more stable optimization with clearer geometric interpretation - The constraint is always exactly satisfied (not approximately)\n\n\nTrade-offs\n\n\n\nMethod\nConstraint Type\nStability\nFlexibility\n\n\n\n\nPenalty (L¬π/L¬≤)\nSoft\nGood\nHigh (tune \\(\\alpha\\))\n\n\nProjection\nHard\nVery Good\nLower (fixed \\(k\\))\n\n\n\nKey insight: Both approaches are equivalent in theory (there exists a correspondence between \\(\\alpha\\) and \\(k\\)), but in practice they may have different optimization properties and convergence behavior.\n\nSource: Deep Learning Book, Chapter 7.2"
  },
  {
    "objectID": "ML/xor-deep-learning.html",
    "href": "ML/xor-deep-learning.html",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.1 shows how ReLU activations let neural networks solve the XOR problem that defeats any linear model.\nüìì For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "ML/xor-deep-learning.html#the-xor-problem-a-challenge-for-linear-models",
    "href": "ML/xor-deep-learning.html#the-xor-problem-a-challenge-for-linear-models",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "The XOR Problem: A Challenge for Linear Models",
    "text": "The XOR Problem: A Challenge for Linear Models\nXOR (Exclusive OR) returns 1 precisely when the two binary inputs differ:\n\\[\\text{XOR}(x_1, x_2) = \\begin{pmatrix}0 & 1\\\\1 & 0\\end{pmatrix}\\]\nThe XOR truth table shows why this is challenging for linear models - the positive class (1) appears at diagonally opposite corners, making it impossible to separate with any single straight line.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Define XOR dataset\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([0, 1, 1, 0])\n\nprint(\"XOR Truth Table:\")\nprint(\"================\")\nprint()\nprint(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\nprint(\"‚îÇ Input   ‚îÇ Output ‚îÇ\")\nprint(\"‚îÇ (x‚ÇÅ,x‚ÇÇ) ‚îÇ  XOR   ‚îÇ\")\nprint(\"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\nfor i in range(4):\n    input_str = f\"({X[i,0]}, {X[i,1]})\"\n    output_str = f\"{y[i]}\"\n    print(f\"‚îÇ {input_str:7} ‚îÇ   {output_str:2}   ‚îÇ\")\nprint(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\nprint()\nprint(\"Notice: XOR = 1 when inputs differ, XOR = 0 when inputs match\")\n\n\nXOR Truth Table:\n================\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Input   ‚îÇ Output ‚îÇ\n‚îÇ (x‚ÇÅ,x‚ÇÇ) ‚îÇ  XOR   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ (0, 0)  ‚îÇ   0    ‚îÇ\n‚îÇ (0, 1)  ‚îÇ   1    ‚îÇ\n‚îÇ (1, 0)  ‚îÇ   1    ‚îÇ\n‚îÇ (1, 1)  ‚îÇ   0    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nNotice: XOR = 1 when inputs differ, XOR = 0 when inputs match"
  },
  {
    "objectID": "ML/xor-deep-learning.html#limitation-1-single-layer-linear-model",
    "href": "ML/xor-deep-learning.html#limitation-1-single-layer-linear-model",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Limitation 1: Single Layer Linear Model",
    "text": "Limitation 1: Single Layer Linear Model\nA single layer perceptron can only create linear decision boundaries. Let‚Äôs see what happens when we try to solve XOR with logistic regression:\n\n\nShow code\n# Demonstrate single layer linear model failure\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\ncolors = ['red', 'blue']\n\n# Plot XOR data\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'Class {i}', edgecolors='black', linewidth=2)\n\n# Overlay representative linear separators to illustrate the impossibility\nx_line = np.linspace(-0.2, 1.2, 100)\nax.plot(x_line, 0.5 * np.ones_like(x_line), '--', color='gray', alpha=0.7, label='candidate lines')\nax.plot(0.5 * np.ones_like(x_line), x_line, '--', color='orange', alpha=0.7)\nax.plot(x_line, x_line, '--', color='green', alpha=0.7)\nax.plot(x_line, 1 - x_line, '--', color='purple', alpha=0.7)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x‚ÇÅ', fontsize=12)\nax.set_ylabel('x‚ÇÇ', fontsize=12)\nax.set_title('XOR Problem: No Linear Solution', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Fit logistic regression just to report its performance\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\naccuracy = log_reg.score(X, y)\nprint(f'Single layer model accuracy: {accuracy:.1%} - still misclassifies XOR.')\n\n\n\n\n\n\n\n\n\nSingle layer model accuracy: 50.0% - still misclassifies XOR."
  },
  {
    "objectID": "ML/xor-deep-learning.html#limitation-2-multiple-layer-linear-model-without-activation",
    "href": "ML/xor-deep-learning.html#limitation-2-multiple-layer-linear-model-without-activation",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Limitation 2: Multiple Layer Linear Model (Without Activation)",
    "text": "Limitation 2: Multiple Layer Linear Model (Without Activation)\nEven stacking multiple linear layers doesn‚Äôt help! Multiple linear transformations are mathematically equivalent to a single linear transformation.\nMathematical proof:\n\\[\\text{Layer 1: } h_1 = W_1 x + b_1\\] \\[\\text{Layer 2: } h_2 = W_2 h_1 + b_2 = W_2(W_1 x + b_1) + b_2 = (W_2W_1)x + (W_2b_1 + b_2)\\]\nResult: Still just \\(Wx + b\\) (a single linear transformation)\nConclusion: Stacking linear layers without activation functions doesn‚Äôt increase the model‚Äôs expressive power!"
  },
  {
    "objectID": "ML/xor-deep-learning.html#the-solution-relu-activation-function",
    "href": "ML/xor-deep-learning.html#the-solution-relu-activation-function",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "The Solution: ReLU Activation Function",
    "text": "The Solution: ReLU Activation Function\nReLU (Rectified Linear Unit) provides the nonlinearity needed to solve XOR: - ReLU(z) = max(0, z) - Clips negative values to zero, keeping positive values unchanged\nUsing the hand-crafted network from the next code cell, the forward pass can be written compactly in matrix form:\n\\[\nX = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix},\n\\quad\nW_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix},\n\\quad\nb_1 = \\begin{bmatrix} 0 & 0 \\end{bmatrix}\n\\]\n\\[\nZ = X W_1^{\\top} + b_1 = \\begin{bmatrix} 0 & 0 \\\\ -1 & 1 \\\\ 1 & -1 \\\\ 0 & 0 \\end{bmatrix},\n\\qquad\nH = \\text{ReLU}(Z) = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \\end{bmatrix}\n\\]\nWith output parameters \\[\nw_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix},\n\\quad\nb_2 = -0.5\n\\] the final linear scores are \\[\na = H w_2^{\\top} + b_2 = \\begin{bmatrix} -0.5 \\\\ 0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}\n\\Rightarrow\n\\text{sign}_+(a) = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\n\\]\nHere \\(\\text{sign}_+(a)\\) maps non-negative entries to 1 and negative entries to 0. Let‚Äôs see how ReLU transforms the XOR problem to make it solvable.\n\n\nShow code\n# Hand-crafted network weights and biases that solve XOR\nfrom IPython.display import display, Math\n\ndef relu(z):\n    return np.maximum(0, z)\n\nW1 = np.array([[1, -1],\n               [-1, 1]])\nb1 = np.array([0, 0])\nw2 = np.array([1, 1])\nb2 = -0.5\n\ndisplay(Math(r\"\\text{Hidden layer: } W_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}\"))\ndisplay(Math(r\"b_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\"))\ndisplay(Math(r\"\\text{Output layer: } w_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix}\"))\ndisplay(Math(r\"b_2 = -0.5\"))\n\ndef forward_pass(X, W1, b1, w2, b2):\n    z1 = X @ W1.T + b1\n    h1 = relu(z1)\n    logits = h1 @ w2 + b2\n    return logits, h1, z1\n\nlogits, hidden_activations, pre_activations = forward_pass(X, W1, b1, w2, b2)\npredictions = (logits &gt;= 0).astype(int)\n\nprint(\"Step-by-step Forward Pass Results:\")\nprint(\"=\" * 80)\nprint()\nprint(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\nprint(\"‚îÇ Input   ‚îÇ  Before ReLU     ‚îÇ  After ReLU      ‚îÇ  Logit  ‚îÇ   Pred   ‚îÇ\")\nprint(\"‚îÇ (x‚ÇÅ,x‚ÇÇ) ‚îÇ    (z‚ÇÅ, z‚ÇÇ)      ‚îÇ    (h‚ÇÅ, h‚ÇÇ)      ‚îÇ  score  ‚îÇ  class   ‚îÇ\")\nprint(\"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\nfor i in range(len(X)):\n    x1, x2 = X[i]\n    z1_vals = pre_activations[i]\n    h1_vals = hidden_activations[i]\n    logit = logits[i]\n    pred = predictions[i]\n    \n    input_str = f\"({x1:.0f}, {x2:.0f})\"\n    pre_relu_str = f\"({z1_vals[0]:4.1f}, {z1_vals[1]:4.1f})\"\n    post_relu_str = f\"({h1_vals[0]:4.1f}, {h1_vals[1]:4.1f})\"\n    logit_str = f\"{logit:6.2f}\"\n    pred_str = f\"{pred:4d}\"\n    \n    print(f\"‚îÇ {input_str:7} ‚îÇ {pre_relu_str:16} ‚îÇ {post_relu_str:16} ‚îÇ {logit_str:7} ‚îÇ {pred_str:8} ‚îÇ\")\nprint(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n\naccuracy = (predictions == y).mean()\nprint(f\"\\nNetwork Accuracy: {accuracy:.0%} ‚úÖ\")\nprint(\"\\nKey transformations:\")\nprint(\"‚Ä¢ (-1, 1) ‚Üí (0, 1) makes XOR(0,1) = 1 separable\")\nprint(\"‚Ä¢ ( 1,-1) ‚Üí (1, 0) makes XOR(1,0) = 1 separable\")\n\n\n\\(\\displaystyle \\text{Hidden layer: } W_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle b_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle \\text{Output layer: } w_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle b_2 = -0.5\\)\n\n\nStep-by-step Forward Pass Results:\n================================================================================\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Input   ‚îÇ  Before ReLU     ‚îÇ  After ReLU      ‚îÇ  Logit  ‚îÇ   Pred   ‚îÇ\n‚îÇ (x‚ÇÅ,x‚ÇÇ) ‚îÇ    (z‚ÇÅ, z‚ÇÇ)      ‚îÇ    (h‚ÇÅ, h‚ÇÇ)      ‚îÇ  score  ‚îÇ  class   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ (0, 0)  ‚îÇ ( 0.0,  0.0)     ‚îÇ ( 0.0,  0.0)     ‚îÇ  -0.50  ‚îÇ    0     ‚îÇ\n‚îÇ (0, 1)  ‚îÇ (-1.0,  1.0)     ‚îÇ ( 0.0,  1.0)     ‚îÇ   0.50  ‚îÇ    1     ‚îÇ\n‚îÇ (1, 0)  ‚îÇ ( 1.0, -1.0)     ‚îÇ ( 1.0,  0.0)     ‚îÇ   0.50  ‚îÇ    1     ‚îÇ\n‚îÇ (1, 1)  ‚îÇ ( 0.0,  0.0)     ‚îÇ ( 0.0,  0.0)     ‚îÇ  -0.50  ‚îÇ    0     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nNetwork Accuracy: 100% ‚úÖ\n\nKey transformations:\n‚Ä¢ (-1, 1) ‚Üí (0, 1) makes XOR(0,1) = 1 separable\n‚Ä¢ ( 1,-1) ‚Üí (1, 0) makes XOR(1,0) = 1 separable\n\n\n\nTransformation Table: How ReLU Solves XOR\nLet‚Äôs trace through exactly what happens to each input:\n\n\nShow code\n\n# Create detailed transformation table\nprint(\"Complete Transformation Table:\")\nprint(\"=============================\")\nprint()\nprint(\"Input   | Pre-ReLU  | Post-ReLU | Logit | Prediction | Target | Correct?\")\nprint(\"(x‚ÇÅ,x‚ÇÇ) | (z‚ÇÅ, z‚ÇÇ)  | (h‚ÇÅ, h‚ÇÇ)  | score | class      | y      |\")\nprint(\"--------|-----------|-----------|-------|------------|--------|----------\")\n\nfor i in range(4):\n    input_str = f\"({X[i,0]},{X[i,1]})\"\n    pre_relu_str = f\"({pre_activations[i,0]:2.0f},{pre_activations[i,1]:2.0f})\"\n    post_relu_str = f\"({hidden_activations[i,0]:.0f},{hidden_activations[i,1]:.0f})\"\n    logit_str = f\"{logits[i]:.2f}\"\n    pred_str = f\"{predictions[i]}\"\n    target_str = f\"{y[i]}\"\n    correct_str = \"‚úì\" if predictions[i] == y[i] else \"‚úó\"\n\n    print(f\"{input_str:7} | {pre_relu_str:9} | {post_relu_str:9} | {logit_str:5} | {pred_str:10} | {target_str:6} | {correct_str}\")\n\nprint()\nprint(\"Key Insight: ReLU transforms (-1,1) ‚Üí (0,1) and (1,-1) ‚Üí (1,0)\")\nprint(\"This makes the XOR classes linearly separable in the hidden space!\")\n\n\nComplete Transformation Table:\n=============================\n\nInput   | Pre-ReLU  | Post-ReLU | Logit | Prediction | Target | Correct?\n(x‚ÇÅ,x‚ÇÇ) | (z‚ÇÅ, z‚ÇÇ)  | (h‚ÇÅ, h‚ÇÇ)  | score | class      | y      |\n--------|-----------|-----------|-------|------------|--------|----------\n(0,0)   | ( 0, 0)   | (0,0)     | -0.50 | 0          | 0      | ‚úì\n(0,1)   | (-1, 1)   | (0,1)     | 0.50  | 1          | 1      | ‚úì\n(1,0)   | ( 1,-1)   | (1,0)     | 0.50  | 1          | 1      | ‚úì\n(1,1)   | ( 0, 0)   | (0,0)     | -0.50 | 0          | 0      | ‚úì\n\nKey Insight: ReLU transforms (-1,1) ‚Üí (0,1) and (1,-1) ‚Üí (1,0)\nThis makes the XOR classes linearly separable in the hidden space!\n\n\n\n\nStep 1: Original Input Space\nThe XOR problem in its raw form - notice how no single line can separate the classes:\n\n\nShow code\n# Step 1 visualization: Original Input Space\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'XOR = {i}', edgecolors='black', linewidth=2)\n\n# Annotate each point\nfor i in range(4):\n    ax.annotate(f'({X[i,0]},{X[i,1]})', X[i], xytext=(10, 10), \n                textcoords='offset points', fontsize=10)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x‚ÇÅ', fontsize=12)\nax.set_ylabel('x‚ÇÇ', fontsize=12)\nax.set_title('Step 1: Original Input Space', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Linear Transformation (Before ReLU)\nThe network applies weights W‚ÇÅ and biases b‚ÇÅ to transform the input space:\n\n\nShow code\n# Step 2 visualization: Pre-activation space\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\nfor i in range(4):\n    ax.scatter(pre_activations[i, 0], pre_activations[i, 1], \n               c=colors[y[i]], s=200, edgecolors='black', linewidth=2)\n\n# Draw ReLU boundaries\nax.axhline(0, color='gray', linestyle='-', alpha=0.8, linewidth=2)\nax.axvline(0, color='gray', linestyle='-', alpha=0.8, linewidth=2)\n\n# Shade all regions where coordinates turn negative (and thus get clipped by ReLU)\nax.axvspan(-1.2, 0, alpha=0.15, color='red')\nax.axhspan(-1.2, 0, alpha=0.15, color='red')\nax.text(-0.75, 0.85, 'Negative z‚ÇÅ ‚Üí ReLU sets to 0', ha='left', fontsize=10,\n        bbox=dict(boxstyle='round', facecolor='red', alpha=0.25))\nax.text(0.95, -0.75, 'Negative z‚ÇÇ ‚Üí ReLU sets to 0', ha='right', fontsize=10,\n        bbox=dict(boxstyle='round', facecolor='red', alpha=0.25))\n\n# Annotate points with input labels\nlabels = ['(0,0)', '(0,1)', '(1,0)', '(1,1)']\nfor i, label in enumerate(labels):\n    pre_coord = f'({pre_activations[i,0]:.0f},{pre_activations[i,1]:.0f})'\n    ax.annotate(f'{label}‚Üí{pre_coord}', pre_activations[i], xytext=(10, 10), \n                textcoords='offset points', fontsize=9)\n\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1.2, 1.2)\nax.set_xlabel('z‚ÇÅ (Pre-activation)', fontsize=12)\nax.set_ylabel('z‚ÇÇ (Pre-activation)', fontsize=12)\nax.set_title('Step 2: Before ReLU (Linear Transform)', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 3: ReLU Transformation\nReLU clips negative values to zero, transforming the space to make it linearly separable:\n\n\nShow code\n# Step 3 visualization: ReLU transformation with arrows\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\n\nfor i in range(4):\n    # Pre-ReLU positions (X marks)\n    ax.scatter(pre_activations[i, 0], pre_activations[i, 1], \n               marker='x', s=150, c=colors[y[i]], alpha=0.5, linewidth=3)\n    # Post-ReLU positions (circles) \n    ax.scatter(hidden_activations[i, 0], hidden_activations[i, 1], \n               marker='o', s=200, c=colors[y[i]], edgecolors='black', linewidth=2)\n    \n    # Draw transformation arrows\n    start = pre_activations[i]\n    end = hidden_activations[i]\n    if not np.array_equal(start, end):\n        ax.annotate('', xy=end, xytext=start,\n                    arrowprops=dict(arrowstyle='-&gt;', lw=2, color=colors[y[i]], alpha=0.8))\n\n\n# Add text box explaining the key transformation\nax.text(0.5, 0.8, 'ReLU clips negative coordinates to zero\\n(-1,1) ‚Üí (0,1) and (1,-1) ‚Üí (1,0)', \n        ha='center', va='center', fontsize=11, \n        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n\nax.set_xlim(-1.2, 1.1)\nax.set_ylim(-1.2, 1.1)\nax.set_xlabel('Hidden dimension 1', fontsize=12)\nax.set_ylabel('Hidden dimension 2', fontsize=12)\nax.set_title('Step 3: ReLU Mapping (Before ‚Üí After)', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Final Classification\nWith the transformed hidden representation, the network can now perfectly classify XOR:\n\n\nShow code\n\n\n# Step 4 visualization: Final classification results\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\n# Create decision boundary\nxx, yy = np.meshgrid(np.linspace(-0.2, 1.2, 100), np.linspace(-0.2, 1.2, 100))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\ngrid_logits, _, _ = forward_pass(grid_points, W1, b1, w2, b2)\ngrid_preds = (grid_logits &gt;= 0).astype(int).reshape(xx.shape)\n\nax.contourf(xx, yy, grid_preds, levels=[-0.5, 0.5, 1.5], \n            colors=['#ffcccc', '#ccccff'], alpha=0.6)\nax.contour(xx, yy, grid_logits.reshape(xx.shape), levels=[0], \n           colors='black', linewidths=2, linestyles='--')\n\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'XOR = {i}', edgecolors='black', linewidth=2)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x‚ÇÅ', fontsize=12)\nax.set_ylabel('x‚ÇÇ', fontsize=12)\nax.set_title('Step 4: Final Classification (100% Accuracy)', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nsample_logits, _, _ = forward_pass(X, W1, b1, w2, b2)\nsample_preds = (sample_logits &gt;= 0).astype(int)\nfor i in range(4):\n    pred_text = f'Pred: {sample_preds[i]}'\n    ax.annotate(pred_text, X[i], xytext=(10, -15), \n                textcoords='offset points', fontsize=9,\n                bbox=dict(boxstyle='round,pad=0.2', facecolor='lightgreen', alpha=0.7))\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ML/xor-deep-learning.html#conclusion",
    "href": "ML/xor-deep-learning.html#conclusion",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Conclusion",
    "text": "Conclusion\nThe XOR problem demonstrates several fundamental principles in deep learning:\n\nNecessity of Nonlinearity: Linear models cannot solve XOR, establishing the critical role of nonlinear activation functions.\nUniversal Approximation: Even simple architectures with sufficient nonlinearity can solve complex classification problems."
  },
  {
    "objectID": "ML/index.html",
    "href": "ML/index.html",
    "title": "Machine Learning Topics",
    "section": "",
    "text": "Understanding Axis(Dim) Operations"
  },
  {
    "objectID": "ML/index.html#numpy-fundamentals",
    "href": "ML/index.html#numpy-fundamentals",
    "title": "Machine Learning Topics",
    "section": "",
    "text": "Understanding Axis(Dim) Operations"
  },
  {
    "objectID": "ML/index.html#clustering-algorithms",
    "href": "ML/index.html#clustering-algorithms",
    "title": "Machine Learning Topics",
    "section": "Clustering Algorithms",
    "text": "Clustering Algorithms\n\nK-Means Clustering"
  },
  {
    "objectID": "ML/index.html#deep-learning-fundamentals",
    "href": "ML/index.html#deep-learning-fundamentals",
    "title": "Machine Learning Topics",
    "section": "Deep Learning Fundamentals",
    "text": "Deep Learning Fundamentals\n\nThe XOR Problem: Nonlinearity in Deep Learning\nLikelihood-Based Loss Functions\nHidden Units and Activation Functions\nArchitecture Design: Depth vs Width\nBack-Propagation and Other Differentiation Algorithms\nChapter 7.11: Bagging and Other Ensemble Methods"
  },
  {
    "objectID": "ML/index.html#classification-algorithms",
    "href": "ML/index.html#classification-algorithms",
    "title": "Machine Learning Topics",
    "section": "Classification Algorithms",
    "text": "Classification Algorithms\n\nLogistic Regression"
  },
  {
    "objectID": "ML/parameter-tying-sharing.html",
    "href": "ML/parameter-tying-sharing.html",
    "title": "Chapter 7.9: Parameter Tying and Parameter Sharing",
    "section": "",
    "text": "Neural networks can benefit from constraints on parameters in two distinct ways:\n\nParameter Tying: Encourages parameters to be similar through regularization\nParameter Sharing: Forces parameters to be identical by design\n\nBoth approaches reduce effective model capacity and improve generalization, but they differ fundamentally in how strictly they enforce parameter relationships."
  },
  {
    "objectID": "ML/parameter-tying-sharing.html#overview",
    "href": "ML/parameter-tying-sharing.html#overview",
    "title": "Chapter 7.9: Parameter Tying and Parameter Sharing",
    "section": "",
    "text": "Neural networks can benefit from constraints on parameters in two distinct ways:\n\nParameter Tying: Encourages parameters to be similar through regularization\nParameter Sharing: Forces parameters to be identical by design\n\nBoth approaches reduce effective model capacity and improve generalization, but they differ fundamentally in how strictly they enforce parameter relationships."
  },
  {
    "objectID": "ML/parameter-tying-sharing.html#parameter-tying",
    "href": "ML/parameter-tying-sharing.html#parameter-tying",
    "title": "Chapter 7.9: Parameter Tying and Parameter Sharing",
    "section": "Parameter Tying",
    "text": "Parameter Tying\n\nDefinition\nParameter tying constrains parameters of different models or layers to be similar by adding a penalty term to the loss function.\n\n\nMathematical Formulation\n\\[\n\\Omega(w^{(A)}, w^{(B)}) = \\|w^{(A)} - w^{(B)}\\|_2^2\n\\]\nHow it works:\n\nAdd this penalty term to the loss function\nForces two models (or layers) to learn similar parameters\nThe parameters are still independent, but regularization encourages similarity\n\n\n\n\n\n\n\nNoteNature of Constraint\n\n\n\nThis is a soft constraint that allows some deviation while encouraging parameter alignment.\n\n\n\n\nReal-World Applications\nNote: The following table is generated by ChatGPT.\n\n\n\n\n\n\n\n\n\nApplication\nMechanism\nPurpose\nReference\n\n\n\n\nWord Embedding Tying\nInput embedding matrix and output softmax matrix tied: \\(W_{\\text{out}} = E^T\\)\nReduce parameters; consistent embedding space\nPress & Wolf, 2017\n\n\nAutoencoder\nDecoder weights tied to encoder transpose: \\(W_{\\text{dec}} = W_{\\text{enc}}^T\\)\nRegularize; stabilize training; mimic PCA\nHinton & Salakhutdinov, 2006\n\n\nMulti-task Learning\nDifferent tasks‚Äô parameters constrained to be similar: \\(\\Omega = \\|w^{(A)} - w^{(B)}\\|^2\\)\nEncourage knowledge sharing between tasks\nCaruana, 1997\n\n\nKnowledge Distillation\nStudent layers tied to teacher via loss constraint: \\(\\|h_s^{(l)} - h_t^{(l)}\\|^2\\)\nTransfer intermediate representations\nSanh et al., 2019 ‚Äî DistilBERT"
  },
  {
    "objectID": "ML/parameter-tying-sharing.html#parameter-sharing",
    "href": "ML/parameter-tying-sharing.html#parameter-sharing",
    "title": "Chapter 7.9: Parameter Tying and Parameter Sharing",
    "section": "Parameter Sharing",
    "text": "Parameter Sharing\n\nDefinition\nParameter sharing uses the exact same set of parameters across multiple locations or time steps.\n\n\nCNN Example\nThe same kernel (set of weights) is applied across all spatial locations of the input:\n\\[\ny_{i,j} = \\sum_{u,v} w_{u,v} x_{i+u, j+v}\n\\]\nBenefits:\n\nPattern detection: Detects the same pattern (e.g., edge or texture) anywhere in the image\nParameter reduction: Dramatically reduces the number of parameters\nTranslation equivariance: Output shifts when input shifts\n\n\n\n\n\n\n\nImportantNature of Constraint\n\n\n\nThis is a hard constraint where parameters are identical by design, not just similar.\n\n\n\n\nReal-World Applications\nNote: The following table is generated by ChatGPT.\n\n\n\n\n\n\n\n\n\nApplication\nMechanism\nPurpose\nReference\n\n\n\n\nCNN\nSame kernel slides across all spatial positions: \\(y_{i,j} = \\sum_{u,v} w_{u,v} x_{i+u, j+v}\\)\nDetect same pattern anywhere; reduce parameters; translation equivariance\nLeCun et al., 1998 ‚Äî LeNet\n\n\nRNN / LSTM / GRU\nSame weights used at each time step: \\(h_t = f(W_h h_{t-1} + W_x x_t)\\)\nTemporal consistency; handle variable-length sequences\nHochreiter & Schmidhuber, 1997\n\n\nTransformer (ALBERT)\nAll encoder layers share parameters: \\(\\theta_1 = \\theta_2 = \\dots = \\theta_L\\)\nReduce memory; efficient deep sharing\nLan et al., 2020 ‚Äî ALBERT\n\n\nSiamese / Twin Networks\nTwo (or more) branches share all parameters: \\(f_\\theta(x_1), f_\\theta(x_2)\\)\nCompare similarity; representation consistency\nBromley et al., 1993 ‚Äî Siamese Nets\n\n\n\n\nSource: Deep Learning Book, Chapter 7.9"
  },
  {
    "objectID": "ML/dataset-augmentation.html",
    "href": "ML/dataset-augmentation.html",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "",
    "text": "When available training data is limited, we can explicitly increase data diversity by generating transformed or perturbed versions of existing samples.\nThis technique, known as dataset augmentation, helps the model generalize better and reduces overfitting."
  },
  {
    "objectID": "ML/dataset-augmentation.html#core-idea",
    "href": "ML/dataset-augmentation.html#core-idea",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "",
    "text": "When available training data is limited, we can explicitly increase data diversity by generating transformed or perturbed versions of existing samples.\nThis technique, known as dataset augmentation, helps the model generalize better and reduces overfitting."
  },
  {
    "objectID": "ML/dataset-augmentation.html#basic-concept",
    "href": "ML/dataset-augmentation.html#basic-concept",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "Basic Concept",
    "text": "Basic Concept\nDataset augmentation is one of the simplest and most effective regularization strategies.\nIt increases both the size and the variability of the training set by applying transformations that do not change the class label.\n\n\n\n\n\n\nImportantKey Principle\n\n\n\nThe augmented data should preserve semantic meaning while introducing variation that reflects real-world conditions."
  },
  {
    "objectID": "ML/dataset-augmentation.html#common-augmentation-methods",
    "href": "ML/dataset-augmentation.html#common-augmentation-methods",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "Common Augmentation Methods",
    "text": "Common Augmentation Methods\n\nGeometric Transformations\nInclude translation, rotation, scaling, and flipping of images.\nExample: For image classification, a horizontally flipped cat image is still a cat.\nNote: Even though convolution provides some degree of translation invariance, explicitly augmenting the dataset with translated copies of the inputs can further improve generalization.\nWhy this helps:\n\nForces the model to learn features that are robust to spatial transformations\nSimulates different camera angles and object positions\nReduces dependence on absolute position in the image\n\n\n\nNoise Injection\nAdd random noise (e.g., Gaussian noise) to the input or hidden layers.\nIntroduced in denoising autoencoders (Vincent et al., 2008), this acts as unsupervised regularization, improving robustness and stability.\nMathematical formulation:\n\\[\n\\tilde{x} = x + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\nResearch finding: Poole et al.¬†(2014) showed that carefully tuning the noise level can lead to strong performance gains.\nWhy this helps:\n\nPrevents the model from memorizing exact pixel values\nImproves robustness to sensor noise and measurement errors\nActs as a form of implicit regularization\n\n\n\nRandom Cropping and Occlusion\nMimic the variability of human perception by randomly cropping or masking parts of the image.\nExample: Randomly crop a 224√ó224 patch from a 256√ó256 image during training.\nWhy this helps:\n\nForces the model to recognize objects from partial views\nSimulates real-world scenarios where objects are partially occluded\nIncreases effective dataset size significantly"
  },
  {
    "objectID": "ML/dataset-augmentation.html#applications-across-domains",
    "href": "ML/dataset-augmentation.html#applications-across-domains",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "Applications Across Domains",
    "text": "Applications Across Domains\n\n\n\n\n\n\n\n\nDomain\nExample of Augmentation\nGoal\n\n\n\n\nComputer Vision\nTranslation, rotation, scaling, flipping\nEncourage spatial invariance\n\n\nSpeech Recognition\nAdd random noise or time masking\nImprove robustness to background noise\n\n\nText / NLP\nWord dropout or synonym replacement\nImprove generalization in low-data settings\n\n\n\nAdditional examples:\n\nComputer Vision: Color jittering, brightness adjustment, elastic distortions\nSpeech: Speed perturbation, pitch shifting, room impulse response simulation\nNLP: Back-translation, paraphrasing, random insertion/deletion"
  },
  {
    "objectID": "ML/dataset-augmentation.html#design-and-evaluation",
    "href": "ML/dataset-augmentation.html#design-and-evaluation",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "Design and Evaluation",
    "text": "Design and Evaluation\nFair comparison principle: When comparing different algorithms, the same data augmentation strategy must be used for a fair comparison.\n\n\n\n\n\n\nWarningWhy This Matters\n\n\n\nIf one algorithm benefits from augmented data and another does not, performance differences may reflect the augmentation strategy, not the algorithm itself.\n\n\nBest practices:\n\nDocument all augmentation techniques used\nAblation studies should isolate augmentation effects\nReport results both with and without augmentation when introducing new methods"
  },
  {
    "objectID": "ML/dataset-augmentation.html#relation-to-other-regularization-methods",
    "href": "ML/dataset-augmentation.html#relation-to-other-regularization-methods",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "Relation to Other Regularization Methods",
    "text": "Relation to Other Regularization Methods\nAdding noise to inputs is conceptually related to weight regularization (Bishop, 1995).\nTheoretical connection:\n\nSmall input noise can be approximated by a penalty on the weights\nFor quadratic loss, input noise is equivalent to Tikhonov regularization\n\nDropout (see Section 7.12) can be interpreted as a stochastic extension of noise-based regularization.\nDataset augmentation can thus be seen as a bridge between:\n\nExplicit data transformation (augmentation)\nImplicit noise regularization (weight decay, dropout)\n\n\n\n\n\n\n\nTipUnified View\n\n\n\nAll these techniques prevent the model from relying too heavily on specific features or exact training examples."
  },
  {
    "objectID": "ML/dataset-augmentation.html#summary",
    "href": "ML/dataset-augmentation.html#summary",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "Summary",
    "text": "Summary\nKey takeaways:\n\nDataset augmentation improves generalization by making the model robust to input variations such as translation, rotation, and noise\nIt is a practical and powerful regularization method that effectively combats overfitting, especially when training data is limited\nAugmentation strategies should preserve semantic labels while introducing realistic variations\nFair algorithm comparisons require consistent augmentation across all methods\n\nWhen to use:\n\nLimited training data\nHigh risk of overfitting\nDomain knowledge suggests specific invariances (e.g., rotation invariance for digit recognition)\n\nTrade-offs:\n\nIncreases training time (more data to process)\nMay introduce unrealistic samples if not carefully designed\nRequires domain expertise to choose appropriate transformations\n\n\nSource: Deep Learning Book, Chapter 7.4"
  },
  {
    "objectID": "ML/logistic_regression.html",
    "href": "ML/logistic_regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "The core idea of logistic regression is to model the probability of a binary outcome.\n\n\nWe model the probability that the target variable (y) is 1, given the features (x), using the sigmoid (or logistic) function, denoted by ().\n\\[\nP(y_i=1 \\mid x_i) = \\hat{y}_i = \\sigma(w^T x_i + b)\n\\]\nThe sigmoid function is defined as: \\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\nSince the outcome is binary, the probability of (y) being 0 is simply: \\[\nP(y_i=0 \\mid x_i) = 1 - \\hat{y}_i\n\\]\nThese two cases can be written compactly as a single equation, which is the probability mass function of a Bernoulli distribution: \\[\nP(y_i \\mid x_i) = \\hat{y}_i^{y_i} (1 - \\hat{y}_i)^{1 - y_i}\n\\]"
  },
  {
    "objectID": "ML/logistic_regression.html#model-formulation",
    "href": "ML/logistic_regression.html#model-formulation",
    "title": "Logistic Regression",
    "section": "",
    "text": "The core idea of logistic regression is to model the probability of a binary outcome.\n\n\nWe model the probability that the target variable (y) is 1, given the features (x), using the sigmoid (or logistic) function, denoted by ().\n\\[\nP(y_i=1 \\mid x_i) = \\hat{y}_i = \\sigma(w^T x_i + b)\n\\]\nThe sigmoid function is defined as: \\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\nSince the outcome is binary, the probability of (y) being 0 is simply: \\[\nP(y_i=0 \\mid x_i) = 1 - \\hat{y}_i\n\\]\nThese two cases can be written compactly as a single equation, which is the probability mass function of a Bernoulli distribution: \\[\nP(y_i \\mid x_i) = \\hat{y}_i^{y_i} (1 - \\hat{y}_i)^{1 - y_i}\n\\]"
  },
  {
    "objectID": "ML/logistic_regression.html#loss-function-binary-cross-entropy",
    "href": "ML/logistic_regression.html#loss-function-binary-cross-entropy",
    "title": "Logistic Regression",
    "section": "Loss Function (Binary Cross-Entropy)",
    "text": "Loss Function (Binary Cross-Entropy)\nTo find the optimal parameters (w) and (b), we use Maximum Likelihood Estimation (MLE). We want to find the parameters that maximize the probability of observing our given dataset.\n\n1. Likelihood\nThe likelihood is the joint probability of observing all (n) data points, assuming they are independent and identically distributed (i.i.d.): \\[\n\\mathcal{L}(w, b) = \\prod_{i=1}^n P(y_i \\mid x_i) = \\prod_{i=1}^n \\hat{y}_i^{y_i} (1 - \\hat{y}_i)^{1 - y_i}\n\\]\n\n\n2. Log-Likelihood\nWorking with products is difficult, so we take the logarithm of the likelihood. Maximizing the log-likelihood is equivalent to maximizing the likelihood.\n\\[\n\\log \\mathcal{L}(w, b) = \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n\\]\n\n\n3. Cost Function\nIn machine learning, we frame problems as minimizing a cost function. The standard convention is to minimize the negative log-likelihood. This gives us the Binary Cross-Entropy loss, (J(w, b)).\n\\[\nJ(w, b) = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n\\] The \\(\\frac{1}{n}\\) term is an average over the training examples and doesn‚Äôt change the minimum, but it helps in stabilizing the training process."
  },
  {
    "objectID": "ML/logistic_regression.html#refernce-bernoulli-distribution",
    "href": "ML/logistic_regression.html#refernce-bernoulli-distribution",
    "title": "Logistic Regression",
    "section": "refernce: Bernoulli Distribution",
    "text": "refernce: Bernoulli Distribution\nFormular \\[\nP(y) = p^y (1 - p)^{1 - y}, \\quad y \\in \\{0, 1\\}\n\\] * when y = 1Ôºö\\(P(y=1) = p^1 (1 - p)^0 = p\\) * when y=0: $P(y=0) = p^0 (1 - p)^1 = 1 - p $"
  },
  {
    "objectID": "ML/representation-sparsity.html",
    "href": "ML/representation-sparsity.html",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "",
    "text": "Previous chapters focused on parameter regularization ‚Äî constraining the weights \\(W\\) of a model (see Chapter 7.1.2: L1 Regularization and Chapter 7.1.1: L2 Regularization).\nThis chapter introduces representation regularization ‚Äî constraining the activations \\(h\\) (the learned representations).\nKey distinction:\n\nParameter sparsity: Makes the weight matrix sparse (few connections)\nRepresentation sparsity: Makes the activation vector sparse (few active neurons)"
  },
  {
    "objectID": "ML/representation-sparsity.html#overview",
    "href": "ML/representation-sparsity.html#overview",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "",
    "text": "Previous chapters focused on parameter regularization ‚Äî constraining the weights \\(W\\) of a model (see Chapter 7.1.2: L1 Regularization and Chapter 7.1.1: L2 Regularization).\nThis chapter introduces representation regularization ‚Äî constraining the activations \\(h\\) (the learned representations).\nKey distinction:\n\nParameter sparsity: Makes the weight matrix sparse (few connections)\nRepresentation sparsity: Makes the activation vector sparse (few active neurons)"
  },
  {
    "objectID": "ML/representation-sparsity.html#parameter-regularization",
    "href": "ML/representation-sparsity.html#parameter-regularization",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "Parameter Regularization",
    "text": "Parameter Regularization\nEquation 7.46 - Linear system with parameter matrix:\n\\[\n\\begin{array}{c}\n\\begin{bmatrix}\n18\\\\\n5\\\\\n15\\\\\n-9\\\\\n-3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n4 & 0 & 0 & -2 & 0\\\\\n0 & -1 & 0 & 3 & 0\\\\\n1 & 0 & 0 & 0 & 3\\\\\n0 & 5 & 0 & -1 & 0\\\\\n1 & 0 & 0 & -5 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n2\\\\\n3\\\\\n-2\\\\\n-5\\\\\n1\n\\end{bmatrix}\n\\\\[6pt]\ny \\in \\mathbb{R}^m,\\quad\nA \\in \\mathbb{R}^{m\\times n},\\quad\nx \\in \\mathbb{R}^n\n\\end{array}\n\\]\nL1 regularization on parameters:\n\\[\n\\Omega(A) = \\|A\\|_1 = \\sum_{i,j} |A_{ij}|\n\\]\nEffect: Encourages sparsity in the parameter matrix \\(A\\) itself, making many weights zero."
  },
  {
    "objectID": "ML/representation-sparsity.html#representation-sparsity",
    "href": "ML/representation-sparsity.html#representation-sparsity",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "Representation Sparsity",
    "text": "Representation Sparsity\nEquation 7.47 - Linear system with sparse representation:\n\\[\n\\begin{array}{c}\n\\begin{bmatrix}\n-14\\\\\n1\\\\\n3\\\\\n2\\\\\n23\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n3 & -1 & 2 & -5 & 4 & 1\\\\\n4 & -2 & -3 & -1 & 3 & 0\\\\\n1 & 5 & 2 & -4 & 0 & 0\\\\\n3 & 4 & -3 & 0 & 2 & 0\\\\\n-5 & -4 & 2 & 5 & -1 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\n0\\\\\n0\\\\\n1\\\\\n0\\\\\n0\\\\\n0\n\\end{bmatrix}\n\\\\[6pt]\ny \\in \\mathbb{R}^m, \\quad\nB \\in \\mathbb{R}^{m\\times n}, \\quad\nh \\in \\mathbb{R}^n\n\\end{array}\n\\]\nL1 regularization on representation:\n\\[\n\\Omega(h) = \\|h\\|_1 = \\sum_{i} |h_i|\n\\]\nEquation 7.48 - Loss function with representation regularization:\n\\[\n\\tilde{J}(\\theta; x, y) = J(\\theta; x, y) + \\alpha \\Omega(h)\n\\]\nwhere \\(\\alpha \\in [0, \\infty)\\) controls the contribution of the norm penalty to the total loss.\n\nKey Difference: Parameter vs Representation Regularization\n\n\n\n\n\n\n\n\n\nType\nWhat is Regularized\nEffect\nExample\n\n\n\n\nParameter regularization\nWeight matrix \\(A\\) or \\(W\\)\nMakes weights sparse\nL1/L2 weight decay\n\n\nRepresentation regularization\nActivation vector \\(h\\)\nMakes activations sparse\nSparse autoencoders\n\n\n\nInterpretation:\n\nParameter sparsity: Few connections between layers (network pruning)\nRepresentation sparsity: Few neurons active at once (sparse coding)"
  },
  {
    "objectID": "ML/representation-sparsity.html#orthogonal-matching-pursuit",
    "href": "ML/representation-sparsity.html#orthogonal-matching-pursuit",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "Orthogonal Matching Pursuit",
    "text": "Orthogonal Matching Pursuit\nEquation 7.49 - Sparse approximation problem:\n\\[\n\\arg\\min_h \\|x - Wh\\|^2 \\quad \\text{subject to} \\quad \\|h\\|_0 &lt; k\n\\]\nGoal: Find both \\(W\\) and \\(h\\) such that:\n\n\\(Wh\\) closely approximates \\(x\\) (reconstruction accuracy)\n\\(h\\) is sparse ‚Äî the number of nonzero elements in \\(h\\) (denoted \\(\\|h\\|_0\\)) is less than \\(k\\)\n\nInterpretation: Use \\(Wh\\) to represent \\(x\\) with only a few active components.\n\n\n\n\n\n\nNoteOrthogonal Matching Pursuit\n\n\n\nWhen the columns of \\(W\\) are orthogonal, this problem can be efficiently solved using orthogonal matching pursuit (OMP) algorithm.\nKey insight: Orthogonality of basis vectors (columns of \\(W\\)) enables efficient sparse decomposition."
  },
  {
    "objectID": "ML/representation-sparsity.html#real-world-applications",
    "href": "ML/representation-sparsity.html#real-world-applications",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nNote: The following table is generated by ChatGPT.\n\n\n\n\n\n\n\n\n\n\nCategory\nTechnique / Model\nRepresentation Type\nHow it Uses Sparsity / Orthogonality\nReal-World Example / Effect\n\n\n\n\nüß† Neuroscience & Vision\nSparse coding (Olshausen & Field, 1996)\nSparse\nThe brain represents visual inputs by activating only a few neurons for each stimulus.\nExplains receptive fields in V1 visual cortex ‚Äî neurons respond only to specific edges or orientations.\n\n\nüßÆ Machine Learning\nLASSO Regression\nSparse\nAdds \\(\\lambda \\|w\\|_1\\) penalty to make weights sparse, selecting only key features.\nFeature selection in predictive models (finance, genomics, etc.).\n\n\nüß© Autoencoders\nSparse Autoencoder\nSparse\nAdds regularizer \\(\\Omega(h) = \\|h\\|_1\\) or KL divergence to force sparse activations.\nUsed in image compression and pretraining deep networks (unsupervised learning).\n\n\nüñºÔ∏è Image Processing / Compression\nDictionary Learning / K-SVD\nSparse\nRepresent an image as a combination of few learned basis patches (atoms).\nJPEG-like compression, denoising, super-resolution.\n\n\nüîä Speech Processing\nNon-negative Matrix Factorization (NMF)\nSparse + Nonnegative\nDecomposes sound spectrograms into few additive components.\nSource separation (e.g., separating voice from music).\n\n\nüß† Transform-based Compression\nDCT (Discrete Cosine Transform)\nOrthogonal\nProjects signals onto orthogonal cosine basis vectors.\nJPEG compression ‚Äî energy concentrated in few coefficients.\n\n\nüì∂ Signal Processing\nFourier Transform\nOrthogonal\nRepresents time-domain signals in orthogonal sinusoidal basis.\nAudio, RF, vibration analysis.\n\n\nüñ•Ô∏è Dimensionality Reduction\nPrincipal Component Analysis (PCA)\nOrthogonal\nFinds orthogonal axes (principal components) that maximize variance.\nDimensionality reduction, data visualization.\n\n\nüéôÔ∏è Source Separation\nIndependent Component Analysis (ICA)\nSparse-like / Orthogonal\nSeeks statistically independent (often sparse) components.\nBlind source separation (‚Äúcocktail party problem‚Äù).\n\n\nü§ñ CNN Filters\nOrthogonal Regularization\nOrthogonal\nForces convolution filters to be orthogonal to prevent redundancy.\nImproves training stability and generalization.\n\n\nüíæ Transformers\nEmbedding Orthogonalization\nOrthogonal or Near-orthogonal\nOrthogonalizes embedding vectors for better separation in latent space.\nImproves language model expressiveness and prevents collapse.\n\n\n\n\nSource: Deep Learning Book, Chapter 7.10"
  },
  {
    "objectID": "ML/early-stopping.html",
    "href": "ML/early-stopping.html",
    "title": "Chapter 7.8: Early Stopping",
    "section": "",
    "text": "Early stopping is a simple yet effective regularization technique that stops training when validation performance begins to degrade."
  },
  {
    "objectID": "ML/early-stopping.html#overview",
    "href": "ML/early-stopping.html#overview",
    "title": "Chapter 7.8: Early Stopping",
    "section": "",
    "text": "Early stopping is a simple yet effective regularization technique that stops training when validation performance begins to degrade."
  },
  {
    "objectID": "ML/early-stopping.html#the-overfitting-problem",
    "href": "ML/early-stopping.html#the-overfitting-problem",
    "title": "Chapter 7.8: Early Stopping",
    "section": "1. The Overfitting Problem",
    "text": "1. The Overfitting Problem\n\n\n\nOverfitting Visualization\n\n\nObservation: Overfitting almost always occurs during training.\nWhat happens:\n\nTraining error continues to decrease\nValidation error initially decreases, then starts increasing\nThe gap between training and validation error grows\nModel memorizes training data instead of learning generalizable patterns\n\nSolution: Stop training when validation error reaches its minimum."
  },
  {
    "objectID": "ML/early-stopping.html#two-approaches-to-early-stopping",
    "href": "ML/early-stopping.html#two-approaches-to-early-stopping",
    "title": "Chapter 7.8: Early Stopping",
    "section": "2. Two Approaches to Early Stopping",
    "text": "2. Two Approaches to Early Stopping\nSplit data into training \\((x, y)\\) and validation \\((x_{\\text{valid}}, y_{\\text{valid}})\\):\n\nApproach 1: Find Optimal Steps, Then Retrain\n\nTrain on \\((x, y)\\) while monitoring \\(y_{\\text{valid}}\\)\nIdentify the optimal number of steps \\(\\tau^*\\) where validation error is minimized\nRetrain from scratch on full dataset for exactly \\(\\tau^*\\) steps\n\nAdvantage: Uses all data for final training\nDisadvantage: Requires two full training runs\n\n\nApproach 2: Keep Best Model\n\nTrain on \\((x, y)\\) while monitoring \\(y_{\\text{valid}}\\)\nKeep a copy of the model whenever validation performance improves\nStop when validation error stops improving\nUse the saved best model\n\nAdvantage: Only requires one training run\nDisadvantage: Validation data is not used for training\n\n\n\nEarly Stopping Approaches"
  },
  {
    "objectID": "ML/early-stopping.html#costs-of-early-stopping",
    "href": "ML/early-stopping.html#costs-of-early-stopping",
    "title": "Chapter 7.8: Early Stopping",
    "section": "3. Costs of Early Stopping",
    "text": "3. Costs of Early Stopping\nData requirements:\n\nNeed to hold out a portion of data as validation set\nReduces effective training data size\nValidation set typically 10-20% of total data\n\nComputational requirements:\n\nNeed to keep a copy of the best-performing model\nRequires periodic evaluation on validation set\nMay need multiple checkpoints if using approach 1"
  },
  {
    "objectID": "ML/early-stopping.html#benefits-of-early-stopping",
    "href": "ML/early-stopping.html#benefits-of-early-stopping",
    "title": "Chapter 7.8: Early Stopping",
    "section": "4. Benefits of Early Stopping",
    "text": "4. Benefits of Early Stopping\nRegularization:\n\nPrevents overfitting without modifying the loss function\nActs as implicit L2 regularization (proven mathematically below)\nNo hyperparameter tuning needed for regularization strength\n\nEfficiency:\n\nSaves computation by stopping early\nAutomatic hyperparameter selection (number of iterations)\nOften faster than training with explicit regularization to convergence\n\nSimplicity:\n\nEasy to implement\nWidely applicable across different model types\nWorks well in practice without fine-tuning"
  },
  {
    "objectID": "ML/early-stopping.html#mathematical-connection-to-l2-regularization",
    "href": "ML/early-stopping.html#mathematical-connection-to-l2-regularization",
    "title": "Chapter 7.8: Early Stopping",
    "section": "5. Mathematical Connection to L2 Regularization",
    "text": "5. Mathematical Connection to L2 Regularization\n\nGradient Descent Update Rule\nStarting from a quadratic approximation around the optimal weights \\(w^*\\):\nEquation 7.33 - Quadratic approximation of loss:\n\\[\n\\hat{J}(\\theta) = J(w^*) + \\frac{1}{2}(w - w^*)^T H(w - w^*)\n\\]\nwhere \\(H\\) is the Hessian matrix at \\(w^*\\).\nEquation 7.34 - Gradient:\n\\[\n\\nabla_w \\hat{J}(w) = H(w - w^*)\n\\]\nEquation 7.35 - Gradient descent update:\n\\[\nw^{(\\tau)} = w^{(\\tau-1)} - \\epsilon \\nabla_w \\hat{J}(w^{(\\tau-1)})\n\\]\nEquation 7.36 - Substituting the gradient:\n\\[\nw^{(\\tau)} = w^{(\\tau-1)} - \\epsilon H(w^{(\\tau-1)} - w^*)\n\\]\nEquation 7.37 - Rearranging:\n\\[\nw^{(\\tau)} - w^* = (I - \\epsilon H)(w^{(\\tau-1)} - w^*)\n\\]\n\n\nEigendecomposition of Hessian\nEquation 7.38 - Decompose \\(H = Q\\Lambda Q^T\\):\n\\[\nw^{(\\tau)} - w^* = (I - \\epsilon Q\\Lambda Q^T)(w^{(\\tau-1)} - w^*)\n\\]\nEquation 7.39 - Multiply both sides by \\(Q^T\\):\n\\[\nQ^T(w^{(\\tau)} - w^*) = (I - \\epsilon \\Lambda)Q^T(w^{(\\tau-1)} - w^*)\n\\]\n\n\nDeriving the Recursive Formula\nSince \\((I - \\epsilon\\Lambda)\\) is a constant diagonal matrix, we can apply this recursion:\nStep 1 - After 1 iteration:\n\\[\nQ^T(w^{(1)} - w^*) = (I - \\epsilon \\Lambda)Q^T(w^{(0)} - w^*)\n\\]\nStep 2 - After 2 iterations:\n\\[\n\\begin{aligned}\nQ^T(w^{(2)} - w^*) &= (I - \\epsilon \\Lambda)Q^T(w^{(1)} - w^*) \\\\\n&= (I - \\epsilon \\Lambda)Q^T(I - \\epsilon \\Lambda)(w^{(0)} - w^*) \\\\\n&= (I - \\epsilon \\Lambda)^2 Q^T(w^{(0)} - w^*)\n\\end{aligned}\n\\]\nStep 3 - After 3 iterations:\n\\[\n\\begin{aligned}\nQ^T(w^{(3)} - w^*) &= (I - \\epsilon \\Lambda)Q^T(w^{(2)} - w^*) \\\\\n&= (I - \\epsilon \\Lambda)Q^T(I - \\epsilon \\Lambda)^2(w^{(0)} - w^*) \\\\\n&= (I - \\epsilon \\Lambda)^3 Q^T(w^{(0)} - w^*)\n\\end{aligned}\n\\]\nGeneral pattern - After \\(\\tau\\) iterations:\n\\[\nQ^T(w^{(\\tau)} - w^*) = (I - \\epsilon \\Lambda)^\\tau Q^T(w^{(0)} - w^*)\n\\]\n\n\nAssuming Zero Initialization\nEquation 7.40 - If \\(w^{(0)} = 0\\):\n\\[\n\\begin{aligned}\nQ^T(w^{(\\tau)} - w^*) &= (I - \\epsilon \\Lambda)^\\tau Q^T(0 - w^*) \\\\\n&= -(I - \\epsilon \\Lambda)^\\tau Q^T w^*\n\\end{aligned}\n\\]\nTherefore:\n\\[\nQ^T w^{(\\tau)} = Q^T w^* - (I - \\epsilon \\Lambda)^\\tau Q^T w^*\n\\]\n\\[\nQ^T w^{(\\tau)} = [I - (I - \\epsilon \\Lambda)^\\tau] Q^T w^*\n\\]\n\n\nL2 Regularization Solution\nEquation 7.41 - L2 regularized solution:\n\\[\nQ^T \\tilde{w} = (\\Lambda + \\alpha I)^{-1} \\Lambda Q^T w^*\n\\]\nEquation 7.42 - Rewriting:\n\\[\nQ^T \\tilde{w} = [I - (\\Lambda + \\alpha I)^{-1} \\alpha] Q^T w^*\n\\]\n\n\nEquivalence Condition\nEquation 7.43 - Comparing equations 7.40 and 7.42:\nIf we can set:\n\\[\n(I - \\epsilon \\Lambda)^\\tau = (\\Lambda + \\alpha I)^{-1} \\alpha\n\\]\nthen early stopping is equivalent to L2 regularization."
  },
  {
    "objectID": "ML/early-stopping.html#relationship-between-training-steps-and-regularization-strength",
    "href": "ML/early-stopping.html#relationship-between-training-steps-and-regularization-strength",
    "title": "Chapter 7.8: Early Stopping",
    "section": "6. Relationship Between Training Steps and Regularization Strength",
    "text": "6. Relationship Between Training Steps and Regularization Strength\n\nDeriving \\(\\tau\\) from \\(\\alpha\\)\nMathematical tools:\n\nLogarithm properties: \\(\\log(ab) = \\log(a) + \\log(b)\\) and \\(\\log(a^b) = b\\log(a)\\)\nTaylor series approximation: \\(\\log(1 + x) \\approx x\\) and \\(\\log(1 - x) \\approx -x\\)\n\nStarting from equation 7.43:\n\\[\n\\tau \\log(I - \\epsilon \\Lambda) = \\log((\\Lambda + \\alpha I)^{-1} \\alpha)\n\\]\nRight side:\n\\[\n\\log((\\Lambda + \\alpha I)^{-1} \\alpha) = -\\log(\\Lambda + \\alpha I) + \\log(\\alpha)\n\\]\nFactor out \\(\\alpha\\):\n\\[\n\\Lambda + \\alpha I = \\alpha\\left(\\frac{\\Lambda}{\\alpha} + I\\right)\n\\]\nTherefore:\n\\[\n-\\log(\\Lambda + \\alpha I) + \\log(\\alpha) = -\\log(\\alpha) - \\log\\left(\\frac{\\Lambda}{\\alpha} + I\\right) + \\log(\\alpha) = -\\log\\left(\\frac{\\Lambda}{\\alpha} + I\\right)\n\\]\nLeft side using Taylor approximation:\n\\[\n\\log(I - \\epsilon \\Lambda) \\approx -\\epsilon \\Lambda\n\\]\nRight side using Taylor approximation:\n\\[\n\\log\\left(I + \\frac{\\Lambda}{\\alpha}\\right) \\approx \\frac{\\Lambda}{\\alpha}\n\\]\nCombining:\n\\[\n\\tau(-\\epsilon \\Lambda) \\approx -\\frac{\\Lambda}{\\alpha}\n\\]\nEquation 7.44 - Solving for \\(\\tau\\):\n\\[\n\\tau \\approx \\frac{1}{\\epsilon \\alpha}\n\\]\nEquation 7.45 - Solving for \\(\\alpha\\):\n\\[\n\\alpha \\approx \\frac{1}{\\epsilon \\tau}\n\\]\n\n\nKey Insight\nThe number of training steps \\(\\tau\\) is inversely proportional to the L2 regularization strength \\(\\alpha\\).\nInterpretation:\n\nMore training steps (\\(\\tau\\) large) ‚ÜîÔ∏é Weaker regularization (\\(\\alpha\\) small)\nFewer training steps (\\(\\tau\\) small) ‚ÜîÔ∏é Stronger regularization (\\(\\alpha\\) large)\nEarly stopping implicitly applies L2 regularization with strength \\(\\alpha \\approx \\frac{1}{\\epsilon \\tau}\\)\n\nPractical implication: Choosing when to stop training is equivalent to choosing the regularization strength.\n\nKey concepts:\n\nEarly stopping: Stop training when validation error stops improving\nTwo approaches: Find optimal steps and retrain, or keep best model\nCosts: Requires validation data and model checkpointing\nBenefits: Simple, effective, computationally efficient regularization\nMathematical equivalence: Early stopping ‚âà implicit L2 regularization\nInverse relationship: \\(\\tau \\approx \\frac{1}{\\epsilon \\alpha}\\) (more steps = less regularization)\n\n\nSource: Deep Learning Book, Chapter 7.8"
  }
]