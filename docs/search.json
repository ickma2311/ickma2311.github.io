[
  {
    "objectID": "ML/k_means_clustering.html",
    "href": "ML/k_means_clustering.html",
    "title": "K-means Clustering",
    "section": "",
    "text": "Setup points and K\nwe will implement a KNN algorithm to cluster the points\n\n\nX=[[1,1],[2,2.1],[3,2.5],[6,7],[7,7.1],[9,7.5]]\nk=2\n\nmax_iter=3\n\n\n# Visualize the data\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter([x[0] for x in X],[x[1] for x in X])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pure python implementation of K-means clustering\ndef knn_iter(X,centroids):\n    # set up new clusters\n    new_clusters=[[] for _ in range(len(centroids))]\n    # k=len(centroids)\n    # assign each point to the nearest centroid\n    for x in X:\n        k,distance=0,(x[0]-centroids[0][0])**2+(x[1]-centroids[0][1])**2\n        for i,c in enumerate(centroids[1:],1):\n            if (x[0]-c[0])**2+(x[1]-c[1])**2&lt;distance:\n                k=i\n                distance=(x[0]-c[0])**2+(x[1]-c[1])**2\n        new_clusters[k].append(x)\n    \n    # calculate new centroids\n    new_centroids=[[\n        sum([x[0] for x in cluster])/len(cluster),\n        sum([x[1] for x in cluster])/len(cluster)\n    ] if cluster else centroids[i] for i,cluster in enumerate(new_clusters)]\n    return new_centroids\n\n\n\n\n\n\n\n\ndef iter_and_draw(X,k,max_iter):\n    centroids=X[:k]  # Randomly select 2 centroids\n    fig, axes = plt.subplots(max_iter//3+(1 if max_iter%3!=0 else 0),\n        3, figsize=(15, 10))\n    axes=axes.flatten()\n    for i in range(max_iter):\n        \n        # Plot points and centroids\n\n\n        # Assign each point to nearest centroid and plot with corresponding color\n        colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n        for j, x in enumerate(X):\n            # Find nearest centroid\n            min_dist = float('inf')\n            nearest_centroid = 0\n            for k, c in enumerate(centroids):\n                dist = (x[0]-c[0])**2 + (x[1]-c[1])**2\n                if dist &lt; min_dist:\n                    min_dist = dist\n                    nearest_centroid = k\n            # Plot point with color corresponding to its cluster\n            axes[i].scatter(x[0], x[1], c=colors[nearest_centroid % len(colors)], label=f'Cluster {nearest_centroid+1}' if j==0 else \"\")\n        axes[i].scatter([c[0] for c in centroids], [c[1] for c in centroids], c='red', marker='*', s=200, label='Centroids')\n        axes[i].set_title(f'Iteration {i}')\n        centroids = knn_iter(X, centroids)\n\n    plt.tight_layout()\n    plt.show()\n\niter_and_draw(X,k,max_iter)\n# print(centroids)\n\n\n\n\n\n\n\n\n\n# A 3 clusters example\n\nimport numpy as np\n\nX1=np.random.rand(20,2)+5 # Some points in the upper right corner\nX2=np.random.rand(20,2)+3 # Some points in the middle\nX3=np.random.rand(20,2) # Some points in the lower left corner\n\niter_and_draw(np.concatenate((X1,X2,X3)),3,5)\n\n\n\n\n\n\n\n\n\n\nA question?\n\nWhat to do if one cluster has no assigned points during iteration?\n\n\n\nFormula Derivation\nThe goal is to minimize the loss of inertia which is sum of the points to cluster centroids.\n\\[\nLoss= \\sum_{i=1}^n \\sum_{x \\in C_i} ||x-\\mu_i||^2\n\\]\nTo iter \\(\\mu\\) for each cluster, let us find the derivative of the following function. \\[\nf(\\mu)=\\sum_{i=1}^n ||x_i-\\mu||^2 =\n\\sum_{i=1}^n {x_i}^2+\\mu^2-2x_i\\mu\n\\]\nGiven a \\(\\nabla \\mu\\), \\[\nf(\\mu + \\nabla \\mu)=\\sum_{i=1}^n ||x_i+\\nabla \\mu -\\mu||^2 =\n\\sum_{i=1}^n  {x_i}^2+\\mu^2+{\\nabla \\mu}^2-2{x_i \\mu}-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\nf(\\mu + \\nabla \\mu)-f(\\mu)=\n\\sum_{i=1}^n {\\nabla \\mu}^2-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\n\\frac {f(\\mu + \\nabla \\mu)-f(\\mu)}{\\nabla \\mu}=\\sum_{i=1}^n {\\nabla \\mu} -2 \\mu +2{x_i} = 2\\sum_{i=1}^n x_i - 2n\\mu\n\\]\nNow we can see if \\(n\\mu = \\sum_{i=1}^n x_i\\), then the derivative is 0, this is why in each iteration, we need to set the center of the cluster as centroid."
  },
  {
    "objectID": "ML/index.html",
    "href": "ML/index.html",
    "title": "Machine Learning Topics",
    "section": "",
    "text": "Understanding Axis(Dim) Operations"
  },
  {
    "objectID": "ML/index.html#numpy-fundamentals",
    "href": "ML/index.html#numpy-fundamentals",
    "title": "Machine Learning Topics",
    "section": "",
    "text": "Understanding Axis(Dim) Operations"
  },
  {
    "objectID": "ML/index.html#clustering-algorithms",
    "href": "ML/index.html#clustering-algorithms",
    "title": "Machine Learning Topics",
    "section": "Clustering Algorithms",
    "text": "Clustering Algorithms\n\nK-Means Clustering"
  },
  {
    "objectID": "ML/index.html#deep-learning-fundamentals",
    "href": "ML/index.html#deep-learning-fundamentals",
    "title": "Machine Learning Topics",
    "section": "Deep Learning Fundamentals",
    "text": "Deep Learning Fundamentals\n\nThe XOR Problem: Nonlinearity in Deep Learning\nLikelihood-Based Loss Functions\nHidden Units and Activation Functions\nArchitecture Design: Depth vs Width\nBack-Propagation and Other Differentiation Algorithms\nChapter 7.11: Bagging and Other Ensemble Methods\nChapter 7.12: Dropout\nChapter 7.13: Adversarial Training\nChapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier\nChapter 8.1: How Learning Differs from Pure Optimization"
  },
  {
    "objectID": "ML/index.html#classification-algorithms",
    "href": "ML/index.html#classification-algorithms",
    "title": "Machine Learning Topics",
    "section": "Classification Algorithms",
    "text": "Classification Algorithms\n\nLogistic Regression"
  },
  {
    "objectID": "Algorithm/index.html",
    "href": "Algorithm/index.html",
    "title": "Algorithm Topics",
    "section": "",
    "text": "DP: Regular Expression Matching"
  },
  {
    "objectID": "Algorithm/index.html#dynamic-programming",
    "href": "Algorithm/index.html#dynamic-programming",
    "title": "Algorithm Topics",
    "section": "",
    "text": "DP: Regular Expression Matching"
  },
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "",
    "section": "Project Overview",
    "text": "Project Overview\nThis is a Quarto-based technical blog hosted on GitHub Pages (ickma2311.github.io). The site covers machine learning, algorithms, and technical tutorials with a focus on mathematical foundations and practical implementations."
  },
  {
    "objectID": "CLAUDE.html#common-commands",
    "href": "CLAUDE.html#common-commands",
    "title": "",
    "section": "Common Commands",
    "text": "Common Commands\n\nDevelopment Workflow\n\n./render-site.sh - Recommended: Clean build of entire website (handles cleanup automatically)\nquarto render - Build the entire website (outputs to docs/ directory)\n\n‚ö†Ô∏è May encounter file movement errors; use render-site.sh instead for reliable builds\n\nquarto preview - Start local development server with live reload\nquarto render &lt;file.qmd&gt; - Render a specific document\nquarto check - Verify Quarto installation and project setup\n\n\n\nContent Management\n\nCreate new ML content in ML/ directory\nCreate new algorithm content in Algorithm/ directory\nUpdate navigation by editing _quarto.yml navbar section\nAdd new content to respective index.qmd files for discoverability"
  },
  {
    "objectID": "CLAUDE.html#project-structure",
    "href": "CLAUDE.html#project-structure",
    "title": "",
    "section": "Project Structure",
    "text": "Project Structure\n‚îú‚îÄ‚îÄ _quarto.yml          # Main configuration file\n‚îú‚îÄ‚îÄ docs/                # Generated output (GitHub Pages source)\n‚îú‚îÄ‚îÄ index.qmd            # Homepage\n‚îú‚îÄ‚îÄ about.qmd            # About page\n‚îú‚îÄ‚îÄ ML/                  # Machine Learning content\n‚îÇ   ‚îú‚îÄ‚îÄ index.qmd        # ML topics overview\n‚îÇ   ‚îú‚îÄ‚îÄ *.qmd            # ML articles\n‚îÇ   ‚îî‚îÄ‚îÄ *.ipynb          # Jupyter notebooks\n‚îú‚îÄ‚îÄ Algorithm/           # Algorithm content\n‚îÇ   ‚îú‚îÄ‚îÄ index.qmd        # Algorithm topics overview\n‚îÇ   ‚îî‚îÄ‚îÄ *.qmd            # Algorithm articles\n‚îú‚îÄ‚îÄ imgs/                # Image assets\n‚îú‚îÄ‚îÄ media/               # Media files\n‚îî‚îÄ‚îÄ styles.css           # Custom CSS styles"
  },
  {
    "objectID": "CLAUDE.html#content-organization",
    "href": "CLAUDE.html#content-organization",
    "title": "",
    "section": "Content Organization",
    "text": "Content Organization\nThe site uses a hierarchical navigation structure defined in _quarto.yml: - Two main sections: ‚ÄúML‚Äù and ‚ÄúAlgorithm‚Äù - Each section has an index page that serves as a directory - Content is categorized by topic (e.g., ‚ÄúNumPy Fundamentals‚Äù, ‚ÄúClustering Algorithms‚Äù)\n\nAdding New Content\n\nCreate the content file in the appropriate directory (ML/ or Math/ or Algorithm/)\nAdd to list page: Update the corresponding list page (e.g., ML/deep-learning-book.qmd, Math/MIT18.06/lectures.qmd)\nUpdate homepage automatically: Run ./update-counts.sh to update section counts on homepage\nAdd navigation entry to _quarto.yml if it should appear in the navbar dropdown\nUse consistent frontmatter with title field\nSet publication date: Always use the current date from the system for the date field in frontmatter\n\nGet current date with: date +\"%Y-%m-%d\" (format: YYYY-MM-DD)\nExample: date: \"2025-10-26\"\n\nImportant: After adding new navbar items, run quarto render (full site render) to update the navbar on ALL existing pages. Individual file renders only update that specific page.\n\n\n\nMaintaining the Homepage\nThe homepage (index.qmd) shows the latest 4 items from each section with a count badge.\nCRITICAL: When adding new content, you MUST:\n\nAdd the new item to the appropriate list page:\n\nDeep Learning: ML/deep-learning-book.qmd\nMIT 18.06SC: Math/MIT18.06/lectures.qmd\nMIT 18.065: Math/MIT18.065/lectures.qmd\n\nUpdate the homepage manually by editing index.qmd:\n\nReplace the oldest item in the ‚ÄúLatest 4‚Äù with the new item\nKeep the 4 most recent items visible\nOrder: newest first (top), oldest last (bottom)\n\nUpdate section counts automatically:\n./update-counts.sh\nThis script automatically counts items in list pages and updates the count badges in index.qmd.\nRender and verify:\nquarto render index.qmd\n\nExample workflow when adding Chapter 9.8:\n# 1. Create the new content file\nvim ML/chapter-9-8.qmd\n\n# 2. Add to list page\nvim ML/deep-learning-book.qmd  # Add Chapter 9.8 entry\n\n# 3. Update homepage\nvim index.qmd  # Replace Chapter 9.4 with 9.8, keep 9.7, 9.6, and 9.5\n\n# 4. Update counts automatically\n./update-counts.sh\n\n# 5. Render\nquarto render index.qmd\nWarning: The homepage will NOT automatically update when you add new content. You must manually update index.qmd to show the latest 4 items."
  },
  {
    "objectID": "CLAUDE.html#configuration-notes",
    "href": "CLAUDE.html#configuration-notes",
    "title": "",
    "section": "Configuration Notes",
    "text": "Configuration Notes\n\nOutput directory is set to docs/ for GitHub Pages compatibility\nTheme: Cosmo with custom branding\nAll pages include table of contents (toc: true)\nSite uses custom CSS from styles.css\nJupyter notebooks are supported alongside Quarto markdown"
  },
  {
    "objectID": "CLAUDE.html#github-pages-deployment",
    "href": "CLAUDE.html#github-pages-deployment",
    "title": "",
    "section": "GitHub Pages Deployment",
    "text": "GitHub Pages Deployment\nThe site is automatically deployed from the docs/ directory. After rendering, commit and push the docs/ folder to trigger GitHub Pages rebuild. - Author is Chao Ma - GitHub Pages URL: https://ickma2311.github.io/\n\nPre-Push Checklist (CRITICAL)\nALWAYS verify locally before pushing to prevent broken images on production:\n\nRun full render: Use ./render-site.sh for automatic cleanup, or quarto render (not individual file renders)\nCRITICAL: Restore ALL deleted images after full render\nFull site renders delete images from multiple locations. You MUST restore them:\n# 1. Restore *_files/ directories (code-generated matplotlib/plotly figures)\ncp -r ML/*_files docs/ML/ 2&gt;/dev/null\ncp -r Math/*_files docs/Math/ 2&gt;/dev/null\ncp -r Algorithm/*_files docs/Algorithm/ 2&gt;/dev/null\n\n# 2. Restore static images in ML/ and Math/\ncp ML/*.png docs/ML/ 2&gt;/dev/null\ncp Math/MIT18.06/*.png docs/Math/MIT18.06/ 2&gt;/dev/null\n\n# 3. Restore media/ directory images (referenced with ../media/image.png)\nmkdir -p docs/media\ncp media/*.png docs/media/ 2&gt;/dev/null\n\n# 4. Restore imgs/ directory images (referenced with ../imgs/image.png)\nmkdir -p docs/imgs\ncp imgs/*.png docs/imgs/ 2&gt;/dev/null\n\n# 5. Move any HTML files that failed to move\nfind ML -maxdepth 1 -name \"*.html\" -exec mv {} docs/ML/ \\; 2&gt;/dev/null\nfind Math/MIT18.06 -maxdepth 1 -name \"*.html\" -exec mv {} docs/Math/MIT18.06/ \\; 2&gt;/dev/null\nfind Algorithm -maxdepth 1 -name \"*.html\" -exec mv {} docs/Algorithm/ \\; 2&gt;/dev/null\n\n# 6. Move index files\nmv Algorithm/index.html docs/Algorithm/ 2&gt;/dev/null\nmv Math/index.html docs/Math/ 2&gt;/dev/null\nmv index-backup.html docs/ 2&gt;/dev/null\nCheck git status: git status - verify all image files are staged\n\nLook for docs/**/*_files/figure-html/*.png files (code-generated)\nLook for docs/media/*.png files (media directory)\nLook for docs/imgs/*.png files (imgs directory)\nLook for docs/ML/*.png and docs/Math/MIT18.06/*.png files (static images)\n\nLocal preview: Open docs/ HTML files in browser to verify images load\n\nCheck pages with Jupyter notebooks (e.g., mit1806-lecture1-geometry.html)\nCheck pages with media references (e.g., dropout.html)\nVerify matplotlib/plotly figures appear correctly\n\nCommit ALL generated files: Don‚Äôt commit .html without their images\nOnly then push: git push\n\nWhy this matters: GitHub Pages serves from the docs/ directory. Quarto‚Äôs full render deletes images from docs/ but keeps them in source directories. If images aren‚Äôt copied back to docs/, the HTML will reference missing files, causing broken images on production even though they work locally.\nImage locations that get deleted: - docs/**/*_files/ - Code-generated figures from Python/matplotlib - docs/media/ - Shared media referenced with ../media/ - docs/imgs/ - Shared images referenced with ../imgs/ - docs/ML/*.png - Static chapter images - docs/Math/MIT18.06/*.png - Static lecture images"
  },
  {
    "objectID": "CLAUDE.html#linkedin-post-guidelines",
    "href": "CLAUDE.html#linkedin-post-guidelines",
    "title": "",
    "section": "LinkedIn Post Guidelines",
    "text": "LinkedIn Post Guidelines\n\nEmoji Usage\nWhen drafting LinkedIn posts for blog content, use these emojis: - Deep Learning topics: ‚àá (delta/nabla symbol) - represents gradients and optimization - Linear Algebra topics: üìê (triangle/ruler) - represents geometric and matrix concepts\n\n\nWriting Process\n\nIdentify the key insight: Focus on the main conceptual connection or ‚Äúaha moment‚Äù from the blog post\nUse ‚Äúconnecting the dots‚Äù tone: Emphasize how concepts link together (e.g., ‚Äúhow linear algebra connects to machine learning‚Äù)\nStructure (Knowledge Card Format):\n\nStart with emoji and chapter reference (e.g., ‚Äú‚àá Deep Learning Book (Chapter 8.1)‚Äù or ‚Äúüìê MIT 18.06SC Linear Algebra (Lecture 19)‚Äù)\nClear statement or equation (e.g., ‚ÄúLearning ‚â† Optimization‚Äù)\n2-4 bullet points (üîπ) with key insights\nPhilosophical closing line (üí°)\nLink to full blog post (üìñ)\nSource attribution at the end (e.g., ‚ÄúMy notes on Deep Learning (Ian Goodfellow) Chapter X.X‚Äù or ‚ÄúMy notes on MIT 18.06SC Linear Algebra - Lecture XX‚Äù)\n\nKeep it concise: Aim for clarity over comprehensiveness - use knowledge card format for quick, digestible insights\nCourse naming:\n\nAlways use ‚ÄúMIT 18.06SC‚Äù (not just ‚ÄúMIT 18.06‚Äù) for linear algebra posts\nUse full course titles to maintain consistency\n\nInclude relevant hashtags: #MachineLearning #LinearAlgebra #DeepLearning"
  },
  {
    "objectID": "Math/index.html",
    "href": "Math/index.html",
    "title": "Math",
    "section": "",
    "text": "Mathematical foundations and explorations.\n\n\n\nThe Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots\n\n\n\n\n\nLecture 1: Geometry of Linear Equations\nLecture 2: Elimination with Matrices\nLecture 3: Matrix Multiplication and Inverse\nLecture 4: LU Decomposition\nLecture 5.1: Permutation Matrices\nLecture 5.2: Transpose\nLecture 5.3: Vector Spaces\nLecture 6: Column Space and Null Space\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nLecture 9: Independence, Basis, and Dimension\nLecture 10: Four Fundamental Subspaces\nLecture 11: Matrix Spaces, Rank-1, and Graphs\nLecture 12: Graphs, Networks, and Incidence Matrices\nLecture 13: Quiz 1 Review\nLecture 14: Orthogonal Vectors and Subspaces\nLecture 15: Projection onto Subspaces\nLecture 16: Projection Matrices and Least Squares\nLecture 17: Orthogonal Matrices and Gram-Schmidt\nLecture 18: Properties of Determinants\nLecture 19: Determinant Formulas and Cofactors\nLecture 20: Cramer‚Äôs Rule, Inverse Matrix, and Volume\nLecture 21: Eigenvalues and Eigenvectors\nLecture 22: Diagonalization and Powers of A\nLecture 23: Differential Equations and exp(At)"
  },
  {
    "objectID": "Math/index.html#reflections-synthesis",
    "href": "Math/index.html#reflections-synthesis",
    "title": "Math",
    "section": "",
    "text": "The Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots"
  },
  {
    "objectID": "Math/index.html#mit-18.06sc-linear-algebra",
    "href": "Math/index.html#mit-18.06sc-linear-algebra",
    "title": "Math",
    "section": "",
    "text": "Lecture 1: Geometry of Linear Equations\nLecture 2: Elimination with Matrices\nLecture 3: Matrix Multiplication and Inverse\nLecture 4: LU Decomposition\nLecture 5.1: Permutation Matrices\nLecture 5.2: Transpose\nLecture 5.3: Vector Spaces\nLecture 6: Column Space and Null Space\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nLecture 9: Independence, Basis, and Dimension\nLecture 10: Four Fundamental Subspaces\nLecture 11: Matrix Spaces, Rank-1, and Graphs\nLecture 12: Graphs, Networks, and Incidence Matrices\nLecture 13: Quiz 1 Review\nLecture 14: Orthogonal Vectors and Subspaces\nLecture 15: Projection onto Subspaces\nLecture 16: Projection Matrices and Least Squares\nLecture 17: Orthogonal Matrices and Gram-Schmidt\nLecture 18: Properties of Determinants\nLecture 19: Determinant Formulas and Cofactors\nLecture 20: Cramer‚Äôs Rule, Inverse Matrix, and Volume\nLecture 21: Eigenvalues and Eigenvectors\nLecture 22: Diagonalization and Powers of A\nLecture 23: Differential Equations and exp(At)"
  },
  {
    "objectID": "Math/MIT18.06/lectures.html",
    "href": "Math/MIT18.06/lectures.html",
    "title": "MIT 18.06SC Linear Algebra",
    "section": "",
    "text": "My journey through MIT‚Äôs Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.\n\n\n\n\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation.\n\n\nFrom Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series The beautiful mathematical journey from Taylor expansions to Euler‚Äôs formula and ultimately to Fourier series, revealing deep connections between polynomials, exponentials, and trigonometric functions.\n\n\n\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations Two powerful perspectives: row picture vs column picture.\n\n\nLecture 2: Elimination with Matrices The systematic algorithm that transforms linear systems into upper triangular form.\n\n\nLecture 3: Matrix Multiplication and Inverse Five different perspectives on matrix multiplication.\n\n\nLecture 4: LU Decomposition Factoring matrices into Lower √ó Upper triangular form.\n\n\nLecture 5.1: Permutation Matrices Permutation matrices reorder rows and columns.\n\n\nLecture 5.2: Transpose The transpose operation switches rows to columns.\n\n\nLecture 5.3: Vector Spaces Vector spaces and subspaces: closed under addition and scalar multiplication.\n\n\nLecture 6: Column Space and Null Space Column space determines which \\(b\\) make \\(Ax = b\\) solvable.\n\n\nLecture 7: Solving Ax=0 Systematic algorithm to find null space using pivot/free variables and RREF.\n\n\nLecture 8: Solving Ax=b Complete solution is particular solution plus null space.\n\n\nLecture 9: Independence, Basis, and Dimension Linear independence, basis, and dimension. Rank-nullity theorem.\n\n\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix.\n\n\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.\n\n\nLecture 12: Graphs, Networks, and Incidence Matrices Applying linear algebra to graph theory: incidence matrices, Kirchhoff‚Äôs laws, and Euler‚Äôs formula.\n\n\nLecture 13: Quiz 1 Review Practice problems reviewing key concepts: subspaces, rank, null spaces, and the four fundamental subspaces.\n\n\n\n\n\n\n\n\nLecture 14: Orthogonal Vectors and Subspaces Orthogonal vectors, orthogonal subspaces, and the fundamental relationships between the four subspaces.\n\n\nLecture 15: Projection onto Subspaces Projecting onto lines and subspaces, the geometry of least squares, and deriving normal equations.\n\n\nLecture 16: Projection Matrices and Least Squares Projection matrices, least squares fitting, and the connection between linear algebra and calculus optimization.\n\n\nLecture 17: Orthogonal Matrices and Gram-Schmidt Orthonormal matrices, the Gram-Schmidt process for orthogonalization, and QR factorization.\n\n\nLecture 18: Properties of Determinants Ten fundamental properties that completely define the determinant and reveal when matrices are invertible.\n\n\nLecture 19: Determinant Formulas and Cofactors Three computational methods for determinants: pivots, the big formula, and cofactor expansion.\n\n\nLecture 20: Inverse & Volume The inverse matrix formula using cofactors, Cramer‚Äôs rule for solving linear systems, and the geometric interpretation of determinants as volume.\n\n\nLecture 21: Eigenvalues and Eigenvectors The directions that matrices can only scale, not rotate: \\(Ax = \\lambda x\\).\n\n\nLecture 22: Diagonalization and Powers of A Computing matrix powers efficiently and solving Fibonacci with eigenvalues.\n\n\nLecture 23: Differential Equations and exp(At) Connecting linear algebra to differential equations: how eigenvalues determine stability and matrix exponentials solve systems.\n\n\nLecture 24: Markov Matrices and Fourier Series Two powerful applications of eigenvalues: Markov matrices for modeling state transitions and Fourier series for decomposing functions into orthogonal components.\n\n\n\n\n\n\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 29: Singular Value Decomposition The most important matrix factorization: SVD provides orthonormal bases for all four fundamental subspaces, revealing how any matrix maps its row space to its column space through scaling by singular values.\n\n\nLecture 30: Linear Transformations and Their Matrices The fundamental connection between abstract linear transformations and concrete matrices: every linear transformation can be represented as matrix multiplication, and the choice of basis determines the matrix representation.\n\n\nLecture 31: Change of Basis and Image Compression How choosing the right basis enables compression: JPEG transforms 512√ó512 images (262,144 pixels) to Fourier basis and discards small coefficients. Three properties of good bases‚Äîfast inverse (FFT in O(n log n)), sparsity (few coefficients enough), and orthogonality (no redundancy).\n\n\nLecture 33: Left and Right Inverse; Pseudo-inverse When matrices aren‚Äôt square: full column rank matrices (\\(r = n &lt; m\\)) have left inverses \\((A^T A)^{-1} A^T\\), full row rank matrices (\\(r = m &lt; n\\)) have right inverses \\(A^T (AA^T)^{-1}\\), and the pseudo-inverse \\(A^+ = V \\Sigma^+ U^T\\) generalizes both using SVD‚Äîinverting non-zero singular values and transposing the shape."
  },
  {
    "objectID": "Math/MIT18.06/lectures.html#all-lectures",
    "href": "Math/MIT18.06/lectures.html#all-lectures",
    "title": "MIT 18.06SC Linear Algebra",
    "section": "",
    "text": "My journey through MIT‚Äôs Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.\n\n\n\n\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation.\n\n\nFrom Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series The beautiful mathematical journey from Taylor expansions to Euler‚Äôs formula and ultimately to Fourier series, revealing deep connections between polynomials, exponentials, and trigonometric functions.\n\n\n\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations Two powerful perspectives: row picture vs column picture.\n\n\nLecture 2: Elimination with Matrices The systematic algorithm that transforms linear systems into upper triangular form.\n\n\nLecture 3: Matrix Multiplication and Inverse Five different perspectives on matrix multiplication.\n\n\nLecture 4: LU Decomposition Factoring matrices into Lower √ó Upper triangular form.\n\n\nLecture 5.1: Permutation Matrices Permutation matrices reorder rows and columns.\n\n\nLecture 5.2: Transpose The transpose operation switches rows to columns.\n\n\nLecture 5.3: Vector Spaces Vector spaces and subspaces: closed under addition and scalar multiplication.\n\n\nLecture 6: Column Space and Null Space Column space determines which \\(b\\) make \\(Ax = b\\) solvable.\n\n\nLecture 7: Solving Ax=0 Systematic algorithm to find null space using pivot/free variables and RREF.\n\n\nLecture 8: Solving Ax=b Complete solution is particular solution plus null space.\n\n\nLecture 9: Independence, Basis, and Dimension Linear independence, basis, and dimension. Rank-nullity theorem.\n\n\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix.\n\n\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.\n\n\nLecture 12: Graphs, Networks, and Incidence Matrices Applying linear algebra to graph theory: incidence matrices, Kirchhoff‚Äôs laws, and Euler‚Äôs formula.\n\n\nLecture 13: Quiz 1 Review Practice problems reviewing key concepts: subspaces, rank, null spaces, and the four fundamental subspaces.\n\n\n\n\n\n\n\n\nLecture 14: Orthogonal Vectors and Subspaces Orthogonal vectors, orthogonal subspaces, and the fundamental relationships between the four subspaces.\n\n\nLecture 15: Projection onto Subspaces Projecting onto lines and subspaces, the geometry of least squares, and deriving normal equations.\n\n\nLecture 16: Projection Matrices and Least Squares Projection matrices, least squares fitting, and the connection between linear algebra and calculus optimization.\n\n\nLecture 17: Orthogonal Matrices and Gram-Schmidt Orthonormal matrices, the Gram-Schmidt process for orthogonalization, and QR factorization.\n\n\nLecture 18: Properties of Determinants Ten fundamental properties that completely define the determinant and reveal when matrices are invertible.\n\n\nLecture 19: Determinant Formulas and Cofactors Three computational methods for determinants: pivots, the big formula, and cofactor expansion.\n\n\nLecture 20: Inverse & Volume The inverse matrix formula using cofactors, Cramer‚Äôs rule for solving linear systems, and the geometric interpretation of determinants as volume.\n\n\nLecture 21: Eigenvalues and Eigenvectors The directions that matrices can only scale, not rotate: \\(Ax = \\lambda x\\).\n\n\nLecture 22: Diagonalization and Powers of A Computing matrix powers efficiently and solving Fibonacci with eigenvalues.\n\n\nLecture 23: Differential Equations and exp(At) Connecting linear algebra to differential equations: how eigenvalues determine stability and matrix exponentials solve systems.\n\n\nLecture 24: Markov Matrices and Fourier Series Two powerful applications of eigenvalues: Markov matrices for modeling state transitions and Fourier series for decomposing functions into orthogonal components.\n\n\n\n\n\n\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 29: Singular Value Decomposition The most important matrix factorization: SVD provides orthonormal bases for all four fundamental subspaces, revealing how any matrix maps its row space to its column space through scaling by singular values.\n\n\nLecture 30: Linear Transformations and Their Matrices The fundamental connection between abstract linear transformations and concrete matrices: every linear transformation can be represented as matrix multiplication, and the choice of basis determines the matrix representation.\n\n\nLecture 31: Change of Basis and Image Compression How choosing the right basis enables compression: JPEG transforms 512√ó512 images (262,144 pixels) to Fourier basis and discards small coefficients. Three properties of good bases‚Äîfast inverse (FFT in O(n log n)), sparsity (few coefficients enough), and orthogonality (no redundancy).\n\n\nLecture 33: Left and Right Inverse; Pseudo-inverse When matrices aren‚Äôt square: full column rank matrices (\\(r = n &lt; m\\)) have left inverses \\((A^T A)^{-1} A^T\\), full row rank matrices (\\(r = m &lt; n\\)) have right inverses \\(A^T (AA^T)^{-1}\\), and the pseudo-inverse \\(A^+ = V \\Sigma^+ U^T\\) generalizes both using SVD‚Äîinverting non-zero singular values and transposing the shape."
  },
  {
    "objectID": "Math/MIT18.065/lectures.html",
    "href": "Math/MIT18.065/lectures.html",
    "title": "MIT 18.065: Linear Algebra Applications",
    "section": "",
    "text": "My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. This course explores how linear algebra powers modern applications in data analysis, signal processing, and machine learning.\n\n\n\nLecture 8: Norms of Vectors and Matrices Understanding vector p-norms (\\(\\|v\\|_p\\)) and when they satisfy the triangle inequality (only for \\(p \\geq 1\\)). The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù creates non-convex unit balls for strong sparsity. S-norms \\(\\sqrt{v^T S v}\\) generalize to ellipses. Matrix norms: spectral \\(\\|A\\|_2 = \\sigma_1\\), Frobenius \\(\\|A\\|_F = \\sqrt{\\sum \\sigma_i^2}\\), nuclear \\(\\|A\\|_N = \\sum \\sigma_i\\)‚Äîall expressed via singular values.\n\n\nLecture 9: Four Ways to Solve Least Squares Problems Four equivalent methods for solving \\(Ax = b\\) when \\(A\\) has no inverse: pseudo-inverse \\(\\hat{x} = A^+ b\\) using SVD, normal equations \\(A^T A \\hat{x} = A^T b\\), algebraic minimization of \\(\\|Ax - b\\|_2^2\\), and geometric projection of \\(b\\) onto \\(C(A)\\). All methods converge to the same solution when \\(A^T A\\) is invertible: \\(\\hat{x} = (A^T A)^{-1}A^T b\\)."
  },
  {
    "objectID": "Math/MIT18.065/lectures.html#all-lectures",
    "href": "Math/MIT18.065/lectures.html#all-lectures",
    "title": "MIT 18.065: Linear Algebra Applications",
    "section": "",
    "text": "My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. This course explores how linear algebra powers modern applications in data analysis, signal processing, and machine learning.\n\n\n\nLecture 8: Norms of Vectors and Matrices Understanding vector p-norms (\\(\\|v\\|_p\\)) and when they satisfy the triangle inequality (only for \\(p \\geq 1\\)). The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù creates non-convex unit balls for strong sparsity. S-norms \\(\\sqrt{v^T S v}\\) generalize to ellipses. Matrix norms: spectral \\(\\|A\\|_2 = \\sigma_1\\), Frobenius \\(\\|A\\|_F = \\sqrt{\\sum \\sigma_i^2}\\), nuclear \\(\\|A\\|_N = \\sum \\sigma_i\\)‚Äîall expressed via singular values.\n\n\nLecture 9: Four Ways to Solve Least Squares Problems Four equivalent methods for solving \\(Ax = b\\) when \\(A\\) has no inverse: pseudo-inverse \\(\\hat{x} = A^+ b\\) using SVD, normal equations \\(A^T A \\hat{x} = A^T b\\), algebraic minimization of \\(\\|Ax - b\\|_2^2\\), and geometric projection of \\(b\\) onto \\(C(A)\\). All methods converge to the same solution when \\(A^T A\\) is invertible: \\(\\hat{x} = (A^T A)^{-1}A^T b\\)."
  },
  {
    "objectID": "Math/EE364A/lectures.html",
    "href": "Math/EE364A/lectures.html",
    "title": "Stanford EE 364A: Convex Optimization",
    "section": "",
    "text": "My notes from Stanford EE 364A: Convex Optimization. This course covers the theory and applications of convex optimization, including least squares, linear programming, and convex optimization problems.\n\n\n\nLecture 1: Introduction to Convex Optimization Introduction to constraint optimization problems: least squares (\\(\\|Ax-b\\|_2^2\\)), linear programming (\\(c^\\top x\\) with linear constraints), and convex optimization (convex objective and constraints). Convex optimization generalizes both while maintaining polynomial-time solvability: \\(O(\\max(n^3, n^2m, F))\\) where \\(F\\) is derivative computation cost.\n\n\nLecture 2: Convex Sets Convex sets (affine combinations, convex combinations, convex hull, convex cone), hyperplanes and halfspaces, Euclidean balls and ellipsoids, norm balls and norm cones (including second-order cone), polyhedra, positive semidefinite cone, operations preserving convexity (intersection, affine functions, perspective, linear-fractional), generalized inequalities (proper cones, componentwise and matrix inequalities), and minimum vs minimal elements."
  },
  {
    "objectID": "Math/EE364A/lectures.html#all-lectures",
    "href": "Math/EE364A/lectures.html#all-lectures",
    "title": "Stanford EE 364A: Convex Optimization",
    "section": "",
    "text": "My notes from Stanford EE 364A: Convex Optimization. This course covers the theory and applications of convex optimization, including least squares, linear programming, and convex optimization problems.\n\n\n\nLecture 1: Introduction to Convex Optimization Introduction to constraint optimization problems: least squares (\\(\\|Ax-b\\|_2^2\\)), linear programming (\\(c^\\top x\\) with linear constraints), and convex optimization (convex objective and constraints). Convex optimization generalizes both while maintaining polynomial-time solvability: \\(O(\\max(n^3, n^2m, F))\\) where \\(F\\) is derivative computation cost.\n\n\nLecture 2: Convex Sets Convex sets (affine combinations, convex combinations, convex hull, convex cone), hyperplanes and halfspaces, Euclidean balls and ellipsoids, norm balls and norm cones (including second-order cone), polyhedra, positive semidefinite cone, operations preserving convexity (intersection, affine functions, perspective, linear-fractional), generalized inequalities (proper cones, componentwise and matrix inequalities), and minimum vs minimal elements."
  },
  {
    "objectID": "ML/deep-learning-book.html",
    "href": "ML/deep-learning-book.html",
    "title": "Deep Learning Book",
    "section": "",
    "text": "My notes and implementations while studying the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\n\n\n\nChapter 6.1: XOR Problem & ReLU Networks How ReLU solves problems that linear models cannot handle.\n\n\nChapter 6.2: Likelihood-Based Loss Functions The mathematical connection between probabilistic models and loss functions.\n\n\nChapter 6.3: Hidden Units and Activation Functions Exploring activation functions and their impact on neural network learning.\n\n\nChapter 6.4: Architecture Design - Depth vs Width How depth enables hierarchical feature reuse and exponential expressiveness.\n\n\nChapter 6.5: Back-Propagation and Other Differentiation Algorithms The algorithm that makes training deep networks computationally feasible.\n\n\n\n\n\n\n\n\nChapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature Essential second-order calculus concepts needed before Chapter 7.\n\n\nChapter 7.1.1: L2 Regularization How L2 regularization shrinks weights based on Hessian eigenvalues.\n\n\nChapter 7.1.2: L1 Regularization L1 regularization uses soft thresholding to create sparse solutions.\n\n\nChapter 7.2: Constrained Optimization View of Regularization Regularization as constrained optimization with KKT conditions.\n\n\nChapter 7.3: Regularization and Under-Constrained Problems Why regularization is mathematically necessary and ensures invertibility.\n\n\nChapter 7.4: Dataset Augmentation How transforming existing data improves generalization.\n\n\nChapter 7.5: Noise Robustness How adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\nChapter 7.6: Semi-Supervised Learning Leveraging unlabeled data to improve model performance when labeled data is scarce.\n\n\nChapter 7.7: Multi-Task Learning Training a single model on multiple related tasks to improve generalization.\n\n\nChapter 7.8: Early Stopping Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.\n\n\nChapter 7.9: Parameter Tying and Parameter Sharing Two strategies for reducing parameters: encouraging similarity vs.¬†enforcing identity.\n\n\nChapter 7.10: Sparse Representations Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.\n\n\nChapter 7.11: Bagging and Other Ensemble Methods How training multiple models on bootstrap samples and averaging their predictions reduces variance.\n\n\nChapter 7.12: Dropout Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.\n\n\nChapter 7.13: Adversarial Training How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.\n\n\nChapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.\n\n\n\n\n\n\n\n\nChapter 8.1: How Learning Differs from Pure Optimization Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.\n\n\nChapter 8.2: Challenges in Deep Learning Optimization Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.\n\n\nChapter 8.3: Basic Algorithms SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.\n\n\nChapter 8.4: Parameter Initialization Strategies Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.\n\n\nChapter 8.5: Algorithms with Adaptive Learning Rates From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.\n\n\nChapter 8.6: Second-Order Optimization Methods Newton‚Äôs method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.\n\n\nChapter 8.7: Optimization Strategies and Meta-Algorithms Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.\n\n\n\n\n\n\n\n\nChapter 9.1: Convolution Computation The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.\n\n\nChapter 9.2: Motivation for Convolutional Networks Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m¬∑n) to O(k¬∑n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere‚Äîachieving 30,000√ó speedup over dense layers.\n\n\nChapter 9.3: Pooling Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28√ó28 feature maps to 14√ó14 (4√ó fewer activations). Comparing three architectures‚Äîstrided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.\n\n\nChapter 9.4: Convolution and Pooling as an Infinitely Strong Prior Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off‚Äîstrong priors reduce sample complexity but only when assumptions match the data structure.\n\n\nChapter 9.5: Convolutional Functions Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula \\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\), stride for downsampling, three padding strategies (valid/same/full), and gradient computation‚Äîkernel gradients via correlation with input, input gradients via convolution with flipped kernel.\n\n\nChapter 9.6: Structured Outputs CNNs can generate high-dimensional structured objects through pixel-level predictions. Preserving spatial dimensions (no pooling, no stride &gt; 1, SAME padding) enables full-resolution outputs. Recurrent convolution refines predictions iteratively: \\(U*X + H(t-1)*W = H(t)\\), producing dense predictions for segmentation, depth estimation, and flow prediction.\n\n\nChapter 9.7: Data Types CNNs can operate on different data types: 1D (audio, time series), 2D (images), and 3D (videos, CT scans) with varying channel counts. Unlike fully connected networks, convolutional kernels handle variable-sized inputs by sliding across spatial dimensions, producing outputs that scale accordingly‚Äîa unique flexibility for diverse domains.\n\n\nChapter 9.8: Efficient Convolution Algorithms Separable convolution reduces computational cost from \\(O(HWk^2)\\) to \\(O(HWk)\\) by decomposing a 2D kernel into two 1D filters (vertical and horizontal). Parameter storage shrinks from \\(k^2\\) to \\(2k\\). This factorization enables faster, more memory-efficient models without sacrificing accuracy‚Äîfoundational for architectures like MobileNet.\n\n\nChapter 9.9: Unsupervised or Semi-Supervised Feature Learning Before CNNs, computer vision relied on hand-crafted kernels (Sobel, Laplacian, Gaussian) and unsupervised methods (sparse coding, autoencoders, k-means). While these captured simple patterns, they couldn‚Äôt match CNNs‚Äô hierarchical, end-to-end feature learning. Modern systems use CNNs to learn features from edges to semantic concepts‚Äîmaking hand-crafted filters largely obsolete.\n\n\nChapter 9.10: Neuroscientific Basis for Convolutional Networks V1 simple cells detect oriented edges (modeled by Gabor filters), complex cells pool over simple cells for translation invariance (like CNN pooling). But CNNs lack key biological features: saccadic attention, multisensory integration, top-down feedback, and dynamic receptive fields. While CNNs excel at feed-forward recognition, biological vision is holistic, context-aware, and adaptive."
  },
  {
    "objectID": "ML/deep-learning-book.html#all-chapters",
    "href": "ML/deep-learning-book.html#all-chapters",
    "title": "Deep Learning Book",
    "section": "",
    "text": "My notes and implementations while studying the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\n\n\n\nChapter 6.1: XOR Problem & ReLU Networks How ReLU solves problems that linear models cannot handle.\n\n\nChapter 6.2: Likelihood-Based Loss Functions The mathematical connection between probabilistic models and loss functions.\n\n\nChapter 6.3: Hidden Units and Activation Functions Exploring activation functions and their impact on neural network learning.\n\n\nChapter 6.4: Architecture Design - Depth vs Width How depth enables hierarchical feature reuse and exponential expressiveness.\n\n\nChapter 6.5: Back-Propagation and Other Differentiation Algorithms The algorithm that makes training deep networks computationally feasible.\n\n\n\n\n\n\n\n\nChapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature Essential second-order calculus concepts needed before Chapter 7.\n\n\nChapter 7.1.1: L2 Regularization How L2 regularization shrinks weights based on Hessian eigenvalues.\n\n\nChapter 7.1.2: L1 Regularization L1 regularization uses soft thresholding to create sparse solutions.\n\n\nChapter 7.2: Constrained Optimization View of Regularization Regularization as constrained optimization with KKT conditions.\n\n\nChapter 7.3: Regularization and Under-Constrained Problems Why regularization is mathematically necessary and ensures invertibility.\n\n\nChapter 7.4: Dataset Augmentation How transforming existing data improves generalization.\n\n\nChapter 7.5: Noise Robustness How adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\nChapter 7.6: Semi-Supervised Learning Leveraging unlabeled data to improve model performance when labeled data is scarce.\n\n\nChapter 7.7: Multi-Task Learning Training a single model on multiple related tasks to improve generalization.\n\n\nChapter 7.8: Early Stopping Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.\n\n\nChapter 7.9: Parameter Tying and Parameter Sharing Two strategies for reducing parameters: encouraging similarity vs.¬†enforcing identity.\n\n\nChapter 7.10: Sparse Representations Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.\n\n\nChapter 7.11: Bagging and Other Ensemble Methods How training multiple models on bootstrap samples and averaging their predictions reduces variance.\n\n\nChapter 7.12: Dropout Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.\n\n\nChapter 7.13: Adversarial Training How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.\n\n\nChapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.\n\n\n\n\n\n\n\n\nChapter 8.1: How Learning Differs from Pure Optimization Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.\n\n\nChapter 8.2: Challenges in Deep Learning Optimization Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.\n\n\nChapter 8.3: Basic Algorithms SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.\n\n\nChapter 8.4: Parameter Initialization Strategies Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.\n\n\nChapter 8.5: Algorithms with Adaptive Learning Rates From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.\n\n\nChapter 8.6: Second-Order Optimization Methods Newton‚Äôs method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.\n\n\nChapter 8.7: Optimization Strategies and Meta-Algorithms Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.\n\n\n\n\n\n\n\n\nChapter 9.1: Convolution Computation The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.\n\n\nChapter 9.2: Motivation for Convolutional Networks Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m¬∑n) to O(k¬∑n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere‚Äîachieving 30,000√ó speedup over dense layers.\n\n\nChapter 9.3: Pooling Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28√ó28 feature maps to 14√ó14 (4√ó fewer activations). Comparing three architectures‚Äîstrided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.\n\n\nChapter 9.4: Convolution and Pooling as an Infinitely Strong Prior Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off‚Äîstrong priors reduce sample complexity but only when assumptions match the data structure.\n\n\nChapter 9.5: Convolutional Functions Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula \\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\), stride for downsampling, three padding strategies (valid/same/full), and gradient computation‚Äîkernel gradients via correlation with input, input gradients via convolution with flipped kernel.\n\n\nChapter 9.6: Structured Outputs CNNs can generate high-dimensional structured objects through pixel-level predictions. Preserving spatial dimensions (no pooling, no stride &gt; 1, SAME padding) enables full-resolution outputs. Recurrent convolution refines predictions iteratively: \\(U*X + H(t-1)*W = H(t)\\), producing dense predictions for segmentation, depth estimation, and flow prediction.\n\n\nChapter 9.7: Data Types CNNs can operate on different data types: 1D (audio, time series), 2D (images), and 3D (videos, CT scans) with varying channel counts. Unlike fully connected networks, convolutional kernels handle variable-sized inputs by sliding across spatial dimensions, producing outputs that scale accordingly‚Äîa unique flexibility for diverse domains.\n\n\nChapter 9.8: Efficient Convolution Algorithms Separable convolution reduces computational cost from \\(O(HWk^2)\\) to \\(O(HWk)\\) by decomposing a 2D kernel into two 1D filters (vertical and horizontal). Parameter storage shrinks from \\(k^2\\) to \\(2k\\). This factorization enables faster, more memory-efficient models without sacrificing accuracy‚Äîfoundational for architectures like MobileNet.\n\n\nChapter 9.9: Unsupervised or Semi-Supervised Feature Learning Before CNNs, computer vision relied on hand-crafted kernels (Sobel, Laplacian, Gaussian) and unsupervised methods (sparse coding, autoencoders, k-means). While these captured simple patterns, they couldn‚Äôt match CNNs‚Äô hierarchical, end-to-end feature learning. Modern systems use CNNs to learn features from edges to semantic concepts‚Äîmaking hand-crafted filters largely obsolete.\n\n\nChapter 9.10: Neuroscientific Basis for Convolutional Networks V1 simple cells detect oriented edges (modeled by Gabor filters), complex cells pool over simple cells for translation invariance (like CNN pooling). But CNNs lack key biological features: saccadic attention, multisensory integration, top-down feedback, and dynamic receptive fields. While CNNs excel at feed-forward recognition, biological vision is holistic, context-aware, and adaptive."
  },
  {
    "objectID": "ML/cnn-pooling.html",
    "href": "ML/cnn-pooling.html",
    "title": "Chapter 9.3: Pooling",
    "section": "",
    "text": "This section explains the pooling operation in convolutional neural networks:\n\nLocal translation invariance: How pooling makes networks insensitive to exact spatial locations\nMax pooling: Downsampling by selecting maximum activations in neighborhoods\nCNN architectures: Comparing no pooling, max pooling, and global average pooling approaches\n\nPooling reduces spatial resolution while preserving important features, making CNNs more robust to translations.\n\n Figure: CNN components - convolution layers detect features, pooling layers aggregate and downsample, creating increasingly abstract representations."
  },
  {
    "objectID": "ML/cnn-pooling.html#overview",
    "href": "ML/cnn-pooling.html#overview",
    "title": "Chapter 9.3: Pooling",
    "section": "",
    "text": "This section explains the pooling operation in convolutional neural networks:\n\nLocal translation invariance: How pooling makes networks insensitive to exact spatial locations\nMax pooling: Downsampling by selecting maximum activations in neighborhoods\nCNN architectures: Comparing no pooling, max pooling, and global average pooling approaches\n\nPooling reduces spatial resolution while preserving important features, making CNNs more robust to translations.\n\n Figure: CNN components - convolution layers detect features, pooling layers aggregate and downsample, creating increasingly abstract representations."
  },
  {
    "objectID": "ML/cnn-pooling.html#local-translation-invariance",
    "href": "ML/cnn-pooling.html#local-translation-invariance",
    "title": "Chapter 9.3: Pooling",
    "section": "1. Local Translation Invariance",
    "text": "1. Local Translation Invariance\nLocal translation invariance means that small shifts in the input produce nearly unchanged outputs. After convolution and pooling, the representation becomes insensitive to the exact spatial location of features, responding mainly to whether a feature is present rather than where it appears within a small neighborhood.\n Figure: How pooling creates translation invariance - different detectors respond strongly to different shifted versions of the same pattern, but the pooling unit aggregates these responses into a single large output. Thus, even when the input ‚Äò5‚Äô appears at different positions, the pooled representation remains nearly unchanged."
  },
  {
    "objectID": "ML/cnn-pooling.html#max-pooling",
    "href": "ML/cnn-pooling.html#max-pooling",
    "title": "Chapter 9.3: Pooling",
    "section": "2. Max Pooling",
    "text": "2. Max Pooling\nPooling reduces the spatial resolution of feature maps by summarizing local neighborhoods (e.g., via max or average). It makes the representation more robust to small translations, reduces sensitivity to exact pixel locations, and improves computational and statistical efficiency.\n Figure: Max pooling operation - the pooling window slides over the feature map, selecting the maximum value in each local neighborhood.\n‚ÄúMax pooling downsamples the activations by summarizing each local neighborhood, so there are fewer pooling units than activation units.‚Äù\n Figure: Downsampling through max pooling - each pooling unit aggregates information from multiple detector units in the feature map, creating a more compact representation."
  },
  {
    "objectID": "ML/cnn-pooling.html#cnn-architecture-structures",
    "href": "ML/cnn-pooling.html#cnn-architecture-structures",
    "title": "Chapter 9.3: Pooling",
    "section": "3. CNN Architecture Structures",
    "text": "3. CNN Architecture Structures\n Figure: Comparison of three CNN architectures - (left) strided convolutions without pooling, (middle) traditional max pooling network, (right) global average pooling network without fully connected layers.\n\nLeft: No Pooling (Stride-Based Downsampling)\nThis architecture removes pooling entirely and relies on strided convolutions to reduce spatial resolution.\nEach convolution layer computes features while simultaneously downsampling the image.\nAfter a few strided convolutions, the feature maps are flattened and fed into a fully connected classifier.\nKey characteristic: This design keeps the structure simple but usually requires more parameters in the classifier, since the flattened representation can be relatively large.\n\n\n\nMiddle: Max Pooling Network\nThis is the classic CNN architecture where pooling layers follow convolution layers.\nMax pooling discards precise spatial information while keeping the strongest local responses, reducing spatial resolution and achieving some translation invariance.\nAfter several conv-pool stages, the final feature maps are flattened and passed through fully connected layers.\nKey characteristic: This design is statistically efficient and was widely used in early CNNs, including LeNet and AlexNet.\n\n\n\nRight: Global Average Pooling (GAP) Network\nInstead of flattening or using dense layers, this architecture uses a final convolution with as many channels as there are classes.\nEach channel then becomes a class-specific activation map.\nGlobal average pooling collapses each map into a single scalar, producing one score per class.\nKey advantage: This removes the need for fully connected layers, greatly reducing parameters and improving robustness.\nGAP-based designs are common in modern architectures such as Network-in-Network and many variants of ResNet."
  },
  {
    "objectID": "ML/tangent-prop-manifold.html",
    "href": "ML/tangent-prop-manifold.html",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "",
    "text": "The assumption is that samples lie on a low-dimensional manifold embedded in a high-dimensional space.\nIf we measure their distance using the Euclidean metric, the points might appear far apart, even though they actually reside on the same manifold."
  },
  {
    "objectID": "ML/tangent-prop-manifold.html#the-manifold-assumption",
    "href": "ML/tangent-prop-manifold.html#the-manifold-assumption",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "",
    "text": "The assumption is that samples lie on a low-dimensional manifold embedded in a high-dimensional space.\nIf we measure their distance using the Euclidean metric, the points might appear far apart, even though they actually reside on the same manifold."
  },
  {
    "objectID": "ML/tangent-prop-manifold.html#tangent-distance-difficulties-and-alternative",
    "href": "ML/tangent-prop-manifold.html#tangent-distance-difficulties-and-alternative",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "Tangent Distance: Difficulties and Alternative",
    "text": "Tangent Distance: Difficulties and Alternative\n\nThe Challenge\nComputing tangent distances directly is computationally expensive. For every pair of samples, it requires solving an optimization problem to find the minimal distance between two tangent planes that approximate their local manifolds. This becomes infeasible with large datasets or high-dimensional input spaces.\n\n\nThe Alternative\nAs an alternative, the method approximates the manifold locally using the tangent plane at a single point. Instead of explicitly finding the closest points between two manifolds, we measure distances using these local linear approximations. This greatly reduces computational cost while still capturing local invariance properties."
  },
  {
    "objectID": "ML/tangent-prop-manifold.html#tangent-prop",
    "href": "ML/tangent-prop-manifold.html#tangent-prop",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "Tangent Prop",
    "text": "Tangent Prop\nThe figure illustrates how Tangent Propagation enforces local invariance of the model output along the tangent directions of the data manifold.\nEach data point lies on a smooth low-dimensional surface embedded in high-dimensional space. The tangent plane represents all directions along which the data can move without changing its semantic meaning (e.g., small translation or rotation).\nThe normal vector points in the direction orthogonal to the manifold ‚Äî changes along this direction correspond to changes in class or semantic meaning.\n\n\n\nTangent Vector and Normal Vector\n\n\nThis approach achieves partial consistency by forcing \\(\\nabla_xf(x) \\perp v^{(i)}\\).\n\nRegularization Term\n\\[\n\\Omega(f)=\\sum_i\\left((\\nabla_xf(x)^\\top v^{(i)})^2\\right)\n\\]\nThis regularization term penalizes the sensitivity of the network‚Äôs output \\(f(x)\\) to small movements along each tangent direction \\(v^{(i)}\\).\nMinimizing this term encourages the model‚Äôs gradient \\(\\nabla_x f(x)\\) to be orthogonal to all tangent vectors, ensuring that \\(f(x)\\) remains approximately constant when the input slides along the manifold.\nIn short: Tangent Propagation achieves local smoothness along the manifold (invariance to small deformations), while still allowing sharp variation in directions orthogonal to the manifold, which separate different classes.\n\n\nTangent Direction Vectors\nIn Tangent Propagation, each \\(v^{(i)}\\) represents a tangent direction of the data manifold at the point \\(x\\).\nIt describes a small, meaningful variation of the input that should not change the output of the network‚Äîfor example, a slight translation, rotation, or scaling of an image.\nMathematically, \\(v^{(i)}\\) can be obtained as the derivative of a transformation \\(T(x, \\alpha_i)\\) with respect to its parameter:\n\\[\nv^{(i)} = \\left.\\frac{\\partial T(x, \\alpha_i)}{\\partial \\alpha_i}\\right|_{\\alpha_i=0}\n\\]\nDuring training, the model is penalized if its output changes along these directions, which enforces invariance and smoothness of \\(f(x)\\) along the manifold."
  },
  {
    "objectID": "ML/tangent-prop-manifold.html#from-tangent-propagation-to-manifold-learning",
    "href": "ML/tangent-prop-manifold.html#from-tangent-propagation-to-manifold-learning",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "From Tangent Propagation to Manifold Learning",
    "text": "From Tangent Propagation to Manifold Learning\nTangent Propagation regularizes the model so that its output remains invariant along directions of known transformations, such as translations or rotations.\nIt can be seen as an analytical version of data augmentation‚Äîrather than generating new samples, it directly penalizes the model‚Äôs sensitivity to those transformations.\n\nConnections to Other Methods\nThis idea connects to bidirectional propagation and adversarial training, both of which encourage the model to stay locally smooth in input space.\nAdversarial training extends Tangent Propagation by finding, for each input, the direction that most changes the model‚Äôs prediction and then enforcing robustness along that direction.\nThe Manifold Tangent Classifier (Rifai et al., 2011d) further removes the need to explicitly specify tangent directions.\nIt uses an autoencoder to learn the manifold structure and derive tangent vectors automatically, allowing the network to regularize itself along the data manifold without handcrafted transformations.\n\n\nManual Specification of Tangent Directions\nA key limitation of Tangent Propagation is that these tangent directions \\(v^{(i)}\\) must be manually specified based on prior knowledge about the task.\nFor example, in handwritten digit recognition, we explicitly define directions corresponding to translation or rotation.\nWhile this makes the method interpretable, it also limits its applicability‚Äîmanual definitions are impractical for complex, high-dimensional data.\n\nSource: Deep Learning (Ian Goodfellow, Yoshua Bengio, Aaron Courville) - Chapter 7.14"
  },
  {
    "objectID": "ML/kmeans.html",
    "href": "ML/kmeans.html",
    "title": "Redirecting‚Ä¶",
    "section": "",
    "text": "Click here if you are not redirected"
  }
]