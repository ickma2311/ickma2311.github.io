[
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "",
    "section": "Project Overview",
    "text": "Project Overview\nThis is a Quarto-based technical blog hosted on GitHub Pages (ickma2311.github.io). The site covers machine learning, algorithms, and technical tutorials with a focus on mathematical foundations and practical implementations."
  },
  {
    "objectID": "CLAUDE.html#common-commands",
    "href": "CLAUDE.html#common-commands",
    "title": "",
    "section": "Common Commands",
    "text": "Common Commands\n\nDevelopment Workflow\n\n./render-site.sh - Recommended: Clean build of entire website (handles cleanup automatically)\nquarto render - Build the entire website (outputs to docs/ directory)\n\n‚ö†Ô∏è May encounter file movement errors; use render-site.sh instead for reliable builds\n\nquarto preview - Start local development server with live reload\nquarto render &lt;file.qmd&gt; - Render a specific document\nquarto check - Verify Quarto installation and project setup\n\n\n\nContent Management\n\nCreate new ML content in ML/ directory\nCreate new algorithm content in Algorithm/ directory\nUpdate navigation by editing _quarto.yml navbar section\nAdd new content to respective index.qmd files for discoverability"
  },
  {
    "objectID": "CLAUDE.html#project-structure",
    "href": "CLAUDE.html#project-structure",
    "title": "",
    "section": "Project Structure",
    "text": "Project Structure\n‚îú‚îÄ‚îÄ _quarto.yml          # Main configuration file\n‚îú‚îÄ‚îÄ docs/                # Generated output (GitHub Pages source)\n‚îú‚îÄ‚îÄ index.qmd            # Homepage\n‚îú‚îÄ‚îÄ about.qmd            # About page\n‚îú‚îÄ‚îÄ ML/                  # Machine Learning content\n‚îÇ   ‚îú‚îÄ‚îÄ index.qmd        # ML topics overview\n‚îÇ   ‚îú‚îÄ‚îÄ *.qmd            # ML articles\n‚îÇ   ‚îî‚îÄ‚îÄ *.ipynb          # Jupyter notebooks\n‚îú‚îÄ‚îÄ Algorithm/           # Algorithm content\n‚îÇ   ‚îú‚îÄ‚îÄ index.qmd        # Algorithm topics overview\n‚îÇ   ‚îî‚îÄ‚îÄ *.qmd            # Algorithm articles\n‚îú‚îÄ‚îÄ imgs/                # Image assets\n‚îú‚îÄ‚îÄ media/               # Media files\n‚îî‚îÄ‚îÄ styles.css           # Custom CSS styles"
  },
  {
    "objectID": "CLAUDE.html#content-organization",
    "href": "CLAUDE.html#content-organization",
    "title": "",
    "section": "Content Organization",
    "text": "Content Organization\nThe site uses a hierarchical navigation structure defined in _quarto.yml: - Two main sections: ‚ÄúML‚Äù and ‚ÄúAlgorithm‚Äù - Each section has an index page that serves as a directory - Content is categorized by topic (e.g., ‚ÄúNumPy Fundamentals‚Äù, ‚ÄúClustering Algorithms‚Äù)\n\nAdding New Content\n\nCreate the content file in the appropriate directory (ML/ or Math/ or Algorithm/)\nAdd to list page: Update the corresponding list page (e.g., ML/deep-learning-book.qmd, Math/MIT18.06/lectures.qmd)\nUpdate homepage automatically: Run ./update-counts.sh to update section counts on homepage\nAdd navigation entry to _quarto.yml if it should appear in the navbar dropdown\nUse consistent frontmatter with title field\nSet publication date: Always use the current date from the system for the date field in frontmatter\n\nGet current date with: date +\"%Y-%m-%d\" (format: YYYY-MM-DD)\nExample: date: \"2025-10-26\"\n\nImportant: After adding new navbar items, run quarto render (full site render) to update the navbar on ALL existing pages. Individual file renders only update that specific page.\n\n\n\nMaintaining the Homepage\nThe homepage (index.qmd) shows the latest 4 items from each section with a count badge.\nCRITICAL: When adding new content, you MUST:\n\nAdd the new item to the appropriate list page:\n\nDeep Learning: ML/deep-learning-book.qmd\nMIT 18.06SC: Math/MIT18.06/lectures.qmd\nMIT 18.065: Math/MIT18.065/lectures.qmd\n\nUpdate the homepage manually by editing index.qmd:\n\nReplace the oldest item in the ‚ÄúLatest 4‚Äù with the new item\nKeep the 4 most recent items visible\nOrder: newest first (top), oldest last (bottom)\n\nUpdate section counts automatically:\n./update-counts.sh\nThis script automatically counts items in list pages and updates the count badges in index.qmd.\nRender and verify:\nquarto render index.qmd\n\nExample workflow when adding Chapter 9.8:\n# 1. Create the new content file\nvim ML/chapter-9-8.qmd\n\n# 2. Add to list page\nvim ML/deep-learning-book.qmd  # Add Chapter 9.8 entry\n\n# 3. Update homepage\nvim index.qmd  # Replace Chapter 9.4 with 9.8, keep 9.7, 9.6, and 9.5\n\n# 4. Update counts automatically\n./update-counts.sh\n\n# 5. Render\nquarto render index.qmd\nWarning: The homepage will NOT automatically update when you add new content. You must manually update index.qmd to show the latest 4 items."
  },
  {
    "objectID": "CLAUDE.html#configuration-notes",
    "href": "CLAUDE.html#configuration-notes",
    "title": "",
    "section": "Configuration Notes",
    "text": "Configuration Notes\n\nOutput directory is set to docs/ for GitHub Pages compatibility\nTheme: Cosmo with custom branding\nAll pages include table of contents (toc: true)\nSite uses custom CSS from styles.css\nJupyter notebooks are supported alongside Quarto markdown"
  },
  {
    "objectID": "CLAUDE.html#github-pages-deployment",
    "href": "CLAUDE.html#github-pages-deployment",
    "title": "",
    "section": "GitHub Pages Deployment",
    "text": "GitHub Pages Deployment\nThe site is automatically deployed from the docs/ directory. After rendering, commit and push the docs/ folder to trigger GitHub Pages rebuild. - Author is Chao Ma - GitHub Pages URL: https://ickma2311.github.io/\n\nPre-Push Checklist (CRITICAL)\nALWAYS verify locally before pushing to prevent broken images on production:\n\nRun full render: Use ./render-site.sh for automatic cleanup, or quarto render (not individual file renders)\nCRITICAL: Restore ALL deleted images after full render\nFull site renders delete images from multiple locations. You MUST restore them:\n# 1. Restore *_files/ directories (code-generated matplotlib/plotly figures)\ncp -r ML/*_files docs/ML/ 2&gt;/dev/null\ncp -r Math/*_files docs/Math/ 2&gt;/dev/null\ncp -r Algorithm/*_files docs/Algorithm/ 2&gt;/dev/null\n\n# 2. Restore static images in ML/ and Math/\ncp ML/*.png docs/ML/ 2&gt;/dev/null\ncp Math/MIT18.06/*.png docs/Math/MIT18.06/ 2&gt;/dev/null\n\n# 3. Restore media/ directory images (referenced with ../media/image.png)\nmkdir -p docs/media\ncp media/*.png docs/media/ 2&gt;/dev/null\n\n# 4. Restore imgs/ directory images (referenced with ../imgs/image.png)\nmkdir -p docs/imgs\ncp imgs/*.png docs/imgs/ 2&gt;/dev/null\n\n# 5. Move any HTML files that failed to move\nfind ML -maxdepth 1 -name \"*.html\" -exec mv {} docs/ML/ \\; 2&gt;/dev/null\nfind Math/MIT18.06 -maxdepth 1 -name \"*.html\" -exec mv {} docs/Math/MIT18.06/ \\; 2&gt;/dev/null\nfind Algorithm -maxdepth 1 -name \"*.html\" -exec mv {} docs/Algorithm/ \\; 2&gt;/dev/null\n\n# 6. Move index files\nmv Algorithm/index.html docs/Algorithm/ 2&gt;/dev/null\nmv Math/index.html docs/Math/ 2&gt;/dev/null\nmv index-backup.html docs/ 2&gt;/dev/null\nCheck git status: git status - verify all image files are staged\n\nLook for docs/**/*_files/figure-html/*.png files (code-generated)\nLook for docs/media/*.png files (media directory)\nLook for docs/imgs/*.png files (imgs directory)\nLook for docs/ML/*.png and docs/Math/MIT18.06/*.png files (static images)\n\nLocal preview: Open docs/ HTML files in browser to verify images load\n\nCheck pages with Jupyter notebooks (e.g., mit1806-lecture1-geometry.html)\nCheck pages with media references (e.g., dropout.html)\nVerify matplotlib/plotly figures appear correctly\n\nCommit ALL generated files: Don‚Äôt commit .html without their images\nOnly then push: git push\n\nWhy this matters: GitHub Pages serves from the docs/ directory. Quarto‚Äôs full render deletes images from docs/ but keeps them in source directories. If images aren‚Äôt copied back to docs/, the HTML will reference missing files, causing broken images on production even though they work locally.\nImage locations that get deleted: - docs/**/*_files/ - Code-generated figures from Python/matplotlib - docs/media/ - Shared media referenced with ../media/ - docs/imgs/ - Shared images referenced with ../imgs/ - docs/ML/*.png - Static chapter images - docs/Math/MIT18.06/*.png - Static lecture images\n\n\nPost-Deployment Cleanup\nAfter successfully pushing changes to GitHub:\n\nArchive source images: Move original images from Downloads to the banana folder for organization\nmv ~/Downloads/&lt;image-name&gt;.png ~/Documents/banana/\nExample: mv ~/Downloads/seq2seq.png ~/Documents/banana/\nVerify deployment: Check GitHub Pages to ensure the site deployed successfully and images load correctly"
  },
  {
    "objectID": "CLAUDE.html#linkedin-post-guidelines",
    "href": "CLAUDE.html#linkedin-post-guidelines",
    "title": "",
    "section": "LinkedIn Post Guidelines",
    "text": "LinkedIn Post Guidelines\n\nEmoji Usage\nWhen drafting LinkedIn posts for blog content, use these emojis: - Deep Learning topics: ‚àá (delta/nabla symbol) - represents gradients and optimization - Linear Algebra topics: üìê (triangle/ruler) - represents geometric and matrix concepts\n\n\nWriting Process\n\nIdentify the key insight: Focus on the main conceptual connection or ‚Äúaha moment‚Äù from the blog post\nUse ‚Äúconnecting the dots‚Äù tone: Emphasize how concepts link together (e.g., ‚Äúhow linear algebra connects to machine learning‚Äù)\nStructure (Knowledge Card Format):\n\nStart with emoji and chapter reference (e.g., ‚Äú‚àá Deep Learning Book (Chapter 8.1)‚Äù or ‚Äúüìê MIT 18.06SC Linear Algebra (Lecture 19)‚Äù)\nClear statement or equation (e.g., ‚ÄúLearning ‚â† Optimization‚Äù)\n2-4 bullet points (üîπ) with key insights\nPhilosophical closing line (üí°)\nLink to full blog post (üìñ)\nSource attribution at the end (e.g., ‚ÄúMy notes on Deep Learning (Ian Goodfellow) Chapter X.X‚Äù or ‚ÄúMy notes on MIT 18.06SC Linear Algebra - Lecture XX‚Äù)\n\nKeep it concise: Aim for clarity over comprehensiveness - use knowledge card format for quick, digestible insights\nCourse naming:\n\nAlways use ‚ÄúMIT 18.06SC‚Äù (not just ‚ÄúMIT 18.06‚Äù) for linear algebra posts\nUse full course titles to maintain consistency\n\nInclude relevant hashtags: #MachineLearning #LinearAlgebra #DeepLearning"
  },
  {
    "objectID": "ML/cnn-pooling.html",
    "href": "ML/cnn-pooling.html",
    "title": "Chapter 9.3: Pooling",
    "section": "",
    "text": "This section explains the pooling operation in convolutional neural networks:\n\nLocal translation invariance: How pooling makes networks insensitive to exact spatial locations\nMax pooling: Downsampling by selecting maximum activations in neighborhoods\nCNN architectures: Comparing no pooling, max pooling, and global average pooling approaches\n\nPooling reduces spatial resolution while preserving important features, making CNNs more robust to translations.\n\n Figure: CNN components - convolution layers detect features, pooling layers aggregate and downsample, creating increasingly abstract representations."
  },
  {
    "objectID": "ML/cnn-pooling.html#overview",
    "href": "ML/cnn-pooling.html#overview",
    "title": "Chapter 9.3: Pooling",
    "section": "",
    "text": "This section explains the pooling operation in convolutional neural networks:\n\nLocal translation invariance: How pooling makes networks insensitive to exact spatial locations\nMax pooling: Downsampling by selecting maximum activations in neighborhoods\nCNN architectures: Comparing no pooling, max pooling, and global average pooling approaches\n\nPooling reduces spatial resolution while preserving important features, making CNNs more robust to translations.\n\n Figure: CNN components - convolution layers detect features, pooling layers aggregate and downsample, creating increasingly abstract representations."
  },
  {
    "objectID": "ML/cnn-pooling.html#local-translation-invariance",
    "href": "ML/cnn-pooling.html#local-translation-invariance",
    "title": "Chapter 9.3: Pooling",
    "section": "1. Local Translation Invariance",
    "text": "1. Local Translation Invariance\nLocal translation invariance means that small shifts in the input produce nearly unchanged outputs. After convolution and pooling, the representation becomes insensitive to the exact spatial location of features, responding mainly to whether a feature is present rather than where it appears within a small neighborhood.\n Figure: How pooling creates translation invariance - different detectors respond strongly to different shifted versions of the same pattern, but the pooling unit aggregates these responses into a single large output. Thus, even when the input ‚Äò5‚Äô appears at different positions, the pooled representation remains nearly unchanged."
  },
  {
    "objectID": "ML/cnn-pooling.html#max-pooling",
    "href": "ML/cnn-pooling.html#max-pooling",
    "title": "Chapter 9.3: Pooling",
    "section": "2. Max Pooling",
    "text": "2. Max Pooling\nPooling reduces the spatial resolution of feature maps by summarizing local neighborhoods (e.g., via max or average). It makes the representation more robust to small translations, reduces sensitivity to exact pixel locations, and improves computational and statistical efficiency.\n Figure: Max pooling operation - the pooling window slides over the feature map, selecting the maximum value in each local neighborhood.\n‚ÄúMax pooling downsamples the activations by summarizing each local neighborhood, so there are fewer pooling units than activation units.‚Äù\n Figure: Downsampling through max pooling - each pooling unit aggregates information from multiple detector units in the feature map, creating a more compact representation."
  },
  {
    "objectID": "ML/cnn-pooling.html#cnn-architecture-structures",
    "href": "ML/cnn-pooling.html#cnn-architecture-structures",
    "title": "Chapter 9.3: Pooling",
    "section": "3. CNN Architecture Structures",
    "text": "3. CNN Architecture Structures\n Figure: Comparison of three CNN architectures - (left) strided convolutions without pooling, (middle) traditional max pooling network, (right) global average pooling network without fully connected layers.\n\nLeft: No Pooling (Stride-Based Downsampling)\nThis architecture removes pooling entirely and relies on strided convolutions to reduce spatial resolution.\nEach convolution layer computes features while simultaneously downsampling the image.\nAfter a few strided convolutions, the feature maps are flattened and fed into a fully connected classifier.\nKey characteristic: This design keeps the structure simple but usually requires more parameters in the classifier, since the flattened representation can be relatively large.\n\n\n\nMiddle: Max Pooling Network\nThis is the classic CNN architecture where pooling layers follow convolution layers.\nMax pooling discards precise spatial information while keeping the strongest local responses, reducing spatial resolution and achieving some translation invariance.\nAfter several conv-pool stages, the final feature maps are flattened and passed through fully connected layers.\nKey characteristic: This design is statistically efficient and was widely used in early CNNs, including LeNet and AlexNet.\n\n\n\nRight: Global Average Pooling (GAP) Network\nInstead of flattening or using dense layers, this architecture uses a final convolution with as many channels as there are classes.\nEach channel then becomes a class-specific activation map.\nGlobal average pooling collapses each map into a single scalar, producing one score per class.\nKey advantage: This removes the need for fully connected layers, greatly reducing parameters and improving robustness.\nGAP-based designs are common in modern architectures such as Network-in-Network and many variants of ResNet."
  },
  {
    "objectID": "ML/kmeans.html",
    "href": "ML/kmeans.html",
    "title": "Redirecting‚Ä¶",
    "section": "",
    "text": "Click here if you are not redirected"
  },
  {
    "objectID": "ML/cnn-motivation.html",
    "href": "ML/cnn-motivation.html",
    "title": "Chapter 9.2: Motivation for Convolutional Networks",
    "section": "",
    "text": "This section explains the three key motivations for using convolutional neural networks:\n\nSparse interactions: Local connectivity reduces parameters and computation\nParameter sharing: Same kernel used at all positions, enforcing translation invariance\nTranslation equivariance: Shifting input shifts output by the same amount\n\nThese properties make CNNs particularly well-suited for processing images and other data with spatial structure."
  },
  {
    "objectID": "ML/cnn-motivation.html#overview",
    "href": "ML/cnn-motivation.html#overview",
    "title": "Chapter 9.2: Motivation for Convolutional Networks",
    "section": "",
    "text": "This section explains the three key motivations for using convolutional neural networks:\n\nSparse interactions: Local connectivity reduces parameters and computation\nParameter sharing: Same kernel used at all positions, enforcing translation invariance\nTranslation equivariance: Shifting input shifts output by the same amount\n\nThese properties make CNNs particularly well-suited for processing images and other data with spatial structure."
  },
  {
    "objectID": "ML/cnn-motivation.html#sparse-interactions-local-connectivity",
    "href": "ML/cnn-motivation.html#sparse-interactions-local-connectivity",
    "title": "Chapter 9.2: Motivation for Convolutional Networks",
    "section": "1. Sparse Interactions (Local Connectivity)",
    "text": "1. Sparse Interactions (Local Connectivity)\n\nDense vs Sparse Connections\nTraditional neural networks use dense connections, where every input unit connects to every output unit.\n\nEach input-output pair has its own unique weight\nTotal parameters: O(m ¬∑ n) where \\(m\\) = input size, \\(n\\) = output size\nComputational cost: O(m ¬∑ n) operations per output\n\nConvolutional neural networks use sparse interactions, because the convolution kernel is much smaller than the input.\n\nEach output unit only interacts with a small local region of the input (its receptive field)\nTotal parameters: O(k ¬∑ n) where \\(k\\) = kernel size, and \\(k \\ll m\\)\nComputational cost: O(k ¬∑ n) operations per output\n\n\n\nBenefits of Sparse Connectivity\n\nFewer parameters: Reduces memory requirements and risk of overfitting\nLess computation: Faster training and inference\nMeaningful local patterns: CNNs can detect edges, corners, textures\nHierarchical features: Deeper layers expand the effective receptive field to model global structure\n\nKey insight: Even though each layer uses sparse connections, units in deeper layers can indirectly connect to a larger region of the input. This allows the network to capture both local and global information efficiently.\n Figure: Sparse connectivity - each output (top) connects to only a small local region of the input (bottom), unlike fully connected layers where every output connects to every input."
  },
  {
    "objectID": "ML/cnn-motivation.html#parameter-sharing",
    "href": "ML/cnn-motivation.html#parameter-sharing",
    "title": "Chapter 9.2: Motivation for Convolutional Networks",
    "section": "2. Parameter Sharing",
    "text": "2. Parameter Sharing\n\nDense Layers: Each Weight Used Once\nIn a traditional neural network, each weight is used exactly once: - It multiplies one input unit at one position - It is never reused elsewhere in the layer - Different positions require different parameters\n\n\nConvolutional Layers: Kernel Reuse\nIn a convolutional neural network, kernel parameters are shared across all spatial positions.\nAs the kernel slides over the input, the same set of parameters is reused at every location.\nParameter count reduction: - Dense layer: O(m) parameters (one weight per input-output connection) - Convolutional layer: O(k) parameters (only kernel weights)\nwhere \\(k \\ll m\\).\n\n\nWhy Parameter Sharing Works\nParameter sharing enforces a useful inductive bias: patterns detected in one region should also be detectable in other regions.\nExamples: - An edge detector that finds vertical edges at the top-left of an image should also detect vertical edges at the bottom-right - A corner detector useful in one part of the image is useful everywhere - The same texture pattern might appear in multiple locations\nThis assumption is natural for images and many other spatial data, but would not hold for arbitrary data (e.g., different parts of a structured database table might require completely different processing).\n Figure: Parameter sharing - the same kernel weights (shown in the same color) are applied at all spatial positions, dramatically reducing the number of parameters compared to fully connected layers.\n\n\nReceptive Fields Grow with Depth\nIn CNNs, deeper layers have larger receptive fields than shallow layers.\n\nLayer 1: Each unit sees a small patch (e.g., 3√ó3 pixels)\nLayer 2: Each unit sees a larger patch through composition (e.g., 5√ó5 or 7√ó7 pixels)\nLayer 3: Even larger effective receptive field (e.g., 11√ó11 or more)\n\nThis hierarchical structure allows the network to build complex features from simple ones: 1. Early layers: Detect simple features (edges, corners, colors) 2. Middle layers: Combine edges into shapes and textures 3. Later layers: Recognize object parts and eventually whole objects\n Figure: Receptive fields increase with depth - deeper layers can ‚Äúsee‚Äù larger regions of the original input through composition of convolutional operations."
  },
  {
    "objectID": "ML/cnn-motivation.html#computational-efficiency",
    "href": "ML/cnn-motivation.html#computational-efficiency",
    "title": "Chapter 9.2: Motivation for Convolutional Networks",
    "section": "3. Computational Efficiency",
    "text": "3. Computational Efficiency\n\nExample: Edge Detection on Images\nConsider performing edge detection on a 320 √ó 280 grayscale image using a 3 √ó 3 kernel.\nConvolutional approach: - Output size: \\(319 \\times 280\\) (with valid padding) - Operations per output: \\(3 \\times 3 = 9\\) multiply-adds - Total operations: \\(319 \\times 280 \\times 3 \\approx 2.68 \\times 10^5\\) operations\nDense matrix multiplication approach: - Unfold each 3√ó3 patch into a vector - Create a weight matrix of size \\((320 \\times 280) \\times (320 \\times 280)\\) - Total operations: \\(320 \\times 280 \\times 320 \\times 280 \\approx 8.03 \\times 10^9\\) operations\n\n\nSpeedup Factor\n\\[\n\\text{Speedup} = \\frac{8.03 \\times 10^9}{2.68 \\times 10^5} \\approx 30,000\\times\n\\]\nConvolution is roughly 30,000 times more efficient than implementing the same operation as a dense matrix multiplication!\nThis massive speedup comes from: 1. Sparse connectivity: Each output only depends on a local patch 2. Parameter sharing: Same kernel weights used at all positions\n Figure: Edge detection example - applying a 3√ó3 edge detection kernel to an image is vastly more efficient than using a fully connected layer.\n Figure: Computational cost comparison - convolutional layers (green) require orders of magnitude fewer operations than dense layers (red) for the same task on images."
  },
  {
    "objectID": "ML/cnn-motivation.html#translation-equivariance",
    "href": "ML/cnn-motivation.html#translation-equivariance",
    "title": "Chapter 9.2: Motivation for Convolutional Networks",
    "section": "4. Translation Equivariance",
    "text": "4. Translation Equivariance\n\nDefinition\nA convolution is translation-equivariant: if the input is shifted, the output shifts by the same amount while preserving its structure.\nFormally, convolution satisfies:\n\\[\nf(g(x)) = g(f(x))\n\\]\nwhere: - \\(f\\) is the convolution operation - \\(g\\) is a spatial translation (shift) - \\(x\\) is the input\n\n\nWhat This Means\nTranslation equivariance means the network detects the same pattern regardless of its position in the input, producing output feature maps that move consistently with the input.\nExample: - If an edge appears at position \\((10, 20)\\) in the input image - And we shift the image by \\((5, 3)\\) pixels - The edge detector will fire at position \\((10+5, 20+3) = (15, 23)\\) in the shifted image\nThe response is the same, just shifted by the same amount.\n\n\nContrast with Translation Invariance\nTranslation equivariance is different from translation invariance:\n\nEquivariance: Output shifts when input shifts (CNNs with convolution)\nInvariance: Output stays the same when input shifts (achieved with pooling)\n\nConvolutional layers provide equivariance. Adding pooling layers creates approximate invariance, which is useful for classification tasks where we care about ‚Äúis there a cat?‚Äù but not ‚Äúwhere exactly is the cat?‚Äù\n Figure: Translation equivariance - when the input is shifted (top), the output feature map shifts by the same amount (bottom), preserving the detection pattern."
  },
  {
    "objectID": "ML/rnn-long-term-dependency.html",
    "href": "ML/rnn-long-term-dependency.html",
    "title": "Chapter 10.7: The Challenge of Long-Term Dependencies",
    "section": "",
    "text": "The fundamental challenge of long-term dependencies is not representational capacity but the difficulty of training recurrent networks.\nDuring backpropagation through time, gradients must be propagated across many steps, causing:\nThe figure shows how repeatedly applying the same nonlinear function causes saturation or steep amplification. Although the x-axis is not time, these compositions mirror the effect of many RNN time steps, explaining why gradients vanish or explode during long-term dependency learning."
  },
  {
    "objectID": "ML/rnn-long-term-dependency.html#mathematical-analysis",
    "href": "ML/rnn-long-term-dependency.html#mathematical-analysis",
    "title": "Chapter 10.7: The Challenge of Long-Term Dependencies",
    "section": "Mathematical Analysis",
    "text": "Mathematical Analysis\n\\[\nh^{t} = W h^{t-1} \\tag{10.36}\n\\]\nHidden state after \\(t\\) steps can be written as the repeated application of the same linear transformation:\n\\[\nh^{t} = W^{t} h^{0} \\tag{10.37}\n\\]\nshowing that long-term behavior is governed by powers of the transition matrix.\n\\[\nW = Q\\Lambda Q^{\\top} \\tag{10.38}\n\\]\nWe can analyze its dynamics in the eigenbasis if \\(W\\) is diagonalizable:\n\\[\nh^{t} = Q \\Lambda^{t} Q^{\\top} h^{0} \\tag{10.39}\n\\]\nEach eigenvalue raised to the \\(t\\)-th power either shrinks (vanishing) or grows (exploding), explaining instability over long time horizons.\n\n\n\nEigenvalue dynamics over time"
  },
  {
    "objectID": "ML/deep-learning-book.html",
    "href": "ML/deep-learning-book.html",
    "title": "Deep Learning Book",
    "section": "",
    "text": "My notes and implementations while studying the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\n\n\n\nChapter 6.1: XOR Problem & ReLU Networks How ReLU solves problems that linear models cannot handle.\n\n\nChapter 6.2: Likelihood-Based Loss Functions The mathematical connection between probabilistic models and loss functions.\n\n\nChapter 6.3: Hidden Units and Activation Functions Exploring activation functions and their impact on neural network learning.\n\n\nChapter 6.4: Architecture Design - Depth vs Width How depth enables hierarchical feature reuse and exponential expressiveness.\n\n\nChapter 6.5: Back-Propagation and Other Differentiation Algorithms The algorithm that makes training deep networks computationally feasible.\n\n\n\n\n\n\n\n\nChapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature Essential second-order calculus concepts needed before Chapter 7.\n\n\nChapter 7.1.1: L2 Regularization How L2 regularization shrinks weights based on Hessian eigenvalues.\n\n\nChapter 7.1.2: L1 Regularization L1 regularization uses soft thresholding to create sparse solutions.\n\n\nChapter 7.2: Constrained Optimization View of Regularization Regularization as constrained optimization with KKT conditions.\n\n\nChapter 7.3: Regularization and Under-Constrained Problems Why regularization is mathematically necessary and ensures invertibility.\n\n\nChapter 7.4: Dataset Augmentation How transforming existing data improves generalization.\n\n\nChapter 7.5: Noise Robustness How adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\nChapter 7.6: Semi-Supervised Learning Leveraging unlabeled data to improve model performance when labeled data is scarce.\n\n\nChapter 7.7: Multi-Task Learning Training a single model on multiple related tasks to improve generalization.\n\n\nChapter 7.8: Early Stopping Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.\n\n\nChapter 7.9: Parameter Tying and Parameter Sharing Two strategies for reducing parameters: encouraging similarity vs.¬†enforcing identity.\n\n\nChapter 7.10: Sparse Representations Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.\n\n\nChapter 7.11: Bagging and Other Ensemble Methods How training multiple models on bootstrap samples and averaging their predictions reduces variance.\n\n\nChapter 7.12: Dropout Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.\n\n\nChapter 7.13: Adversarial Training How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.\n\n\nChapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.\n\n\n\n\n\n\n\n\nChapter 8.1: How Learning Differs from Pure Optimization Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.\n\n\nChapter 8.2: Challenges in Deep Learning Optimization Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.\n\n\nChapter 8.3: Basic Algorithms SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.\n\n\nChapter 8.4: Parameter Initialization Strategies Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.\n\n\nChapter 8.5: Algorithms with Adaptive Learning Rates From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.\n\n\nChapter 8.6: Second-Order Optimization Methods Newton‚Äôs method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.\n\n\nChapter 8.7: Optimization Strategies and Meta-Algorithms Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.\n\n\n\n\n\n\n\n\nChapter 9.1: Convolution Computation The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.\n\n\nChapter 9.2: Motivation for Convolutional Networks Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m¬∑n) to O(k¬∑n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere‚Äîachieving 30,000√ó speedup over dense layers.\n\n\nChapter 9.3: Pooling Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28√ó28 feature maps to 14√ó14 (4√ó fewer activations). Comparing three architectures‚Äîstrided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.\n\n\nChapter 9.4: Convolution and Pooling as an Infinitely Strong Prior Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off‚Äîstrong priors reduce sample complexity but only when assumptions match the data structure.\n\n\nChapter 9.5: Convolutional Functions Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula \\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\), stride for downsampling, three padding strategies (valid/same/full), and gradient computation‚Äîkernel gradients via correlation with input, input gradients via convolution with flipped kernel.\n\n\nChapter 9.6: Structured Outputs CNNs can generate high-dimensional structured objects through pixel-level predictions. Preserving spatial dimensions (no pooling, no stride &gt; 1, SAME padding) enables full-resolution outputs. Recurrent convolution refines predictions iteratively: \\(U*X + H(t-1)*W = H(t)\\), producing dense predictions for segmentation, depth estimation, and flow prediction.\n\n\nChapter 9.7: Data Types CNNs can operate on different data types: 1D (audio, time series), 2D (images), and 3D (videos, CT scans) with varying channel counts. Unlike fully connected networks, convolutional kernels handle variable-sized inputs by sliding across spatial dimensions, producing outputs that scale accordingly‚Äîa unique flexibility for diverse domains.\n\n\nChapter 9.8: Efficient Convolution Algorithms Separable convolution reduces computational cost from \\(O(HWk^2)\\) to \\(O(HWk)\\) by decomposing a 2D kernel into two 1D filters (vertical and horizontal). Parameter storage shrinks from \\(k^2\\) to \\(2k\\). This factorization enables faster, more memory-efficient models without sacrificing accuracy‚Äîfoundational for architectures like MobileNet.\n\n\nChapter 9.9: Unsupervised or Semi-Supervised Feature Learning Before CNNs, computer vision relied on hand-crafted kernels (Sobel, Laplacian, Gaussian) and unsupervised methods (sparse coding, autoencoders, k-means). While these captured simple patterns, they couldn‚Äôt match CNNs‚Äô hierarchical, end-to-end feature learning. Modern systems use CNNs to learn features from edges to semantic concepts‚Äîmaking hand-crafted filters largely obsolete.\n\n\nChapter 9.10: Neuroscientific Basis for Convolutional Networks V1 simple cells detect oriented edges (modeled by Gabor filters), complex cells pool over simple cells for translation invariance (like CNN pooling). But CNNs lack key biological features: saccadic attention, multisensory integration, top-down feedback, and dynamic receptive fields. While CNNs excel at feed-forward recognition, biological vision is holistic, context-aware, and adaptive.\n\n\n\n\n\n\n\n\nChapter 10.1: Unfold Computation Graph Unfolding computation graphs in RNNs enables parameter sharing across time steps. The same function with fixed parameters processes sequences of any length, compressing input history into fixed-size hidden states that retain only task-relevant information for predictions.\n\n\nChapter 10.2: Recurrent Neural Networks RNN architecture with hidden-to-hidden connections, teacher forcing for parallel training, back-propagation through time (BPTT), RNN as directed graphical models with O(œÑ) parameter efficiency, and context-based sequence-to-sequence models.\n\n\nChapter 10.3: Bidirectional RNN Bidirectional RNNs process sequences in both forward and backward directions, allowing predictions to use information from the entire input sequence. Essential for tasks like speech recognition and handwriting recognition where future context matters.\n\n\nChapter 10.4: Encoder-Decoder Sequence-to-Sequence Architecture The seq2seq architecture handles variable-length input and output sequences by compressing the input into a fixed context vector C, then decoding it step-by-step. This enables machine translation, summarization, and dialogue generation where input and output lengths differ.\n\n\nChapter 10.5: Deep Recurrent Networks Three architectural patterns for adding depth to RNNs: hierarchical hidden states (vertical stacking), deep transition RNNs (MLPs replace transformations), and deep transition with skip connections (residual paths for gradient flow).\n\n\nChapter 10.6: Recursive Neural Network Recursive neural networks compute over tree structures rather than linear chains, applying shared composition functions at internal nodes to build hierarchical representations bottom-up. This reduces computation depth from O(œÑ) to O(log œÑ), but requires external tree structure specification.\n\n\nChapter 10.7: The Challenge of Long-Term Dependencies The fundamental challenge of long-term dependencies in RNNs is training difficulty: gradients propagated across many time steps either vanish exponentially (common) or explode (rare but severe). Eigenvalue analysis shows how powers of the transition matrix govern this instability."
  },
  {
    "objectID": "ML/deep-learning-book.html#all-chapters",
    "href": "ML/deep-learning-book.html#all-chapters",
    "title": "Deep Learning Book",
    "section": "",
    "text": "My notes and implementations while studying the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\n\n\n\nChapter 6.1: XOR Problem & ReLU Networks How ReLU solves problems that linear models cannot handle.\n\n\nChapter 6.2: Likelihood-Based Loss Functions The mathematical connection between probabilistic models and loss functions.\n\n\nChapter 6.3: Hidden Units and Activation Functions Exploring activation functions and their impact on neural network learning.\n\n\nChapter 6.4: Architecture Design - Depth vs Width How depth enables hierarchical feature reuse and exponential expressiveness.\n\n\nChapter 6.5: Back-Propagation and Other Differentiation Algorithms The algorithm that makes training deep networks computationally feasible.\n\n\n\n\n\n\n\n\nChapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature Essential second-order calculus concepts needed before Chapter 7.\n\n\nChapter 7.1.1: L2 Regularization How L2 regularization shrinks weights based on Hessian eigenvalues.\n\n\nChapter 7.1.2: L1 Regularization L1 regularization uses soft thresholding to create sparse solutions.\n\n\nChapter 7.2: Constrained Optimization View of Regularization Regularization as constrained optimization with KKT conditions.\n\n\nChapter 7.3: Regularization and Under-Constrained Problems Why regularization is mathematically necessary and ensures invertibility.\n\n\nChapter 7.4: Dataset Augmentation How transforming existing data improves generalization.\n\n\nChapter 7.5: Noise Robustness How adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\nChapter 7.6: Semi-Supervised Learning Leveraging unlabeled data to improve model performance when labeled data is scarce.\n\n\nChapter 7.7: Multi-Task Learning Training a single model on multiple related tasks to improve generalization.\n\n\nChapter 7.8: Early Stopping Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.\n\n\nChapter 7.9: Parameter Tying and Parameter Sharing Two strategies for reducing parameters: encouraging similarity vs.¬†enforcing identity.\n\n\nChapter 7.10: Sparse Representations Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.\n\n\nChapter 7.11: Bagging and Other Ensemble Methods How training multiple models on bootstrap samples and averaging their predictions reduces variance.\n\n\nChapter 7.12: Dropout Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.\n\n\nChapter 7.13: Adversarial Training How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.\n\n\nChapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.\n\n\n\n\n\n\n\n\nChapter 8.1: How Learning Differs from Pure Optimization Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.\n\n\nChapter 8.2: Challenges in Deep Learning Optimization Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.\n\n\nChapter 8.3: Basic Algorithms SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.\n\n\nChapter 8.4: Parameter Initialization Strategies Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.\n\n\nChapter 8.5: Algorithms with Adaptive Learning Rates From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.\n\n\nChapter 8.6: Second-Order Optimization Methods Newton‚Äôs method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.\n\n\nChapter 8.7: Optimization Strategies and Meta-Algorithms Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.\n\n\n\n\n\n\n\n\nChapter 9.1: Convolution Computation The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.\n\n\nChapter 9.2: Motivation for Convolutional Networks Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m¬∑n) to O(k¬∑n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere‚Äîachieving 30,000√ó speedup over dense layers.\n\n\nChapter 9.3: Pooling Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28√ó28 feature maps to 14√ó14 (4√ó fewer activations). Comparing three architectures‚Äîstrided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.\n\n\nChapter 9.4: Convolution and Pooling as an Infinitely Strong Prior Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off‚Äîstrong priors reduce sample complexity but only when assumptions match the data structure.\n\n\nChapter 9.5: Convolutional Functions Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula \\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\), stride for downsampling, three padding strategies (valid/same/full), and gradient computation‚Äîkernel gradients via correlation with input, input gradients via convolution with flipped kernel.\n\n\nChapter 9.6: Structured Outputs CNNs can generate high-dimensional structured objects through pixel-level predictions. Preserving spatial dimensions (no pooling, no stride &gt; 1, SAME padding) enables full-resolution outputs. Recurrent convolution refines predictions iteratively: \\(U*X + H(t-1)*W = H(t)\\), producing dense predictions for segmentation, depth estimation, and flow prediction.\n\n\nChapter 9.7: Data Types CNNs can operate on different data types: 1D (audio, time series), 2D (images), and 3D (videos, CT scans) with varying channel counts. Unlike fully connected networks, convolutional kernels handle variable-sized inputs by sliding across spatial dimensions, producing outputs that scale accordingly‚Äîa unique flexibility for diverse domains.\n\n\nChapter 9.8: Efficient Convolution Algorithms Separable convolution reduces computational cost from \\(O(HWk^2)\\) to \\(O(HWk)\\) by decomposing a 2D kernel into two 1D filters (vertical and horizontal). Parameter storage shrinks from \\(k^2\\) to \\(2k\\). This factorization enables faster, more memory-efficient models without sacrificing accuracy‚Äîfoundational for architectures like MobileNet.\n\n\nChapter 9.9: Unsupervised or Semi-Supervised Feature Learning Before CNNs, computer vision relied on hand-crafted kernels (Sobel, Laplacian, Gaussian) and unsupervised methods (sparse coding, autoencoders, k-means). While these captured simple patterns, they couldn‚Äôt match CNNs‚Äô hierarchical, end-to-end feature learning. Modern systems use CNNs to learn features from edges to semantic concepts‚Äîmaking hand-crafted filters largely obsolete.\n\n\nChapter 9.10: Neuroscientific Basis for Convolutional Networks V1 simple cells detect oriented edges (modeled by Gabor filters), complex cells pool over simple cells for translation invariance (like CNN pooling). But CNNs lack key biological features: saccadic attention, multisensory integration, top-down feedback, and dynamic receptive fields. While CNNs excel at feed-forward recognition, biological vision is holistic, context-aware, and adaptive.\n\n\n\n\n\n\n\n\nChapter 10.1: Unfold Computation Graph Unfolding computation graphs in RNNs enables parameter sharing across time steps. The same function with fixed parameters processes sequences of any length, compressing input history into fixed-size hidden states that retain only task-relevant information for predictions.\n\n\nChapter 10.2: Recurrent Neural Networks RNN architecture with hidden-to-hidden connections, teacher forcing for parallel training, back-propagation through time (BPTT), RNN as directed graphical models with O(œÑ) parameter efficiency, and context-based sequence-to-sequence models.\n\n\nChapter 10.3: Bidirectional RNN Bidirectional RNNs process sequences in both forward and backward directions, allowing predictions to use information from the entire input sequence. Essential for tasks like speech recognition and handwriting recognition where future context matters.\n\n\nChapter 10.4: Encoder-Decoder Sequence-to-Sequence Architecture The seq2seq architecture handles variable-length input and output sequences by compressing the input into a fixed context vector C, then decoding it step-by-step. This enables machine translation, summarization, and dialogue generation where input and output lengths differ.\n\n\nChapter 10.5: Deep Recurrent Networks Three architectural patterns for adding depth to RNNs: hierarchical hidden states (vertical stacking), deep transition RNNs (MLPs replace transformations), and deep transition with skip connections (residual paths for gradient flow).\n\n\nChapter 10.6: Recursive Neural Network Recursive neural networks compute over tree structures rather than linear chains, applying shared composition functions at internal nodes to build hierarchical representations bottom-up. This reduces computation depth from O(œÑ) to O(log œÑ), but requires external tree structure specification.\n\n\nChapter 10.7: The Challenge of Long-Term Dependencies The fundamental challenge of long-term dependencies in RNNs is training difficulty: gradients propagated across many time steps either vanish exponentially (common) or explode (rare but severe). Eigenvalue analysis shows how powers of the transition matrix govern this instability."
  },
  {
    "objectID": "ML/logistic_regression.html",
    "href": "ML/logistic_regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "The core idea of logistic regression is to model the probability of a binary outcome.\n\n\nWe model the probability that the target variable (y) is 1, given the features (x), using the sigmoid (or logistic) function, denoted by ().\n\\[\nP(y_i=1 \\mid x_i) = \\hat{y}_i = \\sigma(w^T x_i + b)\n\\]\nThe sigmoid function is defined as: \\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\nSince the outcome is binary, the probability of (y) being 0 is simply: \\[\nP(y_i=0 \\mid x_i) = 1 - \\hat{y}_i\n\\]\nThese two cases can be written compactly as a single equation, which is the probability mass function of a Bernoulli distribution: \\[\nP(y_i \\mid x_i) = \\hat{y}_i^{y_i} (1 - \\hat{y}_i)^{1 - y_i}\n\\]"
  },
  {
    "objectID": "ML/logistic_regression.html#model-formulation",
    "href": "ML/logistic_regression.html#model-formulation",
    "title": "Logistic Regression",
    "section": "",
    "text": "The core idea of logistic regression is to model the probability of a binary outcome.\n\n\nWe model the probability that the target variable (y) is 1, given the features (x), using the sigmoid (or logistic) function, denoted by ().\n\\[\nP(y_i=1 \\mid x_i) = \\hat{y}_i = \\sigma(w^T x_i + b)\n\\]\nThe sigmoid function is defined as: \\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\nSince the outcome is binary, the probability of (y) being 0 is simply: \\[\nP(y_i=0 \\mid x_i) = 1 - \\hat{y}_i\n\\]\nThese two cases can be written compactly as a single equation, which is the probability mass function of a Bernoulli distribution: \\[\nP(y_i \\mid x_i) = \\hat{y}_i^{y_i} (1 - \\hat{y}_i)^{1 - y_i}\n\\]"
  },
  {
    "objectID": "ML/logistic_regression.html#loss-function-binary-cross-entropy",
    "href": "ML/logistic_regression.html#loss-function-binary-cross-entropy",
    "title": "Logistic Regression",
    "section": "Loss Function (Binary Cross-Entropy)",
    "text": "Loss Function (Binary Cross-Entropy)\nTo find the optimal parameters (w) and (b), we use Maximum Likelihood Estimation (MLE). We want to find the parameters that maximize the probability of observing our given dataset.\n\n1. Likelihood\nThe likelihood is the joint probability of observing all (n) data points, assuming they are independent and identically distributed (i.i.d.): \\[\n\\mathcal{L}(w, b) = \\prod_{i=1}^n P(y_i \\mid x_i) = \\prod_{i=1}^n \\hat{y}_i^{y_i} (1 - \\hat{y}_i)^{1 - y_i}\n\\]\n\n\n2. Log-Likelihood\nWorking with products is difficult, so we take the logarithm of the likelihood. Maximizing the log-likelihood is equivalent to maximizing the likelihood.\n\\[\n\\log \\mathcal{L}(w, b) = \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n\\]\n\n\n3. Cost Function\nIn machine learning, we frame problems as minimizing a cost function. The standard convention is to minimize the negative log-likelihood. This gives us the Binary Cross-Entropy loss, (J(w, b)).\n\\[\nJ(w, b) = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n\\] The \\(\\frac{1}{n}\\) term is an average over the training examples and doesn‚Äôt change the minimum, but it helps in stabilizing the training process."
  },
  {
    "objectID": "ML/logistic_regression.html#refernce-bernoulli-distribution",
    "href": "ML/logistic_regression.html#refernce-bernoulli-distribution",
    "title": "Logistic Regression",
    "section": "refernce: Bernoulli Distribution",
    "text": "refernce: Bernoulli Distribution\nFormular \\[\nP(y) = p^y (1 - p)^{1 - y}, \\quad y \\in \\{0, 1\\}\n\\] * when y = 1Ôºö\\(P(y=1) = p^1 (1 - p)^0 = p\\) * when y=0: $P(y=0) = p^0 (1 - p)^1 = 1 - p $"
  },
  {
    "objectID": "ML/rnn-deep.html",
    "href": "ML/rnn-deep.html",
    "title": "Chapter 10.5: Deep Recurrent Networks",
    "section": "",
    "text": "Deep recurrent networks extend basic RNNs by introducing depth through multiple layers of computation. The experimental literature on deep RNNs emphasizes three main architectural patterns for adding depth:"
  },
  {
    "objectID": "ML/rnn-deep.html#hierarchical-hidden-states",
    "href": "ML/rnn-deep.html#hierarchical-hidden-states",
    "title": "Chapter 10.5: Deep Recurrent Networks",
    "section": "1. Hierarchical Hidden States",
    "text": "1. Hierarchical Hidden States\nThe hidden state at each time step is decomposed into multiple hierarchical layers, forming a vertical stack of RNN cells.\nAt time step \\(t\\), the computation proceeds through layers:\n\\[\n\\begin{align*}\nh_t^{(1)} &= f_1(h_{t-1}^{(1)}, x_t) \\\\\nh_t^{(2)} &= f_2(h_{t-1}^{(2)}, h_t^{(1)}) \\\\\n&\\vdots \\\\\nh_t^{(L)} &= f_L(h_{t-1}^{(L)}, h_t^{(L-1)}) \\\\\n\\end{align*}\n\\]\nEach layer \\(\\ell\\) receives: - the previous hidden state from the same layer \\(h_{t-1}^{(\\ell)}\\) - the current hidden state from the layer below \\(h_t^{(\\ell-1)}\\)\nThis creates a 2D grid of computations over time (horizontal) and depth (vertical).\nKey benefit: Multiple levels of temporal abstraction‚Äîlower layers capture fine-grained patterns, upper layers capture longer-range dependencies.\n\n\n\nHierarchical Hidden States"
  },
  {
    "objectID": "ML/rnn-deep.html#deep-transition-rnn",
    "href": "ML/rnn-deep.html#deep-transition-rnn",
    "title": "Chapter 10.5: Deep Recurrent Networks",
    "section": "2. Deep Transition RNN",
    "text": "2. Deep Transition RNN\nInstead of using simple affine transformations followed by activation functions, the core RNN transformations are replaced by multilayer perceptrons (MLPs).\nThe three key transformations become:\n\nInput-to-hidden: \\(\\text{MLP}_x(x_t)\\)\nHidden-to-hidden (recurrent): \\(\\text{MLP}_h(h_{t-1})\\)\nHidden-to-output: \\(\\text{MLP}_o(h_t)\\)\n\nEach MLP is a small feedforward network with its own hidden layers, adding depth within each time step.\nKey benefit: Richer transformations at each step‚Äîcapable of learning more complex nonlinear mappings between successive hidden states.\n\n\n\nDeep Transition RNN"
  },
  {
    "objectID": "ML/rnn-deep.html#deep-transition-rnn-with-skip-connections",
    "href": "ML/rnn-deep.html#deep-transition-rnn-with-skip-connections",
    "title": "Chapter 10.5: Deep Recurrent Networks",
    "section": "3. Deep Transition RNN with Skip Connections",
    "text": "3. Deep Transition RNN with Skip Connections\nThis architecture extends the deep transition RNN by adding skip connections (residual connections) around the deep MLP blocks.\nAt each time step, the input to an MLP is added directly to its output:\n\\[\nh_t = \\text{MLP}_h(h_{t-1}) + h_{t-1}\n\\]\nThis follows the residual learning principle popularized by ResNets.\nKey benefit: Skip connections enable gradient flow through very deep networks by providing shortcut paths that bypass multiple nonlinear transformations. This mitigates vanishing gradients and allows training of much deeper RNN architectures.\n\n\n\nDeep Transition RNN with Skip Connections"
  },
  {
    "objectID": "ML/rnn-deep.html#comparison-and-trade-offs",
    "href": "ML/rnn-deep.html#comparison-and-trade-offs",
    "title": "Chapter 10.5: Deep Recurrent Networks",
    "section": "Comparison and Trade-offs",
    "text": "Comparison and Trade-offs\n\n\n\n\n\n\n\n\n\n\nArchitecture\nDepth Location\nParameters\nGradient Flow\nUse Case\n\n\n\n\nHierarchical\nVertical stacking\nHigh\nStandard BPTT\nMulti-scale temporal patterns\n\n\nDeep Transition\nWithin transformations\nVery high\nChallenging\nComplex state transitions\n\n\nDeep Transition + Skip\nWithin transformations\nVery high\nImproved\nVery deep networks\n\n\n\nGeneral principle: Adding depth to RNNs increases expressiveness but also increases the risk of optimization difficulties. Skip connections are essential for training very deep recurrent architectures.\n\nThese three patterns can be combined‚Äîe.g., hierarchical RNNs where each layer uses deep transitions with skip connections‚Äîto build highly expressive sequence models at the cost of increased computational requirements."
  },
  {
    "objectID": "ML/convolutional-functions.html",
    "href": "ML/convolutional-functions.html",
    "title": "Chapter 9.5: Convolutional Functions",
    "section": "",
    "text": "This section covers the mathematical details and variations of convolutional operations:\n\nMathematical vs deep learning convolution (commutativity)\nMulti-channel convolution computation\nDownsampling with stride\nPadding strategies (valid, same, full)\nLocal connected layers and tiled convolution\nGradient computation for backpropagation\nEncoder-decoder architectures"
  },
  {
    "objectID": "ML/convolutional-functions.html#overview",
    "href": "ML/convolutional-functions.html#overview",
    "title": "Chapter 9.5: Convolutional Functions",
    "section": "",
    "text": "This section covers the mathematical details and variations of convolutional operations:\n\nMathematical vs deep learning convolution (commutativity)\nMulti-channel convolution computation\nDownsampling with stride\nPadding strategies (valid, same, full)\nLocal connected layers and tiled convolution\nGradient computation for backpropagation\nEncoder-decoder architectures"
  },
  {
    "objectID": "ML/convolutional-functions.html#mathematical-vs-deep-learning-convolution",
    "href": "ML/convolutional-functions.html#mathematical-vs-deep-learning-convolution",
    "title": "Chapter 9.5: Convolutional Functions",
    "section": "1. Mathematical vs Deep Learning Convolution",
    "text": "1. Mathematical vs Deep Learning Convolution\n\nMathematical Convolution (Commutative)\nIn mathematics, convolution is commutative and invertible:\n\\[\n(f * g)(t) = \\sum_\\tau f(\\tau) \\, g(t - \\tau)\n\\]\n\\[\n(g * f)(t) = \\sum_\\tau g(\\tau) \\, f(t - \\tau)\n\\]\nExample: When \\(\\tau = 1\\):\n\\[\nf(1)g(t-1) = g(t-1)f(t-(t-1)) = g(t-1)f(1)\n\\]\nKey property: \\((f * g) = (g * f)\\) (commutative)\n\n\nDeep Learning Convolution (Non-commutative)\nIn deep learning, we use cross-correlation, which is non-commutative:\n\\[\n(f \\star g)(t) = \\sum_\\tau f(t + \\tau) \\, g(\\tau)\n\\]\n\\[\n(g \\star f)(t) = \\sum_\\tau g(t + \\tau) \\, f(\\tau)\n\\]\nExample: When \\(\\tau = 1\\):\n\\[\nf(t+1)g(1) \\neq g(t+1)f(1)\n\\]\nKey property: \\((f \\star g) \\neq (g \\star f)\\) (not commutative)\nWhy this matters: Since kernels are learned, the lack of commutativity doesn‚Äôt affect performance‚Äîthe network will learn the appropriate kernel regardless."
  },
  {
    "objectID": "ML/convolutional-functions.html#multi-channel-convolution-computation",
    "href": "ML/convolutional-functions.html#multi-channel-convolution-computation",
    "title": "Chapter 9.5: Convolutional Functions",
    "section": "2. Multi-Channel Convolution Computation",
    "text": "2. Multi-Channel Convolution Computation\n\nNotation\n\nInput: \\(V \\in \\mathbb{R}^{i \\times m \\times n}\\)\n\n\\(i\\): number of input channels\n\\(m\\): input height\n\\(n\\): input width\n\nKernel: \\(K \\in \\mathbb{R}^{l \\times i \\times j \\times k}\\)\n\n\\(l\\): number of output channels\n\\(i\\): number of input channels (must match input)\n\\(j\\): kernel height\n\\(k\\): kernel width\n\nOutput: \\(Z \\in \\mathbb{R}^{l \\times (m-j+1) \\times (n-k+1)}\\)\n\nAssuming stride = 1, no padding\n\n\n\n\nConvolution Formula\n\\[\nZ_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} \\, K_{l,i,j,k} \\tag{9.7}\n\\]\nInterpretation: - For each output channel \\(l\\) and spatial position \\((x, y)\\) - Sum over all input channels \\(i\\) and all kernel positions \\((j, k)\\) - Multiply corresponding input and kernel values\n Figure: Multi-channel convolution operation - each output channel is computed by convolving all input channels with their respective kernels and summing the results."
  },
  {
    "objectID": "ML/convolutional-functions.html#downsampling-with-stride",
    "href": "ML/convolutional-functions.html#downsampling-with-stride",
    "title": "Chapter 9.5: Convolutional Functions",
    "section": "3. Downsampling with Stride",
    "text": "3. Downsampling with Stride\nWhen stride \\(s \\neq 1\\), the convolution formula becomes:\n\\[\nZ_{l,x,y} = \\sum_{i,j,k} V_{i,(x-1)s+j,(y-1)s+k} \\, K_{l,i,j,k} \\tag{9.8}\n\\]\nEffect: - Stride \\(s = 1\\): No downsampling (default) - Stride \\(s = 2\\): Output dimensions halved - Stride \\(s &gt; 1\\): Output dimensions reduced by factor of \\(s\\)\nOutput size with stride:\n\\[\n\\text{Output height} = \\left\\lfloor \\frac{m - j}{s} \\right\\rfloor + 1\n\\]\n\\[\n\\text{Output width} = \\left\\lfloor \\frac{n - k}{s} \\right\\rfloor + 1\n\\]\n Figure: Strided convolution with stride s=2 - the kernel moves by 2 pixels at a time instead of 1, reducing the output spatial dimensions."
  },
  {
    "objectID": "ML/convolutional-functions.html#padding-strategies",
    "href": "ML/convolutional-functions.html#padding-strategies",
    "title": "Chapter 9.5: Convolutional Functions",
    "section": "4. Padding Strategies",
    "text": "4. Padding Strategies\nWithout padding, the spatial dimensions shrink with each layer. Padding addresses this by adding borders around the input.\n\nValid Convolution (No Padding)\n\nPadding: 0\nOutput height: \\(h \\to h - j + 1\\)\nOutput width: \\(w \\to w - k + 1\\)\nUse case: When dimension reduction is acceptable\n\n\n\nSame Convolution (Preserve Size)\n\nPadding: Add \\(\\frac{j-1}{2}\\) rows to top/bottom, \\(\\frac{k-1}{2}\\) columns to left/right\nOutput height: \\(h \\to h\\)\nOutput width: \\(w \\to w\\)\nUse case: Deep networks where preserving dimensions is important\nNote: Requires \\(j\\) and \\(k\\) to be odd\n\n\n\nFull Convolution (Maximum Padding)\n\nPadding: Add \\(j-1\\) rows to top/bottom, \\(k-1\\) columns to left/right\nOutput height: \\(h \\to h + j - 1\\)\nOutput width: \\(w \\to w + k - 1\\)\nUse case: Transposed convolution (upsampling)\n\n Figure: Three padding strategies - valid (no padding), same (preserve size), and full (maximum padding) convolution."
  },
  {
    "objectID": "ML/convolutional-functions.html#local-connected-layer",
    "href": "ML/convolutional-functions.html#local-connected-layer",
    "title": "Chapter 9.5: Convolutional Functions",
    "section": "5. Local Connected Layer",
    "text": "5. Local Connected Layer\nUnlike standard convolution, the local connected layer does not share parameters across spatial positions.\n\nParameter Tensor\n\\[\nW \\in \\mathbb{R}^{l \\times x \\times y \\times i \\times j \\times k}\n\\]\nDimensions: - \\(l\\): output channels - \\(x\\): output height - \\(y\\): output width - \\(i\\): input channels - \\(j\\): kernel height - \\(k\\): kernel width\n\n\nComputation Formula\n\\[\nZ_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} \\, W_{l,x,y,i,j,k} \\tag{9.9}\n\\]\nKey difference: Each spatial position \\((x, y)\\) has its own unique kernel weights \\(W_{l,x,y,i,j,k}\\) instead of sharing weights across positions.\n Figure: Local connected layer - each spatial position uses different weights (no parameter sharing), requiring a 6-dimensional weight tensor.\nTrade-off: - Advantage: More flexible, can learn position-specific features - Disadvantage: Many more parameters, loses translation equivariance"
  },
  {
    "objectID": "ML/convolutional-functions.html#tiled-convolution",
    "href": "ML/convolutional-functions.html#tiled-convolution",
    "title": "Chapter 9.5: Convolutional Functions",
    "section": "6. Tiled Convolution",
    "text": "6. Tiled Convolution\nTiled convolution is a compromise between standard convolution and locally connected layers.\n\nConcept\nTiled convolution introduces a hyperparameter \\(t\\). In each spatial direction, the layer cycles through \\(t\\) different kernels periodically.\n\n\nParameter Tensor\n\\[\nK \\in \\mathbb{R}^{l \\times t \\times t \\times i \\times j \\times k}\n\\]\n\n\nComputation Formula\n\\[\nZ_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} \\, K_{l,(x \\bmod t)+1,(y \\bmod t)+1,i,j,k} \\tag{9.10}\n\\]\nHow it works: - At position \\((x, y)\\), use kernel indexed by \\((x \\bmod t, y \\bmod t)\\) - Kernels repeat in a tiled pattern across the spatial dimensions - With \\(t=1\\): Reduces to standard convolution (full sharing) - With \\(t = \\text{output size}\\): Becomes locally connected (no sharing)\nModern practice: Tiled convolution is rarely used today. Modern architectures rely on more effective alternatives such as group convolution, depthwise convolution, and dilated convolution."
  },
  {
    "objectID": "ML/convolutional-functions.html#gradient-computation-for-backpropagation",
    "href": "ML/convolutional-functions.html#gradient-computation-for-backpropagation",
    "title": "Chapter 9.5: Convolutional Functions",
    "section": "7. Gradient Computation for Backpropagation",
    "text": "7. Gradient Computation for Backpropagation\nTo train a convolutional network \\(c(K, V, s)\\) with loss \\(J(K, V)\\), we need gradients with respect to both kernel \\(K\\) and input \\(V\\).\n\nGradient Tensor\nGiven:\n\\[\nG_{l,x,y} = \\frac{\\partial}{\\partial Z_{l,x,y}} J(V, K)\n\\]\nThis is the gradient flowing back from the loss to the output \\(Z\\).\n\n\nGradient with Respect to Kernel\n\\[\ng(G, V, s)_{l,i,j,k} = \\frac{\\partial}{\\partial K_{l,i,j,k}} J(V, K) = \\sum_{x,y} G_{l,x,y} \\, V_{i,(x-1)s+j,(y-1)s+k} \\tag{9.11}\n\\]\nInterpretation: The gradient for each kernel element is computed by correlating the output gradient \\(G\\) with the corresponding input regions.\n\n\nGradient with Respect to Input\nWe also need gradients with respect to input \\(V\\) for backpropagation through earlier layers:\n\\[\nh(K, G, s)_{i,x,y} = \\frac{\\partial}{\\partial V_{i,x,y}} J(V, K) \\tag{9.12}\n\\]\nFull formula:\n\\[\nH_{i,a,b} = \\sum_{l} \\sum_{x} \\sum_{y} K_{l,i,a-(x-1)s,b-(y-1)s} \\, G_{l,x,y} \\tag{9.13}\n\\]\nInterpretation: The gradient for each input element is computed by convolving the flipped kernel with the output gradient.\n Figure: Gradient computation - gradients flow backward through the convolution operation, requiring correlation for kernel gradients and convolution with flipped kernel for input gradients."
  },
  {
    "objectID": "ML/convolutional-functions.html#encoder-decoder-networks",
    "href": "ML/convolutional-functions.html#encoder-decoder-networks",
    "title": "Chapter 9.5: Convolutional Functions",
    "section": "8. Encoder-Decoder Networks",
    "text": "8. Encoder-Decoder Networks\nIn an encoder-decoder architecture, \\(h(K, H, s)\\) provides the backward pass of the convolution, propagating gradients from the decoder output back to the encoder input.\n\\[\nR = h(K, H, s)\n\\]\nTraining requirements: - To train the encoder: Need gradients of \\(R\\) (the reconstructed input) - To train the decoder: Need gradients of \\(K\\) (the decoder kernel)\n Figure: Encoder-decoder network - the encoder compresses the input through convolutions, and the decoder reconstructs it. Gradients flow backward through both parts using the formulas above.\nApplications: - Image segmentation - Autoencoders - Image-to-image translation - Super-resolution"
  },
  {
    "objectID": "ML/convolutional-functions.html#summary",
    "href": "ML/convolutional-functions.html#summary",
    "title": "Chapter 9.5: Convolutional Functions",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\nConcept\nKey Formula\nKey Idea\n\n\n\n\nMath Convolution\n\\((f * g)(t) = \\sum_\\tau f(\\tau) g(t-\\tau)\\)\nCommutative, invertible\n\n\nDL Convolution\n\\((f \\star g)(t) = \\sum_\\tau f(t+\\tau) g(\\tau)\\)\nCross-correlation, non-commutative\n\n\nMulti-channel\n\\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\)\nSum over input channels\n\n\nStride\n\\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,(x-1)s+j,(y-1)s+k} K_{l,i,j,k}\\)\nDownsampling\n\n\nPadding\nValid/Same/Full\nControl output size\n\n\nLocal Connected\n\\(W \\in \\mathbb{R}^{l \\times x \\times y \\times i \\times j \\times k}\\)\nNo parameter sharing\n\n\nTiled Conv\nKernel indexed by \\((x \\bmod t, y \\bmod t)\\)\nPeriodic parameter sharing\n\n\nKernel Gradient\n\\(\\nabla_K J = \\sum_{x,y} G_{l,x,y} V_{i,(x-1)s+j,(y-1)s+k}\\)\nCorrelation of output gradient with input\n\n\nInput Gradient\n\\(\\nabla_V J = \\sum_{l,x,y} K_{l,i,a-(x-1)s,b-(y-1)s} G_{l,x,y}\\)\nConvolution with flipped kernel\n\n\n\nKey takeaways: - Deep learning uses cross-correlation (not true mathematical convolution) - Stride enables downsampling without pooling - Padding controls output spatial dimensions - Gradient computation requires different operations for kernel vs input - Modern architectures favor standard convolution over tiled or locally connected variants"
  },
  {
    "objectID": "ML/dataset-augmentation.html",
    "href": "ML/dataset-augmentation.html",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "",
    "text": "When available training data is limited, we can explicitly increase data diversity by generating transformed or perturbed versions of existing samples.\nThis technique, known as dataset augmentation, helps the model generalize better and reduces overfitting."
  },
  {
    "objectID": "ML/dataset-augmentation.html#core-idea",
    "href": "ML/dataset-augmentation.html#core-idea",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "",
    "text": "When available training data is limited, we can explicitly increase data diversity by generating transformed or perturbed versions of existing samples.\nThis technique, known as dataset augmentation, helps the model generalize better and reduces overfitting."
  },
  {
    "objectID": "ML/dataset-augmentation.html#basic-concept",
    "href": "ML/dataset-augmentation.html#basic-concept",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "Basic Concept",
    "text": "Basic Concept\nDataset augmentation is one of the simplest and most effective regularization strategies.\nIt increases both the size and the variability of the training set by applying transformations that do not change the class label.\n\n\n\n\n\n\nImportantKey Principle\n\n\n\nThe augmented data should preserve semantic meaning while introducing variation that reflects real-world conditions."
  },
  {
    "objectID": "ML/dataset-augmentation.html#common-augmentation-methods",
    "href": "ML/dataset-augmentation.html#common-augmentation-methods",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "Common Augmentation Methods",
    "text": "Common Augmentation Methods\n\nGeometric Transformations\nInclude translation, rotation, scaling, and flipping of images.\nExample: For image classification, a horizontally flipped cat image is still a cat.\nNote: Even though convolution provides some degree of translation invariance, explicitly augmenting the dataset with translated copies of the inputs can further improve generalization.\nWhy this helps:\n\nForces the model to learn features that are robust to spatial transformations\nSimulates different camera angles and object positions\nReduces dependence on absolute position in the image\n\n\n\nNoise Injection\nAdd random noise (e.g., Gaussian noise) to the input or hidden layers.\nIntroduced in denoising autoencoders (Vincent et al., 2008), this acts as unsupervised regularization, improving robustness and stability.\nMathematical formulation:\n\\[\n\\tilde{x} = x + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\nResearch finding: Poole et al.¬†(2014) showed that carefully tuning the noise level can lead to strong performance gains.\nWhy this helps:\n\nPrevents the model from memorizing exact pixel values\nImproves robustness to sensor noise and measurement errors\nActs as a form of implicit regularization\n\n\n\nRandom Cropping and Occlusion\nMimic the variability of human perception by randomly cropping or masking parts of the image.\nExample: Randomly crop a 224√ó224 patch from a 256√ó256 image during training.\nWhy this helps:\n\nForces the model to recognize objects from partial views\nSimulates real-world scenarios where objects are partially occluded\nIncreases effective dataset size significantly"
  },
  {
    "objectID": "ML/dataset-augmentation.html#applications-across-domains",
    "href": "ML/dataset-augmentation.html#applications-across-domains",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "Applications Across Domains",
    "text": "Applications Across Domains\n\n\n\n\n\n\n\n\nDomain\nExample of Augmentation\nGoal\n\n\n\n\nComputer Vision\nTranslation, rotation, scaling, flipping\nEncourage spatial invariance\n\n\nSpeech Recognition\nAdd random noise or time masking\nImprove robustness to background noise\n\n\nText / NLP\nWord dropout or synonym replacement\nImprove generalization in low-data settings\n\n\n\nAdditional examples:\n\nComputer Vision: Color jittering, brightness adjustment, elastic distortions\nSpeech: Speed perturbation, pitch shifting, room impulse response simulation\nNLP: Back-translation, paraphrasing, random insertion/deletion"
  },
  {
    "objectID": "ML/dataset-augmentation.html#design-and-evaluation",
    "href": "ML/dataset-augmentation.html#design-and-evaluation",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "Design and Evaluation",
    "text": "Design and Evaluation\nFair comparison principle: When comparing different algorithms, the same data augmentation strategy must be used for a fair comparison.\n\n\n\n\n\n\nWarningWhy This Matters\n\n\n\nIf one algorithm benefits from augmented data and another does not, performance differences may reflect the augmentation strategy, not the algorithm itself.\n\n\nBest practices:\n\nDocument all augmentation techniques used\nAblation studies should isolate augmentation effects\nReport results both with and without augmentation when introducing new methods"
  },
  {
    "objectID": "ML/dataset-augmentation.html#relation-to-other-regularization-methods",
    "href": "ML/dataset-augmentation.html#relation-to-other-regularization-methods",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "Relation to Other Regularization Methods",
    "text": "Relation to Other Regularization Methods\nAdding noise to inputs is conceptually related to weight regularization (Bishop, 1995).\nTheoretical connection:\n\nSmall input noise can be approximated by a penalty on the weights\nFor quadratic loss, input noise is equivalent to Tikhonov regularization\n\nDropout (see Section 7.12) can be interpreted as a stochastic extension of noise-based regularization.\nDataset augmentation can thus be seen as a bridge between:\n\nExplicit data transformation (augmentation)\nImplicit noise regularization (weight decay, dropout)\n\n\n\n\n\n\n\nTipUnified View\n\n\n\nAll these techniques prevent the model from relying too heavily on specific features or exact training examples."
  },
  {
    "objectID": "ML/dataset-augmentation.html#summary",
    "href": "ML/dataset-augmentation.html#summary",
    "title": "Dataset Augmentation: Regularization Through Data Diversity",
    "section": "Summary",
    "text": "Summary\nKey takeaways:\n\nDataset augmentation improves generalization by making the model robust to input variations such as translation, rotation, and noise\nIt is a practical and powerful regularization method that effectively combats overfitting, especially when training data is limited\nAugmentation strategies should preserve semantic labels while introducing realistic variations\nFair algorithm comparisons require consistent augmentation across all methods\n\nWhen to use:\n\nLimited training data\nHigh risk of overfitting\nDomain knowledge suggests specific invariances (e.g., rotation invariance for digit recognition)\n\nTrade-offs:\n\nIncreases training time (more data to process)\nMay introduce unrealistic samples if not carefully designed\nRequires domain expertise to choose appropriate transformations\n\n\nSource: Deep Learning Book, Chapter 7.4"
  },
  {
    "objectID": "ML/axis.html",
    "href": "ML/axis.html",
    "title": "Understanding Axis(dim) Operations Smartly",
    "section": "",
    "text": "import numpy as np\n\nx = np.array([[1, 2, 3], [4, 5, 6]])\nThe 2D array is: \\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\]\n\n\nprint(x.sum(axis=0))\nThe result is:\narray([5, 7, 9])\nWhen axis(dim) is 0, it means the operation is performed along 0 dimension. Items along 0 dimension are each sub-array. Then the result is just two vectors added together.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n4 & 5 & 6\n\\end{bmatrix}\n\\]\nx.sum(axis=1)==x[0]+x[1]\nOperations along axis 0 is just operate on all sub-arrays. For example,\nsum(x,axis=0) is just \\(\\vec{x[0]}+\\vec{x[1]}+...+\\vec{x[n]}\\)\n\n\n\nprint(x.sum(axis=1))\nThe result is:\narray([6, 15])\nWhen axis(dim) is 1, it means the operation is performed along 1 dimension.\n\\[\n\\begin{bmatrix}\n1+2+3 \\\\\n4+5+6\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "ML/axis.html#a-2d-example",
    "href": "ML/axis.html#a-2d-example",
    "title": "Understanding Axis(dim) Operations Smartly",
    "section": "",
    "text": "import numpy as np\n\nx = np.array([[1, 2, 3], [4, 5, 6]])\nThe 2D array is: \\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\]\n\n\nprint(x.sum(axis=0))\nThe result is:\narray([5, 7, 9])\nWhen axis(dim) is 0, it means the operation is performed along 0 dimension. Items along 0 dimension are each sub-array. Then the result is just two vectors added together.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n4 & 5 & 6\n\\end{bmatrix}\n\\]\nx.sum(axis=1)==x[0]+x[1]\nOperations along axis 0 is just operate on all sub-arrays. For example,\nsum(x,axis=0) is just \\(\\vec{x[0]}+\\vec{x[1]}+...+\\vec{x[n]}\\)\n\n\n\nprint(x.sum(axis=1))\nThe result is:\narray([6, 15])\nWhen axis(dim) is 1, it means the operation is performed along 1 dimension.\n\\[\n\\begin{bmatrix}\n1+2+3 \\\\\n4+5+6\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "ML/axis.html#a-3d-example",
    "href": "ML/axis.html#a-3d-example",
    "title": "Understanding Axis(dim) Operations Smartly",
    "section": "A 3D example",
    "text": "A 3D example\nx_3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nThe 3D array looks like:\n\\[\nX = \\left[\\begin{array}{c|c}\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix} &\n\\begin{bmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{bmatrix}\n\\end{array}\\right]\n\\]\n\nsum along axis 0\nprint(x_3d.sum(axis=0))\nThe result is:\narray([[8, 10, 12], [18, 20, 22]])\nThe result is the sum of two matrices.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n8 & 10 & 12 \\\\\n18 & 20 & 22\n\\end{bmatrix}\n\\]\nWhen axis(dim) is 0, given each element in this dimension is the matrix, so the sum is the sum of two matrices.\n\n\nsum along axis 1\nprint(x_3d.sum(axis=1))\nThe result is:\n\nWhen axis(dim) is 1, given each element in this dimension is the rows of the matrix, so the sum is the sum of all the rows in each matrix.\n\\[\n[\\begin{array}{c|c}\n\\begin{bmatrix}\n1 & 2 & 3\n\\end{bmatrix} +\n\\begin{bmatrix}\n4 & 5 & 6\n\\end{bmatrix} &\n\\begin{bmatrix}\n7 & 8 & 9\n\\end{bmatrix} +\n\\begin{bmatrix}\n10 & 11 & 12\n\\end{bmatrix}\n\\end{array}]\n\\]\nso the result is:\narray([[5,7,9], [17, 19, 21]])\n\n\nsum along axis 2\nWhen axis(dim) is 2, given each element in this dimension is the elements of the matrix, so the sum is the sum of the elements.\n\\[\n\\begin{array}{c|c}\n\\begin{bmatrix}\n1+2+3,4+5+6\n\\end{bmatrix} &\n\\begin{bmatrix}\n7+8+9,10+11+12\n\\end{bmatrix}\n\\end{array}\n\\]\nso the result is:\narray([[6, 15], [24, 33]])\nAso, when axis(dim) is -1, it means the operation is performed along the last dimension. So for 2d array, axis -1 is the same as axis 1."
  },
  {
    "objectID": "ML/axis.html#rules",
    "href": "ML/axis.html#rules",
    "title": "Understanding Axis(dim) Operations Smartly",
    "section": "Rules",
    "text": "Rules\n\nwhen operate on axis(dim) N, it means the operation is performed along the elements of dimension [0‚Ä¶N].\nfor 2d array, axis 0 is the sum of vectors, because each element of the array is a vector, computer sees m vectors at once for a (m,n) shape array.\nfor 3d array, axis 0 is the sum of matrices, because each element of the array is a matrix. Computer sees m matrices at once for a (m,n,p) shape array.\nfor 2d array, axis 1 is the sum of elements in each vector and merge back to (m,1) shape array. Computer sees 1 vector with n elements at once for m times.\nfor 3d array, axis 1 is the sum of all vectors in each matrix and merge back to (m,1,p) shape array. Computer sees n vectors at once for m times.\nfor 3d array, axis 2 is the sum of each vector in each matrix and merge back to (m,n,1) shape array. Computer sees p vectors for m*n times."
  },
  {
    "objectID": "ML/cnn-data-types.html",
    "href": "ML/cnn-data-types.html",
    "title": "Chapter 9.7: Data Types",
    "section": "",
    "text": "Convolutional networks can operate on many different kinds of data, depending on the number of channels and the dimensionality of the input."
  },
  {
    "objectID": "ML/cnn-data-types.html#overview",
    "href": "ML/cnn-data-types.html#overview",
    "title": "Chapter 9.7: Data Types",
    "section": "",
    "text": "Convolutional networks can operate on many different kinds of data, depending on the number of channels and the dimensionality of the input."
  },
  {
    "objectID": "ML/cnn-data-types.html#input-formats",
    "href": "ML/cnn-data-types.html#input-formats",
    "title": "Chapter 9.7: Data Types",
    "section": "Input Formats",
    "text": "Input Formats\nTable 9.1 illustrates how 1D, 2D, and 3D inputs can each appear in both single-channel and multi-channel forms.\n Figure: Examples of different data types CNNs can process. 1D inputs include audio waveforms and time series. 2D inputs include grayscale images (single-channel) and RGB images (multi-channel). 3D inputs correspond to volumes such as CT scans (single-channel) or videos and skeleton animations (multi-channel spatiotemporal data).\n\n1D inputs include audio waveforms, time series, or multi-sensor signals.\n2D inputs include grayscale images (single-channel) and RGB or multispectral images (multi-channel).\n3D inputs correspond to volumes such as CT scans (single-channel) or videos and skeleton animations (multi-channel spatiotemporal data)."
  },
  {
    "objectID": "ML/cnn-data-types.html#flexibility-of-convolution",
    "href": "ML/cnn-data-types.html#flexibility-of-convolution",
    "title": "Chapter 9.7: Data Types",
    "section": "Flexibility of Convolution",
    "text": "Flexibility of Convolution\nOne advantage of convolution is that it naturally supports these different input structures without requiring a fixed dimensionality or a fixed number of channels.\nThe same convolutional kernel can be applied across various data types because convolution is fundamentally a translation-equivariant, local operation."
  },
  {
    "objectID": "ML/cnn-data-types.html#variable-sized-inputs",
    "href": "ML/cnn-data-types.html#variable-sized-inputs",
    "title": "Chapter 9.7: Data Types",
    "section": "Variable-Sized Inputs",
    "text": "Variable-Sized Inputs\nAdditionally, convolution can also process variable-sized inputs.\nAs long as the kernel size is fixed, the convolution operation slides across the input regardless of its spatial extent, producing outputs whose dimensions scale accordingly.\nThis flexibility enables CNNs to handle inputs with different widths or heights‚Äîa property that fully connected networks do not have.\n Figure: CNNs can process inputs of different sizes. The same convolutional kernel slides across inputs regardless of spatial extent, producing outputs that scale with the input dimensions. This flexibility is unique to convolutional architectures‚Äîfully connected networks require fixed-size inputs."
  },
  {
    "objectID": "ML/cnn-data-types.html#key-insight",
    "href": "ML/cnn-data-types.html#key-insight",
    "title": "Chapter 9.7: Data Types",
    "section": "Key Insight",
    "text": "Key Insight\nThe power of CNNs lies in their flexibility. Unlike fully connected networks that require fixed input dimensions, convolutional networks can handle varying channel counts, different spatial dimensionalities (1D, 2D, 3D), and variable-sized inputs‚Äîall with the same kernel. This versatility makes CNNs applicable to a wide range of domains beyond computer vision, from audio processing to volumetric medical imaging."
  },
  {
    "objectID": "ML/basic-optimization-algorithms.html",
    "href": "ML/basic-optimization-algorithms.html",
    "title": "Chapter 8.3: Basic Algorithms",
    "section": "",
    "text": "Stochastic Gradient Descent (SGD) is the most fundamental optimization algorithm in deep learning. Unlike batch gradient descent, which computes gradients over the entire training set, SGD uses small random samples (minibatches) to estimate the gradient, making it computationally efficient for large datasets.\n\n\nHyperparameters:\n\nLearning rate \\(\\epsilon\\)\nInitial parameters \\(\\theta\\)\n\nTraining procedure:\nWhile stopping criterion not met, do:\n\nSample a minibatch of m examples from the training set\nCompute the loss and gradient on the minibatch\nCompute the gradient estimate:\n\n\\[\n\\hat{g} \\leftarrow +\\frac{1}{m}\\nabla_{\\theta}\\sum_i L(f(x^{(i)};\\theta), y^{(i)})\n\\]\n\nUpdate parameters:\n\n\\[\n\\theta \\leftarrow \\theta - \\epsilon \\hat{g}\n\\]\n\n\n\nA critical question in SGD is: how should we choose the learning rate schedule to guarantee convergence? For SGD to converge to a minimum, the learning rate schedule must satisfy two complementary conditions that balance exploration and stability:\nCondition 1: The sum of learning rates must diverge (equation 8.12):\n\\[\n\\sum_{k=1}^{\\infty}\\epsilon_k = \\infty \\tag{8.12}\n\\]\nIn other words, \\(\\epsilon_1 + \\epsilon_2 + \\ldots\\) must be unbounded.\n\n\n\nDivergence\n\n\nThis condition ensures that the algorithm can reach any point in parameter space, no matter how far from the initialization.\nCondition 2: The sum of squared learning rates must converge (equation 8.13):\n\\[\n\\sum_{k=1}^{\\infty} \\epsilon_k^2 &lt; \\infty \\tag{8.13}\n\\]\nIn other words, \\(\\epsilon_1^2 + \\epsilon_2^2 + \\ldots = C\\) (a finite constant).\n\n\n\nConvergence\n\n\nThis condition ensures that the accumulated noise from stochastic gradient estimates remains bounded, allowing the algorithm to settle into a minimum rather than bouncing around indefinitely.\nSummary: Together, these two conditions ensure that SGD can initially make large steps to explore parameter space (Condition 1), while eventually taking smaller steps that allow convergence to a minimum (Condition 2).\n\n\n\nWhile the theoretical conditions above provide convergence guarantees, they are often impractical for real-world training.\nA commonly used learning rate schedule in practice is linear decay (equation 8.14):\n\\[\n\\epsilon_k = (1-\\alpha)\\epsilon_0 + \\alpha \\epsilon_{\\tau} \\tag{8.14}\n\\]\nwhere \\(\\alpha = \\frac{k}{\\tau}\\).\nThis schedule starts at \\(\\epsilon_0\\) and linearly decreases to \\(\\epsilon_{\\tau}\\) over \\(\\tau\\) iterations. After \\(\\tau\\) iterations, when \\(\\alpha = 1\\), the learning rate remains fixed at \\(\\epsilon_{\\tau}\\)."
  },
  {
    "objectID": "ML/basic-optimization-algorithms.html#stochastic-gradient-descent",
    "href": "ML/basic-optimization-algorithms.html#stochastic-gradient-descent",
    "title": "Chapter 8.3: Basic Algorithms",
    "section": "",
    "text": "Stochastic Gradient Descent (SGD) is the most fundamental optimization algorithm in deep learning. Unlike batch gradient descent, which computes gradients over the entire training set, SGD uses small random samples (minibatches) to estimate the gradient, making it computationally efficient for large datasets.\n\n\nHyperparameters:\n\nLearning rate \\(\\epsilon\\)\nInitial parameters \\(\\theta\\)\n\nTraining procedure:\nWhile stopping criterion not met, do:\n\nSample a minibatch of m examples from the training set\nCompute the loss and gradient on the minibatch\nCompute the gradient estimate:\n\n\\[\n\\hat{g} \\leftarrow +\\frac{1}{m}\\nabla_{\\theta}\\sum_i L(f(x^{(i)};\\theta), y^{(i)})\n\\]\n\nUpdate parameters:\n\n\\[\n\\theta \\leftarrow \\theta - \\epsilon \\hat{g}\n\\]\n\n\n\nA critical question in SGD is: how should we choose the learning rate schedule to guarantee convergence? For SGD to converge to a minimum, the learning rate schedule must satisfy two complementary conditions that balance exploration and stability:\nCondition 1: The sum of learning rates must diverge (equation 8.12):\n\\[\n\\sum_{k=1}^{\\infty}\\epsilon_k = \\infty \\tag{8.12}\n\\]\nIn other words, \\(\\epsilon_1 + \\epsilon_2 + \\ldots\\) must be unbounded.\n\n\n\nDivergence\n\n\nThis condition ensures that the algorithm can reach any point in parameter space, no matter how far from the initialization.\nCondition 2: The sum of squared learning rates must converge (equation 8.13):\n\\[\n\\sum_{k=1}^{\\infty} \\epsilon_k^2 &lt; \\infty \\tag{8.13}\n\\]\nIn other words, \\(\\epsilon_1^2 + \\epsilon_2^2 + \\ldots = C\\) (a finite constant).\n\n\n\nConvergence\n\n\nThis condition ensures that the accumulated noise from stochastic gradient estimates remains bounded, allowing the algorithm to settle into a minimum rather than bouncing around indefinitely.\nSummary: Together, these two conditions ensure that SGD can initially make large steps to explore parameter space (Condition 1), while eventually taking smaller steps that allow convergence to a minimum (Condition 2).\n\n\n\nWhile the theoretical conditions above provide convergence guarantees, they are often impractical for real-world training.\nA commonly used learning rate schedule in practice is linear decay (equation 8.14):\n\\[\n\\epsilon_k = (1-\\alpha)\\epsilon_0 + \\alpha \\epsilon_{\\tau} \\tag{8.14}\n\\]\nwhere \\(\\alpha = \\frac{k}{\\tau}\\).\nThis schedule starts at \\(\\epsilon_0\\) and linearly decreases to \\(\\epsilon_{\\tau}\\) over \\(\\tau\\) iterations. After \\(\\tau\\) iterations, when \\(\\alpha = 1\\), the learning rate remains fixed at \\(\\epsilon_{\\tau}\\)."
  },
  {
    "objectID": "ML/basic-optimization-algorithms.html#momentum",
    "href": "ML/basic-optimization-algorithms.html#momentum",
    "title": "Chapter 8.3: Basic Algorithms",
    "section": "Momentum",
    "text": "Momentum\nWhile SGD provides a solid foundation for optimization, it can struggle with ill-conditioned loss surfaces where gradients in different directions have vastly different magnitudes. Momentum addresses this problem by adding ‚Äúinertia‚Äù to parameter updates. It accelerates learning in directions with consistent gradients and smooths oscillations in steep or noisy directions.\nThe momentum method introduces a velocity variable \\(v\\) that accumulates an exponentially decaying moving average of past gradients:\n\\[\nv_t = \\beta v_{t-1} + (1-\\beta)\\nabla_\\theta L(\\theta)\n\\]\n\\[\n\\theta_{t+1} = \\theta_t - \\epsilon v_t\n\\]\nwhere:\n\n\\(\\beta\\) is the momentum coefficient (typically 0.9)\n\\(v_t\\) is the velocity, representing the weighted average of gradients\n\\(\\epsilon\\) is the learning rate\n\n\nAlternative Formulation\nThe momentum algorithm can equivalently be written in a slightly different form (equations 8.15-8.16):\n\\[\nv \\leftarrow \\alpha v - \\epsilon \\nabla_\\theta \\left(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)}; \\theta), y^{(i)}) \\right) \\tag{8.15}\n\\]\n\\[\n\\theta \\leftarrow \\theta + v \\tag{8.16}\n\\]\nIn this formulation, \\(\\epsilon\\) represents the learning rate scaled by \\((1-\\alpha)\\). Note that here we use \\(\\alpha\\) instead of \\(\\beta\\) as the momentum coefficient, and the parameter update adds the velocity instead of subtracting it.\n\n\nMaximum Step Length\nEquation 8.17 gives the upper bound of the effective step size in momentum-based SGD:\n\\[\n\\text{Max step length} = \\frac{\\epsilon \\lVert g \\rVert}{1 - \\alpha} \\tag{8.17}\n\\]\nIntuition: Assuming the gradients \\(g_t\\) remain in a consistent direction, the velocity accumulates geometrically as:\n\\[\nv_t = -\\epsilon(g_t + \\alpha g_{t-1} + \\alpha^2 g_{t-2} + \\cdots)\n\\]\nwhich converges to \\(\\frac{\\epsilon g}{1 - \\alpha}\\).\nThis shows that momentum effectively amplifies the step length by a factor of \\(\\frac{1}{1-\\alpha}\\).\nExample: With \\(\\alpha = 0.9\\), the effective speed can be up to 10√ó faster than standard SGD.\n\n\nPhysical Interpretation: Newton‚Äôs Law of Acceleration\nThe momentum method is not just a mathematical trick ‚Äî it has a deep connection to classical physics. We can understand momentum optimization by drawing an analogy with a particle moving on a physical surface under the influence of forces.\nEquation 8.18 - Newton‚Äôs Second Law of Motion:\nIn physics, force equals mass times acceleration:\n\\[\nF = ma = m\\frac{\\partial^2 x}{\\partial t^2}\n\\]\nIn deep learning, assuming \\(m = 1\\) and viewing the parameters \\(\\theta\\) as the position of a particle on the loss surface, we obtain:\n\\[\nf(t) = \\frac{\\partial^2 \\theta(t)}{\\partial t^2} \\tag{8.18}\n\\]\nThis expresses that the force acting on parameters corresponds to their acceleration, forming the physical foundation of momentum optimization.\nEquation 8.19 - Velocity as Rate of Change:\nTo further express the relationship between force, velocity, and position, we define velocity \\(v(t)\\) as the first derivative of the position \\(\\theta(t)\\) with respect to time:\n\\[\nv(t) = \\frac{\\partial \\theta(t)}{\\partial t} \\tag{8.19}\n\\]\n\\[\nf(t) = \\frac{\\partial v(t)}{\\partial t}\n\\]\nThe velocity is the instantaneous rate of change ‚Äî the first derivative of position. Likewise, the force (or acceleration) is the instantaneous rate of change of velocity ‚Äî the first derivative of velocity.\nThis physical interpretation helps us understand why momentum works: the negative gradient acts as a force pushing the parameters downhill, while the velocity variable accumulates this force over time, allowing the optimizer to build up speed in consistent directions.\n\n\nDamping in Momentum\nIn the physical analogy, we can also understand the momentum coefficient \\(\\alpha\\) as a damping or friction term that prevents the optimizer from accelerating indefinitely.\nIn momentum optimization, the damping (or friction) effect is implicitly controlled by the factor \\(\\alpha\\) in the velocity update rule:\n\\[\nv_t = \\alpha v_{t-1} - \\epsilon \\nabla_\\theta L(\\theta_t)\n\\]\nThis formulation can be seen as a discrete version of Newton‚Äôs second law with viscous damping:\n\\[\n\\frac{d^2 \\theta}{dt^2} = -\\nabla_\\theta L(\\theta) - \\mu \\frac{d\\theta}{dt}\n\\]\nwhere the damping coefficient \\(\\mu\\) corresponds to \\(1 - \\alpha\\).\nIntuition:\n\n\\(\\alpha\\) determines how much of the previous velocity is retained (the inertia)\n\\(1 - \\alpha\\) represents the proportion of velocity lost due to friction\n\nEffect of \\(\\alpha\\):\n\nWhen \\(\\alpha\\) is close to 1 (e.g., 0.99), damping is weak ‚Äî the optimizer moves faster but tends to oscillate near minima\nWhen \\(\\alpha\\) is smaller (e.g., 0.5), damping is strong ‚Äî the motion is smoother but slower"
  },
  {
    "objectID": "ML/basic-optimization-algorithms.html#nesterov-momentum",
    "href": "ML/basic-optimization-algorithms.html#nesterov-momentum",
    "title": "Chapter 8.3: Basic Algorithms",
    "section": "Nesterov Momentum",
    "text": "Nesterov Momentum\nWhile standard momentum provides significant improvements over vanilla SGD, it still has a limitation: it computes gradients at the current position without considering where the momentum will take us next. Nesterov momentum addresses this by introducing a ‚Äúlookahead‚Äù mechanism.\nStandard momentum looks backward ‚Äî it uses gradients from the current position. Nesterov momentum looks ahead ‚Äî it first predicts the next position \\(\\theta + \\alpha v\\) and then calculates the gradient there.\nThis leads to the modified update rule (equation 8.21):\n\\[\nv_{t+1} = \\alpha v_t - \\epsilon \\nabla_\\theta L(\\theta_t + \\alpha v_t)\n\\]\n\\[\nv \\leftarrow \\alpha v - \\epsilon \\nabla_{\\theta}\\left[\\frac{1}{m} \\sum_{i=1}^m L(f(x^{(i)};\\theta + \\alpha v), y^{(i)}) \\right] \\tag{8.21}\n\\]\nFinally, the parameters are updated using this new velocity (equation 8.22):\n\\[\n\\theta_{t+1} = \\theta_t + v_{t+1} \\tag{8.22}\n\\]\nThis ‚Äúlookahead‚Äù strategy allows the optimizer to anticipate its next move, resulting in faster convergence and less oscillation near minima compared to standard momentum.\nIntuition: Imagine you‚Äôre rolling a ball down a hill. Standard momentum uses the slope at your current position to decide how fast to roll. Nesterov momentum is smarter ‚Äî it first looks ahead to where the momentum will carry you, checks the slope there, and then adjusts the velocity accordingly. This prevents overshooting and makes the optimizer more responsive to the local curvature of the loss surface."
  },
  {
    "objectID": "ML/regularization-underconstrained.html",
    "href": "ML/regularization-underconstrained.html",
    "title": "Regularization and Under-Constrained Problems",
    "section": "",
    "text": "In linear regression, we often encounter scenarios where the standard solution breaks down:\nUnder-constrained scenarios:\n\n\\(m &lt; n\\) (fewer samples than dimensions)\nFeature-dependent columns (linearly dependent features)\n\nConsequence: Matrix \\(X^T X\\) is not invertible, preventing direct solution of linear regression.\nThis post explores why this happens and how regularization provides a mathematically sound solution."
  },
  {
    "objectID": "ML/regularization-underconstrained.html#problem-context",
    "href": "ML/regularization-underconstrained.html#problem-context",
    "title": "Regularization and Under-Constrained Problems",
    "section": "",
    "text": "In linear regression, we often encounter scenarios where the standard solution breaks down:\nUnder-constrained scenarios:\n\n\\(m &lt; n\\) (fewer samples than dimensions)\nFeature-dependent columns (linearly dependent features)\n\nConsequence: Matrix \\(X^T X\\) is not invertible, preventing direct solution of linear regression.\nThis post explores why this happens and how regularization provides a mathematically sound solution."
  },
  {
    "objectID": "ML/regularization-underconstrained.html#linear-regression-and-the-normal-equations",
    "href": "ML/regularization-underconstrained.html#linear-regression-and-the-normal-equations",
    "title": "Regularization and Under-Constrained Problems",
    "section": "Linear Regression and the Normal Equations",
    "text": "Linear Regression and the Normal Equations\n\nObjective Function\nIn linear regression, we want to minimize the squared error:\n\\[\n\\min_w ||Xw - y||^2\n\\]\nwhere: - \\(X\\) is the \\(m \\times n\\) data matrix (m samples, n features) - \\(w\\) is the \\(n \\times 1\\) weight vector - \\(y\\) is the \\(m \\times 1\\) target vector\n\n\nLoss Function\nThe squared error can be written as:\n\\[\nJ(w; X, y) = ||Xw - y||^2 = (Xw - y)^T (Xw - y)\n\\]"
  },
  {
    "objectID": "ML/regularization-underconstrained.html#derivation-of-the-normal-equations",
    "href": "ML/regularization-underconstrained.html#derivation-of-the-normal-equations",
    "title": "Regularization and Under-Constrained Problems",
    "section": "Derivation of the Normal Equations",
    "text": "Derivation of the Normal Equations\nLet‚Äôs derive the analytical solution step by step.\n\nStep 1: Expand the Loss\n\\[\nJ(w; X, y) = (Xw)^T Xw - (Xw)^T y - y^T Xw + y^T y\n\\]\n\n\nStep 2: Simplify Using Vector Dot Product Symmetry\nSince \\(Xw\\) and \\(y\\) are vectors, their dot product is commutative:\n\\[\n(Xw)^T y = y^T Xw \\quad \\text{(because } \\vec{v}_1 \\cdot \\vec{v}_2 = \\vec{v}_2 \\cdot \\vec{v}_1\\text{)}\n\\]\nTherefore:\n\\[\nJ(w; X, y) = w^T X^T Xw - 2y^T Xw + y^T y\n\\]\n\n\nStep 3: Compute the Gradient\nWe need \\(\\nabla_w J(w; X, y)\\). The gradient has three parts:\nPart 1: Quadratic Term \\(w^T X^T Xw\\)\nThis is a quadratic form. Using the matrix derivative rule \\(\\nabla_w (w^T A w) = 2Aw\\) for symmetric \\(A\\):\n\\[\n\\nabla_w (w^T X^T Xw) = 2X^T Xw\n\\]\n\n\n\n\n\n\nNoteDetailed Derivation\n\n\n\n\\[\n\\begin{aligned}\nw^T X^T Xw &= (Xw)^T (Xw) = ||Xw||^2 \\\\\n\\nabla_w ||Xw||^2 &= 2X^T Xw\n\\end{aligned}\n\\]\n\n\nPart 2: Linear Term \\(-2y^T Xw\\)\nThis is a linear term. Using the rule \\(\\nabla_w (a^T w) = a\\):\n\\[\n\\nabla_w (-2y^T Xw) = -2X^T y\n\\]\nExplanation of dimensions: - If \\(X\\) is \\(m \\times n\\) and \\(w\\) is \\(n \\times 1\\), then \\(Xw\\) is \\(m \\times 1\\) - \\(y\\) is \\(m \\times 1\\) - For the gradient with respect to \\(w\\) (which is \\(n \\times 1\\)), we need \\(X^T\\) (which is \\(n \\times m\\)) to match dimensions - The linear derivative formula \\(\\nabla_w (a^T w) = a\\) tells us the gradient is the transpose of the coefficient\nPart 3: Constant Term \\(y^T y\\)\nThis is constant with respect to \\(w\\), so its gradient is 0.\n\n\nStep 4: Complete Gradient\n\\[\n\\nabla_w J(w; X, y) = 2X^T Xw - 2X^T y\n\\]\n\n\nStep 5: Solve for Optimal \\(w\\)\nSet the gradient to zero:\n\\[\n\\begin{aligned}\n2X^T Xw - 2X^T y &= 0 \\\\\nX^T Xw &= X^T y \\\\\nw &= (X^T X)^{-1} X^T y \\quad \\text{(if } X^T X \\text{ is invertible)}\n\\end{aligned}\n\\]\nThis is the normal equation - the closed-form solution to linear regression."
  },
  {
    "objectID": "ML/regularization-underconstrained.html#the-problem-when-xt-x-is-not-invertible",
    "href": "ML/regularization-underconstrained.html#the-problem-when-xt-x-is-not-invertible",
    "title": "Regularization and Under-Constrained Problems",
    "section": "The Problem: When \\(X^T X\\) is Not Invertible",
    "text": "The Problem: When \\(X^T X\\) is Not Invertible\nThe normal equation requires \\((X^T X)^{-1}\\) to exist. However, \\(X^T X\\) is not invertible when:\n\nUnder-constrained: \\(m &lt; n\\) (fewer samples than features)\nRank deficient: Columns of \\(X\\) are linearly dependent\n\nIn these cases, we cannot compute \\((X^T X)^{-1}\\), and the standard solution fails."
  },
  {
    "objectID": "ML/regularization-underconstrained.html#solution-regularization",
    "href": "ML/regularization-underconstrained.html#solution-regularization",
    "title": "Regularization and Under-Constrained Problems",
    "section": "Solution: Regularization",
    "text": "Solution: Regularization\nThe key insight is to add a small positive constant \\(\\alpha\\) to the diagonal of \\(X^T X\\):\n\\[\nw = (X^T X + \\alpha I_n)^{-1} X^T y\n\\]\nwhere: - \\(\\alpha &gt; 0\\) is the regularization parameter - \\(I_n\\) is the \\(n \\times n\\) identity matrix\nEffect: \\(X^T X + \\alpha I_n\\) is always invertible for \\(\\alpha &gt; 0\\), even when \\(X^T X\\) is singular.\n\n\n\n\n\n\nTipWhy This Works\n\n\n\nAdding \\(\\alpha I_n\\) increases all eigenvalues of \\(X^T X\\) by \\(\\alpha\\). Since \\(\\alpha &gt; 0\\), no eigenvalue can be zero, ensuring invertibility.\n\n\n\nTwo Variants: Ridge vs Pseudo-Inverse\nThis approach gives us two related solutions depending on the value of \\(\\alpha\\):\n1. Ridge Regression (L2 Regularization): Use \\(\\alpha &gt; 0\\) in practice. This is the common approach where you choose a small positive regularization parameter.\n2. Moore-Penrose Pseudo-Inverse: The limiting case as \\(\\alpha \\to 0\\):\n\\[\nX^+ = \\lim_{\\alpha \\to 0} (X^T X + \\alpha I_n)^{-1} X^T\n\\]\nThis is the pseudo-inverse of \\(X\\), which exists even when \\(X^T X\\) is singular.\nProperties:\n\nWhen \\(X^T X\\) is invertible: \\(X^+ = (X^T X)^{-1} X^T\\) (standard inverse)\nWhen \\(X^T X\\) is singular: \\(X^+\\) provides the minimum-norm solution"
  },
  {
    "objectID": "ML/regularization-underconstrained.html#intuitive-derivation-from-scalar-to-matrix",
    "href": "ML/regularization-underconstrained.html#intuitive-derivation-from-scalar-to-matrix",
    "title": "Regularization and Under-Constrained Problems",
    "section": "Intuitive Derivation: From Scalar to Matrix",
    "text": "Intuitive Derivation: From Scalar to Matrix\nUnderstanding the matrix form becomes easier if we start from a simple scalar case.\n\nScalar Case\nFor a simple quadratic loss \\((ax - c)^2\\) where \\(a\\) is input, \\(x\\) is weight, and \\(c\\) is target:\nExpand:\n\\[\n\\text{Loss} = a^2 x^2 - 2acx + c^2\n\\]\nDerivative:\n\\[\n\\frac{\\partial \\text{Loss}}{\\partial x} = 2a^2 x - 2ac\n\\]\nSet to zero:\n\\[\n2a^2 x = 2ac \\quad \\Rightarrow \\quad x = \\frac{c}{a}\n\\]\n\n\nExtension to Matrix Form\nBy analogy, replace:\n\n\\(a^2 \\to X^T X\\) (scalar squared becomes matrix product)\n\\(x \\to w\\) (scalar weight becomes weight vector)\n\\(ac \\to X^T y\\) (scalar product becomes matrix-vector product)\n\nThis gives:\n\\[\n\\nabla_w J(w) = 2X^T Xw - 2X^T y\n\\]\n\n\n\n\n\n\nImportantKey Insight\n\n\n\nThe matrix derivative follows the same pattern as the scalar derivative, with appropriate transpose operations to maintain dimensional consistency."
  },
  {
    "objectID": "ML/regularization-underconstrained.html#summary",
    "href": "ML/regularization-underconstrained.html#summary",
    "title": "Regularization and Under-Constrained Problems",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\nScenario\n\\(X^T X\\) Status\nSolution Method\n\n\n\n\n\\(m \\geq n\\), full rank\nInvertible\n\\(w = (X^T X)^{-1} X^T y\\)\n\n\n\\(m &lt; n\\) (under-constrained)\nNot invertible\n\\(w = (X^T X + \\alpha I_n)^{-1} X^T y\\)\n\n\nRank deficient\nNot invertible\nUse pseudo-inverse \\(X^+\\)\n\n\n\nKey principle: Regularization \\(X^T X + \\alpha I_n\\) ensures invertibility by adding a small positive value to all eigenvalues of \\(X^T X\\), making the system solvable even in under-constrained scenarios."
  },
  {
    "objectID": "ML/regularization-underconstrained.html#related-posts",
    "href": "ML/regularization-underconstrained.html#related-posts",
    "title": "Regularization and Under-Constrained Problems",
    "section": "Related Posts",
    "text": "Related Posts\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots - Exploring how invertibility connects to fundamental linear algebra concepts\n\n\nSource: Deep Learning Book, Chapter 7.3"
  },
  {
    "objectID": "ML/dropout.html",
    "href": "ML/dropout.html",
    "title": "Chapter 7.12: Dropout",
    "section": "",
    "text": "When training a very large neural network, it is often impractical to train and average multiple models because the computational cost is too high."
  },
  {
    "objectID": "ML/dropout.html#limitation-of-bagging",
    "href": "ML/dropout.html#limitation-of-bagging",
    "title": "Chapter 7.12: Dropout",
    "section": "",
    "text": "When training a very large neural network, it is often impractical to train and average multiple models because the computational cost is too high."
  },
  {
    "objectID": "ML/dropout.html#dropout",
    "href": "ML/dropout.html#dropout",
    "title": "Chapter 7.12: Dropout",
    "section": "Dropout",
    "text": "Dropout\nDropout is a computationally efficient alternative to bagging that trains an ensemble of subnetworks by randomly dropping units during training.\nIf we have \\(n\\) droppable units, each of them can be either kept or dropped independently, we have \\(2^n\\) subnetworks.\n\\[\n2\\times2\\times2\\text{...}\\times2=2^n\n\\]\n\n\n\nDropout Training"
  },
  {
    "objectID": "ML/dropout.html#comparing-with-bagging",
    "href": "ML/dropout.html#comparing-with-bagging",
    "title": "Chapter 7.12: Dropout",
    "section": "Comparing with Bagging",
    "text": "Comparing with Bagging\nAssume our task is to output the probability.\nBagging (Equation 7.52): Averages predictions from \\(k\\) independently trained models.\n\\[\n\\frac{1}{k}\\sum_{i=1}^kp^{(i)}(y|x)\n\\]\nDropout (Equation 7.53): Takes a weighted sum over all possible mask configurations, where \\(p(\\mu)\\) is the probability to sample mask \\(\\mu\\).\n\\[\n\\sum_\\mu p(\\mu)p(y|x,\\mu)\n\\]"
  },
  {
    "objectID": "ML/dropout.html#masks",
    "href": "ML/dropout.html#masks",
    "title": "Chapter 7.12: Dropout",
    "section": "Masks",
    "text": "Masks\nWe use a vector to represent masks for each unit:\n\\[\n\\mu=[\\mu_1,\\mu_2,...\\mu_n]\n\\]\nDuring training, we sample masks:\n\\[\n\\begin{aligned}\nh &= [h_1,h_2,...,h_n] \\\\\nh' &= h\\odot\\mu\n\\end{aligned}\n\\]\nFor example, if \\(\\mu=[1,0,1]\\), then \\(h'=[h_1,0,h_3]\\)"
  },
  {
    "objectID": "ML/dropout.html#feasibility-of-simple-forward-propagation-in-dropout-inference",
    "href": "ML/dropout.html#feasibility-of-simple-forward-propagation-in-dropout-inference",
    "title": "Chapter 7.12: Dropout",
    "section": "Feasibility of Simple Forward Propagation in Dropout Inference",
    "text": "Feasibility of Simple Forward Propagation in Dropout Inference\nUsing geometric average (Equation 7.54):\n\\[\n\\tilde{p}_{\\text{ensemble}}(y|x)=\\sqrt[2^d]{\\prod_{\\mu}p(y|x,\\mu)}\n\\]\nNormalization (Equation 7.55): Assuming the distribution is uniform:\n\\[\np_{\\text{ensemble}}(y|x)=\\frac{\\tilde{p}(y|x)}{\\sum_{y'}\\tilde{p}(y'|x)}\n\\]\n\nDeriving the Weight Scaling Rule\nFor model families without non-linear hidden units, we can derive an exact solution.\nStandard softmax output (Equation 7.56):\n\\[\nP(y=y|v)=\\text{softmax}(W^\\top v+b)_y\n\\]\nWith dropout mask (Equation 7.57):\n\\[\nP(y=y|\\mathbf{v};\\mathbf{d})=\\text{softmax}(\\mathbf{W}^\\top (\\mathbf{d}\\odot\\mathbf{v})+b)_y\n\\]\nEnsemble prediction (Equation 7.58):\n\\[\nP_{\\text{ensemble}}(y=y|\\mathbf{v})=\\frac{\\tilde{P}_{\\text{ensemble}}(y=y|\\mathbf{v})}{\\sum_{y'}\\tilde{P}_{\\text{ensemble}}(y=y'|\\mathbf{v})}\n\\]\nGeometric average (Equation 7.59):\n\\[\n\\tilde{P}_{\\text{ensemble}}(y=y\\mid\\mathbf{v})=\\sqrt[2^n]{\\prod_{\\mathbf{d}\\in\\{0,1\\}^n}P(y=y\\mid \\mathbf{v};\\mathbf{d})}\n\\]\nExpanding the softmax (Equation 7.62):\n\\[\n\\tilde{P}_{\\text{ensemble}}(y|\\mathbf{v}) = \\sqrt[2^n]{\\prod_{\\mathbf{d} \\in \\{0,1\\}^n} \\frac{\\exp(\\mathbf{W}_y^\\top(\\mathbf{d} \\odot\\mathbf{v})+b_y)}{\\sum_{y'}\\exp(\\mathbf{W}_{y'}^\\top(\\mathbf{d}\\odot\\mathbf{v})+b_{y'})}}\n\\]\nSeparating numerator and denominator (Equation 7.63):\n\\[\n\\tilde{P}_{\\text{ensemble}}(y \\mid \\mathbf{v}) = \\frac{\\sqrt[2^n]{\\displaystyle \\prod_{\\mathbf{d} \\in \\{0,1\\}^n} \\exp\\big(\\mathbf{W}_{y}^{\\top}(\\mathbf{d}\\odot \\mathbf{v}) + b_y\\big)}}{\\sqrt[2^n]{\\displaystyle \\prod_{\\mathbf{d} \\in \\{0,1\\}^n} \\sum_{y'} \\exp\\big(\\mathbf{W}_{y'}^{\\top}(\\mathbf{d}\\odot \\mathbf{v}) + b_{y'}\\big)}}\n\\]\nKey properties of exponentials:\n\\[\n\\begin{aligned}\n\\prod_i e^{a_i} &= e^{\\sum_i a_i} \\\\\n\\sqrt[k]{\\prod_i^k x_i} &= \\exp\\left(\\frac{1}{k}\\sum_i\\log x_i\\right)\n\\end{aligned}\n\\]\nSince the denominator is constant with respect to \\(y\\) (Equation 7.64):\n\\[\n\\tilde{P}_{\\text{ensemble}}(y=y|v) \\propto \\sqrt[2^n]{\\prod_{\\mathbf{d} \\in \\{0,1\\}^n} \\exp(\\mathbf{W}_y^\\top(\\mathbf{d}\\odot v)+b_y)}\n\\]\nApplying the geometric average property (Equation 7.65):\n\\[\n= \\exp\\left(\\frac{1}{2^n}\\sum_{\\mathbf{d} \\in \\{0,1\\}^n} (\\mathbf{W}^\\top(\\mathbf{d}\\odot v)+b_y)\\right)\n\\]\nFinal result (Equation 7.66):\n\\[\n= \\exp\\left(\\frac{1}{2}\\mathbf{W}^\\top v+b_y\\right)\n\\]\nThis shows that at inference, we can simply scale weights by the keep probability (e.g., 0.5) instead of sampling multiple masks.\nIntuition: Each unit has probability 0.5 of being active, so the expected input is \\(0.5 \\times v\\). Therefore, multiplying by 0.5 at inference approximates the ensemble average."
  },
  {
    "objectID": "ML/dropout.html#computational-efficiency-of-dropout",
    "href": "ML/dropout.html#computational-efficiency-of-dropout",
    "title": "Chapter 7.12: Dropout",
    "section": "Computational Efficiency of Dropout",
    "text": "Computational Efficiency of Dropout\nDropout acts as an implicit ensemble, where all subnetworks share the same parameters within one network.\nDuring training: Each unit has a probability (e.g., 0.5) of being active, so all \\(2^n\\) subnetworks are trained efficiently within a single forward/backward pass.\nDuring inference: Only one forward pass is required ‚Äî we simply multiply the activations (or equivalently the weights) by the keep probability (e.g., 0.5).\nAlternative approach: Gal and Ghahramani (2015) found that some models can achieve better classification accuracy by using Monte Carlo approximation with around 20 dropout samples. The optimal number of samples for inference approximation appears to be problem-dependent.\nDropout outperforms traditional low-cost regularization methods (e.g., weight decay, norm or sparsity constraints) and can be combined with them for additional gains."
  },
  {
    "objectID": "ML/dropout.html#limitations-of-dropout",
    "href": "ML/dropout.html#limitations-of-dropout",
    "title": "Chapter 7.12: Dropout",
    "section": "Limitations of Dropout",
    "text": "Limitations of Dropout\n\nRequires a sufficiently large model capacity\nDropout is most effective when the network has enough parameters to compensate for the random removal of units. Small models may underfit when dropout is applied.\nMay be less effective with small training datasets\nWhen the dataset is small, the stochastic noise introduced by dropout can overwhelm the learning signal, leading to unstable training or degraded performance."
  },
  {
    "objectID": "ML/dropout.html#intuition-and-insights-behind-dropout",
    "href": "ML/dropout.html#intuition-and-insights-behind-dropout",
    "title": "Chapter 7.12: Dropout",
    "section": "Intuition and Insights Behind Dropout",
    "text": "Intuition and Insights Behind Dropout\nDropout forces each unit to perform well independently, without relying on the presence of specific other units. This encourages the network to learn redundant yet complementary representations, so that every subnetwork formed during dropout can perform reasonably well. As a result, combining many of these ‚Äúgood-enough‚Äù subnetworks produces a more powerful ensemble.\n\nBiological inspiration: Hinton proposed that dropout resembles the process of gene exchange between organisms. Evolutionary pressure not only rewards strong genes but also favors genes that remain effective after recombination. Similarly, dropout encourages units to learn features that are robust to co-adaptation and can function well under many combinations.\n\nAdaptive destruction: By randomly ‚Äúcorrupting‚Äù its own input during training, dropout teaches the network to adapt to noise and missing information. This adaptive destruction mechanism leads to features that are more stable and robust to input perturbations and unseen conditions.\n\nSource: Deep Learning (Ian Goodfellow, Yoshua Bengio, Aaron Courville), Chapter 7.12"
  },
  {
    "objectID": "ML/index.html",
    "href": "ML/index.html",
    "title": "Machine Learning Topics",
    "section": "",
    "text": "Understanding Axis(Dim) Operations"
  },
  {
    "objectID": "ML/index.html#numpy-fundamentals",
    "href": "ML/index.html#numpy-fundamentals",
    "title": "Machine Learning Topics",
    "section": "",
    "text": "Understanding Axis(Dim) Operations"
  },
  {
    "objectID": "ML/index.html#clustering-algorithms",
    "href": "ML/index.html#clustering-algorithms",
    "title": "Machine Learning Topics",
    "section": "Clustering Algorithms",
    "text": "Clustering Algorithms\n\nK-Means Clustering"
  },
  {
    "objectID": "ML/index.html#deep-learning-fundamentals",
    "href": "ML/index.html#deep-learning-fundamentals",
    "title": "Machine Learning Topics",
    "section": "Deep Learning Fundamentals",
    "text": "Deep Learning Fundamentals\n\nThe XOR Problem: Nonlinearity in Deep Learning\nLikelihood-Based Loss Functions\nHidden Units and Activation Functions\nArchitecture Design: Depth vs Width\nBack-Propagation and Other Differentiation Algorithms\nChapter 7.11: Bagging and Other Ensemble Methods\nChapter 7.12: Dropout\nChapter 7.13: Adversarial Training\nChapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier\nChapter 8.1: How Learning Differs from Pure Optimization"
  },
  {
    "objectID": "ML/index.html#classification-algorithms",
    "href": "ML/index.html#classification-algorithms",
    "title": "Machine Learning Topics",
    "section": "Classification Algorithms",
    "text": "Classification Algorithms\n\nLogistic Regression"
  },
  {
    "objectID": "ML/cnn-structured-outputs.html",
    "href": "ML/cnn-structured-outputs.html",
    "title": "Chapter 9.6: Structured Outputs",
    "section": "",
    "text": "CNNs can generate high-dimensional structured objects, enabling pixel-level predictions for tasks like segmentation, depth estimation, and flow prediction."
  },
  {
    "objectID": "ML/cnn-structured-outputs.html#overview",
    "href": "ML/cnn-structured-outputs.html#overview",
    "title": "Chapter 9.6: Structured Outputs",
    "section": "",
    "text": "CNNs can generate high-dimensional structured objects, enabling pixel-level predictions for tasks like segmentation, depth estimation, and flow prediction."
  },
  {
    "objectID": "ML/cnn-structured-outputs.html#preserving-spatial-dimensions",
    "href": "ML/cnn-structured-outputs.html#preserving-spatial-dimensions",
    "title": "Chapter 9.6: Structured Outputs",
    "section": "Preserving Spatial Dimensions",
    "text": "Preserving Spatial Dimensions\nTo generate pixel-level (full-resolution) outputs, the convolutions must preserve spatial dimensions. This means:\n\nNo pooling layers\nNo stride &gt; 1\nConvolution uses SAME padding (e.g., padding=1 for 3√ó3 kernels)\n\nBy maintaining spatial resolution throughout the network, we can produce outputs that match the input dimensions pixel-for-pixel."
  },
  {
    "objectID": "ML/cnn-structured-outputs.html#recurrent-convolution",
    "href": "ML/cnn-structured-outputs.html#recurrent-convolution",
    "title": "Chapter 9.6: Structured Outputs",
    "section": "Recurrent Convolution",
    "text": "Recurrent Convolution\nRecurrent convolution repeatedly refines pixel-level predictions by applying the same convolutional transform across time, combining high-resolution input with the previous hidden state to produce increasingly accurate structured outputs.\n\nThe Process\nThe recurrent convolution follows this pattern:\nStep 1: \\[U * X = H(1), \\quad H(1) * V = \\hat{Y}(1)\\]\nStep 2: \\[U * X + H(1) * W = H(2), \\quad H(2) * V = \\hat{Y}(2)\\]\nStep 3: \\[U * X + H(2) * W = H(3), \\quad H(3) * V = \\hat{Y}(3)\\]\nWhere: - \\(U\\): Input convolution kernel - \\(X\\): Input image - \\(H(t)\\): Hidden state at time \\(t\\) - \\(W\\): Recurrent kernel (processes previous hidden state) - \\(V\\): Output convolution kernel - \\(\\hat{Y}(t)\\): Predicted output at time \\(t\\)\n Figure: Recurrent convolution repeatedly refines predictions by combining the input with previous hidden states through convolutional operations."
  },
  {
    "objectID": "ML/cnn-structured-outputs.html#applications-dense-pixel-level-predictions",
    "href": "ML/cnn-structured-outputs.html#applications-dense-pixel-level-predictions",
    "title": "Chapter 9.6: Structured Outputs",
    "section": "Applications: Dense Pixel-Level Predictions",
    "text": "Applications: Dense Pixel-Level Predictions\nOnce a model can produce structured outputs, we can design networks that generate full spatial maps‚Äîpredicting an entire image-like object rather than a single scalar.\nThis enables tasks such as:\n\nSemantic Segmentation: Classifying every pixel into object categories\nDepth Estimation: Predicting distance from camera for each pixel\nOptical Flow Prediction: Estimating motion vectors between frames\nDense Correspondence: Finding pixel-level matches across images\n\n Figure: Examples of structured output tasks where CNNs generate full spatial maps with pixel-level predictions, moving beyond single scalar outputs to dense, coherent predictions."
  },
  {
    "objectID": "ML/cnn-structured-outputs.html#key-insight",
    "href": "ML/cnn-structured-outputs.html#key-insight",
    "title": "Chapter 9.6: Structured Outputs",
    "section": "Key Insight",
    "text": "Key Insight\nThe power of structured outputs lies in generating spatially coherent predictions. Rather than treating each output pixel independently, recurrent convolution allows information to flow across the spatial map, ensuring that neighboring predictions are consistent and that the network can refine its outputs iteratively based on context from the entire image."
  },
  {
    "objectID": "ML/adversarial-training.html",
    "href": "ML/adversarial-training.html",
    "title": "MIT Deep Learning Chapter 7.13: Adversarial Training",
    "section": "",
    "text": "Regularization methods like dropout or noise injection aim to make models less sensitive to small input changes and improve generalization.\nYet, this does not guarantee real robustness.\nEven tiny, imperceptible perturbations can completely change a model‚Äôs prediction while keeping its confidence extremely high."
  },
  {
    "objectID": "ML/adversarial-training.html#the-robustness-problem",
    "href": "ML/adversarial-training.html#the-robustness-problem",
    "title": "MIT Deep Learning Chapter 7.13: Adversarial Training",
    "section": "",
    "text": "Regularization methods like dropout or noise injection aim to make models less sensitive to small input changes and improve generalization.\nYet, this does not guarantee real robustness.\nEven tiny, imperceptible perturbations can completely change a model‚Äôs prediction while keeping its confidence extremely high."
  },
  {
    "objectID": "ML/adversarial-training.html#adversarial-example",
    "href": "ML/adversarial-training.html#adversarial-example",
    "title": "MIT Deep Learning Chapter 7.13: Adversarial Training",
    "section": "Adversarial Example",
    "text": "Adversarial Example\n\n\n\n\n\n\n\n\n\n\nImage\nMathematical Expression\nModel Prediction\nConfidence\nInterpretation\n\n\n\n\nOriginal image\n\\(x\\)\n‚Äúpanda‚Äù\n57.7%\nCorrect classification.\n\n\nAdversarial noise (imperceptible)\n\\(0.007 \\times \\operatorname{sign}(\\nabla_x J(\\theta, x, y))\\)\n‚Äúnematode‚Äù\n8.2%\nDirection of perturbation; visually meaningless.\n\n\nOriginal + Noise (adversarial example)\n\\(x + 0.007 \\times \\operatorname{sign}(\\nabla_x J(\\theta, x, y))\\)\n‚Äúgibbon‚Äù\n99.3%\nThe model is completely fooled ‚Äî confidently wrong.\n\n\n\n\nFigure 7.8 ‚Äì Adversarial Example (Goodfellow et al., 2014b): A panda image classified correctly by GoogLeNet (57.7%) becomes misclassified as a gibbon (99.3%) after adding an imperceptible perturbation \\(\\epsilon = 0.007\\) in the gradient direction. Demonstrates how a small, structured noise can completely fool a high-performing neural network."
  },
  {
    "objectID": "ML/adversarial-training.html#adversarial-training",
    "href": "ML/adversarial-training.html#adversarial-training",
    "title": "MIT Deep Learning Chapter 7.13: Adversarial Training",
    "section": "Adversarial Training",
    "text": "Adversarial Training\n\nMotivation and Regularization Perspective\nOriginally proposed as a defense method, adversarial training also serves as an effective form of regularization.\nTraining on adversarially perturbed samples reduces test error on i.i.d. datasets (Szegedy et al., 2014b; Goodfellow et al., 2014b).\n\n\nKey Insight‚ÄîThe Linearity of Neural Networks\nGoodfellow et al.¬†(2014b) observed that adversarial examples arise mainly from the high linearity of modern neural networks in high-dimensional space.\nEven tiny perturbations on each input dimension accumulate into large linear responses \\(w^\\top \\epsilon\\).\nSmall pixel-level changes can cause drastic prediction shifts.\nIn short: deep nets are ‚Äútoo linear‚Äù in very high dimensions.\n\n\nMechanism‚ÄîHow Adversarial Training Works\nAdversarial training penalizes the model for being overly sensitive to local perturbations in the input space.\nIt enforces local smoothness in the learned mapping \\(f(x)\\).\nThis encourages the desirable property: ‚ÄúSimilar inputs should lead to similar outputs.‚Äù\n\n\nMathematical Connection‚ÄîRegularization by Smoothing\nAdversarial training effectively adds a smoothness term to the loss function, constraining the magnitude of input gradients.\nThis reduces local curvature in the input‚Äìoutput mapping.\nThe idea aligns with traditional regularization methods like weight decay or noise injection‚Äîbut applied directly in input space rather than parameter space."
  },
  {
    "objectID": "ML/adversarial-training.html#real-world-applications",
    "href": "ML/adversarial-training.html#real-world-applications",
    "title": "MIT Deep Learning Chapter 7.13: Adversarial Training",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nGenerated by ChatGPT\n\nComputer Vision\n\n\n\n\n\n\n\n\n\nScenario\nReal-world Risk\nHow Adversarial Training Helps\nExamples\n\n\n\n\nAutonomous Driving\nAdversarial stickers on traffic signs can cause misclassification\nTrain perception networks with adversarially perturbed images\nTesla, Waymo, Baidu Apollo\n\n\nImage Classification / Detection\nSlight pixel noise or compression artifacts can flip labels\nFGSM / PGD-based adversarial augmentation improves robustness\nGoogle Brain (ImageNet robustness experiments)\n\n\nFace Recognition Security\nGlasses or patches can fool face-ID systems\nAdds physically realizable perturbations during training\nFace++, SenseTime, Apple FaceID R&D\n\n\n\n\n\nFinance and Security\n\n\n\n\n\n\n\n\n\nScenario\nRisk\nRole of Adversarial Training\nIndustry Examples\n\n\n\n\nFraud Detection\nAttackers subtly alter transaction features to evade detection\nLearn robust boundaries for borderline transactions\nPayPal, Square\n\n\nCredit Scoring Models\nSynthetic feature manipulation fools scoring algorithms\nImproves resilience to adversarial tabular data\nICLR 2021/2022 ‚ÄúAdversarial Robustness for Tabular Data‚Äù\n\n\nIntrusion Detection (IDS)\nMalicious traffic mimics normal patterns\nImproves anomaly detection robustness\nUsed in cybersecurity monitoring pipelines\n\n\n\n\n\nMedical\n\n\n\n\n\n\n\n\n\nScenario\nChallenge\nBenefit of Adversarial Training\nResearch/Industry\n\n\n\n\nTumor Detection, CT Segmentation\nNoise or scanner differences change predictions\nImproves robustness across hospitals/devices\nMICCAI 2020: Adversarial Training for Robust Medical Image Segmentation\n\n\nHistopathology & Genomic Classification\nDistribution shift between labs\nCombines domain adaptation + adversarial robustness\nPathAI, DeepMind Health\n\n\n\n\n\nNatural Language Processing\n\n\n\n\n\n\n\n\n\nScenario\nPerturbation Type\nRobustness Goal\nExamples\n\n\n\n\nBERT / GPT models\nSmall embedding or token perturbations\nStay stable under paraphrasing or synonym replacement\nText Adversarial Training (Jin et al., 2020)\n\n\nSentiment & QA Models\nSpelling errors, emojis, paraphrases\nMaintain consistent predictions\nAdversarial Training for BERT (Zhu et al., 2020)\n\n\nSpeech Recognition (ASR)\nBackground noise or adversarial audio\nIncrease noise tolerance\nAmazon Alexa, Apple Siri robustness training\n\n\n\n\nSource: Deep Learning (Ian Goodfellow, Yoshua Bengio, Aaron Courville) - Chapter 7.13"
  },
  {
    "objectID": "ML/l1-regularization.html",
    "href": "ML/l1-regularization.html",
    "title": "Deep Learning Book Chapter 7.1.2: L1 Regularization",
    "section": "",
    "text": "My lecture notes\nL1 regularization penalizes the absolute values of weights, creating sparse solutions where many weights become exactly zero. This post derives the analytical solution and shows how soft thresholding leads to feature selection."
  },
  {
    "objectID": "ML/l1-regularization.html#context",
    "href": "ML/l1-regularization.html#context",
    "title": "Deep Learning Book Chapter 7.1.2: L1 Regularization",
    "section": "",
    "text": "My lecture notes\nL1 regularization penalizes the absolute values of weights, creating sparse solutions where many weights become exactly zero. This post derives the analytical solution and shows how soft thresholding leads to feature selection."
  },
  {
    "objectID": "ML/l1-regularization.html#definition",
    "href": "ML/l1-regularization.html#definition",
    "title": "Deep Learning Book Chapter 7.1.2: L1 Regularization",
    "section": "Definition",
    "text": "Definition\nFor L1 regularization, the penalty term is defined as:\nFormula 7.18: \\[\n\\Omega(\\theta) = ||w||_1 = \\sum_i |w_i|\n\\]\nUsing a regularization parameter \\(\\alpha\\), our total loss becomes:\nFormula 7.19: \\[\n\\tilde{J}(w;X,y) = \\alpha||w||_1 + J(w;X,y)\n\\]"
  },
  {
    "objectID": "ML/l1-regularization.html#gradient-calculation",
    "href": "ML/l1-regularization.html#gradient-calculation",
    "title": "Deep Learning Book Chapter 7.1.2: L1 Regularization",
    "section": "Gradient Calculation",
    "text": "Gradient Calculation\nFor the absolute value function \\(f = |w|\\), the derivative (subgradient) is: \\[\n\\frac{\\partial f}{\\partial w} = \\begin{cases}\n1, & w &gt; 0 \\\\\n-1, & w &lt; 0\n\\end{cases}\n\\]\nTherefore, the gradient of \\(\\alpha||w||_1\\) is \\(\\alpha \\text{sign}(w)\\), and we get:\nFormula 7.20: \\[\n\\nabla_w \\tilde{J}(w;X,y) = \\alpha \\text{sign}(w) + \\nabla_w J(w;X,y)\n\\]"
  },
  {
    "objectID": "ML/l1-regularization.html#analytical-solution",
    "href": "ML/l1-regularization.html#analytical-solution",
    "title": "Deep Learning Book Chapter 7.1.2: L1 Regularization",
    "section": "Analytical Solution",
    "text": "Analytical Solution\nFor the model, assuming the gradient of the unregularized loss can be approximated as:\nFormula 7.21: \\[\n\\nabla_w J(w;X,y) \\approx H(w - w^*)\n\\]\nwhere \\(w^*\\) is the optimal solution without regularization (where \\(\\nabla_w J(w^*;X,y) = 0\\)), and \\(H\\) is the Hessian matrix.\nUsing a quadratic approximation around \\(w^*\\), the regularized loss becomes:\nFormula 7.22: \\[\n\\hat{J}(w;X,y) = J(w^*;X,y) + \\sum_{i=1}^n \\left[\\frac{1}{2}H_{i,i}(w_i - w_i^*)^2 + \\alpha|w_i|\\right]\n\\]\nThe gradient with respect to \\(w_i\\) is: \\[\n\\frac{\\partial \\hat{J}}{\\partial w_i} = H_{i,i}(w_i - w_i^*) + \\alpha \\text{sign}(w_i)\n\\]\nSetting the gradient to zero: \\[\nH_{i,i}(w_i - w_i^*) + \\alpha \\text{sign}(w_i) = 0\n\\]\n\\[\nH_{i,i}w_i = H_{i,i}w_i^* - \\alpha \\text{sign}(w_i)\n\\]\nDividing both sides by \\(H_{i,i}\\): \\[\nw_i = w_i^* - \\frac{\\alpha}{H_{i,i}} \\text{sign}(w_i)\n\\]\nSince \\(\\alpha\\) and \\(H_{i,i}\\) are both positive:\n\nWhen \\(w_i^* &gt; 0\\): We expect \\(w_i &gt; 0\\), so \\(\\text{sign}(w_i) = +1\\): \\[\nw_i = w_i^* - \\frac{\\alpha}{H_{i,i}}\n\\]\nWhen \\(w_i^* &lt; 0\\): We expect \\(w_i &lt; 0\\), so \\(\\text{sign}(w_i) = -1\\): \\[\nw_i = w_i^* + \\frac{\\alpha}{H_{i,i}}\n\\]\nNote that for \\(w_i^* &lt; 0\\), we have \\(|w_i^*| = -w_i^*\\), so this can be written as: \\[\nw_i = -(|w_i^*| - \\frac{\\alpha}{H_{i,i}})\n\\]\n\nCombining both cases with the sign function:\nFormula 7.23: \\[\nw_i = \\text{sign}(w_i^*) \\max\\left(|w_i^*| - \\frac{\\alpha}{H_{i,i}}, 0\\right)\n\\]\nImportant note: The \\(\\max(\\cdot, 0)\\) prevents sign reversal. When \\(|w_i^*| &lt; \\frac{\\alpha}{H_{i,i}}\\), the regularization is strong enough to push \\(w_i\\) to exactly zero, rather than changing its sign. This is the soft thresholding operation."
  },
  {
    "objectID": "ML/l1-regularization.html#sparsity-property",
    "href": "ML/l1-regularization.html#sparsity-property",
    "title": "Deep Learning Book Chapter 7.1.2: L1 Regularization",
    "section": "Sparsity Property",
    "text": "Sparsity Property\nL1 regularization has a sparse solution property: it tends to push many weights to exactly zero, effectively performing feature selection. This is in contrast to L2 regularization, which shrinks weights toward zero but rarely sets them exactly to zero.\nThis sparsity arises from the soft thresholding effect shown in Formula 7.23, where weights smaller than the threshold \\(\\frac{\\alpha}{H_{i,i}}\\) are set to zero.\n\nSource: Deep Learning Book, Chapter 7.1.2"
  },
  {
    "objectID": "ML/optimization-strategies.html",
    "href": "ML/optimization-strategies.html",
    "title": "Chapter 8.7: Optimization Strategies and Meta-Algorithms",
    "section": "",
    "text": "This section covers advanced optimization strategies and meta-algorithms that enhance training efficiency and stability:\n\nBatch Normalization: Normalizing intermediate activations to stabilize training\nCoordinate Descent: Decomposing optimization into simpler sub-problems\nPolyak Averaging: Smoothing parameter trajectories for better convergence\nSupervised Pretraining: Greedy layer-wise initialization for deep networks\nContinuation Methods: Gradually transitioning from easy to hard optimization problems\n\nThese meta-algorithms provide higher-level strategies that can be combined with basic optimizers like SGD or Adam."
  },
  {
    "objectID": "ML/optimization-strategies.html#overview",
    "href": "ML/optimization-strategies.html#overview",
    "title": "Chapter 8.7: Optimization Strategies and Meta-Algorithms",
    "section": "",
    "text": "This section covers advanced optimization strategies and meta-algorithms that enhance training efficiency and stability:\n\nBatch Normalization: Normalizing intermediate activations to stabilize training\nCoordinate Descent: Decomposing optimization into simpler sub-problems\nPolyak Averaging: Smoothing parameter trajectories for better convergence\nSupervised Pretraining: Greedy layer-wise initialization for deep networks\nContinuation Methods: Gradually transitioning from easy to hard optimization problems\n\nThese meta-algorithms provide higher-level strategies that can be combined with basic optimizers like SGD or Adam."
  },
  {
    "objectID": "ML/optimization-strategies.html#batch-normalization",
    "href": "ML/optimization-strategies.html#batch-normalization",
    "title": "Chapter 8.7: Optimization Strategies and Meta-Algorithms",
    "section": "Batch Normalization",
    "text": "Batch Normalization\nBatch normalization normalizes the activations of each layer to have zero mean and unit variance across the mini-batch. This stabilizes training by reducing internal covariate shift and allows higher learning rates.\n\nNormalization Formula\nFor a layer‚Äôs activations \\(H\\), the normalized output is:\n\\[\nH' = \\frac{H - \\mu}{\\sigma} \\tag{8.35}\n\\]\nwhere:\n\n\\(\\mu\\): The batch mean\n\\[\n\\mu = \\frac{1}{m} \\sum_{i=1}^{m} H_i\n\\]\n\\(\\sigma\\): The batch standard deviation\n\\[\n\\sigma = \\sqrt{\\delta + \\frac{1}{m} \\sum_{i=1}^{m} (H_i - \\mu)^2}\n\\]\nThe small constant \\(\\delta\\) (typically \\(10^{-5}\\)) prevents division by zero.\n\n\n\nLearnable Parameters\nBatch normalization introduces two learnable parameters per feature:\n\n\\(\\gamma\\): The scale parameter that controls the variance\n\\(\\beta\\): The shift parameter that controls the mean\n\nThe final batch-normalized output is:\n\\[\n\\hat{H} = \\gamma \\cdot \\frac{H - \\mu}{\\sigma} + \\beta\n\\]\nWhy learnable parameters?\nThese parameters allow the network to undo the normalization if needed. For example: - Setting \\(\\gamma = \\sigma\\) and \\(\\beta = \\mu\\) recovers the original unnormalized activations - The network can learn the optimal mean and variance for each layer - This ensures normalization doesn‚Äôt limit the expressive power of the network\nBenefits: 1. Faster training: Reduces internal covariate shift, allowing higher learning rates 2. Better gradient flow: Prevents vanishing/exploding gradients in deep networks 3. Regularization effect: Mini-batch statistics introduce noise, acting as implicit regularization 4. Reduced sensitivity: Less dependence on careful weight initialization"
  },
  {
    "objectID": "ML/optimization-strategies.html#coordinate-descent",
    "href": "ML/optimization-strategies.html#coordinate-descent",
    "title": "Chapter 8.7: Optimization Strategies and Meta-Algorithms",
    "section": "Coordinate Descent",
    "text": "Coordinate Descent\nCoordinate descent optimizes a complex multi-variable function by breaking it into simpler single-variable problems.\n\nAlgorithm\nInstead of updating all parameters simultaneously, coordinate descent:\n\nFixes all variables except one (\\(x_i\\))\nMinimizes the objective function with respect to \\(x_i\\)\nCycles through all variables, repeating the process\n\nBlock Coordinate Descent: A generalization where we optimize a subset (block) of variables jointly at each step, rather than just one variable.\n\n\nWhen Coordinate Descent Works Well\nCoordinate descent is effective when:\n\nVariables are separable: The problem can be naturally partitioned into independent groups\nSub-problems are easier: Optimizing one subset is much simpler than the full problem\nClosed-form solutions exist: Each sub-problem can be solved analytically\n\n\n\nExample: Sparse Coding\n\\[\nJ(H, W) = \\sum_{i,j} |H_{i,j}| + \\sum_{i,j} (X - W^\\top H)_{i,j}^2 \\tag{8.38}\n\\]\nThis objective learns a sparse representation:\n\n\\(X\\): Input data (observations)\n\\(W\\): Dictionary matrix (basis vectors)\n\\(H\\): Sparse activation matrix (coefficients)\nFirst term: \\(\\ell_1\\) regularization encourages sparsity in \\(H\\)\nSecond term: Reconstruction error measures how well \\(W^\\top H\\) approximates \\(X\\)\n\nCoordinate descent strategy: 1. Fix \\(W\\), optimize \\(H\\): This becomes a LASSO problem with a closed-form solution 2. Fix \\(H\\), optimize \\(W\\): This becomes a least-squares problem, also solvable analytically\nThe sparsity constraint prevents ill-conditioning (e.g., arbitrarily large \\(W\\) with tiny \\(H\\) values).\n\n\nWhen Coordinate Descent Is Inefficient\nCoordinate descent can be slow when variables are strongly coupled, even though it still converges. For convex quadratic functions, the convergence rate depends on how elongated the level curves are.\nExample: For \\(f(x_1, x_2) = (x_1 - x_2)^2 + \\alpha(x_1^2 + x_2^2)\\) with optimal solution \\((0, 0)\\):\n\nPer-coordinate updates:\n\n\\(x_1 = \\frac{x_2}{1+\\alpha}\\) (minimizing over \\(x_1\\) while fixing \\(x_2\\))\n\\(x_2 = \\frac{x_1}{1+\\alpha}\\) (minimizing over \\(x_2\\) while fixing \\(x_1\\))\n\nEach full cycle multiplies both coordinates by \\(\\frac{1}{1+\\alpha}\\), so coordinate descent does converge to \\((0,0)\\), but the convergence is only linear (geometric).\n\nWhen coordinate descent truly fails:\n\nNon-smooth objectives: For \\(f(x_1, x_2) = |x_1 - x_2|\\), the derivative is undefined at the minimum, preventing coordinate descent from finding the exact solution.\nDegenerate problems: For \\(f(x_1, x_2) = x_1^2\\), the minimum forms a line (\\(x_1 = 0\\), any \\(x_2\\)). Coordinate descent on \\(x_2\\) makes no progress since \\(f\\) doesn‚Äôt depend on \\(x_2\\).\n\nKey insight: Coordinate descent works best when the problem is well-conditioned and separable. For strongly coupled or ill-conditioned problems, joint optimization methods (e.g., conjugate gradient, Newton‚Äôs method) converge much faster."
  },
  {
    "objectID": "ML/optimization-strategies.html#polyak-averaging",
    "href": "ML/optimization-strategies.html#polyak-averaging",
    "title": "Chapter 8.7: Optimization Strategies and Meta-Algorithms",
    "section": "Polyak Averaging",
    "text": "Polyak Averaging\nPolyak averaging smooths the optimization trajectory by maintaining an exponential moving average of parameter values.\n\nFormula\n\\[\n\\hat{\\theta}^{(t)} = \\alpha \\hat{\\theta}^{(t-1)} + (1 - \\alpha) \\theta^{(t)} \\tag{8.39}\n\\]\nwhere: - \\(\\theta^{(t)}\\): Current parameter values at iteration \\(t\\) - \\(\\hat{\\theta}^{(t)}\\): Averaged parameter values - \\(\\alpha\\): Momentum coefficient (typically 0.9 or 0.99)\n\n\nWhy Averaging Helps\nProblem: Stochastic gradient descent oscillates around the optimum due to gradient noise from mini-batches.\nSolution: The averaged parameters \\(\\hat{\\theta}\\) lie closer to the true minimum than any individual iterate \\(\\theta^{(t)}\\).\nGeometric interpretation: - SGD follows a noisy path that zigzags around the optimum - Polyak averaging smooths this path by averaging out the oscillations - The result is a more stable estimate of the optimal parameters\nPractical benefits: 1. Better generalization: Averaged parameters often generalize better than final iterates 2. Reduced variance: Smoothing removes high-frequency noise in the parameter trajectory 3. Free improvement: No additional hyperparameter tuning required\nImplementation note: In practice, start averaging after an initial burn-in period to avoid biasing the average with early, poorly-initialized parameters."
  },
  {
    "objectID": "ML/optimization-strategies.html#supervised-pretraining",
    "href": "ML/optimization-strategies.html#supervised-pretraining",
    "title": "Chapter 8.7: Optimization Strategies and Meta-Algorithms",
    "section": "Supervised Pretraining",
    "text": "Supervised Pretraining\nWhen optimizing a very deep or complex model is difficult, we can first train simpler versions and use them to initialize the full model.\n\nGreedy Layer-wise Pretraining\nStrategy: Train the network one layer at a time, where each layer learns a useful representation before adding the next layer.\nAlgorithm:\nflowchart LR\n    a[\"(a) x -&gt; h1 -&gt; y\"]\n    b[\"(b) x -&gt; h1 (frozen) -&gt; y_old, y_new\"]\n    c[\"(c) x -&gt; h1 (frozen) -&gt; h2 -&gt; y\"]\n    d[\"(d) x -&gt; h1 -&gt; h2 -&gt; y (fine-tune)\"]\n\n    a --&gt; b --&gt; c --&gt; d\nSteps:\n\n(a) Train a shallow network: \\(x \\to h_1 \\to y\\)\n(b) Freeze \\(h_1\\), add new output heads to verify it learned good features\n(c) Add second layer \\(h_2\\) on top of frozen \\(h_1\\), train new layer: \\(x \\to h_1 \\to h_2 \\to y\\)\n(d) Fine-tune: Jointly optimize all layers end-to-end\n\n\n\nWhy Pretraining Helps\n\nBetter initialization: Each layer starts from meaningful features, not random weights\nEasier optimization: Training one layer at a time is simpler than training all layers jointly\nBetter representations: Lower layers learn general features that help higher layers\nPrevents poor local minima: Greedy initialization guides the network toward better regions\n\nHistorical context: Pretraining was crucial for training deep networks before the advent of: - Batch normalization - Residual connections - Better initialization schemes (e.g., He initialization)\nModern usage: Still useful for: - Transfer learning: Pretrain on large dataset, fine-tune on small target task - Domain adaptation: Pretrain on source domain, adapt to target domain - Few-shot learning: Pretrain general features, quickly adapt to new tasks"
  },
  {
    "objectID": "ML/optimization-strategies.html#continuation-methods",
    "href": "ML/optimization-strategies.html#continuation-methods",
    "title": "Chapter 8.7: Optimization Strategies and Meta-Algorithms",
    "section": "Continuation Methods",
    "text": "Continuation Methods\nContinuation methods solve difficult optimization problems by first optimizing an easier, smoothed version of the objective, then gradually transforming it into the true objective.\n\nMathematical Formulation\n\\[\nJ^{(i)}(\\theta) = \\mathbb{E}_{\\theta' \\sim \\mathcal{N}(\\theta', \\theta, \\sigma_i^2)} [J(\\theta')] \\tag{8.40}\n\\]\nThis formula smooths the loss by:\n\nSampling nearby parameters: Draw \\(\\theta' \\sim \\mathcal{N}(\\theta, \\sigma_i^2)\\) from a Gaussian centered at \\(\\theta\\)\nAveraging their losses: Compute \\(\\mathbb{E}[J(\\theta')]\\) by taking the expectation over the Gaussian neighborhood\nGradually reducing \\(\\sigma_i\\): Start with large \\(\\sigma_0\\) (very smooth) and decrease toward \\(\\sigma_{\\text{final}} \\approx 0\\) (original loss)\n\nInterpretation: This is equivalent to convolving the loss landscape with a Gaussian kernel of variance \\(\\sigma_i^2\\).\n\n\nIntuition\nEarly stages (large \\(\\sigma\\)): - The smoothed objective \\(J^{(i)}(\\theta)\\) blurs out small local minima and sharp valleys - The loss landscape becomes easier to navigate - Gradient descent can find the basin of a good global minimum\nLater stages (small \\(\\sigma\\)): - As \\(\\sigma_i \\to 0\\), the smoothed objective converges to the true loss \\(J(\\theta)\\) - The optimization becomes more precise - Fine details and sharp features of the loss re-emerge\n\n\nBenefits\n\nEscapes poor local minima: Smoothing removes small, shallow minima\nReduces gradient noise: Averaging over nearby parameters stabilizes gradients\nMore stable convergence: Gradual transition prevents sudden jumps in the loss landscape\nBetter final solution: Guides optimization toward broader, more robust minima\n\n\n\nPractical Considerations\nComputational cost: Evaluating \\(\\mathbb{E}_{\\theta'}[J(\\theta')]\\) requires sampling multiple \\(\\theta'\\) values, making each step expensive.\nApproximations: - Finite samples: Approximate the expectation with a small number of Monte Carlo samples - Deterministic annealing: Use a predefined schedule for \\(\\sigma_i\\) that decreases over time - Adaptive scheduling: Adjust \\(\\sigma_i\\) based on convergence metrics (e.g., gradient norm)\nConnection to other methods: - Simulated annealing: Similar idea of starting with high ‚Äútemperature‚Äù (randomness) and cooling - Curriculum learning: Start with easier examples and gradually introduce harder ones - Gaussian smoothing in computer vision: Blurring images to remove noise"
  },
  {
    "objectID": "ML/optimization-strategies.html#summary",
    "href": "ML/optimization-strategies.html#summary",
    "title": "Chapter 8.7: Optimization Strategies and Meta-Algorithms",
    "section": "Summary",
    "text": "Summary\nThese meta-algorithms operate at a higher level than basic optimizers:\n\n\n\n\n\n\n\n\nStrategy\nKey Idea\nWhen to Use\n\n\n\n\nBatch Normalization\nNormalize activations to stabilize training\nDeep networks, high learning rates\n\n\nCoordinate Descent\nOptimize one variable/block at a time\nSeparable problems, closed-form sub-problems\n\n\nPolyak Averaging\nSmooth parameter trajectory via averaging\nNoisy optimization, better final models\n\n\nSupervised Pretraining\nTrain layers greedily, then fine-tune\nVery deep networks, transfer learning\n\n\nContinuation Methods\nStart with smoothed objective, gradually sharpen\nHighly non-convex losses, many local minima\n\n\n\nCombining strategies: These techniques are complementary and can be used together: - Batch normalization + Polyak averaging for stable training - Pretraining + continuation methods for difficult initializations - Coordinate descent + Polyak averaging for sparse coding problems\nThe choice of strategy depends on the problem structure, computational budget, and desired properties of the solution."
  },
  {
    "objectID": "ML/constrained-optimization-regularization.html",
    "href": "ML/constrained-optimization-regularization.html",
    "title": "Deep Learning Book Chapter 7.2: Constrained Optimization View of Regularization",
    "section": "",
    "text": "My lecture notes\nRegularization can be viewed as either adding a penalty term or enforcing a constraint. This post shows the equivalence between these two perspectives using Lagrange multipliers and dual optimization."
  },
  {
    "objectID": "ML/constrained-optimization-regularization.html#context",
    "href": "ML/constrained-optimization-regularization.html#context",
    "title": "Deep Learning Book Chapter 7.2: Constrained Optimization View of Regularization",
    "section": "",
    "text": "My lecture notes\nRegularization can be viewed as either adding a penalty term or enforcing a constraint. This post shows the equivalence between these two perspectives using Lagrange multipliers and dual optimization."
  },
  {
    "objectID": "ML/constrained-optimization-regularization.html#from-penalty-to-constraint",
    "href": "ML/constrained-optimization-regularization.html#from-penalty-to-constraint",
    "title": "Deep Learning Book Chapter 7.2: Constrained Optimization View of Regularization",
    "section": "From Penalty to Constraint",
    "text": "From Penalty to Constraint\nRegularization can be viewed from two equivalent perspectives:\n\nPenalty Form (Unconstrained)\nFormula 7.25: \\[\n\\tilde{J}(\\theta; X, y) = J(\\theta; X, y) + \\alpha \\Omega(\\theta)\n\\]\nThis adds a penalty term \\(\\alpha \\Omega(\\theta)\\) to the original loss, where \\(\\alpha\\) controls the strength of regularization.\n\n\nConstraint Form (Lagrangian)\nWe can equivalently express regularization as a constrained optimization problem requiring \\(\\Omega(\\theta) \\leq k\\).\nFormula 7.26 (Lagrangian): \\[\n\\mathcal{L}(\\theta, \\alpha; X, y) = J(\\theta; X, y) + \\alpha(\\Omega(\\theta) - k)\n\\]\nwhere \\(\\alpha \\geq 0\\) is the Lagrange multiplier.\nFormula 7.27 (Optimization Problem): \\[\n\\theta^* = \\arg\\min_{\\theta} \\max_{\\alpha \\geq 0} \\mathcal{L}(\\theta, \\alpha)\n\\]\nThis is equivalent to the constrained optimization: \\[\n\\theta^* = \\arg\\min_{\\theta} J(\\theta; X, y) \\quad \\text{subject to} \\quad \\Omega(\\theta) \\leq k\n\\]\nInterpretation: The solution corresponds to finding parameters \\(\\theta\\) that minimize the loss while satisfying the constraint. When \\(\\alpha\\) is large, it strongly enforces the constraint \\(\\Omega(\\theta) \\leq k\\)."
  },
  {
    "objectID": "ML/constrained-optimization-regularization.html#lagrange-multiplier-method",
    "href": "ML/constrained-optimization-regularization.html#lagrange-multiplier-method",
    "title": "Deep Learning Book Chapter 7.2: Constrained Optimization View of Regularization",
    "section": "Lagrange Multiplier Method",
    "text": "Lagrange Multiplier Method\nThe Lagrangian formulation transforms the problem into a min-max optimization: \\[\n\\min_{\\theta} \\max_{\\alpha \\geq 0} \\mathcal{L}(\\theta, \\alpha)\n\\]\n\nDual Direction Training\nInstead of a single gradient descent on \\(\\min \\mathcal{L}(\\theta)\\), we now have two opposing optimization directions:\n\\[\n\\begin{aligned}\n\\theta &\\downarrow \\quad \\text{(minimize w.r.t. } \\theta\\text{)} \\\\\n\\alpha &\\uparrow \\quad \\text{(maximize w.r.t. } \\alpha\\text{)}\n\\end{aligned}\n\\]\n\n\nTraining Dynamics\nThe training process balances two forces:\n\nWhen \\(\\Omega(\\theta) &gt; k\\) (constraint violated):\n\n\\(\\alpha\\) increases to penalize the violation\nLarger \\(\\alpha\\) pushes \\(\\theta\\) toward smaller norms\nThis enforces the constraint\n\nWhen \\(\\Omega(\\theta) &lt; k\\) (constraint satisfied):\n\n\\(\\alpha\\) decreases toward 0\nThe constraint is not active\nOptimization focuses on minimizing \\(J(\\theta)\\)\n\n\n\n\nSaddle Point Solution\nThe training converges to a saddle point satisfying the KKT (Karush-Kuhn-Tucker) conditions:\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\mathcal{L}(\\theta, \\alpha) &= 0 \\quad \\text{(stationarity)} \\\\\n\\alpha(\\Omega(\\theta) - k) &= 0 \\quad \\text{(complementary slackness)} \\\\\n\\alpha &\\geq 0 \\quad \\text{(dual feasibility)}\n\\end{aligned}\n\\]\nThe complementary slackness condition means: - Either \\(\\alpha = 0\\) (constraint inactive, \\(\\Omega(\\theta) &lt; k\\)) - Or \\(\\Omega(\\theta) = k\\) (constraint active, \\(\\alpha &gt; 0\\))"
  },
  {
    "objectID": "ML/constrained-optimization-regularization.html#penalty-vs-projection-methods",
    "href": "ML/constrained-optimization-regularization.html#penalty-vs-projection-methods",
    "title": "Deep Learning Book Chapter 7.2: Constrained Optimization View of Regularization",
    "section": "Penalty vs Projection Methods",
    "text": "Penalty vs Projection Methods\n\nWeight Norm Penalties (Soft Constraint)\nWhen using weight norm penalties such as L¬π or L¬≤ regularization: - The penalty term \\(\\alpha \\Omega(\\theta)\\) provides a ‚Äúsoft‚Äù constraint - The optimal solution may be locally optimal for the regularized objective \\(\\tilde{J}(\\theta)\\) - Even if increasing weights could reduce the original loss \\(J(\\theta)\\), the penalty prevents this - The regularization strength \\(\\alpha\\) determines how strictly the constraint is enforced\n\n\nExplicit Constraints (Hard Constraint)\nIn contrast, explicit constraint or projection methods enforce \\(\\Omega(\\theta) \\leq k\\) directly: - Provide a hard boundary on the weight norm - After each gradient step, project \\(\\theta\\) back onto the constraint set if needed - Often lead to more stable optimization with clearer geometric interpretation - The constraint is always exactly satisfied (not approximately)\n\n\nTrade-offs\n\n\n\nMethod\nConstraint Type\nStability\nFlexibility\n\n\n\n\nPenalty (L¬π/L¬≤)\nSoft\nGood\nHigh (tune \\(\\alpha\\))\n\n\nProjection\nHard\nVery Good\nLower (fixed \\(k\\))\n\n\n\nKey insight: Both approaches are equivalent in theory (there exists a correspondence between \\(\\alpha\\) and \\(k\\)), but in practice they may have different optimization properties and convergence behavior.\n\nSource: Deep Learning Book, Chapter 7.2"
  },
  {
    "objectID": "ML/backpropagation.html",
    "href": "ML/backpropagation.html",
    "title": "Deep Learning Book 6.5: Back-Propagation and Other Differentiation Algorithms",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.5 explores backpropagation‚Äîthe algorithm that makes training deep neural networks computationally feasible by efficiently computing gradients through the chain rule.\nüìì For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub.\nüöÄ Want to see backpropagation in action? Check out picograd‚Äîa minimal automatic differentiation engine I built (inspired by Andrej Karpathy‚Äôs micrograd) that implements reverse-mode autodiff from scratch. It demonstrates how computational graphs enable automatic gradient computation through backpropagation!"
  },
  {
    "objectID": "ML/backpropagation.html#the-gradient-problem-why-backpropagation",
    "href": "ML/backpropagation.html#the-gradient-problem-why-backpropagation",
    "title": "Deep Learning Book 6.5: Back-Propagation and Other Differentiation Algorithms",
    "section": "The Gradient Problem: Why Backpropagation?",
    "text": "The Gradient Problem: Why Backpropagation?\nTraining a neural network requires computing gradients of the loss function with respect to every parameter‚Äîpotentially millions of them. The naive approach of computing each gradient independently would be computationally intractable.\nBackpropagation solves this by exploiting the chain rule in a clever way: it reuses intermediate computations to calculate all gradients in a single backward pass through the network. This transforms an exponentially expensive problem into a linear one.\n\nQuick Reference: Understanding Backpropagation\nFor context on the mathematical foundations of backpropagation, see the Backpropagation summary.\nKey Concepts:\n\nChain Rule in Vector Form: For composite mapping \\(z = f(y), y = g(x)\\): \\[\\nabla_x z = \\left( \\frac{\\partial y}{\\partial x} \\right)^{\\top} \\nabla_y z\\]\nForward Pass: Cache all activations \\(h^{(l)} = f^{(l)}(W^{(l)} h^{(l-1)} + b^{(l)})\\)\nBackward Pass: Propagate gradients layer by layer: \\[\\nabla_{h^{(l-1)}} L = (W^{(l)})^{\\top} (\\nabla_{h^{(l)}} L \\odot f'^{(l)}(z^{(l)}))\\]\nParameter Gradients: \\[\\frac{\\partial L}{\\partial W^{(l)}} = (\\nabla_{h^{(l)}} L \\odot f'^{(l)}(z^{(l)})) (h^{(l-1)})^{\\top}\\]\n\n\n\n\n\n\n\n\n\nConcept\nDescription\nKey Insight\n\n\n\n\nComputational Graph\nDAG representing operations\nEnables reverse-mode automatic differentiation\n\n\nGradient Reuse\nShare intermediate computations\nReduces complexity from exponential to linear\n\n\nLocal Gradients\nEach operation computes local derivative\nChain rule combines them for global gradient"
  },
  {
    "objectID": "ML/backpropagation.html#exercise-1-understanding-jacobian-matrices",
    "href": "ML/backpropagation.html#exercise-1-understanding-jacobian-matrices",
    "title": "Deep Learning Book 6.5: Back-Propagation and Other Differentiation Algorithms",
    "section": "üî¨ Exercise 1: Understanding Jacobian Matrices",
    "text": "üî¨ Exercise 1: Understanding Jacobian Matrices\nHow do gradients flow through multiple layers? Let‚Äôs trace the chain rule through a 2-layer linear network.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\nprint(\"‚úì Setup complete\")\n\n\n‚úì Setup complete\n\n\n\nThe 2-Layer Network\nLayer 1 (Hidden layer): \\[\n\\begin{align}\nh_1 &= 2x_1 + x_2 \\\\\nh_2 &= x_1 + 3x_2 \\\\\nh_3 &= -x_1 + x_2\n\\end{align}\n\\]\nLayer 2 (Output layer): \\[\n\\begin{align}\ny_1 &= h_1 + 2h_2 - h_3 \\\\\ny_2 &= 3h_1 - h_2 + h_3\n\\end{align}\n\\]\n\n\nForward Pass Implementation\n\n\nShow code\ndef forward_pass(x: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute forward pass through the 2-layer network.\n\n    Args:\n        x: Input vector [x1, x2]\n\n    Returns:\n        h: Hidden layer output [h1, h2, h3]\n        y: Output layer [y1, y2]\n    \"\"\"\n    x1, x2 = x[0], x[1]\n\n    # Layer 1 (linear)\n    h1 = 2*x1 + x2\n    h2 = x1 + 3*x2\n    h3 = -x1 + x2\n    h = np.array([h1, h2, h3])\n\n    # Layer 2 (linear)\n    y1 = h1 + 2*h2 - h3\n    y2 = 3*h1 - h2 + h3\n    y = np.array([y1, y2])\n\n    return h, y\n\nprint(\"‚úì Forward pass defined\")\n\n\n‚úì Forward pass defined\n\n\n\n\nComputing Jacobian Matrices\nThe chain rule states: \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{h}} \\cdot \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{x}}\\)\n\n\nShow code\ndef compute_jacobian_h_x(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute ‚àÇh/‚àÇx analytically.\n\n    Args:\n        x: Input vector [x1, x2]\n\n    Returns:\n        Jacobian matrix (3x2)\n    \"\"\"\n    J = np.array([[2, 1], [1, 3], [-1, 1]])\n    return J\n\ndef compute_jacobian_y_h(x: np.ndarray, h: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute ‚àÇy/‚àÇh analytically.\n\n    Args:\n        x: Input vector [x1, x2]\n        h: Hidden layer [h1, h2, h3]\n\n    Returns:\n        Jacobian matrix (2x3)\n    \"\"\"\n    J = np.array([[1, 2, -1], [3, -1, 1]])\n    return J\n\ndef compute_jacobian_y_x(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute ‚àÇy/‚àÇx using chain rule.\n\n    Args:\n        x: Input vector [x1, x2]\n\n    Returns:\n        Jacobian matrix (2x2)\n    \"\"\"\n    return compute_jacobian_y_h(x, forward_pass(x)[0]) @ compute_jacobian_h_x(x)\n\nprint(\"‚úì Jacobian functions defined\")\n\n\n‚úì Jacobian functions defined\n\n\n\n\nVerification with Numerical Gradients\n\n\nShow code\ndef numerical_gradient(func, x, epsilon=1e-7):\n    \"\"\"\n    Compute numerical gradient using finite differences.\n    \"\"\"\n    grad = np.zeros((len(func(x)), len(x)))\n    for i in range(len(x)):\n        x_plus = x.copy()\n        x_minus = x.copy()\n        x_plus[i] += epsilon\n        x_minus[i] -= epsilon\n        grad[:, i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n    return grad\n\n# Test at point (1.0, 2.0)\nx_test = np.array([1.0, 2.0])\nh_test, y_test = forward_pass(x_test)\n\nprint(f\"Test point: x = {x_test}\")\nprint(f\"Hidden layer: h = {h_test}\")\nprint(f\"Output: y = {y_test}\")\nprint(\"\\n\" + \"=\"*50)\n\n# Compute analytical Jacobians\nJ_h_x_analytical = compute_jacobian_h_x(x_test)\nJ_y_h_analytical = compute_jacobian_y_h(x_test, h_test)\nJ_y_x_analytical = compute_jacobian_y_x(x_test)\n\n# Compute numerical Jacobians\nJ_h_x_numerical = numerical_gradient(lambda x: forward_pass(x)[0], x_test)\nJ_y_x_numerical = numerical_gradient(lambda x: forward_pass(x)[1], x_test)\n\nprint(\"\\n‚àÇh/‚àÇx (Analytical):\")\nprint(J_h_x_analytical)\nprint(\"\\n‚àÇh/‚àÇx (Numerical):\")\nprint(J_h_x_numerical)\nprint(f\"\\nDifference: {np.max(np.abs(J_h_x_analytical - J_h_x_numerical)):.2e}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"\\n‚àÇy/‚àÇh (Analytical):\")\nprint(J_y_h_analytical)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"\\n‚àÇy/‚àÇx (Analytical):\")\nprint(J_y_x_analytical)\nprint(\"\\n‚àÇy/‚àÇx (Numerical):\")\nprint(J_y_x_numerical)\nprint(f\"\\nDifference: {np.max(np.abs(J_y_x_analytical - J_y_x_numerical)):.2e}\")\n\n\nTest point: x = [1. 2.]\nHidden layer: h = [4. 7. 1.]\nOutput: y = [17.  6.]\n\n==================================================\n\n‚àÇh/‚àÇx (Analytical):\n[[ 2  1]\n [ 1  3]\n [-1  1]]\n\n‚àÇh/‚àÇx (Numerical):\n[[ 2.  1.]\n [ 1.  3.]\n [-1.  1.]]\n\nDifference: 2.80e-09\n\n==================================================\n\n‚àÇy/‚àÇh (Analytical):\n[[ 1  2 -1]\n [ 3 -1  1]]\n\n==================================================\n\n‚àÇy/‚àÇx (Analytical):\n[[5 6]\n [4 1]]\n\n‚àÇy/‚àÇx (Numerical):\n[[5.00000002 6.        ]\n [4.         0.99999999]]\n\nDifference: 2.29e-08"
  },
  {
    "objectID": "ML/backpropagation.html#exercise-2-gradient-descent-with-backpropagation",
    "href": "ML/backpropagation.html#exercise-2-gradient-descent-with-backpropagation",
    "title": "Deep Learning Book 6.5: Back-Propagation and Other Differentiation Algorithms",
    "section": "üî¨ Exercise 2: Gradient Descent with Backpropagation",
    "text": "üî¨ Exercise 2: Gradient Descent with Backpropagation\nLet‚Äôs implement a complete 3-layer network with backpropagation and watch how the parameters evolve during training.\n\nNetwork Architecture\nLayer 1: \\(h^{(1)}_i = \\tanh(w^{(1)}_{i1} x_1 + w^{(1)}_{i2} x_2 + b^{(1)}_i)\\)\nLayer 2: \\(h^{(2)}_i = \\tanh(w^{(2)}_{i1} a^{(1)}_1 + w^{(2)}_{i2} a^{(1)}_2 + b^{(2)}_i)\\)\nOutput: \\(\\hat{y} = w^{(3)}_1 a^{(2)}_1 + w^{(3)}_2 a^{(2)}_2 + b^{(3)}\\)\nLoss: \\(L = \\frac{1}{2}(\\hat{y} - y_{\\text{target}})^2\\)\n\n\nImplementation\n\n\nShow code\nclass ThreeLayerNetwork:\n    def __init__(self, seed=42):\n        \"\"\"\n        Initialize a 3-layer neural network with random weights.\n        \"\"\"\n        np.random.seed(seed)\n\n        # Layer 1: 2 inputs -&gt; 2 hidden units\n        self.W1 = np.random.randn(2, 2) * 0.5\n        self.b1 = np.random.randn(2) * 0.5\n\n        # Layer 2: 2 hidden -&gt; 2 hidden units\n        self.W2 = np.random.randn(2, 2) * 0.5\n        self.b2 = np.random.randn(2) * 0.5\n\n        # Layer 3: 2 hidden -&gt; 1 output\n        self.W3 = np.random.randn(2) * 0.5\n        self.b3 = np.random.randn() * 0.5\n\n        # For storing intermediate values during forward pass\n        self.cache = {}\n\n    def forward(self, x: np.ndarray) -&gt; float:\n        \"\"\"\n        Forward propagation through the network.\n\n        Args:\n            x: Input vector [x1, x2]\n\n        Returns:\n            y_hat: Predicted output (scalar)\n        \"\"\"\n        # Store input\n        self.cache['x'] = x\n\n        # Layer 1: h1 = W1 @ x + b1, a1 = tanh(h1)\n        self.cache['h1'] = self.W1 @ x + self.b1\n        self.cache['a1'] = np.tanh(self.cache['h1'])\n\n        # Layer 2: h2 = W2 @ a1 + b2, a2 = tanh(h2)\n        self.cache['h2'] = self.W2 @ self.cache['a1'] + self.b2\n        self.cache['a2'] = np.tanh(self.cache['h2'])\n\n        # Layer 3: y_hat = W3 @ a2 + b3 (linear output)\n        y_hat = self.W3 @ self.cache['a2'] + self.b3\n        self.cache['y_hat'] = y_hat\n\n        return y_hat\n\n    def backward(self, y_target: float, learning_rate: float) -&gt; dict:\n        \"\"\"\n        Backpropagation to compute gradients.\n\n        Args:\n            y_target: Target output (scalar)\n            learning_rate: Learning rate for gradient descent\n\n        Returns:\n            grads: Dictionary containing gradients for all parameters\n        \"\"\"\n        # Get cached values\n        x = self.cache['x']\n        a1 = self.cache['a1']\n        a2 = self.cache['a2']\n        y_hat = self.cache['y_hat']\n\n        # Output gradient: dL/dy_hat = y_hat - y_target\n        dL_dy = y_hat - y_target\n\n        # Layer 3 gradients\n        dL_dW3 = dL_dy * a2  # shape: (2,)\n        dL_db3 = dL_dy       # scalar\n        dL_da2 = dL_dy * self.W3  # shape: (2,)\n\n        # Layer 2 gradients\n        # tanh derivative: d(tanh(x))/dx = 1 - tanh(x)^2\n        dL_dh2 = dL_da2 * (1 - a2**2)  # shape: (2,)\n        dL_dW2 = np.outer(dL_dh2, a1)  # shape: (2, 2)\n        dL_db2 = dL_dh2                # shape: (2,)\n        dL_da1 = self.W2.T @ dL_dh2    # shape: (2,)\n\n        # Layer 1 gradients\n        dL_dh1 = dL_da1 * (1 - a1**2)  # shape: (2,)\n        dL_dW1 = np.outer(dL_dh1, x)   # shape: (2, 2)\n        dL_db1 = dL_dh1                # shape: (2,)\n\n        # Store gradients\n        grads = {\n            'dW3': dL_dW3,\n            'db3': dL_db3,\n            'dW2': dL_dW2,\n            'db2': dL_db2,\n            'dW1': dL_dW1,\n            'db1': dL_db1\n        }\n\n        # Update parameters\n        self.W3 -= learning_rate * dL_dW3\n        self.b3 -= learning_rate * dL_db3\n        self.W2 -= learning_rate * dL_dW2\n        self.b2 -= learning_rate * dL_db2\n        self.W1 -= learning_rate * dL_dW1\n        self.b1 -= learning_rate * dL_db1\n\n        return grads\n\n    def compute_loss(self, y_hat: float, y_target: float) -&gt; float:\n        \"\"\"\n        Compute MSE loss.\n        \"\"\"\n        return 0.5 * (y_hat - y_target)**2\n\n    def get_params(self) -&gt; dict:\n        \"\"\"\n        Get all parameters as a dictionary.\n        \"\"\"\n        return {\n            'W1': self.W1.copy(),\n            'b1': self.b1.copy(),\n            'W2': self.W2.copy(),\n            'b2': self.b2.copy(),\n            'W3': self.W3.copy(),\n            'b3': self.b3\n        }\n\nprint(\"‚úì Network class defined\")\n\n\n‚úì Network class defined\n\n\n\n\nTraining Loop\n\n\nShow code\n# Training configuration\nx_input = np.array([0.5, -0.3])\ny_target = 1.0\nlearning_rate = 0.01\nnum_iterations = 1000\n\n# Initialize network\nnetwork = ThreeLayerNetwork(seed=42)\n\n# Storage for logging\nloss_history = []\nparam_history = {\n    'W1': [],\n    'b1': [],\n    'W2': [],\n    'b2': [],\n    'W3': [],\n    'b3': []\n}\n\n# Training loop\nfor i in range(num_iterations):\n    # Forward pass\n    y_hat = network.forward(x_input)\n\n    # Compute loss\n    loss = network.compute_loss(y_hat, y_target)\n    loss_history.append(loss)\n\n    # Backward pass and parameter update\n    grads = network.backward(y_target, learning_rate)\n\n    # Log parameters\n    params = network.get_params()\n    param_history['W1'].append(params['W1'])\n    param_history['b1'].append(params['b1'])\n    param_history['W2'].append(params['W2'])\n    param_history['b2'].append(params['b2'])\n    param_history['W3'].append(params['W3'])\n    param_history['b3'].append(params['b3'])\n\n    # Print loss every 100 iterations\n    if (i + 1) % 100 == 0:\n        print(f\"Iteration {i+1:4d}: Loss = {loss:.6f}, y_hat = {y_hat:.6f}\")\n\nprint(\"\\nTraining completed!\")\n\n\nIteration  100: Loss = 0.019820, y_hat = 0.800902\nIteration  200: Loss = 0.000292, y_hat = 0.975849\nIteration  300: Loss = 0.000004, y_hat = 0.997066\nIteration  400: Loss = 0.000000, y_hat = 0.999644\nIteration  500: Loss = 0.000000, y_hat = 0.999957\nIteration  600: Loss = 0.000000, y_hat = 0.999995\nIteration  700: Loss = 0.000000, y_hat = 0.999999\nIteration  800: Loss = 0.000000, y_hat = 1.000000\nIteration  900: Loss = 0.000000, y_hat = 1.000000\nIteration 1000: Loss = 0.000000, y_hat = 1.000000\n\nTraining completed!\n\n\n\n\nVisualization: Loss Curve\n\n\nShow code\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(loss_history)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Training Loss over Time')\nplt.grid(True)\nplt.yscale('log')\n\nplt.subplot(1, 2, 2)\nplt.plot(loss_history[-500:])\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss (Last 500 iterations)')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nVisualization: Parameter Evolution\n\n\nShow code\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\n\n# W1\naxes[0, 0].plot(np.array(param_history['W1'])[:, 0, 0], label='W1[0,0]')\naxes[0, 0].plot(np.array(param_history['W1'])[:, 0, 1], label='W1[0,1]')\naxes[0, 0].plot(np.array(param_history['W1'])[:, 1, 0], label='W1[1,0]')\naxes[0, 0].plot(np.array(param_history['W1'])[:, 1, 1], label='W1[1,1]')\naxes[0, 0].set_title('Layer 1 Weights (W1)')\naxes[0, 0].legend()\naxes[0, 0].grid(True)\n\n# b1\naxes[0, 1].plot(np.array(param_history['b1'])[:, 0], label='b1[0]')\naxes[0, 1].plot(np.array(param_history['b1'])[:, 1], label='b1[1]')\naxes[0, 1].set_title('Layer 1 Biases (b1)')\naxes[0, 1].legend()\naxes[0, 1].grid(True)\n\n# W2\naxes[0, 2].plot(np.array(param_history['W2'])[:, 0, 0], label='W2[0,0]')\naxes[0, 2].plot(np.array(param_history['W2'])[:, 0, 1], label='W2[0,1]')\naxes[0, 2].plot(np.array(param_history['W2'])[:, 1, 0], label='W2[1,0]')\naxes[0, 2].plot(np.array(param_history['W2'])[:, 1, 1], label='W2[1,1]')\naxes[0, 2].set_title('Layer 2 Weights (W2)')\naxes[0, 2].legend()\naxes[0, 2].grid(True)\n\n# b2\naxes[1, 0].plot(np.array(param_history['b2'])[:, 0], label='b2[0]')\naxes[1, 0].plot(np.array(param_history['b2'])[:, 1], label='b2[1]')\naxes[1, 0].set_title('Layer 2 Biases (b2)')\naxes[1, 0].legend()\naxes[1, 0].grid(True)\n\n# W3\naxes[1, 1].plot(np.array(param_history['W3'])[:, 0], label='W3[0]')\naxes[1, 1].plot(np.array(param_history['W3'])[:, 1], label='W3[1]')\naxes[1, 1].set_title('Layer 3 Weights (W3)')\naxes[1, 1].legend()\naxes[1, 1].grid(True)\n\n# b3\naxes[1, 2].plot(param_history['b3'], label='b3')\naxes[1, 2].set_title('Layer 3 Bias (b3)')\naxes[1, 2].legend()\naxes[1, 2].grid(True)\n\nfor ax in axes.flat:\n    ax.set_xlabel('Iteration')\n    ax.set_ylabel('Parameter Value')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFinal Results\n\n\nShow code\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Initial Loss: {loss_history[0]:.6f}\")\nprint(f\"Final Loss: {loss_history[-1]:.6f}\")\nprint(f\"Loss Reduction: {(1 - loss_history[-1]/loss_history[0])*100:.2f}%\")\nprint(f\"\\nTarget: {y_target}\")\nfinal_prediction = network.forward(x_input)\nprint(f\"Final Prediction: {final_prediction:.6f}\")\nprint(f\"Prediction Error: {abs(final_prediction - y_target):.6f}\")\n\n\n\n============================================================\nTRAINING SUMMARY\n============================================================\nInitial Loss: 1.323064\nFinal Loss: 0.000000\nLoss Reduction: 100.00%\n\nTarget: 1.0\nFinal Prediction: 1.000000\nPrediction Error: 0.000000\n\n\n\nThis implementation demonstrates the power of backpropagation: efficiently computing gradients through the chain rule enables training deep neural networks that would otherwise be computationally intractable."
  },
  {
    "objectID": "ML/multi-task-learning.html",
    "href": "ML/multi-task-learning.html",
    "title": "Chapter 7.7: Multi-Task Learning",
    "section": "",
    "text": "Multi-task learning trains a single model to perform multiple related tasks simultaneously by sharing representations across tasks. This approach:\n\nImproves generalization by learning shared features\nReduces overfitting through implicit regularization\nEnables knowledge transfer between related tasks"
  },
  {
    "objectID": "ML/multi-task-learning.html#overview",
    "href": "ML/multi-task-learning.html#overview",
    "title": "Chapter 7.7: Multi-Task Learning",
    "section": "",
    "text": "Multi-task learning trains a single model to perform multiple related tasks simultaneously by sharing representations across tasks. This approach:\n\nImproves generalization by learning shared features\nReduces overfitting through implicit regularization\nEnables knowledge transfer between related tasks"
  },
  {
    "objectID": "ML/multi-task-learning.html#concept",
    "href": "ML/multi-task-learning.html#concept",
    "title": "Chapter 7.7: Multi-Task Learning",
    "section": "1. Concept",
    "text": "1. Concept\n\nModel Architecture\nMulti-task learning uses a shared representation with task-specific outputs:\nTraining: \\[\n\\begin{aligned}\nh &= f(x; \\theta_{\\text{shared}}) \\\\\n\\hat{y}^{(t)} &= g(h; \\theta_t)\n\\end{aligned}\n\\]\nwhere:\n\n\\(h\\): Shared representation (common features learned across all tasks)\n\\(f(x; \\theta_{\\text{shared}})\\): Shared layers (e.g., CNN encoder, Transformer)\n\\(g(h; \\theta_t)\\): Task-specific head for task \\(t\\)\n\\(\\theta_{\\text{shared}}\\): Shared parameters\n\\(\\theta_t\\): Task-specific parameters\n\n\n\nLoss Function\nCombined loss: \\[\n\\mathcal{L} = \\sum_t \\lambda_t \\mathcal{L}_t(g(f(x; \\theta_{\\text{shared}}); \\theta_t))\n\\]\nwhere:\n\n\\(\\mathcal{L}_t\\): Loss function for task \\(t\\)\n\\(\\lambda_t\\): Weight for task \\(t\\) (controls importance)\nSum is over all tasks\n\nInterpretation: The model minimizes a weighted combination of task-specific losses, forcing the shared representation to be useful for all tasks.\n\n\n\nMulti-Task Learning Architecture"
  },
  {
    "objectID": "ML/multi-task-learning.html#benefit",
    "href": "ML/multi-task-learning.html#benefit",
    "title": "Chapter 7.7: Multi-Task Learning",
    "section": "2. Benefit",
    "text": "2. Benefit\nMulti-task learning improves generalization ability and reduces generalization error.\nWhy this works:\n\nShared representations: Common features learned from multiple tasks are more robust and general\nImplicit regularization: Training on multiple tasks prevents overfitting to any single task\nData efficiency: Each task benefits from the data of other tasks\nInductive bias: The model is encouraged to learn features that are useful across tasks\n\nExample:\n\nTraining on both face recognition and age estimation helps the model learn better facial features than training on either task alone\nThe shared features capture general facial characteristics useful for both tasks"
  },
  {
    "objectID": "ML/multi-task-learning.html#limitation",
    "href": "ML/multi-task-learning.html#limitation",
    "title": "Chapter 7.7: Multi-Task Learning",
    "section": "3. Limitation",
    "text": "3. Limitation\nMulti-task learning works only when the assumption that the tasks are related statistically holds.\nWhen it fails:\n\nUnrelated tasks: If tasks are not related, sharing representations can hurt performance\nNegative transfer: A poorly performing task can degrade the shared representation\nTask interference: Conflicting objectives can prevent convergence\n\nExamples of unrelated tasks:\n\nFace recognition + financial fraud detection (no shared structure)\nMedical diagnosis + game playing (different domains entirely)\n\nKey principle: Tasks should share some underlying structure or statistical properties for multi-task learning to be beneficial."
  },
  {
    "objectID": "ML/multi-task-learning.html#real-world-cases",
    "href": "ML/multi-task-learning.html#real-world-cases",
    "title": "Chapter 7.7: Multi-Task Learning",
    "section": "4. Real-World Cases",
    "text": "4. Real-World Cases\nNote: The following table is generated by ChatGPT.\n\n\n\n\n\n\n\n\n\n\nDomain\nTasks Learned Together\nShared Representation / Model\nPractical Benefit\nExample / Source\n\n\n\n\nFace Analysis\nFace recognition ¬∑ Age estimation ¬∑ Gender / Emotion classification\nShared CNN backbone (e.g., ResNet) with multiple output heads\nImproves accuracy and robustness by using shared facial features\nZhang et al., MTL-CNN for Face Analysis, CVPR 2014\n\n\nAutonomous Driving\nObject detection ¬∑ Lane segmentation ¬∑ Depth estimation\nShared encoder in perception network\nEnables one network to handle multiple perception tasks ‚Üí reduced compute & latency\nUber ATG, MultiNet, CVPR 2017\n\n\nMedical Imaging\nTumor segmentation ¬∑ Disease classification\nShared U-Net encoder with task-specific decoders\nCombines fine-grained segmentation and diagnosis ‚Üí less labeled data needed\nLiu et al., MT-UNet, MICCAI 2019\n\n\nSpeech Processing\nPhoneme recognition ¬∑ Speaker ID ¬∑ Emotion detection\nShared acoustic encoder (e.g., wav2vec backbone)\nImproves noise robustness and transfer learning across tasks\nBaevski et al., wav2vec 2.0, 2020\n\n\nNatural Language Processing\nPOS tagging ¬∑ NER ¬∑ Parsing ¬∑ Sentiment analysis\nShared Transformer encoder (e.g., BERT) with task-specific heads\nLearns richer linguistic features; boosts low-data tasks\nCollobert et al., Unified NLP with MTL, 2008; Devlin et al., BERT, 2019\n\n\nSearch & Recommendation\nClick prediction ¬∑ Conversion rate ¬∑ Dwell-time estimation\nShared user-embedding network\nCaptures user intent across tasks ‚Üí higher CTR and ranking precision\nGoogle Ads / YouTube Recommender Systems\n\n\nFinancial Risk Modeling\nCredit default ¬∑ Fraud detection ¬∑ Customer churn\nShared behavior-feature extractor\nReduces training cost, improves detection of rare events\nAnt Financial Research Team, 2020\n\n\nRobotics / Reinforcement Learning\nNavigation ¬∑ Object manipulation ¬∑ Balance control\nShared policy network or shared latent state\nLearns transferable motor skills across tasks\nDeepMind IMPALA (2018), Gato (2022)\n\n\n\n\nSource: Deep Learning Book (Goodfellow et al.), Chapter 7.7"
  },
  {
    "objectID": "ML/hessian-prerequisites.html",
    "href": "ML/hessian-prerequisites.html",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "",
    "text": "My notebook\nBefore diving into optimization algorithms for deep learning (Chapter 7), we need to understand second-order derivatives in multiple dimensions. The Hessian matrix is the key tool that generalizes the concept of curvature to high-dimensional spaces."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#context",
    "href": "ML/hessian-prerequisites.html#context",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "",
    "text": "My notebook\nBefore diving into optimization algorithms for deep learning (Chapter 7), we need to understand second-order derivatives in multiple dimensions. The Hessian matrix is the key tool that generalizes the concept of curvature to high-dimensional spaces."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#why-second-derivatives-matter",
    "href": "ML/hessian-prerequisites.html#why-second-derivatives-matter",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Why Second Derivatives Matter",
    "text": "Why Second Derivatives Matter\nIn one dimension, optimizing \\(f(x)\\) involves:\n\nFirst derivative \\(f'(x) = 0\\) ‚Üí Find critical points\n\nWhy set \\(f'(x) = 0\\)? At a minimum or maximum, the slope is flat (zero)\nThink of a hill: at the very top, you stop going up ‚Üí slope = 0\nAt the bottom of a valley, you stop going down ‚Üí slope = 0\nExample: For \\(f(x) = x^2\\), we have \\(f'(x) = 2x\\). Setting \\(f'(x) = 0\\) gives \\(x = 0\\) (the minimum)\n\nSecond derivative \\(f''(x)\\) ‚Üí Classify the critical point:\n\n\\(f''(x) &gt; 0\\) ‚Üí Local minimum (curves upward like a bowl)\n\\(f''(x) &lt; 0\\) ‚Üí Local maximum (curves downward like a dome)\n\\(f''(x) = 0\\) ‚Üí Inconclusive (could be an inflection point)\nWhy needed? Not all points where \\(f'(x) = 0\\) are minima! For \\(f(x) = x^3\\), we have \\(f'(0) = 0\\) but it‚Äôs neither a min nor max.\n\n\nThe challenge: How do we extend this to functions of many variables \\(f(x_1, x_2, \\ldots, x_n)\\)?\nThe answer: The Hessian matrix captures all second-order information.\n\nVisualizing Second Derivatives\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = np.linspace(-3, 3, 200)\n\n# Three functions with different second derivatives\nf1 = x**2           # f''(x) = 2 (positive, curves up)\nf2 = -x**2          # f''(x) = -2 (negative, curves down)\nf3 = x**3           # f''(x) = 6x (changes sign at x=0)\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\n\n# Function 1: f(x) = x¬≤\naxes[0, 0].plot(x, f1, 'b-', linewidth=2)\naxes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 0].set_title(\"f(x) = x¬≤\\nf''(x) = 2 &gt; 0\\n(Curves UP)\", fontsize=10)\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel('f(x)')\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].annotate('Minimum', xy=(0, 0), xytext=(0.5, 2),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Function 2: f(x) = -x¬≤\naxes[0, 1].plot(x, f2, 'r-', linewidth=2)\naxes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 1].set_title(\"f(x) = -x¬≤\\nf''(x) = -2 &lt; 0\\n(Curves DOWN)\", fontsize=10)\naxes[0, 1].set_xlabel('x')\naxes[0, 1].set_ylabel('f(x)')\naxes[0, 1].grid(True, alpha=0.3)\naxes[0, 1].annotate('Maximum', xy=(0, 0), xytext=(0.5, -2),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Function 3: f(x) = x¬≥\naxes[1, 0].plot(x, f3, 'g-', linewidth=2)\naxes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[1, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[1, 0].set_title(\"f(x) = x¬≥\\nf''(x) = 6x\\n(Changes sign)\", fontsize=10)\naxes[1, 0].set_xlabel('x')\naxes[1, 0].set_ylabel('f(x)')\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].annotate('Inflection point', xy=(0, 0), xytext=(1, -10),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Hide the unused subplot\naxes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nKey observations:\n\n\n\n\n\n\n\n\n\nSecond Derivative\nCurvature\nShape\nPoint Type\n\n\n\n\n\\(f''(x) &gt; 0\\)\nCurves upward\nBowl shape\nPotential minimum\n\n\n\\(f''(x) &lt; 0\\)\nCurves downward\nDome shape\nPotential maximum\n\n\n\\(f''(x) = 0\\) (at critical point)\nChanges sign\nFlat at that point\nInflection point\n\n\n\nNote on the third example: For \\(f(x) = x^3\\), we have \\(f''(x) = 6x\\). At the critical point \\(x = 0\\), \\(f''(0) = 0\\), which is inconclusive. The curvature changes sign: negative for \\(x &lt; 0\\) and positive for \\(x &gt; 0\\)."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#the-hessian-matrix",
    "href": "ML/hessian-prerequisites.html#the-hessian-matrix",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "The Hessian Matrix",
    "text": "The Hessian Matrix\n\nDefinition\nFor a scalar function \\(f(\\mathbf{x}) = f(x_1, x_2, \\ldots, x_n)\\), the Hessian matrix is the square matrix of all second-order partial derivatives:\n\\[\nH(f) =\n\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n\\]\n\n\nKey Properties\n\nSymmetric: If mixed partial derivatives are continuous, then \\(\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}\\), so \\(H = H^T\\).\nShape: Always \\(n \\times n\\) (determined by number of variables, not terms in the function)\nDescribes curvature in all directions simultaneously\nEigenvalue decomposition: Since the Hessian is symmetric, it can be decomposed as \\(H = Q\\Lambda Q^T\\) where \\(Q\\) contains orthonormal eigenvectors and \\(\\Lambda\\) is a diagonal matrix of eigenvalues\n\n\n\nSimple Example\nFor \\(f(x, y) = x^2 + 3y^2\\):\nStep 1: Compute first derivatives \\[\n\\frac{\\partial f}{\\partial x} = 2x, \\quad \\frac{\\partial f}{\\partial y} = 6y\n\\]\nStep 2: Compute second derivatives \\[\n\\frac{\\partial^2 f}{\\partial x^2} = 2, \\quad \\frac{\\partial^2 f}{\\partial y^2} = 6, \\quad \\frac{\\partial^2 f}{\\partial x \\partial y} = 0\n\\]\nStep 3: Build Hessian \\[\nH = \\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}\n\\]\nInterpretation: - Curvature along \\(x\\)-axis: 2 - Curvature along \\(y\\)-axis: 6 - No cross-dependency (off-diagonal = 0)\n\n\nExample with Cross Terms\nFor \\(f(x, y) = x^2 + xy + y^2\\):\n\\[\nH = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\n\\]\nThe off-diagonal term (1) indicates that \\(x\\) and \\(y\\) are coupled‚Äîchanging one affects the rate of change of the other."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#matrix-definiteness",
    "href": "ML/hessian-prerequisites.html#matrix-definiteness",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Matrix Definiteness",
    "text": "Matrix Definiteness\nFor a symmetric matrix \\(A\\), its definiteness is determined by the signs of its eigenvalues.\n\n\n\n\n\n\n\n\n\nType\nEigenvalues\nQuadratic Form \\(x^T A x\\)\nGeometric Shape\n\n\n\n\nPositive definite (PD)\nall \\(&gt; 0\\)\n\\(&gt; 0\\) for all \\(x \\neq 0\\)\nBowl (curves upward)\n\n\nNegative definite (ND)\nall \\(&lt; 0\\)\n\\(&lt; 0\\) for all \\(x \\neq 0\\)\nDome (curves downward)\n\n\nIndefinite\nsome \\(+\\), some \\(-\\)\ndepends on direction\nSaddle\n\n\nPositive semi-definite (PSD)\nall \\(\\geq 0\\)\n\\(\\geq 0\\) for all \\(x\\)\nFlat-bottom bowl\n\n\nNegative semi-definite (NSD)\nall \\(\\leq 0\\)\n\\(\\leq 0\\) for all \\(x\\)\nFlat-top dome\n\n\n\n\nQuick Test (2√ó2 case)\nFor \\(A = \\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}\\):\n\nPositive definite if \\(a &gt; 0\\) and \\(ac - b^2 &gt; 0\\)\nNegative definite if \\(a &lt; 0\\) and \\(ac - b^2 &gt; 0\\)\nIndefinite if \\(ac - b^2 &lt; 0\\)\n\n\n\nExamples\n\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}\\): eigenvalues = [2, 6] ‚Üí Positive definite\n\\(\\begin{bmatrix} -2 & 0 \\\\ 0 & -3 \\end{bmatrix}\\): eigenvalues = [-2, -3] ‚Üí Negative definite\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\\): eigenvalues = [2, -2] ‚Üí Indefinite\n\\(\\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix}\\): eigenvalues = [4, 0] ‚Üí Positive semi-definite"
  },
  {
    "objectID": "ML/hessian-prerequisites.html#interpreting-hessian-at-critical-points",
    "href": "ML/hessian-prerequisites.html#interpreting-hessian-at-critical-points",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Interpreting Hessian at Critical Points",
    "text": "Interpreting Hessian at Critical Points\nAt a critical point where \\(\\nabla f = 0\\), the Hessian determines the nature of the point:\n\n\n\n\n\n\n\n\n\nHessian Type\nEigenvalues\nSurface Shape\nPoint Type\n\n\n\n\nPositive definite\nall positive\nBowl (convex)\nLocal minimum\n\n\nNegative definite\nall negative\nDome (concave)\nLocal maximum\n\n\nIndefinite\nmixed signs\nSaddle\nNeither min nor max\n\n\nSemi-definite\nsome zero\nFlat in some directions\nInconclusive\n\n\n\n\nVisualization: Different Surface Types\n\n\nShow code\n# Create grid for plotting\nx_grid = np.linspace(-2, 2, 100)\ny_grid = np.linspace(-2, 2, 100)\nX, Y = np.meshgrid(x_grid, y_grid)\n\n# Define different functions with different Hessian types\ndef positive_definite(x, y):\n    \"\"\"Minimum: f = x¬≤ + y¬≤\"\"\"\n    return x**2 + y**2\n\ndef negative_definite(x, y):\n    \"\"\"Maximum: f = -x¬≤ - y¬≤\"\"\"\n    return -x**2 - y**2\n\ndef indefinite(x, y):\n    \"\"\"Saddle: f = x¬≤ - y¬≤\"\"\"\n    return x**2 - y**2\n\ndef semi_definite(x, y):\n    \"\"\"Flat direction: f = x¬≤\"\"\"\n    return x**2\n\n# Create 3D surface plots in 2x2 grid\nfig = plt.figure(figsize=(12, 10))\n\nfunctions = [\n    (positive_definite, \"Positive Definite\\n(Bowl - Minimum)\", \"Greens\"),\n    (negative_definite, \"Negative Definite\\n(Dome - Maximum)\", \"Reds\"),\n    (indefinite, \"Indefinite\\n(Saddle Point)\", \"RdBu\"),\n    (semi_definite, \"Semi-Definite\\n(Flat Direction)\", \"YlOrRd\")\n]\n\nfor idx, (func, title, cmap) in enumerate(functions, 1):\n    ax = fig.add_subplot(2, 2, idx, projection='3d')\n    Z = func(X, Y)\n\n    surf = ax.plot_surface(X, Y, Z, cmap=cmap, alpha=0.8,\n                           linewidth=0, antialiased=True)\n\n    ax.set_xlabel('x', fontsize=9)\n    ax.set_ylabel('y', fontsize=9)\n    ax.set_zlabel('f(x,y)', fontsize=9)\n    ax.set_title(title, fontsize=10)\n    ax.view_init(elev=25, azim=45)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nContour plots for better understanding:\n\n\nShow code\n# Contour plots in 2x2 grid\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\naxes = axes.flatten()\n\nfor idx, (func, title, cmap) in enumerate(functions):\n    ax = axes[idx]\n    Z = func(X, Y)\n\n    contour = ax.contour(X, Y, Z, levels=15, cmap=cmap)\n    ax.clabel(contour, inline=True, fontsize=7)\n\n    # Mark the critical point at origin\n    ax.plot(0, 0, 'r*', markersize=12, label='Critical point')\n\n    ax.set_xlabel('x', fontsize=9)\n    ax.set_ylabel('y', fontsize=9)\n    ax.set_title(title, fontsize=10)\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=8)\n    ax.set_aspect('equal')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ML/semi-supervised-learning.html",
    "href": "ML/semi-supervised-learning.html",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "",
    "text": "When labeled data is scarce, semi-supervised learning leverages both labeled and unlabeled data to improve model performance. This approach combines:\n\nGenerative modeling to learn data distribution \\(P(x)\\)\nSupervised classification to learn \\(P(y|x)\\)\nJoint optimization that balances both objectives"
  },
  {
    "objectID": "ML/semi-supervised-learning.html#overview",
    "href": "ML/semi-supervised-learning.html#overview",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "",
    "text": "When labeled data is scarce, semi-supervised learning leverages both labeled and unlabeled data to improve model performance. This approach combines:\n\nGenerative modeling to learn data distribution \\(P(x)\\)\nSupervised classification to learn \\(P(y|x)\\)\nJoint optimization that balances both objectives"
  },
  {
    "objectID": "ML/semi-supervised-learning.html#the-problem-limited-labeled-data",
    "href": "ML/semi-supervised-learning.html#the-problem-limited-labeled-data",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "1. The Problem: Limited Labeled Data",
    "text": "1. The Problem: Limited Labeled Data\nIn many real-world scenarios:\n\nLabeled data is expensive to obtain (requires human annotation)\nUnlabeled data is abundant and cheap\nModels trained only on limited labeled data tend to overfit\n\nSolution: Use unlabeled data to learn better representations and regularize the model."
  },
  {
    "objectID": "ML/semi-supervised-learning.html#two-learning-objectives",
    "href": "ML/semi-supervised-learning.html#two-learning-objectives",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "2. Two Learning Objectives",
    "text": "2. Two Learning Objectives\n\nGenerative Model (Unsupervised)\nObjective: Maximize the probability of generating correct inputs \\[\nP(x)\n\\]\nWhat this learns:\n\nThe underlying distribution of the data\nUseful representations of the input space\nStructure and patterns in unlabeled data\n\n\n\nClassification Model (Supervised)\nObjective: Maximize the probability of correct predictions given inputs \\[\nP(y|x)\n\\]\nWhat this learns:\n\nDecision boundaries between classes\nTask-specific features\nDirect mapping from inputs to labels"
  },
  {
    "objectID": "ML/semi-supervised-learning.html#joint-learning-objective",
    "href": "ML/semi-supervised-learning.html#joint-learning-objective",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "3. Joint Learning Objective",
    "text": "3. Joint Learning Objective\nCombined loss function: \\[\n\\mathcal{L} = -\\log P(y|x) - \\lambda \\log P(x)\n\\]\nwhere:\n\nFirst term: Supervised loss (classification accuracy)\nSecond term: Unsupervised loss (generative modeling)\n\\(\\lambda\\): Trade-off parameter controlling the balance\n\nInterpretation:\n\nThe model must simultaneously:\n\nPredict labels correctly (supervised term)\nModel the data distribution well (unsupervised term)\n\nThe unsupervised term acts as regularization, preventing overfitting to the small labeled set\n\n\n\n\nSemi-Supervised Learning"
  },
  {
    "objectID": "ML/semi-supervised-learning.html#why-this-works",
    "href": "ML/semi-supervised-learning.html#why-this-works",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "4. Why This Works",
    "text": "4. Why This Works\nKey insight: When the model learns how to represent \\(P(x)\\), it discovers where the data is dense. Decision boundaries should avoid cutting through high-density regions ‚Äî they should instead pass through low-density areas between clusters.\nGeometric interpretation:\n\nLearning \\(P(x)\\) reveals the natural clustering structure of the data\nClassification boundaries are encouraged to lie in low-density regions\nThis prevents the decision boundary from crossing through dense data manifolds\n\nBenefits:\n\nBetter representations: Unlabeled data reveals the structure of the input space\nCluster assumption: Decision boundaries naturally form between clusters, not through them\nRegularization: The generative term prevents the classifier from focusing only on labeled examples\nData efficiency: Can achieve high accuracy with significantly fewer labeled samples\n\nExample:\n\nWith only 10% labeled data, semi-supervised learning can match the performance of fully supervised learning with 100% labels"
  },
  {
    "objectID": "ML/semi-supervised-learning.html#real-world-applications",
    "href": "ML/semi-supervised-learning.html#real-world-applications",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "5. Real-World Applications",
    "text": "5. Real-World Applications\nNote: The following content is generated by ChatGPT.\n\n\n\n\n\n\n\n\n\n\n\nDomain\nTask / Problem\nUnlabeled Data Used\nMethod Family\nReal-World Benefit\nReference\n\n\n\n\nImage Recognition\nClassifying natural images (CIFAR-10, ImageNet-100)\nMillions of unlabeled web images\nConsistency Regularization (FixMatch, Mean Teacher)\n+15‚Äì25% accuracy with 10√ó fewer labeled samples\nSohn et al., FixMatch, 2020\n\n\nMedical Imaging\nTumor or lesion segmentation (MRI / CT)\nThousands of unlabeled scans\nGenerative / Consistency Hybrid (VAE, U-Net)\n~80% annotation cost reduction; works well with rare cases\nBai et al., MedIA, 2019\n\n\nSpeech Recognition\nAutomatic speech recognition (ASR)\nLarge amounts of raw audio\nRepresentation Learning (wav2vec 2.0)\nMatches full supervision using &lt;10% labeled data\nBaevski et al., wav2vec 2.0, 2020\n\n\nNatural Language Processing\nText classification, sentiment analysis\nBillions of unlabeled sentences\nSelf-Supervised Pretraining (BERT, RoBERTa)\nMassive improvement in downstream \\(P(y \\mid x)\\) tasks\nDevlin et al., BERT, 2018\n\n\nAutonomous Driving\nScene understanding, lane detection\nContinuous unlabeled video streams\nConsistency + Pseudo-Labeling\nRobust to lighting/weather; reduces manual labels\nFrench et al., 2020\n\n\nFinancial Fraud Detection\nDetecting anomalous transactions\nTransaction logs without labels\nGenerative Modeling (VAE / GAN)\nLearns normal patterns ‚Üí better anomaly detection\nXu et al., KDD, 2018\n\n\nRecommendation Systems\nPredicting user preferences\nUser‚Äìitem logs without explicit feedback\nRepresentation Learning (Autoencoder / Contrastive)\nImproves cold-start and leverages implicit signals\n‚Äî"
  },
  {
    "objectID": "ML/semi-supervised-learning.html#common-semi-supervised-learning-methods",
    "href": "ML/semi-supervised-learning.html#common-semi-supervised-learning-methods",
    "title": "Chapter 7.6: Semi-Supervised Learning",
    "section": "6. Common Semi-Supervised Learning Methods",
    "text": "6. Common Semi-Supervised Learning Methods\nNote: The following content is generated by ChatGPT.\n\nConsistency Regularization\n\nIdea: Model should produce similar predictions for perturbed versions of the same input\nExamples: FixMatch, Mean Teacher, Virtual Adversarial Training\n\n\n\nPseudo-Labeling\n\nIdea: Use model‚Äôs confident predictions on unlabeled data as ‚Äúsoft labels‚Äù\nProcess: Train ‚Üí predict on unlabeled ‚Üí retrain with pseudo-labels\n\n\n\nGenerative Models\n\nIdea: Learn \\(P(x)\\) and \\(P(y|x)\\) jointly\nExamples: VAE, GAN-based approaches\n\n\n\nSelf-Supervised Pretraining\n\nIdea: Pretrain on unlabeled data with pretext tasks, then fine-tune on labeled data\nExamples: BERT (masked language modeling), wav2vec 2.0 (contrastive learning)\n\n\nSource: Deep Learning Book (Goodfellow et al.), Chapter 7.6"
  },
  {
    "objectID": "ML/architecture-design.html",
    "href": "ML/architecture-design.html",
    "title": "Deep Learning Book 6.4: Architecture Design - Depth vs Width",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.4 explores how network architecture‚Äîdepth versus width‚Äîfundamentally shapes what neural networks can learn and how efficiently they learn it.\nüìì For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "ML/architecture-design.html#the-architecture-question-deep-or-wide",
    "href": "ML/architecture-design.html#the-architecture-question-deep-or-wide",
    "title": "Deep Learning Book 6.4: Architecture Design - Depth vs Width",
    "section": "The Architecture Question: Deep or Wide?",
    "text": "The Architecture Question: Deep or Wide?\nWhen designing a neural network, one of the most fundamental decisions is choosing between depth (many layers) and width (many units per layer). Should you build a shallow network with many units, or a deep network with fewer units per layer?\nThe answer reveals something profound about how neural networks represent functions: deep networks can achieve exponentially greater expressiveness than shallow networks with the same number of parameters. This isn‚Äôt just theoretical‚Äîit has practical implications for model efficiency and performance.\n\nQuick Reference: Understanding Depth vs Width\nFor context on the fundamental concepts of network architecture, see the Architecture Design summary.\nKey insight: A deep ReLU network with \\(n\\) units per layer and depth \\(L\\) can create \\(\\mathcal{O}(n^L)\\) distinct linear regions in the input space. A shallow network would need exponentially many units (\\(\\mathcal{O}(n^L)\\) units in a single layer) to achieve the same expressiveness.\n\n\n\n\n\n\n\n\n\nArchitecture\nCharacteristic\nAdvantage\nChallenge\n\n\n\n\nDeep (many layers)\nHierarchical feature reuse\nExponential expressiveness with fewer parameters\nHarder to optimize (vanishing/exploding gradients)\n\n\nWide (many units per layer)\nIncreased capacity per layer\nEasier optimization\nParameter inefficient; requires exponentially more units"
  },
  {
    "objectID": "ML/architecture-design.html#experiment-shallow-vs-deep-network-comparison",
    "href": "ML/architecture-design.html#experiment-shallow-vs-deep-network-comparison",
    "title": "Deep Learning Book 6.4: Architecture Design - Depth vs Width",
    "section": "üî¨ Experiment: Shallow vs Deep Network Comparison",
    "text": "üî¨ Experiment: Shallow vs Deep Network Comparison\nLet‚Äôs explore whether depth provides an advantage in practice by comparing two networks: - Shallow Network: 1 hidden layer with 128 units - Deep Network: 3 hidden layers (16 ‚Üí 8 ‚Üí output)\nBoth networks are trained on the same regression task: \\(y = \\sin^2(x) + x^3\\)\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Configure plotting\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['axes.grid'] = True\nplt.rcParams['grid.alpha'] = 0.3\n\nprint(\"‚úì Setup complete\")\n\n\n‚úì Setup complete\n\n\n\nStep 1: Generate Training and Test Data\n\n\nShow code\n# Training data\nx_train = np.random.rand(200, 1)\ny_train = np.square(np.sin(x_train)) + np.power(x_train, 3)\n\n# Test data\nx_test = np.random.rand(100, 1)\ny_test = np.square(np.sin(x_test)) + np.power(x_test, 3)\n\n# Convert to PyTorch tensors\nx_train_tensor = torch.FloatTensor(x_train)\ny_train_tensor = torch.FloatTensor(y_train)\nx_test_tensor = torch.FloatTensor(x_test)\ny_test_tensor = torch.FloatTensor(y_test)\n\nprint(f\"Training samples: {len(x_train)}\")\nprint(f\"Test samples: {len(x_test)}\")\nprint(f\"Input range: [{x_train.min():.2f}, {x_train.max():.2f}]\")\nprint(f\"Target range: [{y_train.min():.2f}, {y_train.max():.2f}]\")\n\n\nTraining samples: 200\nTest samples: 100\nInput range: [0.01, 0.99]\nTarget range: [0.00, 1.66]\n\n\n\n\nStep 2: Define Model Architectures\n\n\nShow code\n# Shallow model: 1 hidden layer with 128 units\nshallow_model = nn.Sequential(\n    nn.Linear(1, 128),\n    nn.ReLU(),\n    nn.Linear(128, 1)\n)\n\n# Deep model: 3 hidden layers (16 ‚Üí 8 ‚Üí output)\ndeep_model = nn.Sequential(\n    nn.Linear(1, 16),\n    nn.ReLU(),\n    nn.Linear(16, 8),\n    nn.ReLU(),\n    nn.Linear(8, 1)\n)\n\nprint(\"‚úì Models created\")\nprint(f\"\\nShallow model architecture:\")\nprint(shallow_model)\nprint(f\"\\nDeep model architecture:\")\nprint(deep_model)\n\n\n‚úì Models created\n\nShallow model architecture:\nSequential(\n  (0): Linear(in_features=1, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=1, bias=True)\n)\n\nDeep model architecture:\nSequential(\n  (0): Linear(in_features=1, out_features=16, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=16, out_features=8, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=8, out_features=1, bias=True)\n)\n\n\n\n\nStep 3: Count Parameters\nHow many trainable parameters does each architecture use?\n\n\nShow code\ndef count_parameters(model):\n    \"\"\"Count total trainable parameters in a model\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nshallow_params = count_parameters(shallow_model)\ndeep_params = count_parameters(deep_model)\n\nprint(\"Parameter Counts:\")\nprint(\"-\" * 50)\nprint(f\"Shallow model (1 layer √ó 128 units): {shallow_params:,} parameters\")\nprint(f\"Deep model (3 layers):                {deep_params:,} parameters\")\nprint(\"-\" * 50)\nprint(f\"Ratio (shallow/deep): {shallow_params/deep_params:.2f}x\")\n\n# Visualize parameter counts\nfig, ax = plt.subplots(figsize=(8, 5))\nmodels = ['Shallow\\n(1√ó128)', 'Deep\\n(3 layers)']\nparams = [shallow_params, deep_params]\ncolors = ['#ff7f0e', '#1f77b4']\n\nbars = ax.bar(models, params, color=colors, alpha=0.7, edgecolor='black')\nax.set_ylabel('Number of Parameters', fontsize=12)\nax.set_title('Model Parameter Comparison', fontsize=14, fontweight='bold')\nax.grid(axis='y', alpha=0.3)\n\n# Add value labels on bars\nfor bar, param in zip(bars, params):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{param:,}',\n            ha='center', va='bottom', fontsize=11, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\nParameter Counts:\n--------------------------------------------------\nShallow model (1 layer √ó 128 units): 385 parameters\nDeep model (3 layers):                177 parameters\n--------------------------------------------------\nRatio (shallow/deep): 2.18x\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Train Both Models\n\n\nShow code\n# Training configuration\nn_epochs = 500\nlearning_rate = 0.01\nloss_fn = nn.MSELoss()\n\n# Track training history\nhistory = {\n    'Shallow': {'train_loss': [], 'test_loss': []},\n    'Deep': {'train_loss': [], 'test_loss': []}\n}\n\nmodels = {\n    'Shallow': shallow_model,\n    'Deep': deep_model\n}\n\n# Train each model\nfor name, model in models.items():\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    for epoch in range(n_epochs):\n        # Training\n        model.train()\n        y_pred = model(x_train_tensor)\n        loss = loss_fn(y_pred, y_train_tensor)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        history[name]['train_loss'].append(loss.item())\n\n        # Evaluation on test set\n        model.eval()\n        with torch.no_grad():\n            y_test_pred = model(x_test_tensor)\n            test_loss = loss_fn(y_test_pred, y_test_tensor).item()\n            history[name]['test_loss'].append(test_loss)\n\n    print(f\"‚úì {name} model trained\")\n\nprint(\"\\n‚úì Training complete\")\n\n\n‚úì Shallow model trained\n‚úì Deep model trained\n\n‚úì Training complete\n\n\n\n\nStep 5: Compare Model Performance\n\n\nShow code\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\ncolors = {'Shallow': '#ff7f0e', 'Deep': '#1f77b4'}\n\n# Plot training loss\nfor name, data in history.items():\n    axes[0].plot(data['train_loss'], label=name, linewidth=2, color=colors[name])\n\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Training Loss (MSE)', fontsize=12)\naxes[0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\naxes[0].set_yscale('log')\n\n# Plot test loss\nfor name, data in history.items():\n    axes[1].plot(data['test_loss'], label=name, linewidth=2, color=colors[name])\n\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Test Loss (MSE)', fontsize=12)\naxes[1].set_title('Test Loss Comparison', fontsize=14, fontweight='bold')\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\naxes[1].set_yscale('log')\n\nplt.tight_layout()\nplt.show()\n\n# Print final metrics\nprint(\"\\nFinal Performance (after {} epochs):\".format(n_epochs))\nprint(\"-\" * 70)\nprint(f\"{'Model':&lt;15} {'Parameters':&lt;15} {'Train Loss':&lt;15} {'Test Loss':&lt;15}\")\nprint(\"-\" * 70)\nfor name in models.keys():\n    params = count_parameters(models[name])\n    train_loss = history[name]['train_loss'][-1]\n    test_loss = history[name]['test_loss'][-1]\n    print(f\"{name:&lt;15} {params:&lt;15,} {train_loss:&lt;15.6f} {test_loss:&lt;15.6f}\")\n\n\n\n\n\n\n\n\n\n\nFinal Performance (after 500 epochs):\n----------------------------------------------------------------------\nModel           Parameters      Train Loss      Test Loss      \n----------------------------------------------------------------------\nShallow         385             0.000017        0.000022       \nDeep            177             0.000012        0.000013       \n\n\n\nThis experiment demonstrates the practical implications of depth versus width in neural network architecture design, showing how deeper networks can achieve competitive performance with fewer parameters."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I‚Äôm Chao Ma (aka ickma), a passionate developer and researcher focused on machine learning, algorithms, and problem-solving.\n\n\nI enjoy exploring the intersection of mathematics and computer science, with a particular interest in:\n\nü§ñ Machine Learning & AI - From fundamental concepts to practical implementations\nüßÆ Algorithms & Data Structures - Solving complex problems efficiently\n\nüìä Data Science - Extracting insights from data using Python and NumPy\nüíª Software Development - Building robust, scalable solutions\n\n\n\n\nThis site serves as my digital notebook where I share:\n\nAlgorithm explanations with visual examples and code implementations\nProblem-solving approaches for coding challenges and mathematical concepts\nTechnical insights from my learning journey\n\nI believe in learning by doing and explaining concepts clearly with code examples, visualizations, and mathematical foundations.\n\n\n\nMy content covers:\n\nAlgorithms - Dynamic programming, optimization, data structures, and algorithmic problem solving\nMathematics - Linear algebra, calculus, statistics, and mathematical foundations for CS\nReinforcement Learning & Deep Learning - Neural networks, policy optimization, and AI agents\nParallel Computation - Distributed systems, GPU computing, and performance optimization\n\n\n\n\nI‚Äôm always excited to discuss technology, collaborate on projects, or help fellow learners!\n\nüìß Email: ickma2311@gmail.com\nüíª GitHub: @ickma2311\nüê¶ Twitter: @ickma2311\n\nFeel free to reach out if you have questions about any of my posts, want to collaborate, or just want to chat about machine learning and algorithms!\n\nThis blog is built with Quarto and hosted on GitHub Pages. All code examples are available in my repositories."
  },
  {
    "objectID": "about.html#what-i-do",
    "href": "about.html#what-i-do",
    "title": "About",
    "section": "",
    "text": "I enjoy exploring the intersection of mathematics and computer science, with a particular interest in:\n\nü§ñ Machine Learning & AI - From fundamental concepts to practical implementations\nüßÆ Algorithms & Data Structures - Solving complex problems efficiently\n\nüìä Data Science - Extracting insights from data using Python and NumPy\nüíª Software Development - Building robust, scalable solutions"
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "This site serves as my digital notebook where I share:\n\nAlgorithm explanations with visual examples and code implementations\nProblem-solving approaches for coding challenges and mathematical concepts\nTechnical insights from my learning journey\n\nI believe in learning by doing and explaining concepts clearly with code examples, visualizations, and mathematical foundations."
  },
  {
    "objectID": "about.html#technical-focus",
    "href": "about.html#technical-focus",
    "title": "About",
    "section": "",
    "text": "My content covers:\n\nAlgorithms - Dynamic programming, optimization, data structures, and algorithmic problem solving\nMathematics - Linear algebra, calculus, statistics, and mathematical foundations for CS\nReinforcement Learning & Deep Learning - Neural networks, policy optimization, and AI agents\nParallel Computation - Distributed systems, GPU computing, and performance optimization"
  },
  {
    "objectID": "about.html#connect-with-me",
    "href": "about.html#connect-with-me",
    "title": "About",
    "section": "",
    "text": "I‚Äôm always excited to discuss technology, collaborate on projects, or help fellow learners!\n\nüìß Email: ickma2311@gmail.com\nüíª GitHub: @ickma2311\nüê¶ Twitter: @ickma2311\n\nFeel free to reach out if you have questions about any of my posts, want to collaborate, or just want to chat about machine learning and algorithms!\n\nThis blog is built with Quarto and hosted on GitHub Pages. All code examples are available in my repositories."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture9-least-squares.html",
    "href": "Math/MIT18.065/mit18065-lecture9-least-squares.html",
    "title": "Lecture 9: Four Ways to Solve Least Squares Problems",
    "section": "",
    "text": "This lecture presents four methods for solving least squares problems \\(Ax = b\\) when \\(A\\) has no inverse, exploring the connections between pseudo-inverse, normal equations, and projection geometry."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture9-least-squares.html#overview",
    "href": "Math/MIT18.065/mit18065-lecture9-least-squares.html#overview",
    "title": "Lecture 9: Four Ways to Solve Least Squares Problems",
    "section": "",
    "text": "This lecture presents four methods for solving least squares problems \\(Ax = b\\) when \\(A\\) has no inverse, exploring the connections between pseudo-inverse, normal equations, and projection geometry."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture9-least-squares.html#projection-matrices",
    "href": "Math/MIT18.065/mit18065-lecture9-least-squares.html#projection-matrices",
    "title": "Lecture 9: Four Ways to Solve Least Squares Problems",
    "section": "Projection Matrices",
    "text": "Projection Matrices\nUnderstanding projections is key to least squares:\n\n\\(A^+A\\): Projection onto row space \\(C(A^T)\\)\n\nIf \\(x\\) is in the row space, we get \\(x\\) back\nIf \\(x\\) is in the null space, it gets mapped to \\(0\\)\n\n\\(AA^+\\): Projection onto column space \\(C(A)\\)\n\nProjects vectors onto the column space of \\(A\\)\n\n\n Figure: The four fundamental subspaces and how projection matrices \\(A^+A\\) and \\(AA^+\\) map between them."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture9-least-squares.html#method-1-pseudo-inverse",
    "href": "Math/MIT18.065/mit18065-lecture9-least-squares.html#method-1-pseudo-inverse",
    "title": "Lecture 9: Four Ways to Solve Least Squares Problems",
    "section": "Method 1: Pseudo-Inverse",
    "text": "Method 1: Pseudo-Inverse\nUsing the SVD decomposition \\(A = U\\Sigma V^T\\):\nFor invertible matrices: \\[A^{-1} = V\\Sigma^{-1}U^T\\]\nFor non-invertible matrices: \\[A^+ = V\\Sigma^+ U^T\\]\nwhere \\(\\Sigma^+\\) inverts only the non-zero singular values:\n\\[\n\\Sigma^+ = \\begin{bmatrix}\n\\frac{1}{\\sigma_1} & 0 & \\cdots & 0 \\\\\n0 & \\frac{1}{\\sigma_2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\n\\end{bmatrix}\n\\]\nSolution: \\(\\hat{x} = A^+ b\\)"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture9-least-squares.html#method-2-normal-equations",
    "href": "Math/MIT18.065/mit18065-lecture9-least-squares.html#method-2-normal-equations",
    "title": "Lecture 9: Four Ways to Solve Least Squares Problems",
    "section": "Method 2: Normal Equations",
    "text": "Method 2: Normal Equations\nSolve \\(A^T A \\hat{x} = A^T b\\) when \\(A\\) is \\(m \\times n\\) with rank \\(r\\).\n\nLinear Regression Example\nGiven data points \\((x_1, y_1), (x_2, y_2), \\ldots, (x_m, y_m)\\), we want to find \\(C, D\\) such that:\n\\[C + Dx \\approx y\\]\nBuild the matrix: \\[A = \\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_m\n\\end{bmatrix}, \\quad\nA\\begin{bmatrix}C \\\\ D\\end{bmatrix} =\nC\\begin{bmatrix}1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{bmatrix} +\nD\\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m\\end{bmatrix}\n\\]\n Figure: Setting up the least squares problem for linear regression‚Äîfinding the line that best fits the data points.\n\n\nThree Approaches to the Solution\n\n1. Algebraic Approach\nMinimize \\(\\|Ax - b\\|_2^2\\):\n\\[\n\\|Ax - b\\|_2^2 = (Ax - b)^T(Ax - b) = x^T A^T A x - 2x^T A^T b + b^T b\n\\]\nGradient: \\[\\nabla_x \\|Ax - b\\|_2^2 = 2A^T A x - 2A^T b\\]\nSetting gradient to zero: \\[A^T A \\hat{x} = A^T b\\]\n\n\n2. Geometric Approach\n\nThe vector \\(Ax\\) always lies in the column space \\(C(A)\\)\nIn general, \\(b \\notin C(A)\\), so \\(Ax = b\\) has no exact solution\nThe least-squares solution \\(\\hat{x}\\) satisfies: \\(A\\hat{x} = P_{C(A)} b\\)\n\ni.e., the orthogonal projection of \\(b\\) onto the column space of \\(A\\)\n\nThis projection condition is equivalent to: \\(A^T(A\\hat{x} - b) = 0\\)\nHence the normal equation: \\(A^T A \\hat{x} = A^T b\\)\n\n Figure: Geometric view of least squares‚Äîprojecting \\(b\\) onto the column space of \\(A\\) to find the closest solution \\(A\\hat{x}\\).\n\n\n3. Connection to Pseudo-Inverse\nWhen \\(A^T A\\) is invertible:\n\\[A^+ = V\\Sigma^{-1}U^T = (A^T A)^{-1}A^T\\]\nThe pseudo-inverse \\((A^T A)^{-1}A^T\\) is the left inverse because: \\[(A^T A)^{-1}A^T \\cdot A = I_n\\]\nTherefore: \\[\\hat{x} = A^+ b = (A^T A)^{-1}A^T b\\]\nThis shows that the pseudo-inverse method and normal equations give the same solution!\n Figure: Summary of four equivalent ways to solve least squares problems, all converging to the same optimal solution."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture9-least-squares.html#key-insights",
    "href": "Math/MIT18.065/mit18065-lecture9-least-squares.html#key-insights",
    "title": "Lecture 9: Four Ways to Solve Least Squares Problems",
    "section": "Key Insights",
    "text": "Key Insights\n\nMultiple perspectives, same solution: The pseudo-inverse, normal equations, algebraic minimization, and geometric projection all lead to the same least-squares solution.\nWhen \\(A^T A\\) is invertible: \\(\\hat{x} = (A^T A)^{-1}A^T b\\) provides a closed-form solution.\nProjection interpretation: Least squares finds the point in \\(C(A)\\) closest to \\(b\\)‚Äîthis is the fundamental geometric insight.\nPseudo-inverse generalizes inversion: \\(A^+ = V\\Sigma^+ U^T\\) extends matrix inversion to non-invertible matrices by inverting only non-zero singular values."
  },
  {
    "objectID": "Math/index.html",
    "href": "Math/index.html",
    "title": "Math",
    "section": "",
    "text": "Mathematical foundations and explorations.\n\n\n\nThe Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots\n\n\n\n\n\nLecture 1: Geometry of Linear Equations\nLecture 2: Elimination with Matrices\nLecture 3: Matrix Multiplication and Inverse\nLecture 4: LU Decomposition\nLecture 5.1: Permutation Matrices\nLecture 5.2: Transpose\nLecture 5.3: Vector Spaces\nLecture 6: Column Space and Null Space\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nLecture 9: Independence, Basis, and Dimension\nLecture 10: Four Fundamental Subspaces\nLecture 11: Matrix Spaces, Rank-1, and Graphs\nLecture 12: Graphs, Networks, and Incidence Matrices\nLecture 13: Quiz 1 Review\nLecture 14: Orthogonal Vectors and Subspaces\nLecture 15: Projection onto Subspaces\nLecture 16: Projection Matrices and Least Squares\nLecture 17: Orthogonal Matrices and Gram-Schmidt\nLecture 18: Properties of Determinants\nLecture 19: Determinant Formulas and Cofactors\nLecture 20: Cramer‚Äôs Rule, Inverse Matrix, and Volume\nLecture 21: Eigenvalues and Eigenvectors\nLecture 22: Diagonalization and Powers of A\nLecture 23: Differential Equations and exp(At)"
  },
  {
    "objectID": "Math/index.html#reflections-synthesis",
    "href": "Math/index.html#reflections-synthesis",
    "title": "Math",
    "section": "",
    "text": "The Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots"
  },
  {
    "objectID": "Math/index.html#mit-18.06sc-linear-algebra",
    "href": "Math/index.html#mit-18.06sc-linear-algebra",
    "title": "Math",
    "section": "",
    "text": "Lecture 1: Geometry of Linear Equations\nLecture 2: Elimination with Matrices\nLecture 3: Matrix Multiplication and Inverse\nLecture 4: LU Decomposition\nLecture 5.1: Permutation Matrices\nLecture 5.2: Transpose\nLecture 5.3: Vector Spaces\nLecture 6: Column Space and Null Space\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nLecture 9: Independence, Basis, and Dimension\nLecture 10: Four Fundamental Subspaces\nLecture 11: Matrix Spaces, Rank-1, and Graphs\nLecture 12: Graphs, Networks, and Incidence Matrices\nLecture 13: Quiz 1 Review\nLecture 14: Orthogonal Vectors and Subspaces\nLecture 15: Projection onto Subspaces\nLecture 16: Projection Matrices and Least Squares\nLecture 17: Orthogonal Matrices and Gram-Schmidt\nLecture 18: Properties of Determinants\nLecture 19: Determinant Formulas and Cofactors\nLecture 20: Cramer‚Äôs Rule, Inverse Matrix, and Volume\nLecture 21: Eigenvalues and Eigenvectors\nLecture 22: Diagonalization and Powers of A\nLecture 23: Differential Equations and exp(At)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html",
    "href": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html",
    "title": "MIT 18.06 Lecture 26: Complex Matrices and Fast Fourier Transform",
    "section": "",
    "text": "This lecture extends linear algebra to complex vectors and matrices, introducing key concepts like Hermitian matrices and unitary matrices. The highlight is the Fast Fourier Transform (FFT), which exploits the recursive structure of Fourier matrices to reduce complexity from \\(O(N^2)\\) to \\(O(N\\log N)\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html#complex-vectors-and-inner-products",
    "href": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html#complex-vectors-and-inner-products",
    "title": "MIT 18.06 Lecture 26: Complex Matrices and Fast Fourier Transform",
    "section": "Complex Vectors and Inner Products",
    "text": "Complex Vectors and Inner Products\n\nLength of Complex Vectors\nFor a vector \\(z \\in \\mathbb{C}^n\\), the length is defined using the conjugate transpose:\n\\[\n\\|z\\|^2 = \\bar{z}^{\\top}z = \\begin{bmatrix}\\bar{z}_1 & \\bar{z}_2 & \\cdots & \\bar{z}_n\\end{bmatrix}\\begin{bmatrix}z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_n\\end{bmatrix} = \\sum_{i=1}^n \\bar{z}_i z_i = \\sum_{i=1}^n |z_i|^2\n\\]\nWhy conjugate? For complex numbers, \\(\\bar{z}_i z_i = |z_i|^2\\) is always real and non-negative. Without the conjugate, we would get \\(z_i z_i = z_i^2\\), which can be complex or negative.\nExample:\n\\[\nz = \\begin{bmatrix}1 + i \\\\ 2 - i\\end{bmatrix}\n\\]\n\\[\n\\|z\\|^2 = (1-i)(1+i) + (2+i)(2-i) = (1 + 1) + (4 + 1) = 2 + 5 = 7\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html#hermitian-matrices",
    "href": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html#hermitian-matrices",
    "title": "MIT 18.06 Lecture 26: Complex Matrices and Fast Fourier Transform",
    "section": "Hermitian Matrices",
    "text": "Hermitian Matrices\n\nDefinition\nA matrix \\(A\\) is Hermitian if it equals its conjugate transpose:\n\\[\nA^H = \\bar{A}^{\\top} = A\n\\]\nwhere \\(A^H\\) denotes the Hermitian transpose (also called conjugate transpose).\nHermitian matrices are the complex analog of symmetric matrices.\n\n\nExample\n\\[\nA = \\begin{bmatrix}2 & 3+i \\\\ 3-i & 5\\end{bmatrix}\n\\]\nCheck:\n\\[\nA^H = \\overline{A^{\\top}} = \\overline{\\begin{bmatrix}2 & 3-i \\\\ 3+i & 5\\end{bmatrix}} = \\begin{bmatrix}2 & 3+i \\\\ 3-i & 5\\end{bmatrix} = A\n\\]\nKey property: The diagonal entries of a Hermitian matrix must be real, because \\(\\bar{a}_{ii} = a_{ii}\\) implies \\(a_{ii} \\in \\mathbb{R}\\).\n\n\nProperties of Hermitian Matrices\nJust like real symmetric matrices, Hermitian matrices have: - Real eigenvalues - Orthogonal (unitary) eigenvectors - Spectral decomposition: \\(A = Q\\Lambda Q^H\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html#unitary-matrices",
    "href": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html#unitary-matrices",
    "title": "MIT 18.06 Lecture 26: Complex Matrices and Fast Fourier Transform",
    "section": "Unitary Matrices",
    "text": "Unitary Matrices\n\nDefinition\nA matrix \\(Q\\) is unitary if its conjugate transpose equals its inverse:\n\\[\nQ^H = Q^{-1}\n\\]\nor equivalently:\n\\[\nQ^HQ = I\n\\]\nUnitary matrices are the complex analog of orthogonal matrices.\n\n\nProperties\n\nColumns are orthonormal under the complex inner product: \\(q_i^H q_j = \\delta_{ij}\\)\nPreserve length: \\(\\|Qx\\| = \\|x\\|\\) for all \\(x\\)\nPreserve angles: The inner product \\(\\langle Qx, Qy \\rangle = \\langle x, y \\rangle\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html#the-fourier-matrix",
    "href": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html#the-fourier-matrix",
    "title": "MIT 18.06 Lecture 26: Complex Matrices and Fast Fourier Transform",
    "section": "The Fourier Matrix",
    "text": "The Fourier Matrix\nThe Fourier matrix \\(F_n\\) is one of the most important matrices in applied mathematics.\n\nGeneral Form\n\\[\nF_n = \\begin{bmatrix}\n1 & 1 & 1 & \\cdots & 1 \\\\\n1 & w & w^2 & \\cdots & w^{n-1} \\\\\n1 & w^2 & w^4 & \\cdots & w^{2(n-1)} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & w^{n-1} & w^{2(n-1)} & \\cdots & w^{(n-1)^2}\n\\end{bmatrix}\n\\]\nwhere:\n\\[\nw = e^{-i\\frac{2\\pi}{n}}\n\\]\nThe general entry is:\n\\[\n(F_n)_{i,j} = w^{ij}, \\quad i,j = 0, 1, \\ldots, n-1\n\\]\n\n\nThe nth Roots of Unity\nThe key to the Fourier matrix is \\(w = e^{-i\\frac{2\\pi}{n}}\\), which is a primitive nth root of unity:\n\\[\nw^n = e^{-i2\\pi} = 1\n\\]\nGeometric interpretation: The powers \\(\\{1, w, w^2, \\ldots, w^{n-1}\\}\\) are equally spaced points on the unit circle in the complex plane, rotating clockwise at angles \\(-\\frac{2\\pi k}{n}\\) for \\(k = 0, 1, \\ldots, n-1\\).\n\n\n\nRoots of Unity\n\n\nThe six points shown here are the 6th roots of unity‚Äîthe complex numbers whose 6th power equals 1. They sit on the unit circle at angles \\(2\\pi k / 6\\). For the DFT, we use \\(w = e^{-i2\\pi/n}\\), which rotates clockwise. These rotations are exactly the numbers used as entries of the Fourier matrix \\(F_n\\). The DFT is built from these unit roots: every row is a different power of the same rotation \\(w_n = e^{-i2\\pi/n}\\). This is the algebraic heart of the Fast Fourier Transform.\n\n\nExample: \\(F_4\\) (4√ó4 Fourier Matrix)\nWhen \\(n = 4\\):\n\\[\nw = e^{-i\\frac{2\\pi}{4}} = e^{-i\\frac{\\pi}{2}} = \\cos\\left(-\\frac{\\pi}{2}\\right) + i\\sin\\left(-\\frac{\\pi}{2}\\right) = -i\n\\]\nPowers of \\(-i\\): - \\((-i)^0 = 1\\) - \\((-i)^1 = -i\\) - \\((-i)^2 = -1\\) - \\((-i)^3 = i\\) - \\((-i)^4 = 1\\) (cycle repeats)\nThe Fourier matrix becomes:\n\\[\nF_4 = \\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & -i & (-i)^2 & (-i)^3 \\\\\n1 & (-i)^2 & (-i)^4 & (-i)^6 \\\\\n1 & (-i)^3 & (-i)^6 & (-i)^9\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & -i & -1 & i \\\\\n1 & -1 & 1 & -1 \\\\\n1 & i & -1 & -i\n\\end{bmatrix}\n\\]\n\n\nOrthogonality of Columns\nThe columns of the Fourier matrix are orthogonal under the complex inner product.\nVerification: For columns \\(c_i\\) and \\(c_j\\) with \\(i \\neq j\\), we compute:\n\\[\n\\bar{c}_i^{\\top}c_j = \\sum_{k=0}^{n-1} \\overline{w^{ki}} \\cdot w^{kj} = \\sum_{k=0}^{n-1} w^{-ki} \\cdot w^{kj} = \\sum_{k=0}^{n-1} w^{k(j-i)}\n\\]\nThis is a geometric series with ratio \\(w^{j-i}\\) where \\(j - i \\neq 0 \\pmod{n}\\):\n\\[\n\\sum_{k=0}^{n-1} w^{k(j-i)} = \\frac{1 - w^{n(j-i)}}{1 - w^{j-i}} = \\frac{1 - (w^n)^{j-i}}{1 - w^{j-i}} = \\frac{1 - 1}{1 - w^{j-i}} = 0\n\\]\nTherefore, all columns are orthogonal.\n\n\nNormalization\nEach column has length \\(\\sqrt{n}\\) because:\n\\[\n\\|c_i\\|^2 = \\sum_{k=0}^{n-1} |w^{ki}|^2 = \\sum_{k=0}^{n-1} 1 = n\n\\]\nTo make the matrix unitary (orthonormal columns), we normalize:\n\\[\nF_n^{\\text{normalized}} = \\frac{1}{\\sqrt{n}}F_n\n\\]\nFor \\(F_4\\):\n\\[\nF_4 = \\frac{1}{2}\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & -i & -1 & i \\\\\n1 & -1 & 1 & -1 \\\\\n1 & i & -1 & -i\n\\end{bmatrix}\n\\]\n\n\nInverse of the Fourier Matrix\nSince the normalized Fourier matrix is unitary:\n\\[\nF_n^H F_n = I\n\\]\nTherefore:\n\\[\nF_n^{-1} = F_n^H\n\\]\nKey insight: The inverse Fourier transform is just the conjugate transpose, which corresponds to replacing \\(w\\) with \\(\\bar{w} = w^{-1}\\) (rotating in the opposite direction)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html#discrete-fourier-transform-dft",
    "href": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html#discrete-fourier-transform-dft",
    "title": "MIT 18.06 Lecture 26: Complex Matrices and Fast Fourier Transform",
    "section": "Discrete Fourier Transform (DFT)",
    "text": "Discrete Fourier Transform (DFT)\n\nDefinition\nGiven a vector \\(x = (x_0, x_1, x_2, \\ldots, x_{N-1})\\), the Discrete Fourier Transform computes:\n\\[\ny_k = \\sum_{n=0}^{N-1} x_n e^{-i2\\pi kn / N}, \\quad k = 0, 1, \\ldots, N-1\n\\]\nMatrix form:\n\\[\ny = F_N x\n\\]\nComplexity: Direct computation requires \\(O(N^2)\\) operations (each of \\(N\\) output values requires summing \\(N\\) terms).\n\n\nWhy DFT Matters\nThe DFT decomposes a signal into its frequency components: - \\(y_0\\) = DC component (average value) - \\(y_k\\) = amplitude and phase of frequency \\(k/N\\)\nThis is fundamental in signal processing, audio compression, image processing, and solving PDEs."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html#fast-fourier-transform-fft",
    "href": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html#fast-fourier-transform-fft",
    "title": "MIT 18.06 Lecture 26: Complex Matrices and Fast Fourier Transform",
    "section": "Fast Fourier Transform (FFT)",
    "text": "Fast Fourier Transform (FFT)\nThe Fast Fourier Transform is an algorithm that computes the DFT in \\(O(N\\log N)\\) time instead of \\(O(N^2)\\).\n\nKey Connection: Squaring Roots of Unity\nIf we square \\(w_n = e^{i2\\pi/n}\\), we get \\(w_{n/2}\\):\n\\[\nw_n^2 = e^{i2\\pi/n} \\cdot e^{i2\\pi/n} = e^{i4\\pi/n} = e^{i2\\pi/(n/2)} = w_{n/2}\n\\]\nExamples: - \\(w_{64}^2 = w_{32}\\) - \\(w_{32}^2 = w_{16}\\) - \\(w_{16}^2 = w_8\\) - \\(w_8^2 = w_4\\) - \\(w_4^2 = w_2\\) - \\(w_2^2 = w_1 = 1\\)\nThis recursive relationship is the foundation of the FFT.\n\n\nEven-Odd Split\nFor an \\(N\\)-point DFT, split the input into even-indexed and odd-indexed elements.\nExample with \\(N = 8\\):\nOriginal DFT:\n\\[\nY_k = \\sum_{n=0}^{7} x_n e^{-i2\\pi kn/8}\n\\]\nSplit into even and odd:\n\\[\nY_k = \\sum_{m=0}^{3} x_{2m} e^{-i2\\pi k(2m)/8} + \\sum_{m=0}^{3} x_{2m+1} e^{-i2\\pi k(2m+1)/8}\n\\]\nSimplify even terms:\n\\[\nE_k = \\sum_{m=0}^{3} x_{2m} e^{-i2\\pi km/4}\n\\]\nThis is a 4-point DFT of the even-indexed elements.\nSimplify odd terms:\n\\[\n\\sum_{m=0}^{3} x_{2m+1} e^{-i2\\pi k(2m+1)/8} = e^{-i2\\pi k/8} \\sum_{m=0}^{3} x_{2m+1} e^{-i2\\pi km/4}\n\\]\n\\[\nO_k = \\sum_{m=0}^{3} x_{2m+1} e^{-i2\\pi km/4}\n\\]\nThis is a 4-point DFT of the odd-indexed elements.\nCombine:\n\\[\nY_k = E_k + e^{-i2\\pi k/N} O_k\n\\]\n\n\nThe Second Half: \\(Y_{k+N/2}\\)\nFor the second half of outputs, we use the periodicity and symmetry:\n\\[\nY_{k+N/2} = E_k - e^{-i2\\pi k/N} O_k\n\\]\nWhy the sign change?\n\\[\ne^{-i2\\pi(k+N/2)/N} = e^{-i2\\pi k/N} \\cdot e^{-i\\pi} = -e^{-i2\\pi k/N}\n\\]\nKey insight: The even part \\(E_k\\) is periodic with period \\(N/2\\), so \\(E_{k+N/2} = E_k\\). The odd part picks up a minus sign due to the rotation by \\(\\pi\\).\n\n\nMatrix Factorization\nThe FFT can be expressed as a matrix factorization:\n\\[\nF_N = \\begin{bmatrix}I & D \\\\ I & -D\\end{bmatrix}\\begin{bmatrix}F_{N/2} & 0 \\\\ 0 & F_{N/2}\\end{bmatrix}P\n\\]\nwhere: - \\(P\\) is a permutation matrix that groups even and odd indices together - The block diagonal contains two smaller Fourier transforms - \\(D\\) is a diagonal matrix of twiddle factors: \\(D = \\text{diag}(1, e^{-i2\\pi/N}, e^{-i4\\pi/N}, \\ldots, e^{-i2\\pi(N/2-1)/N})\\) - The first matrix combines the results: top half is \\(E_k + D \\cdot O_k\\), bottom half is \\(E_k - D \\cdot O_k\\)\n\n\nPermutation Matrix\nThe permutation matrix \\(P\\) reorders the input to group even and odd indices:\n\\[\nx = \\begin{bmatrix}x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7\\end{bmatrix}\n\\]\n\\[\nPx = \\begin{bmatrix}x_0, x_2, x_4, x_6, x_1, x_3, x_5, x_7\\end{bmatrix}\n\\]\n\n\nRecursive Structure\nEach \\(F_{N/2}\\) can be further split into two \\(F_{N/4}\\) transforms, and so on:\n\\[\nF_{64} = \\begin{bmatrix}I & D \\\\ I & -D\\end{bmatrix}\\begin{bmatrix}F_{32} & 0 \\\\ 0 & F_{32}\\end{bmatrix}P\n\\]\n\\[\nF_{32} = \\begin{bmatrix}I & D \\\\ I & -D\\end{bmatrix}\\begin{bmatrix}F_{16} & 0 \\\\ 0 & F_{16}\\end{bmatrix}P\n\\]\n\\[\nF_{16} = \\begin{bmatrix}I & D \\\\ I & -D\\end{bmatrix}\\begin{bmatrix}F_8 & 0 \\\\ 0 & F_8\\end{bmatrix}P\n\\]\n\\[\nF_8 = \\begin{bmatrix}I & D \\\\ I & -D\\end{bmatrix}\\begin{bmatrix}F_4 & 0 \\\\ 0 & F_4\\end{bmatrix}P\n\\]\n\\[\nF_4 = \\begin{bmatrix}I & D \\\\ I & -D\\end{bmatrix}\\begin{bmatrix}F_2 & 0 \\\\ 0 & F_2\\end{bmatrix}P\n\\]\n\n\nComplexity Analysis\nRecurrence relation:\n\\[\nT(N) = 2T(N/2) + O(N)\n\\]\nwhere: - \\(2T(N/2)\\) comes from two smaller FFTs - \\(O(N)\\) comes from the permutation, multiplication by twiddle factors, and combining results\nSolution:\n\\[\nT(N) = O(N\\log N)\n\\]\nWhy? There are \\(\\log_2 N\\) levels of recursion, and each level performs \\(O(N)\\) operations.\nComparison: - Direct DFT: \\(O(N^2)\\) operations - FFT: \\(O(N\\log N)\\) operations - For \\(N = 1024\\): FFT is about 100 times faster - For \\(N = 1{,}000{,}000\\): FFT is about 50,000 times faster"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html#summary",
    "title": "MIT 18.06 Lecture 26: Complex Matrices and Fast Fourier Transform",
    "section": "Summary",
    "text": "Summary\nThis lecture introduced complex linear algebra and the Fast Fourier Transform:\n\nComplex vectors: Inner product uses conjugate transpose \\(\\bar{z}^{\\top}w\\)\nHermitian matrices: \\(A^H = A\\) (complex analog of symmetric)\nUnitary matrices: \\(Q^H = Q^{-1}\\) (complex analog of orthogonal)\nFourier matrix: Built from nth roots of unity, with orthogonal columns\nDFT: Transforms signals into frequency domain in \\(O(N^2)\\) time\nFFT: Exploits recursive structure to achieve \\(O(N\\log N)\\) complexity through even-odd splitting and twiddle factors\n\nThe FFT is one of the most important algorithms in computational science, enabling real-time signal processing, fast convolution, and efficient solutions to PDEs. Its \\(O(N\\log N)\\) complexity has made digital signal processing practical across countless applications."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture23-differential-equations.html",
    "href": "Math/MIT18.06/mit1806-lecture23-differential-equations.html",
    "title": "MIT 18.06 Lecture 23: Differential Equations and exp(At)",
    "section": "",
    "text": "This lecture connects linear algebra to differential equations, showing how eigenvalues and eigenvectors provide powerful tools for solving systems of differential equations. The key insight is that matrix exponentials \\(e^{At}\\) behave analogously to scalar exponentials \\(e^{at}\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#differential-equations-the-foundation",
    "href": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#differential-equations-the-foundation",
    "title": "MIT 18.06 Lecture 23: Differential Equations and exp(At)",
    "section": "Differential Equations: The Foundation",
    "text": "Differential Equations: The Foundation\n\nBasic Theorem\nThe exponential function has a remarkable property:\n\\[\n\\frac{d}{dx}e^x = e^x\n\\]\nThis can also be expressed as a limit:\n\\[\ne^x = \\lim_{n\\to\\infty} \\left(1 + \\frac{x}{n}\\right)^n\n\\]\n\n\nCalculus Rules\nTwo essential rules for differentiation:\n\nConstant multiplication: \\(\\frac{d}{dt}(c \\cdot f(t)) = c \\cdot \\frac{d}{dt}f(t)\\)\nChain rule: \\(\\frac{d}{dt} f(g(t)) = f'(g(t)) \\cdot g'(t)\\)\n\n\n\nConstant-Coefficient Linear Equations\nWhen the coefficients in a differential equation do not depend on \\(t\\), we have a constant-coefficient linear equation.\nFor systems, if the matrix \\(A\\) in \\(\\frac{du}{dt} = Au\\) is constant (not a function of \\(t\\)), we call it a constant-coefficient linear system:\n\\[\n\\frac{du}{dt} = Au\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#example-1-scalar-differential-equation",
    "href": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#example-1-scalar-differential-equation",
    "title": "MIT 18.06 Lecture 23: Differential Equations and exp(At)",
    "section": "Example 1: Scalar Differential Equation",
    "text": "Example 1: Scalar Differential Equation\nConsider the simple equation \\(\\frac{du}{dt} = 3u\\).\nSolution method using separation of variables:\n\\[\ndu = 3u\\,dt\n\\]\n\\[\n\\frac{du}{u} = 3\\,dt\n\\]\nIntegrating both sides using \\(\\int f'(x)\\,dx = f(x) + C\\):\n\nLeft side: Since the derivative of \\(\\ln(u)\\) is \\(\\frac{1}{u}\\), the integral of \\(\\frac{1}{u}du\\) is \\(\\ln|u| + C_1\\)\nRight side: The derivative of \\(3t\\) is \\(3\\), so the integral is \\(3t + C_2\\)\n\nThis gives us:\n\\[\n\\ln|u| = 3t + C\n\\]\nEliminating the logarithm:\n\\[\ne^{\\ln|u|} = e^{3t + C} \\Rightarrow |u| = e^C e^{3t}\n\\]\n\\[\nu = C'' e^{3t}\n\\]\nwhere \\(C''\\) absorbs the sign and constant \\(e^C\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#example-2-system-of-differential-equations",
    "href": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#example-2-system-of-differential-equations",
    "title": "MIT 18.06 Lecture 23: Differential Equations and exp(At)",
    "section": "Example 2: System of Differential Equations",
    "text": "Example 2: System of Differential Equations\nConsider the system:\n\\[\n\\frac{du_1}{dt} = -u_1 + 2u_2\n\\]\n\\[\n\\frac{du_2}{dt} = u_1 - 2u_2\n\\]\nIn matrix form:\n\\[\n\\frac{du}{dt} = Au \\quad \\text{where} \\quad A = \\begin{bmatrix}-1 & 2 \\\\ 1 & -2\\end{bmatrix}\n\\]\n\nFinding Eigenvalues and Eigenvectors\nObserve that:\n\nThe matrix is singular (columns are linearly dependent)\nTherefore, \\(\\lambda_1 = 0\\)\nFrom the trace: \\(\\operatorname{tr}(A) = -1 + (-2) = -3 = \\lambda_1 + \\lambda_2\\), so \\(\\lambda_2 = -3\\)\n\nThe eigenvectors are:\n\n\\(x_1 = \\begin{bmatrix}2 \\\\ 1\\end{bmatrix}\\) for \\(\\lambda_1 = 0\\)\n\\(x_2 = \\begin{bmatrix}-1 \\\\ 1\\end{bmatrix}\\) for \\(\\lambda_2 = -3\\)\n\n\n\nGeneral Solution\nThe general solution is a linear combination of exponential solutions:\n\\[\nu(t) = c_1 e^{\\lambda_1 t} x_1 + c_2 e^{\\lambda_2 t} x_2 = c_1 e^0 x_1 + c_2 e^{-3t} x_2\n\\]\n\\[\nu(t) = c_1 \\begin{bmatrix}2 \\\\ 1\\end{bmatrix} + c_2 e^{-3t} \\begin{bmatrix}-1 \\\\ 1\\end{bmatrix}\n\\]\n\n\nVerification\nLet‚Äôs verify that \\(u(t) = e^{\\lambda_1 t} x_1\\) is indeed a solution:\n\\[\n\\frac{du}{dt} = \\frac{d}{dt}(e^{\\lambda_1 t} x_1) = \\lambda_1 e^{\\lambda_1 t} x_1\n\\]\nThis must equal \\(Au = A(e^{\\lambda_1 t} x_1)\\):\n\\[\n\\lambda_1 \\cancel{e^{\\lambda_1 t}} x_1 = A \\cancel{e^{\\lambda_1 t}} x_1\n\\]\n\\[\n\\lambda_1 x_1 = Ax_1\n\\]\nThis is exactly the eigenvalue equation!\nBreaking down the derivative step:\n\n\\(\\frac{d}{dt}(e^{\\lambda_1 t} x_1) = x_1 \\frac{d}{dt}(e^{\\lambda_1 t})\\) (since \\(x_1\\) is constant)\n\\(\\frac{d}{dt}(e^{\\lambda_1 t}) = \\lambda_1 e^{\\lambda_1 t}\\) (chain rule: \\(\\frac{d}{dt} f(g(t)) = f'(g(t)) \\cdot g'(t)\\))\n\n\n\nConnection to Discrete Systems\nThe continuous solution:\n\\[\nu(t) = c_1 e^{\\lambda_1 t} x_1 + c_2 e^{\\lambda_2 t} x_2\n\\]\ncorresponds to the discrete version:\n\\[\nu_k \\approx c_1 \\lambda_1^k x_1 + c_2 \\lambda_2^k x_2\n\\]\nThis shows the deep connection between continuous and discrete dynamical systems.\n\n\nSolving for Constants \\(c_1\\) and \\(c_2\\)\nGiven the initial condition \\(u(0) = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\\) at \\(t = 0\\):\n\\[\nc_1 \\begin{bmatrix}2 \\\\ 1\\end{bmatrix} + c_2 \\begin{bmatrix}-1 \\\\ 1\\end{bmatrix} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\n\\]\nThis gives us:\n\n\\(2c_1 - c_2 = 1\\)\n\\(c_1 + c_2 = 0\\)\n\nSolving: \\(c_1 = c_2 = \\frac{1}{3}\\)\n\n\nSteady State\nAs \\(t \\to \\infty\\), the term \\(e^{-3t} \\to 0\\), leaving only:\n\\[\nu(\\infty) = \\frac{1}{3} \\begin{bmatrix}2 \\\\ 1\\end{bmatrix}\n\\]\nThis steady state corresponds to the eigenvector with eigenvalue \\(\\lambda = 0\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#information-from-eigenvalues",
    "href": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#information-from-eigenvalues",
    "title": "MIT 18.06 Lecture 23: Differential Equations and exp(At)",
    "section": "Information from Eigenvalues",
    "text": "Information from Eigenvalues\nEigenvalues tell us everything about the long-term behavior:\n\nStability: \\(u(t) \\to 0\\)\n\nCondition: All \\(\\lambda_i &lt; 0\\) (all eigenvalues have negative real parts)\nThe solution decays exponentially to zero\n\n\n\nSteady State: \\(u(t) \\to\\) constant\n\nCondition: One \\(\\lambda_i = 0\\) and all other \\(\\lambda_j &lt; 0\\)\nThe solution approaches a non-zero constant\n\n\n\nBlow Up: \\(u(t) \\to \\infty\\)\n\nCondition: Any \\(\\lambda_i &gt; 0\\) (at least one positive eigenvalue)\nThe solution grows exponentially\n\n\n\nExample: 2√ó2 Stability Criterion\nFor a \\(2 \\times 2\\) matrix to be stable (both eigenvalues negative):\n\nTrace &lt; 0: \\(\\operatorname{tr}(A) = \\lambda_1 + \\lambda_2 &lt; 0\\)\nDeterminant &gt; 0: \\(\\det(A) = \\lambda_1 \\lambda_2 &gt; 0\\)\n\nThese two conditions guarantee both eigenvalues are negative."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#diagonalization-and-the-change-of-variables",
    "href": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#diagonalization-and-the-change-of-variables",
    "title": "MIT 18.06 Lecture 23: Differential Equations and exp(At)",
    "section": "Diagonalization and the Change of Variables",
    "text": "Diagonalization and the Change of Variables\nWe can simplify the system by introducing the eigenvector matrix \\(S\\). Since \\(u = Sv\\), we have:\n\\[\n\\frac{du}{dt} = Au = ASv\n\\]\nMultiplying both sides by \\(S^{-1}\\):\n\\[\nS\\frac{dv}{dt} = ASv\n\\]\n\\[\n\\frac{dv}{dt} = S^{-1}ASv = \\Lambda v\n\\]\nwhere \\(\\Lambda\\) is the diagonal matrix of eigenvalues.\n\nSolution in Eigenvector Coordinates\nUsing \\(\\frac{d}{dt}e^x = e^x\\), the solution in \\(v\\)-coordinates is:\n\\[\nv(t) = e^{\\Lambda t} v_0\n\\]\nTransforming back to \\(u\\)-coordinates:\n\\[\nu(t) = S v(t) = S e^{\\Lambda t} v_0\n\\]\nSince \\(v_0 = S^{-1}u(0)\\):\n\\[\nu(t) = S e^{\\Lambda t} S^{-1} u(0)\n\\]\nThis reveals the fundamental formula:\n\\[\ne^{At} = S e^{\\Lambda t} S^{-1}\n\\]\n\\[\nu(t) = e^{At} u(0)\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#matrix-exponential-eat",
    "href": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#matrix-exponential-eat",
    "title": "MIT 18.06 Lecture 23: Differential Equations and exp(At)",
    "section": "Matrix Exponential \\(e^{At}\\)",
    "text": "Matrix Exponential \\(e^{At}\\)\nThe matrix exponential extends the scalar exponential to matrices.\n\nTaylor Series Definition\nJust as \\(e^x = \\sum_{n=0}^\\infty \\frac{x^n}{n!}\\), we define:\n\\[\ne^{At} = I + At + \\frac{(At)^2}{2!} + \\frac{(At)^3}{3!} + \\cdots + \\frac{(At)^n}{n!} + \\cdots\n\\]\n\n\nAlternative Series\nSimilarly to \\(\\frac{1}{1-x} = \\sum_{n=0}^\\infty x^n\\), we have:\n\\[\n(I - At)^{-1} = I + At + (At)^2 + (At)^3 + \\cdots\n\\]\n(valid for small \\(t\\))\n\n\nConnection to Diagonalization\nExpanding \\(e^{At}\\) using \\(A = S\\Lambda S^{-1}\\):\n\\[\ne^{At} = I + At + \\frac{(At)^2}{2!} + \\cdots + \\frac{(At)^n}{n!} + \\cdots\n\\]\n\\[\n= SS^{-1} + S\\Lambda S^{-1}t + \\frac{S\\Lambda^2 S^{-1} t^2}{2!} + \\cdots + \\frac{S\\Lambda^n S^{-1} t^n}{n!} + \\cdots\n\\]\nFactoring out \\(S\\) and \\(S^{-1}\\):\n\\[\n= S\\left(I + \\Lambda t + \\frac{\\Lambda^2 t^2}{2!} + \\cdots + \\frac{\\Lambda^n t^n}{n!} + \\cdots\\right)S^{-1}\n\\]\n\\[\n= S e^{\\Lambda t} S^{-1}\n\\]\nThis shows the power of diagonalization: to compute \\(e^{At}\\), we only need to compute \\(e^{\\Lambda t}\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#exponential-of-a-diagonal-matrix",
    "href": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#exponential-of-a-diagonal-matrix",
    "title": "MIT 18.06 Lecture 23: Differential Equations and exp(At)",
    "section": "Exponential of a Diagonal Matrix",
    "text": "Exponential of a Diagonal Matrix\nFor a diagonal matrix, the exponential is simple:\n\\[\ne^{\\Lambda t} = \\begin{bmatrix}\ne^{\\lambda_1 t} & 0 & \\cdots & 0 \\\\\n0 & e^{\\lambda_2 t} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & e^{\\lambda_n t}\n\\end{bmatrix}\n\\]\nEach diagonal entry is simply the exponential of the corresponding eigenvalue times \\(t\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#converting-higher-order-to-first-order-systems",
    "href": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#converting-higher-order-to-first-order-systems",
    "title": "MIT 18.06 Lecture 23: Differential Equations and exp(At)",
    "section": "Converting Higher-Order to First-Order Systems",
    "text": "Converting Higher-Order to First-Order Systems\n\nExample: Second-Order Equation\nConsider the second-order differential equation:\n\\[\ny'' + by' + ky = 0\n\\]\nWe can convert this to a first-order system by defining:\n\\[\nu = \\begin{bmatrix}y' \\\\ y\\end{bmatrix}\n\\]\nThen:\n\\[\nu' = \\begin{bmatrix}y'' \\\\ y'\\end{bmatrix} = \\begin{bmatrix}-by' - ky \\\\ y'\\end{bmatrix} = \\begin{bmatrix}-b & -k \\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix}y' \\\\ y\\end{bmatrix}\n\\]\nGeneral principle: We can convert any \\(n\\)-th order differential equation into an \\(n \\times n\\) first-order system. For example, a 5th-order equation becomes a \\(5 \\times 5\\) first-order system.\nThis technique allows us to use the powerful machinery of linear algebra (eigenvalues, eigenvectors, matrix exponentials) to solve high-order differential equations."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture23-differential-equations.html#summary",
    "title": "MIT 18.06 Lecture 23: Differential Equations and exp(At)",
    "section": "Summary",
    "text": "Summary\nThe key insights of this lecture:\n\nEigenvalues determine stability: Negative eigenvalues ‚Üí decay, zero eigenvalue ‚Üí steady state, positive eigenvalue ‚Üí blow up\nMatrix exponentials solve systems: \\(u(t) = e^{At}u(0)\\) is the solution to \\(\\frac{du}{dt} = Au\\)\nDiagonalization simplifies computation: \\(e^{At} = Se^{\\Lambda t}S^{-1}\\), where \\(e^{\\Lambda t}\\) is trivial to compute\nHigher-order reduces to first-order: Any differential equation can be converted to a first-order system\n\nThese connections between linear algebra and differential equations are fundamental to understanding dynamical systems in physics, engineering, and applied mathematics."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html",
    "href": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html",
    "title": "Lecture 33: Left and Right Inverse; Pseudo-inverse",
    "section": "",
    "text": "This lecture explores inverses for non-square and singular matrices:\n\nTwo-sided inverse for square full-rank matrices\nWhy rectangular matrices cannot have two-sided inverses\nLeft inverse for full column rank matrices\nRight inverse for full row rank matrices\nPseudo-inverse using SVD for singular matrices"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#overview",
    "href": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#overview",
    "title": "Lecture 33: Left and Right Inverse; Pseudo-inverse",
    "section": "",
    "text": "This lecture explores inverses for non-square and singular matrices:\n\nTwo-sided inverse for square full-rank matrices\nWhy rectangular matrices cannot have two-sided inverses\nLeft inverse for full column rank matrices\nRight inverse for full row rank matrices\nPseudo-inverse using SVD for singular matrices"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#two-sided-inverse",
    "href": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#two-sided-inverse",
    "title": "Lecture 33: Left and Right Inverse; Pseudo-inverse",
    "section": "1. Two-Sided Inverse",
    "text": "1. Two-Sided Inverse\nFor a square matrix with full rank, we have a two-sided inverse:\n\\[\nAA^{-1} = I = A^{-1}A\n\\]\nRequirements: \\(r = m = n\\) (A is full rank)\nUniqueness: The two-sided inverse is unique when it exists."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#rectangular-matrices",
    "href": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#rectangular-matrices",
    "title": "Lecture 33: Left and Right Inverse; Pseudo-inverse",
    "section": "2. Rectangular Matrices",
    "text": "2. Rectangular Matrices\nKey fact: Rectangular matrices cannot have a two-sided inverse.\nReason: Rectangular matrices must have either a nullspace or a left nullspace (or both), preventing a true two-sided inverse from existing.\n Figure: Rectangular matrices have one-sided inverses depending on their rank - full column rank matrices have left inverses, while full row rank matrices have right inverses.\nIntuition: - If \\(m &gt; n\\) (tall matrix): nullspace of \\(A^T\\) is non-empty - If \\(m &lt; n\\) (wide matrix): nullspace of \\(A\\) is non-empty - Either way, information is lost in one direction"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#left-inverse",
    "href": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#left-inverse",
    "title": "Lecture 33: Left and Right Inverse; Pseudo-inverse",
    "section": "3. Left Inverse",
    "text": "3. Left Inverse\nFor a full column rank matrix (\\(r = n &lt; m\\)):\n\nProperties\n\nRank: \\(r = n\\) (full column rank)\nNullspace: \\(\\text{null}(A) = \\{0\\}\\) (only the zero vector)\nColumns: All columns are independent\nSolutions to \\(Ax = b\\): Either 0 or 1 solution (never infinite solutions)\n\\(A^T A\\): Invertible (\\(n \\times n\\) matrix)\n\n\n\nLeft Inverse Formula\n\\[\n(A^T A)^{-1} A^T A = I\n\\]\nTherefore:\n\\[\nA_{\\text{left}}^{-1} = (A^T A)^{-1} A^T\n\\]\nVerification:\n\\[\nA_{\\text{left}}^{-1} A = (A^T A)^{-1} A^T A = I_n\n\\]\nNote: We get \\(I_n\\) (the \\(n \\times n\\) identity), not \\(I_m\\).\n\n\nWhy \\(A^T A\\) Is Invertible\nIf \\((A^T A)x = 0\\), then:\n\\[\nx^T (A^T A) x = 0 \\implies (Ax)^T (Ax) = 0 \\implies \\|Ax\\|^2 = 0\n\\]\nSince \\(A\\) has full column rank, \\(Ax = 0\\) only when \\(x = 0\\).\nTherefore, \\(A^T A\\) has trivial nullspace and is invertible."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#right-inverse",
    "href": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#right-inverse",
    "title": "Lecture 33: Left and Right Inverse; Pseudo-inverse",
    "section": "4. Right Inverse",
    "text": "4. Right Inverse\nFor a full row rank matrix (\\(r = m &lt; n\\)):\n\nProperties\n\nRank: \\(r = m\\) (full row rank)\nLeft nullspace: \\(\\text{null}(A^T) = \\{0\\}\\)\nPivots: \\(m\\) pivots\nFree variables: \\(n - m\\) free variables\nSolutions to \\(Ax = b\\): Infinite solutions (when solution exists)\n\\(AA^T\\): Invertible (\\(m \\times m\\) matrix)\n\n\n\nRight Inverse Formula\nWe want to find \\(B\\) such that:\n\\[\nAB = I_m\n\\]\nSince \\(AA^T\\) is invertible:\n\\[\nAA^T (AA^T)^{-1} = I_m\n\\]\nTherefore:\n\\[\nA_{\\text{right}}^{-1} = A^T (AA^T)^{-1}\n\\]\nVerification:\n\\[\nA A_{\\text{right}}^{-1} = A A^T (AA^T)^{-1} = I_m\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#revisit-ax-b",
    "href": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#revisit-ax-b",
    "title": "Lecture 33: Left and Right Inverse; Pseudo-inverse",
    "section": "5. Revisit \\(Ax = b\\)",
    "text": "5. Revisit \\(Ax = b\\)\n\nUnderstanding the Mapping\n\nInput: \\(x \\in \\mathbb{R}^n\\)\nOutput: \\(Ax\\) is a linear combination of columns, so \\(Ax \\in \\mathbb{R}^m\\)\nRange: Output is in the column space of \\(A\\)\n\n\n\nWhen Is \\(A\\) Invertible on the Row Space?\nClaim: If \\(x, y\\) are both in the row space and \\(x \\neq y\\), then \\(Ax \\neq Ay\\) when \\(A\\) has full rank.\n\n\nProof\nAssume \\(Ax = Ay\\) for \\(x \\neq y\\) in the row space.\nThen:\n\\[\nA(x - y) = 0\n\\]\nThis means: - \\(x - y\\) is in the nullspace of \\(A\\) - \\(x - y\\) is in the row space (since it‚Äôs a combination of \\(x\\) and \\(y\\))\nBut the nullspace and row space are orthogonal complements, so:\n\\[\nx - y \\in \\text{null}(A) \\cap \\text{row}(A) = \\{0\\}\n\\]\nTherefore \\(x - y = 0\\), which contradicts \\(x \\neq y\\).\nConclusion: \\(A\\) is one-to-one (injective) when restricted to its row space."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#pseudo-inverse",
    "href": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#pseudo-inverse",
    "title": "Lecture 33: Left and Right Inverse; Pseudo-inverse",
    "section": "6. Pseudo-Inverse",
    "text": "6. Pseudo-Inverse\nThe pseudo-inverse provides a general way to ‚Äúinvert‚Äù singular or rectangular matrices.\n\nCases Where Pseudo-Inverse Is Useful\n\n\\(r = n, r &lt; m\\) (full column rank, tall matrix)\n\\(r = m, r &lt; n\\) (full row rank, wide matrix)\n\\(r &lt; m, r &lt; n\\) (rank deficient in both dimensions)\n\n\n\nComputing \\(A^+\\) Using SVD\nFrom the Singular Value Decomposition:\n\\[\nA = U \\Sigma V^T\n\\]\nThe pseudo-inverse is:\n\\[\nA^+ = V \\Sigma^+ U^T\n\\]\nwhere \\(\\Sigma^+\\) is the pseudo-inverse of the diagonal matrix \\(\\Sigma\\).\n\n\nConstructing \\(\\Sigma^+\\)\nIf \\(\\Sigma\\) is an \\(m \\times n\\) diagonal matrix with singular values \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r\\):\n\\[\n\\Sigma = \\begin{bmatrix}\n\\sigma_1 & 0 & 0 & \\cdots & 0 \\\\\n0 & \\sigma_2 & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0\n\\end{bmatrix}_{m \\times n}\n\\]\nThen \\(\\Sigma^+\\) is the \\(n \\times m\\) matrix:\n\\[\n\\Sigma^+ = \\begin{bmatrix}\n\\frac{1}{\\sigma_1} & 0 & 0 & \\cdots & 0 \\\\\n0 & \\frac{1}{\\sigma_2} & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0\n\\end{bmatrix}_{n \\times m}\n\\]\nConstruction rule: - Take reciprocals of non-zero singular values: \\(\\sigma_i \\to \\frac{1}{\\sigma_i}\\) - Keep zeros as zeros - Transpose the shape: \\(m \\times n \\to n \\times m\\)\n\n\nProperties of Pseudo-Inverse\n\nReduces to regular inverse when \\(A\\) is invertible: If \\(A\\) is square and full rank, \\(A^+ = A^{-1}\\)\nReduces to left inverse: If \\(A\\) has full column rank, \\(A^+ = (A^T A)^{-1} A^T\\)\nReduces to right inverse: If \\(A\\) has full row rank, \\(A^+ = A^T (AA^T)^{-1}\\)\nMoore-Penrose conditions: \\(A^+\\) is the unique matrix satisfying:\n\n\\(A A^+ A = A\\)\n\\(A^+ A A^+ = A^+\\)\n\\((A A^+)^T = A A^+\\)\n\\((A^+ A)^T = A^+ A\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture33-left-right-inverse.html#summary",
    "title": "Lecture 33: Left and Right Inverse; Pseudo-inverse",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\n\nMatrix Type\nRank\nInverse Type\nFormula\n\n\n\n\nSquare full rank\n\\(r = m = n\\)\nTwo-sided\n\\(A^{-1}\\)\n\n\nTall full column rank\n\\(r = n &lt; m\\)\nLeft inverse\n\\((A^T A)^{-1} A^T\\)\n\n\nWide full row rank\n\\(r = m &lt; n\\)\nRight inverse\n\\(A^T (AA^T)^{-1}\\)\n\n\nRank deficient\n\\(r &lt; \\min(m,n)\\)\nPseudo-inverse\n\\(V \\Sigma^+ U^T\\)\n\n\n\nKey insights: - Rectangular matrices cannot have two-sided inverses - One-sided inverses exist when the matrix has full rank in one dimension - The pseudo-inverse generalizes all these cases using SVD - The pseudo-inverse always exists, even for singular matrices"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture6-column-null-space.html",
    "href": "Math/MIT18.06/mit1806-lecture6-column-null-space.html",
    "title": "MIT 18.06SC Lecture 6: Column Space and Null Space",
    "section": "",
    "text": "My lecture notes\nColumn space and null space are two fundamental subspaces associated with any matrix. This lecture shows which vectors \\(b\\) make \\(Ax = b\\) solvable and which vectors \\(x\\) satisfy \\(Ax = 0\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#context",
    "href": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#context",
    "title": "MIT 18.06SC Lecture 6: Column Space and Null Space",
    "section": "",
    "text": "My lecture notes\nColumn space and null space are two fundamental subspaces associated with any matrix. This lecture shows which vectors \\(b\\) make \\(Ax = b\\) solvable and which vectors \\(x\\) satisfy \\(Ax = 0\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#subspace-properties",
    "href": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#subspace-properties",
    "title": "MIT 18.06SC Lecture 6: Column Space and Null Space",
    "section": "Subspace Properties",
    "text": "Subspace Properties\n\n\\(P \\cup L\\) is not a subspace (P and L are two subspaces)\n\\(P \\cap L\\) is a subspace"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#column-space",
    "href": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#column-space",
    "title": "MIT 18.06SC Lecture 6: Column Space and Null Space",
    "section": "Column Space",
    "text": "Column Space\n\nDefinition\nGiven a matrix A: \\[\nA = \\begin{bmatrix}\n1 & 1 & 2 \\\\\n2 & 1 & 3 \\\\\n3 & 1 & 4 \\\\\n4 & 1 & 5\n\\end{bmatrix}\n\\]\nThe column space of A consists of all possible linear combinations of the columns of A.\n\n\nKey Observation\nBecause the column space has 4 dimensions (rows) but only 3 columns (subspaces/lines), the entire space cannot be filled. There are many vectors \\(b\\) outside the column space.\nTherefore: We cannot say that for every \\(Ax = b\\), there is a solution.\n\n\nSolutions to \\(Ax = b\\)\n\nSpecial Case: \\(b = \\mathbf{0}\\)\n\n\\(b = [0, 0, 0, 0]\\) always has a solution\nThis is the origin point, and all subspaces pass through the origin\nThe solution is \\(x = [0, 0, 0]\\)\n\n\n\nWhich \\(b\\) Can Be Solved?\nGeneral Rule: \\(Ax = b\\) has a solution if and only if \\(b\\) is in the column space of A.\n\n\nExamples of Solvable \\(b\\):\n\n\\(b = [1, 2, 3, 4]\\) has solution \\(x = [1, 0, 0]\\)\n\nThis is the first column of A\n\n\\(b = [1, 1, 1, 1]\\) has solution \\(x = [0, 1, 0]\\)\n\nThis is the second column of A\n\nAny linear combination of columns has a solution\n\nIf \\(b = c_1 \\cdot \\text{col}_1 + c_2 \\cdot \\text{col}_2 + c_3 \\cdot \\text{col}_3\\)\nThen \\(x = [c_1, c_2, c_3]\\) is the solution"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#null-space",
    "href": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#null-space",
    "title": "MIT 18.06SC Lecture 6: Column Space and Null Space",
    "section": "Null Space",
    "text": "Null Space\n\nDefinition\nThe null space of A, denoted \\(N(A)\\), contains all vectors \\(x\\) that satisfy: \\[\nAx = \\mathbf{0}\n\\]\n\n\nProof: \\(N(A)\\) is a Subspace\nTo prove that the null space is a subspace, we must show it satisfies two properties:\n\n1. Closed Under Addition\nIf \\(v\\) and \\(w\\) are in \\(N(A)\\), then \\(v + w\\) is also in \\(N(A)\\).\nProof: \\[\n\\begin{align}\nAv &= \\mathbf{0} \\\\\nAw &= \\mathbf{0} \\\\\nA(v + w) &= Av + Aw = \\mathbf{0} + \\mathbf{0} = \\mathbf{0}\n\\end{align}\n\\]\nTherefore, \\(v + w \\in N(A)\\).\n\n\n2. Closed Under Scalar Multiplication\nIf \\(x\\) is in \\(N(A)\\) and \\(c\\) is any scalar, then \\(cx\\) is also in \\(N(A)\\).\nProof: \\[\n\\begin{align}\nAx &= \\mathbf{0} \\\\\nA(cx) &= c(Ax) = c \\cdot \\mathbf{0} = \\mathbf{0}\n\\end{align}\n\\]\nTherefore, \\(cx \\in N(A)\\).\n\n\n\nImportant Contrast: When \\(b \\neq \\mathbf{0}\\)\nThe solution set of \\(Ax = b\\) (when \\(b \\neq \\mathbf{0}\\)) is not a subspace.\n\nProof: Not Closed Under Scalar Multiplication\nGiven: \\[\nAx = b, \\quad b \\neq \\mathbf{0}\n\\]\nFor scalar \\(c \\neq 1\\): \\[\nA(cx) = c(Ax) = cb \\neq b\n\\]\nTherefore, if \\(x\\) is a solution, \\(cx\\) is not a solution (unless \\(c = 1\\)).\nConclusion: The solution set fails the scalar multiplication property, so it is not a subspace."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture6-column-null-space.html#summary",
    "title": "MIT 18.06SC Lecture 6: Column Space and Null Space",
    "section": "Summary",
    "text": "Summary\nColumn Space: - Column space = all possible linear combinations of columns - \\(Ax = b\\) is solvable \\(\\Leftrightarrow\\) \\(b\\) is in the column space - Not every \\(b \\in \\mathbb{R}^4\\) is in the column space of this particular A\nNull Space: - Null space \\(N(A) = \\{x : Ax = \\mathbf{0}\\}\\) is a subspace - Solution set of \\(Ax = b\\) (when \\(b \\neq \\mathbf{0}\\)) is not a subspace - The null space always contains the zero vector - The null space is closed under addition and scalar multiplication\n\nSource: MIT 18.06SC Linear Algebra, Lecture 6"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html",
    "href": "Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html",
    "title": "MIT 18.06 Lecture 21: Eigenvalues and Eigenvectors",
    "section": "",
    "text": "An eigenvector of A is a vector such that \\(Ax = \\lambda x\\), meaning A only scales (not rotates) x.\n\\[\nAx \\parallel x\\\\\nAx=\\lambda x\n\\]\nFor a singular matrix, \\(\\lambda=0\\) and x is in the null space."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html#special-matrices",
    "href": "Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html#special-matrices",
    "title": "MIT 18.06 Lecture 21: Eigenvalues and Eigenvectors",
    "section": "Special Matrices",
    "text": "Special Matrices\n\nProjection Matrix\nIf P is the projection matrix onto plane A, then:\n\nAny vector on the plane is an eigenvector of P with eigenvalue 1\nAny vector perpendicular to plane A is also an eigenvector of P with eigenvalue 0\n\n\n\nPermutation Matrix\nFor a permutation matrix, it permutes the items in the vector into another order. Vectors that have the same items remain unchanged after multiplication.\n\\[\nA=\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\n\\]\n\n\\(x_1=\\begin{bmatrix}1\\\\1\\end{bmatrix}\\), \\(\\lambda=1\\)\n\nSince A reverses the direction of the vector, if we choose x with the same magnitudes but opposite signs, we obtain another eigenvector corresponding to eigenvalue \\(-1\\):\n\n\\(x_2=\\begin{bmatrix}-1\\\\1\\end{bmatrix}\\), \\(\\lambda=-1\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html#key-facts",
    "href": "Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html#key-facts",
    "title": "MIT 18.06 Lecture 21: Eigenvalues and Eigenvectors",
    "section": "Key Facts",
    "text": "Key Facts\n\nAny \\(n \\times n\\) matrix has n eigenvalues (counting algebraic multiplicities, possibly complex)\nFor a real symmetric (or Hermitian) matrix, all eigenvalues are real, and the eigenvectors corresponding to distinct eigenvalues are orthogonal\nThe sum of the eigenvalues equals the trace of the matrix:\n\n\\[\n\\operatorname{tr}(A) = \\sum_{i=1}^n \\lambda_i\n\\]\n\nThe product of the eigenvalues equals the determinant of the matrix:\n\n\\[\n\\det(A) = \\prod_{i=1}^n \\lambda_i\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html#solving-axlambda-x",
    "href": "Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html#solving-axlambda-x",
    "title": "MIT 18.06 Lecture 21: Eigenvalues and Eigenvectors",
    "section": "Solving \\(Ax=\\lambda x\\)",
    "text": "Solving \\(Ax=\\lambda x\\)\n\nMethod\nRewrite the eigenvalue equation:\n\\[\n(A-\\lambda I)x=\\mathbf{0}\n\\]\nFor non-trivial solutions to exist, \\(A-\\lambda I\\) must be singular, so x is in the null space. This requires:\n\\[\n\\det (A-\\lambda I)=0\n\\]\nThis is called the characteristic equation.\n\n\nExample\nFind the eigenvalues and eigenvectors of:\n\\[\nA=\\begin{bmatrix}3&1\\\\1&3\\end{bmatrix}\n\\]\nFinding \\(\\lambda\\):\n\\[\n\\det(A-\\lambda I)=\\begin{vmatrix}3-\\lambda & 1\\\\ 1&3-\\lambda\\end{vmatrix}=(3-\\lambda)^2-1=0\n\\]\nSolving: \\((3-\\lambda)^2=1\\), so \\(3-\\lambda=\\pm 1\\)\n\\[\n\\lambda_1=4\\\\\n\\lambda_2=2\n\\]\nFinding eigenvectors:\nFor \\(\\lambda_1=4\\):\n\\[\nA-4I=\\begin{bmatrix}-1&1\\\\1&-1\\end{bmatrix}\\\\\nx_1=\\begin{bmatrix}1\\\\1\\end{bmatrix}\n\\]\nFor \\(\\lambda_2=2\\):\n\\[\nA-2I=\\begin{bmatrix}1&1\\\\1&1\\end{bmatrix}\\\\\nx_2=\\begin{bmatrix}-1\\\\1\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html#effect-of-shifting-the-matrix",
    "href": "Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html#effect-of-shifting-the-matrix",
    "title": "MIT 18.06 Lecture 21: Eigenvalues and Eigenvectors",
    "section": "Effect of Shifting the Matrix",
    "text": "Effect of Shifting the Matrix\nIf we add \\(nI\\) to matrix A, the eigenvectors remain unchanged while the eigenvalues increase by n:\n\\[\n(A+3I)x=Ax+3x=\\lambda x+3x=(\\lambda+3)x\n\\]\nSo the new eigenvalues are \\(\\lambda+3\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html#complex-eigenvalues",
    "href": "Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html#complex-eigenvalues",
    "title": "MIT 18.06 Lecture 21: Eigenvalues and Eigenvectors",
    "section": "Complex Eigenvalues",
    "text": "Complex Eigenvalues\n\nRotation Matrix\nFor a \\(90¬∞\\) rotation matrix:\n\\[\nA=\\begin{bmatrix}0&-1\\\\1&0\\end{bmatrix}\n\\]\nFrom the key facts, we know:\n\\[\n\\lambda_1+\\lambda_2=0\\\\\n\\lambda_1\\lambda_2=1\n\\]\nComputing the characteristic equation:\n\\[\n\\det(A- \\lambda I)=\\begin{vmatrix}-\\lambda &-1\\\\1& - \\lambda \\end{vmatrix}=\\lambda^2+1=0\n\\]\n\\[\n\\lambda_1=i\\\\\n\\lambda_2=-i\n\\]\nBoth eigenvalues are complex numbers, reflecting the fact that rotation cannot be represented by simple scaling along real directions."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html#triangular-matrix",
    "href": "Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html#triangular-matrix",
    "title": "MIT 18.06 Lecture 21: Eigenvalues and Eigenvectors",
    "section": "Triangular Matrix",
    "text": "Triangular Matrix\n\\[\nA=\\begin{bmatrix}3&1\\\\0&3\\end{bmatrix}\n\\]\n\\[\n\\det(A-\\lambda I)=\\begin{vmatrix}3-\\lambda&1\\\\0&3-\\lambda\\end{vmatrix}=(3-\\lambda)^2=0\n\\]\n\\[\n\\lambda_1=\\lambda_2=3\n\\]\nFor a triangular matrix, the determinant of \\(A-\\lambda I\\) is determined by the diagonal only, so the eigenvalues are simply the diagonal entries.\nFinding the eigenvector:\n\\[\n(A-3I)x=\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}x=\\mathbf{0}\n\\]\nSince \\(0x_1+1x_2=x_2=0\\), we must have \\(x_2=0\\) and \\(x_1\\) is free.\n\\[\nx=c\\begin{bmatrix}1\\\\0\\end{bmatrix}\n\\]\nIn this case, we have a repeated eigenvalue \\(\\lambda=3\\), but only one independent eigenvector. This indicates that the matrix is not diagonalizable."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#overview",
    "href": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#overview",
    "title": "MIT 18.06SC Lecture 28: Similar Matrices and Jordan Form",
    "section": "Overview",
    "text": "Overview\nThis lecture covers: - Properties of \\(A^{\\top}A\\) (symmetric, positive semi-definite) - Similar matrices: definition, eigenvalue preservation, and eigenvector transformation - Repeated eigenvalues and the failure of diagonalization - Jordan canonical form for non-diagonalizable matrices - Jordan blocks and their geometric interpretation"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#the-matrix-atopa",
    "href": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#the-matrix-atopa",
    "title": "MIT 18.06SC Lecture 28: Similar Matrices and Jordan Form",
    "section": "1. The Matrix \\(A^{\\top}A\\)",
    "text": "1. The Matrix \\(A^{\\top}A\\)\nFor any \\(m \\times n\\) matrix \\(A\\), the product \\(A^{\\top}A\\) has special properties.\n\nProperties\n\\(A^{\\top}A\\) is:\n\nSquare: \\(n \\times n\\) (even if \\(A\\) is rectangular)\nSymmetric: \\((A^{\\top}A)^{\\top} = A^{\\top}(A^{\\top})^{\\top} = A^{\\top}A\\)\nPositive semi-definite: \\(x^{\\top}A^{\\top}Ax \\geq 0\\) for all \\(x\\)\nPositive definite if \\(A\\) has full column rank\n\n\n\nProof of Positive Semi-definiteness\nFor any vector \\(x \\in \\mathbb{R}^n\\):\n\\[\nx^{\\top}A^{\\top}Ax = (x^{\\top}A^{\\top})(Ax) = (Ax)^{\\top}(Ax) = \\|Ax\\|^2 \\geq 0\n\\]\nSince \\(\\|Ax\\|^2\\) is the squared length of a vector, it‚Äôs always non-negative.\n\n\nWhen Is \\(A^{\\top}A\\) Positive Definite?\n\\(A^{\\top}A\\) is positive definite (strictly \\(&gt; 0\\) for \\(x \\neq 0\\)) if and only if:\n\\[\nx^{\\top}A^{\\top}Ax = \\|Ax\\|^2 &gt; 0 \\quad \\text{for all } x \\neq 0\n\\]\nThis holds when:\n\n\\(A\\) has no nullspace: \\(Ax \\neq 0\\) for all \\(x \\neq 0\\)\nColumns are independent: \\(A\\) has full column rank\nRank of \\(A\\) is \\(n\\): All \\(n\\) columns are linearly independent\n\nKey insight: The Gram matrix \\(A^{\\top}A\\) encodes information about the inner products between columns of \\(A\\). If \\(A = [a_1, a_2, \\ldots, a_n]\\), then:\n\\[\nA^{\\top}A = \\begin{bmatrix}\na_1^{\\top}a_1 & a_1^{\\top}a_2 & \\cdots & a_1^{\\top}a_n \\\\\na_2^{\\top}a_1 & a_2^{\\top}a_2 & \\cdots & a_2^{\\top}a_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_n^{\\top}a_1 & a_n^{\\top}a_2 & \\cdots & a_n^{\\top}a_n\n\\end{bmatrix}\n\\]\nThe diagonal entries are \\(\\|a_i\\|^2\\), and off-diagonal entries measure how aligned the columns are."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#similar-matrices",
    "href": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#similar-matrices",
    "title": "MIT 18.06SC Lecture 28: Similar Matrices and Jordan Form",
    "section": "2. Similar Matrices",
    "text": "2. Similar Matrices\n\nDefinition\nMatrices \\(A\\) and \\(B\\) are similar if there exists an invertible matrix \\(M\\) such that:\n\\[\nB = M^{-1}AM\n\\]\nNotation: \\(A \\sim B\\)\nInterpretation: \\(A\\) and \\(B\\) represent the same linear transformation in different coordinate systems (different bases). The matrix \\(M\\) performs the change of basis.\n\n\nExample 1: Diagonalization\nEvery diagonalizable matrix is similar to a diagonal matrix.\nIf \\(A\\) has eigenvector matrix \\(S\\) and eigenvalue matrix \\(\\Lambda\\), then:\n\\[\nS^{-1}AS = \\Lambda\n\\]\nSo \\(A \\sim \\Lambda\\).\nConcrete example:\n\\[\nA = \\begin{bmatrix}2 & 1 \\\\ 1 & 2\\end{bmatrix}\n\\]\nThe eigenvalues are \\(\\lambda_1 = 3, \\lambda_2 = 1\\), so:\n\\[\n\\Lambda = \\begin{bmatrix}3 & 0 \\\\ 0 & 1\\end{bmatrix}\n\\]\nThus \\(A \\sim \\Lambda\\).\n\n\nExample 2: Different Similarity Transformation\nUsing a different choice of \\(M\\), we can find other matrices similar to \\(A\\).\nLet \\(M = \\begin{bmatrix}1 & 4 \\\\ 0 & 1\\end{bmatrix}\\). Then:\n\\[\nB = M^{-1}AM = \\begin{bmatrix}1 & -4 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}2 & 1 \\\\ 1 & 2\\end{bmatrix} \\begin{bmatrix}1 & 4 \\\\ 0 & 1\\end{bmatrix} = \\begin{bmatrix}-2 & -15 \\\\ 1 & 6\\end{bmatrix}\n\\]\nSo \\(A \\sim B\\), even though \\(B\\) looks very different from \\(A\\).\nKey observation: Similar matrices form equivalence classes. All matrices in the same class represent the same transformation, just in different bases."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#properties-of-similar-matrices",
    "href": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#properties-of-similar-matrices",
    "title": "MIT 18.06SC Lecture 28: Similar Matrices and Jordan Form",
    "section": "3. Properties of Similar Matrices",
    "text": "3. Properties of Similar Matrices\n\nMain Facts\nIf \\(A \\sim B\\) (i.e., \\(B = M^{-1}AM\\)), then:\n\nSame eigenvalues: \\(A\\) and \\(B\\) have identical eigenvalues\nDifferent eigenvectors: The eigenvectors transform as \\(x_B = M^{-1}x_A\\)\nSame trace: \\(\\text{tr}(A) = \\text{tr}(B)\\)\nSame determinant: \\(\\det(A) = \\det(B)\\)\nSame rank: \\(\\text{rank}(A) = \\text{rank}(B)\\)\n\n\n\nProof: Eigenvalues Are Preserved\nSuppose \\(Ax = \\lambda x\\) (so \\(\\lambda\\) is an eigenvalue of \\(A\\) with eigenvector \\(x\\)).\nWe want to show that \\(\\lambda\\) is also an eigenvalue of \\(B = M^{-1}AM\\).\nStep 1: Start with the eigenvalue equation for \\(A\\):\n\\[\nAx = \\lambda x\n\\]\nStep 2: Insert \\(M M^{-1} = I\\) to the left of \\(x\\):\n\\[\nA M M^{-1} x = \\lambda x\n\\]\nStep 3: Multiply both sides by \\(M^{-1}\\) on the left:\n\\[\nM^{-1} A M M^{-1} x = \\lambda M^{-1} x\n\\]\nStep 4: Recognize that \\(M^{-1}AM = B\\):\n\\[\nB (M^{-1} x) = \\lambda (M^{-1} x)\n\\]\nConclusion: \\(\\lambda\\) is an eigenvalue of \\(B\\) with eigenvector \\(M^{-1}x\\).\nKey insight: The eigenvalues stay unchanged, but the eigenvectors transform according to the change of basis:\n\\[\nx_B = M^{-1} x_A\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#repeated-eigenvalues-the-bad-case",
    "href": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#repeated-eigenvalues-the-bad-case",
    "title": "MIT 18.06SC Lecture 28: Similar Matrices and Jordan Form",
    "section": "4. Repeated Eigenvalues: The Bad Case",
    "text": "4. Repeated Eigenvalues: The Bad Case\nWhen a matrix has repeated eigenvalues, we must check whether it has enough eigenvectors to be diagonalizable.\n\nExample: Two Contrasting Matrices\nConsider two matrices with the same eigenvalues \\(\\lambda_1 = \\lambda_2 = 4\\):\n\\[\nA = \\begin{bmatrix}4 & 0 \\\\ 0 & 4\\end{bmatrix}, \\quad B = \\begin{bmatrix}4 & 1 \\\\ 0 & 4\\end{bmatrix}\n\\]\nBoth have characteristic polynomial \\((\\lambda - 4)^2 = 0\\), so both have double eigenvalue \\(\\lambda = 4\\).\n\n\nEigenvectors\nMatrix \\(A\\):\n\\[\n(A - 4I)x = \\begin{bmatrix}0 & 0 \\\\ 0 & 0\\end{bmatrix}x = 0\n\\]\nEvery vector is an eigenvector! The eigenspace is all of \\(\\mathbb{R}^2\\).\nMatrix \\(B\\):\n\\[\n(B - 4I)x = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}x = 0\n\\]\nThis requires \\(x_2 = 0\\), so the eigenvectors are:\n\\[\nx = c\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\n\\]\nThe eigenspace is only 1-dimensional, even though the eigenvalue has algebraic multiplicity 2.\n\n\nClassification\n\nMatrix \\(A\\): ‚ÄúSmall family‚Äù ‚Äî diagonalizable, full set of eigenvectors\nMatrix \\(B\\): ‚ÄúBig family‚Äù ‚Äî non-diagonalizable, deficient in eigenvectors\n\nImportant: \\(A\\) and \\(B\\) are not similar because they have different numbers of independent eigenvectors."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#jordan-canonical-form",
    "href": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#jordan-canonical-form",
    "title": "MIT 18.06SC Lecture 28: Similar Matrices and Jordan Form",
    "section": "5. Jordan Canonical Form",
    "text": "5. Jordan Canonical Form\nWhen a matrix is not diagonalizable, the best we can do is put it in Jordan form.\n\nThe Family of \\(2 \\times 2\\) Matrices with \\(\\lambda_1 = \\lambda_2 = 4\\)\nNon-diagonalizable matrices with double eigenvalue 4:\n\\[\n\\begin{bmatrix}4 & 1 \\\\ 0 & 4\\end{bmatrix}, \\quad \\begin{bmatrix}3 & 1 \\\\ -1 & 5\\end{bmatrix}, \\quad \\ldots\n\\]\nThese all share:\n\nTrace: \\(\\text{tr}(A) = 8\\)\nDeterminant: \\(\\det(A) = 16\\)\nNon-diagonalizable: Only one independent eigenvector\n\nAll matrices in this family are similar to the Jordan block:\n\\[\nJ = \\begin{bmatrix}4 & 1 \\\\ 0 & 4\\end{bmatrix}\n\\]\n\n\nJordan Block\nA Jordan block of size \\(n\\) for eigenvalue \\(\\lambda\\) has the form:\n\\[\nJ_i = \\begin{bmatrix}\n\\lambda_i & 1 & 0 & \\cdots & 0 \\\\\n0 & \\lambda_i & 1 & \\cdots & 0 \\\\\n0 & 0 & \\lambda_i & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & \\lambda_i\n\\end{bmatrix}\n\\]\nKey property: A Jordan block of size \\(n\\) has exactly one eigenvector (corresponding to the repeated eigenvalue \\(\\lambda_i\\)).\nStructure:\n\nMain diagonal: eigenvalue \\(\\lambda_i\\) repeated\nSuperdiagonal: all 1‚Äôs\nEverywhere else: 0‚Äôs"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#case-study-nilpotent-matrices",
    "href": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#case-study-nilpotent-matrices",
    "title": "MIT 18.06SC Lecture 28: Similar Matrices and Jordan Form",
    "section": "6. Case Study: Nilpotent Matrices",
    "text": "6. Case Study: Nilpotent Matrices\nConsider two \\(4 \\times 4\\) nilpotent matrices (all eigenvalues are 0):\n\nMatrix \\(H_1\\)\n\\[\nH_1 = \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nJordan form: This has one Jordan block of size 3 and one block of size 1:\n\\[\nJ_1 = \\begin{bmatrix}\n0 & 1 & 0 & | & 0 \\\\\n0 & 0 & 1 & | & 0 \\\\\n0 & 0 & 0 & | & 0 \\\\\n\\hline\n0 & 0 & 0 & | & 0\n\\end{bmatrix}\n\\]\nEigenspace: The eigenvectors (for \\(\\lambda = 0\\)) are 2-dimensional.\n\n\nMatrix \\(H_2\\)\n\\[\nH_2 = \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nJordan form: This has two Jordan blocks of size 2:\n\\[\nJ_2 = \\begin{bmatrix}\n0 & 1 & | & 0 & 0 \\\\\n0 & 0 & | & 0 & 0 \\\\\n\\hline\n0 & 0 & | & 0 & 1 \\\\\n0 & 0 & | & 0 & 0\n\\end{bmatrix}\n\\]\nEigenspace: The eigenvectors are also 2-dimensional.\n\n\nAre \\(H_1\\) and \\(H_2\\) Similar?\nNo! Even though they have:\n\nSame eigenvalues: \\(\\lambda_1 = \\lambda_2 = \\lambda_3 = \\lambda_4 = 0\\)\nSame dimension of eigenspace: 2\n\nThey have different Jordan block structures, so they are not similar.\nKey insight: Two matrices are similar if and only if they have the same Jordan form (up to reordering of blocks)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#general-jordan-canonical-form",
    "href": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#general-jordan-canonical-form",
    "title": "MIT 18.06SC Lecture 28: Similar Matrices and Jordan Form",
    "section": "7. General Jordan Canonical Form",
    "text": "7. General Jordan Canonical Form\nEvery square matrix \\(A\\) is similar to a unique Jordan form:\n\\[\nJ = \\begin{bmatrix}\nJ_1 & & & \\\\\n& J_2 & & \\\\\n& & \\ddots & \\\\\n& & & J_d\n\\end{bmatrix}\n\\]\nwhere each \\(J_i\\) is a Jordan block.\n\nThe Good Case\nWhen \\(A\\) is diagonalizable, the Jordan form is diagonal:\n\\[\nJ = \\Lambda = \\begin{bmatrix}\n\\lambda_1 & & & \\\\\n& \\lambda_2 & & \\\\\n& & \\ddots & \\\\\n& & & \\lambda_n\n\\end{bmatrix}\n\\]\nThis happens when every eigenvalue‚Äôs geometric multiplicity (dimension of eigenspace) equals its algebraic multiplicity (number of times it appears as a root of the characteristic polynomial).\n\n\nThe Bad Case\nWhen \\(A\\) is not diagonalizable, some Jordan blocks have size greater than 1. These blocks correspond to eigenvalues with deficient eigenspaces.\nExample: If \\(\\lambda = 4\\) has algebraic multiplicity 3 but geometric multiplicity 1, the Jordan form might be:\n\\[\nJ = \\begin{bmatrix}\n4 & 1 & 0 \\\\\n0 & 4 & 1 \\\\\n0 & 0 & 4\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html#summary",
    "title": "MIT 18.06SC Lecture 28: Similar Matrices and Jordan Form",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\nConcept\nKey Idea\n\n\n\n\n\\(A^{\\top}A\\)\nAlways symmetric and positive semi-definite; positive definite if \\(A\\) has full column rank\n\n\nSimilar Matrices\n\\(B = M^{-1}AM\\) represents the same transformation in a different basis\n\n\nEigenvalue Preservation\nSimilar matrices have identical eigenvalues\n\n\nEigenvector Transformation\nEigenvectors transform as \\(x_B = M^{-1}x_A\\)\n\n\nDiagonalizable\n\\(A\\) is diagonalizable if it has \\(n\\) independent eigenvectors\n\n\nJordan Block\n\\(n \\times n\\) block with eigenvalue \\(\\lambda\\) on diagonal, 1‚Äôs on superdiagonal\n\n\nJordan Form\nEvery matrix is similar to a block-diagonal matrix of Jordan blocks\n\n\nGood Case\nJordan form is diagonal \\(\\Leftrightarrow\\) diagonalizable\n\n\n\nMain theorem: Two matrices are similar if and only if they have the same Jordan canonical form (up to reordering of blocks)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture3-multiplication.html",
    "href": "Math/MIT18.06/mit1806-lecture3-multiplication.html",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strang‚Äôs third lecture reveals a profound insight: matrix multiplication isn‚Äôt just one operation‚Äîit‚Äôs five different perspectives on the same computation, each revealing different structural properties. Then we explore matrix inverses and why some matrices fundamentally cannot be inverted."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture3-multiplication.html#context",
    "href": "Math/MIT18.06/mit1806-lecture3-multiplication.html#context",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strang‚Äôs third lecture reveals a profound insight: matrix multiplication isn‚Äôt just one operation‚Äîit‚Äôs five different perspectives on the same computation, each revealing different structural properties. Then we explore matrix inverses and why some matrices fundamentally cannot be inverted."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture3-multiplication.html#the-five-ways-of-matrix-multiplication",
    "href": "Math/MIT18.06/mit1806-lecture3-multiplication.html#the-five-ways-of-matrix-multiplication",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "The Five Ways of Matrix Multiplication",
    "text": "The Five Ways of Matrix Multiplication\nGiven \\(C = AB\\) where \\(A\\) is \\(m \\times n\\) and \\(B\\) is \\(n \\times p\\):\n\n1. Element-by-Element View (Row √ó Column)\nThe standard definition: each entry \\(C_{ij}\\) is the dot product of row \\(i\\) of \\(A\\) with column \\(j\\) of \\(B\\):\n\\[\nC_{ij} = \\sum_{k=1}^n A_{ik} B_{kj}\n\\]\nExample: \\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n=\n\\begin{bmatrix} 1 \\cdot 5+2 \\cdot 7 & 1 \\cdot 6+2 \\cdot 8 \\\\ 3 \\cdot 5+4 \\cdot 7 & 3 \\cdot 6+4 \\cdot 8 \\end{bmatrix}\n=\n\\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n\\]\n\n\n2. Column View\nKey insight: Each column of \\(C\\) is a linear combination of the columns of \\(A\\).\n\\[\nC = A \\begin{bmatrix} | & | & & | \\\\ b_1 & b_2 & \\cdots & b_p \\\\ | & | & & | \\end{bmatrix}\n= \\begin{bmatrix} | & | & & | \\\\ Ab_1 & Ab_2 & \\cdots & Ab_p \\\\ | & | & & | \\end{bmatrix}\n\\]\nInterpretation: Multiply \\(A\\) by each column of \\(B\\) to get each column of \\(C\\).\nThis perspective is crucial for understanding: - The column space of \\(C\\) lies in the column space of \\(A\\) - Matrix-vector multiplication \\(Ax\\) as a linear combination of \\(A\\)‚Äôs columns\n\n\n3. Row View\nKey insight: Each row of \\(C\\) is a linear combination of the rows of \\(B\\).\n\\[\nC = \\begin{bmatrix} ‚Äî & a_1 & ‚Äî \\\\ ‚Äî & a_2 & ‚Äî \\\\ & \\vdots & \\\\ ‚Äî & a_m & ‚Äî \\end{bmatrix} B\n= \\begin{bmatrix} ‚Äî & a_1B & ‚Äî \\\\ ‚Äî & a_2B & ‚Äî \\\\ & \\vdots & \\\\ ‚Äî & a_mB & ‚Äî \\end{bmatrix}\n\\]\nInterpretation: Each row of \\(A\\) multiplies the entire matrix \\(B\\) to produce a row of \\(C\\).\nThis shows that the row space of \\(C\\) lies in the row space of \\(B\\).\n\n\n4. Column √ó Row View (Sum of Rank-1 Matrices)\nThe most powerful perspective:\n\\[\nAB = \\sum_{k=1}^n (\\text{column } k \\text{ of } A) \\times (\\text{row } k \\text{ of } B)\n\\]\nEach term is a rank-1 matrix (outer product of a column vector with a row vector).\nExample: \\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n=\n\\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\\begin{bmatrix} 5 & 6 \\end{bmatrix}\n+\n\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\n\\begin{bmatrix} 7 & 8 \\end{bmatrix}\n\\]\n\\[\n=\n\\begin{bmatrix} 5 & 6 \\\\ 15 & 18 \\end{bmatrix}\n+\n\\begin{bmatrix} 14 & 16 \\\\ 28 & 32 \\end{bmatrix}\n=\n\\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n\\]\nWhy this matters: - Each rank-1 matrix has all rows as multiples of each other - Matrix multiplication is a sum of simple, structured pieces - Foundation for understanding matrix rank and decompositions (SVD, eigendecomposition)\n\n\n5. Block Multiplication\nMatrices can be partitioned into blocks and multiplied block-wise:\n\\[\n\\begin{bmatrix} A_1 & A_2 \\\\ A_3 & A_4 \\end{bmatrix}\n\\begin{bmatrix} B_1 & B_2 \\\\ B_3 & B_4 \\end{bmatrix}\n=\n\\begin{bmatrix} A_1B_1 + A_2B_3 & A_1B_2 + A_2B_4 \\\\ A_3B_1 + A_4B_3 & A_3B_2 + A_4B_4 \\end{bmatrix}\n\\]\nCondition: Block dimensions must be compatible for multiplication.\nApplications: - Efficient computation for sparse or structured matrices - Recursive algorithms for large matrices - Parallel computing strategies"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture3-multiplication.html#matrix-inverse",
    "href": "Math/MIT18.06/mit1806-lecture3-multiplication.html#matrix-inverse",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "Matrix Inverse",
    "text": "Matrix Inverse\n\nDefinition\nFor a square matrix \\(A\\), if there exists a matrix \\(A^{-1}\\) such that:\n\\[\nA^{-1}A = I \\quad \\text{and} \\quad AA^{-1} = I\n\\]\nthen \\(A\\) is invertible (or non-singular).\n\n\nWhen Does an Inverse NOT Exist?\nA matrix \\(A\\) has no inverse if any of these equivalent conditions hold:\n\nZero determinant: \\(\\det(A) = 0\\)\nDependent columns: Some column is a linear combination of others\nNon-trivial null space: \\(Ax = 0\\) for some non-zero \\(x\\)\n\n\n\nWhy Singular Matrices Have No Inverse: A Proof\nConsider the singular matrix:\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix}\n\\]\nObservation: The second column is 2√ó the first (columns are dependent).\nWe can find \\(x = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} \\neq 0\\) such that:\n\\[\nAx = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n\\]\nProof by contradiction:\nSuppose \\(A^{-1}\\) exists. Then:\n\nWe have \\(Ax = 0\\) where \\(x \\neq 0\\)\nMultiply both sides by \\(A^{-1}\\): \\(A^{-1}(Ax) = A^{-1} \\cdot 0\\)\nLeft side simplifies: \\(A^{-1}(Ax) = (A^{-1}A)x = Ix = x\\)\nRight side: \\(A^{-1} \\cdot 0 = 0\\)\nTherefore: \\(x = 0\\)\n\nContradiction! We started with \\(x = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} \\neq 0\\), but our logic says \\(x = 0\\).\nThis proves \\(A^{-1}\\) cannot exist. The fundamental issue: if \\(A\\) maps different inputs to the same output (like both \\(x\\) and \\(0\\) to \\(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)), we cannot uniquely reverse the operation.\n\n\nComputing the Inverse\nFor 2√ó2 matrices:\n\\[\nA = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n\\quad \\Rightarrow \\quad\nA^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\\]\nNote: \\(ad - bc\\) is the determinant. If \\(\\det(A) = 0\\), no inverse exists.\nFor larger matrices, use Gauss-Jordan elimination:\n\\[\n[A | I] \\xrightarrow{\\text{row operations}} [I | A^{-1}]\n\\]\nStart with \\(A\\) augmented with the identity matrix, then use row operations to transform \\(A\\) into \\(I\\). The same operations transform \\(I\\) into \\(A^{-1}\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture3-multiplication.html#key-takeaways",
    "href": "Math/MIT18.06/mit1806-lecture3-multiplication.html#key-takeaways",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nMatrix multiplication has five perspectives:\n\nElement-by-element (computational)\nColumn view (output columns from input columns)\nRow view (output rows from input rows)\nSum of rank-1 matrices (most insightful)\nBlock multiplication (practical for large matrices)\n\nMatrix inverse exists if and only if:\n\nDeterminant is non-zero\nColumns are independent\n\\(Ax = 0\\) only when \\(x = 0\\)\n\nWhy this matters:\n\nSolving \\(Ax = b\\): if \\(A^{-1}\\) exists, then \\(x = A^{-1}b\\)\nHowever, Gaussian elimination is usually more efficient than computing \\(A^{-1}\\)\nUnderstanding when inverses don‚Äôt exist reveals the structure of linear transformations\n\n\nThe rank-1 decomposition (view 4) is particularly powerful‚Äîit‚Äôs the foundation for understanding matrix rank, the Four Fundamental Subspaces, and advanced topics like Singular Value Decomposition."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture3-multiplication.html#exercises",
    "href": "Math/MIT18.06/mit1806-lecture3-multiplication.html#exercises",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "Exercises",
    "text": "Exercises\n\nExample 1: Matrix Multiplication (All Five Perspectives)\n\n\nShow code\nimport numpy as np\nfrom IPython.display import display, Markdown, Latex\n\ndef matrix_to_latex(M, name=\"\"):\n    \"\"\"Convert numpy matrix to LaTeX bmatrix format\"\"\"\n    if len(M.shape) == 1:\n        M = M.reshape(-1, 1)\n    rows = [\" & \".join([f\"{x:.0f}\" if x == int(x) else f\"{x:.2f}\" for x in row]) for row in M]\n    latex_str = r\"\\begin{bmatrix}\" + r\" \\\\ \".join(rows) + r\"\\end{bmatrix}\"\n    if name:\n        latex_str = f\"{name} = \" + latex_str\n    return latex_str\n\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\nC = A @ B\n\ndisplay(Markdown(\"**Given matrices:**\"))\ndisplay(Latex(f\"$${matrix_to_latex(A, 'A')}$$\"))\ndisplay(Latex(f\"$${matrix_to_latex(B, 'B')}$$\"))\n\n\nGiven matrices:\n\n\n\\[A = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}\\]\n\n\n\\[B = \\begin{bmatrix}5 & 6 \\\\ 7 & 8\\end{bmatrix}\\]\n\n\nMethod 1: Element-by-element\n\n\nShow code\ndisplay(Latex(f\"$${matrix_to_latex(C, 'C = AB')}$$\"))\ndisplay(Latex(f\"$$C_{{11}} = (1)(5) + (2)(7) = {C[0,0]}$$\"))\ndisplay(Latex(f\"$$C_{{12}} = (1)(6) + (2)(8) = {C[0,1]}$$\"))\ndisplay(Latex(f\"$$C_{{21}} = (3)(5) + (4)(7) = {C[1,0]}$$\"))\ndisplay(Latex(f\"$$C_{{22}} = (3)(6) + (4)(8) = {C[1,1]}$$\"))\n\n\n\\[C = AB = \\begin{bmatrix}19 & 22 \\\\ 43 & 50\\end{bmatrix}\\]\n\n\n\\[C_{11} = (1)(5) + (2)(7) = 19\\]\n\n\n\\[C_{12} = (1)(6) + (2)(8) = 22\\]\n\n\n\\[C_{21} = (3)(5) + (4)(7) = 43\\]\n\n\n\\[C_{22} = (3)(6) + (4)(8) = 50\\]\n\n\nMethod 2: Column view\n\n\nShow code\ncol1 = A @ B[:, 0]\ncol2 = A @ B[:, 1]\n\ndisplay(Markdown(\"Column 1 of C is a linear combination of A's columns:\"))\ndisplay(Latex(f\"$$A \\\\begin{{bmatrix}} 5 \\\\\\\\ 7 \\\\end{{bmatrix}} = {matrix_to_latex(col1)}$$\"))\n\ndisplay(Markdown(\"Column 2 of C is a linear combination of A's columns:\"))\ndisplay(Latex(f\"$$A \\\\begin{{bmatrix}} 6 \\\\\\\\ 8 \\\\end{{bmatrix}} = {matrix_to_latex(col2)}$$\"))\n\n\nColumn 1 of C is a linear combination of A‚Äôs columns:\n\n\n\\[A \\begin{bmatrix} 5 \\\\ 7 \\end{bmatrix} = \\begin{bmatrix}19 \\\\ 43\\end{bmatrix}\\]\n\n\nColumn 2 of C is a linear combination of A‚Äôs columns:\n\n\n\\[A \\begin{bmatrix} 6 \\\\ 8 \\end{bmatrix} = \\begin{bmatrix}22 \\\\ 50\\end{bmatrix}\\]\n\n\nMethod 3: Row view\n\n\nShow code\nrow1 = A[0, :] @ B\nrow2 = A[1, :] @ B\n\ndisplay(Markdown(\"Row 1 of C is a linear combination of B's rows:\"))\ndisplay(Latex(f\"$$\\\\begin{{bmatrix}} 1 & 2 \\\\end{{bmatrix}} B = {matrix_to_latex(row1.reshape(1, -1))}$$\"))\n\ndisplay(Markdown(\"Row 2 of C is a linear combination of B's rows:\"))\ndisplay(Latex(f\"$$\\\\begin{{bmatrix}} 3 & 4 \\\\end{{bmatrix}} B = {matrix_to_latex(row2.reshape(1, -1))}$$\"))\n\n\nRow 1 of C is a linear combination of B‚Äôs rows:\n\n\n\\[\\begin{bmatrix} 1 & 2 \\end{bmatrix} B = \\begin{bmatrix}19 & 22\\end{bmatrix}\\]\n\n\nRow 2 of C is a linear combination of B‚Äôs rows:\n\n\n\\[\\begin{bmatrix} 3 & 4 \\end{bmatrix} B = \\begin{bmatrix}43 & 50\\end{bmatrix}\\]\n\n\nMethod 4: Sum of rank-1 matrices (most powerful!)\n\n\nShow code\nrank1_1 = np.outer(A[:, 0], B[0, :])\nrank1_2 = np.outer(A[:, 1], B[1, :])\n\ndisplay(Markdown(\"**Rank-1 term 1:**\"))\ndisplay(Latex(f\"$$\\\\begin{{bmatrix}} 1 \\\\\\\\ 3 \\\\end{{bmatrix}} \\\\begin{{bmatrix}} 5 & 6 \\\\end{{bmatrix}} = {matrix_to_latex(rank1_1)}$$\"))\n\ndisplay(Markdown(\"**Rank-1 term 2:**\"))\ndisplay(Latex(f\"$$\\\\begin{{bmatrix}} 2 \\\\\\\\ 4 \\\\end{{bmatrix}} \\\\begin{{bmatrix}} 7 & 8 \\\\end{{bmatrix}} = {matrix_to_latex(rank1_2)}$$\"))\n\ndisplay(Markdown(\"**Sum of rank-1 matrices:**\"))\ndisplay(Latex(f\"$${matrix_to_latex(rank1_1)} + {matrix_to_latex(rank1_2)} = {matrix_to_latex(rank1_1 + rank1_2)}$$\"))\n\ndisplay(Markdown(\"‚úì **All methods give the same result!**\"))\n\n\nRank-1 term 1:\n\n\n\\[\\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} \\begin{bmatrix} 5 & 6 \\end{bmatrix} = \\begin{bmatrix}5 & 6 \\\\ 15 & 18\\end{bmatrix}\\]\n\n\nRank-1 term 2:\n\n\n\\[\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} \\begin{bmatrix} 7 & 8 \\end{bmatrix} = \\begin{bmatrix}14 & 16 \\\\ 28 & 32\\end{bmatrix}\\]\n\n\nSum of rank-1 matrices:\n\n\n\\[\\begin{bmatrix}5 & 6 \\\\ 15 & 18\\end{bmatrix} + \\begin{bmatrix}14 & 16 \\\\ 28 & 32\\end{bmatrix} = \\begin{bmatrix}19 & 22 \\\\ 43 & 50\\end{bmatrix}\\]\n\n\n‚úì All methods give the same result!\n\n\n\n\nExample 2: Matrix Inverse\nInvertible matrix:\n\n\nShow code\nA_inv = np.array([[2, 1], [5, 3]])\nA_inv_inverse = np.linalg.inv(A_inv)\ndet_A = np.linalg.det(A_inv)\n\ndisplay(Latex(f\"$${matrix_to_latex(A_inv, 'A')}$$\"))\ndisplay(Latex(f\"$$\\\\det(A) = {det_A:.0f} \\\\neq 0 \\\\quad \\\\checkmark \\\\text{{ Invertible!}}$$\"))\ndisplay(Latex(f\"$${matrix_to_latex(A_inv_inverse, 'A^{-1}')}$$\"))\n\ndisplay(Markdown(\"**Verification:**\"))\nidentity = A_inv @ A_inv_inverse\ndisplay(Latex(f\"$$A A^{{-1}} = {matrix_to_latex(np.round(identity, 10), '')} = I$$\"))\n\n\n\\[A = \\begin{bmatrix}2 & 1 \\\\ 5 & 3\\end{bmatrix}\\]\n\n\n\\[\\det(A) = 1 \\neq 0 \\quad \\checkmark \\text{ Invertible!}\\]\n\n\n\\[A^{-1} = \\begin{bmatrix}3.00 & -1.00 \\\\ -5.00 & 2.00\\end{bmatrix}\\]\n\n\nVerification:\n\n\n\\[A A^{-1} = \\begin{bmatrix}1 & 0 \\\\ -0 & 1\\end{bmatrix} = I\\]\n\n\nSingular matrix (no inverse):\n\n\nShow code\nA_singular = np.array([[1, 2], [2, 4]])\ndet_singular = np.linalg.det(A_singular)\n\ndisplay(Latex(f\"$${matrix_to_latex(A_singular, 'A')}$$\"))\ndisplay(Latex(f\"$$\\\\det(A) = 0 \\\\quad \\\\times \\\\text{{ Singular!}}$$\"))\n\ndisplay(Markdown(\"**Observation:** Column 2 = 2 √ó Column 1 (linearly dependent)\"))\ndisplay(Latex(f\"$${matrix_to_latex(A_singular[:, 1])} = 2 \\\\times {matrix_to_latex(A_singular[:, 0])}$$\"))\n\nx = np.array([2, -1])\nAx = A_singular @ x\n\ndisplay(Markdown(f\"**Non-zero vector satisfying $Ax = 0$:**\"))\ndisplay(Latex(f\"$$A {matrix_to_latex(x, 'x')} = {matrix_to_latex(Ax)} = 0$$\"))\ndisplay(Markdown(\"‚üπ **No inverse exists** (proven by contradiction earlier)\"))\n\n\n\\[A = \\begin{bmatrix}1 & 2 \\\\ 2 & 4\\end{bmatrix}\\]\n\n\n\\[\\det(A) = 0 \\quad \\times \\text{ Singular!}\\]\n\n\nObservation: Column 2 = 2 √ó Column 1 (linearly dependent)\n\n\n\\[\\begin{bmatrix}2 \\\\ 4\\end{bmatrix} = 2 \\times \\begin{bmatrix}1 \\\\ 2\\end{bmatrix}\\]\n\n\nNon-zero vector satisfying \\(Ax = 0\\):\n\n\n\\[A x = \\begin{bmatrix}2 \\\\ -1\\end{bmatrix} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix} = 0\\]\n\n\n‚üπ No inverse exists (proven by contradiction earlier)"
  },
  {
    "objectID": "Math/MIT18.06/lectures.html",
    "href": "Math/MIT18.06/lectures.html",
    "title": "MIT 18.06SC Linear Algebra",
    "section": "",
    "text": "My journey through MIT‚Äôs Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.\n\n\n\n\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation.\n\n\nFrom Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series The beautiful mathematical journey from Taylor expansions to Euler‚Äôs formula and ultimately to Fourier series, revealing deep connections between polynomials, exponentials, and trigonometric functions.\n\n\n\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations Two powerful perspectives: row picture vs column picture.\n\n\nLecture 2: Elimination with Matrices The systematic algorithm that transforms linear systems into upper triangular form.\n\n\nLecture 3: Matrix Multiplication and Inverse Five different perspectives on matrix multiplication.\n\n\nLecture 4: LU Decomposition Factoring matrices into Lower √ó Upper triangular form.\n\n\nLecture 5.1: Permutation Matrices Permutation matrices reorder rows and columns.\n\n\nLecture 5.2: Transpose The transpose operation switches rows to columns.\n\n\nLecture 5.3: Vector Spaces Vector spaces and subspaces: closed under addition and scalar multiplication.\n\n\nLecture 6: Column Space and Null Space Column space determines which \\(b\\) make \\(Ax = b\\) solvable.\n\n\nLecture 7: Solving Ax=0 Systematic algorithm to find null space using pivot/free variables and RREF.\n\n\nLecture 8: Solving Ax=b Complete solution is particular solution plus null space.\n\n\nLecture 9: Independence, Basis, and Dimension Linear independence, basis, and dimension. Rank-nullity theorem.\n\n\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix.\n\n\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.\n\n\nLecture 12: Graphs, Networks, and Incidence Matrices Applying linear algebra to graph theory: incidence matrices, Kirchhoff‚Äôs laws, and Euler‚Äôs formula.\n\n\nLecture 13: Quiz 1 Review Practice problems reviewing key concepts: subspaces, rank, null spaces, and the four fundamental subspaces.\n\n\n\n\n\n\n\n\nLecture 14: Orthogonal Vectors and Subspaces Orthogonal vectors, orthogonal subspaces, and the fundamental relationships between the four subspaces.\n\n\nLecture 15: Projection onto Subspaces Projecting onto lines and subspaces, the geometry of least squares, and deriving normal equations.\n\n\nLecture 16: Projection Matrices and Least Squares Projection matrices, least squares fitting, and the connection between linear algebra and calculus optimization.\n\n\nLecture 17: Orthogonal Matrices and Gram-Schmidt Orthonormal matrices, the Gram-Schmidt process for orthogonalization, and QR factorization.\n\n\nLecture 18: Properties of Determinants Ten fundamental properties that completely define the determinant and reveal when matrices are invertible.\n\n\nLecture 19: Determinant Formulas and Cofactors Three computational methods for determinants: pivots, the big formula, and cofactor expansion.\n\n\nLecture 20: Inverse & Volume The inverse matrix formula using cofactors, Cramer‚Äôs rule for solving linear systems, and the geometric interpretation of determinants as volume.\n\n\nLecture 21: Eigenvalues and Eigenvectors The directions that matrices can only scale, not rotate: \\(Ax = \\lambda x\\).\n\n\nLecture 22: Diagonalization and Powers of A Computing matrix powers efficiently and solving Fibonacci with eigenvalues.\n\n\nLecture 23: Differential Equations and exp(At) Connecting linear algebra to differential equations: how eigenvalues determine stability and matrix exponentials solve systems.\n\n\nLecture 24: Markov Matrices and Fourier Series Two powerful applications of eigenvalues: Markov matrices for modeling state transitions and Fourier series for decomposing functions into orthogonal components.\n\n\n\n\n\n\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 29: Singular Value Decomposition The most important matrix factorization: SVD provides orthonormal bases for all four fundamental subspaces, revealing how any matrix maps its row space to its column space through scaling by singular values.\n\n\nLecture 30: Linear Transformations and Their Matrices The fundamental connection between abstract linear transformations and concrete matrices: every linear transformation can be represented as matrix multiplication, and the choice of basis determines the matrix representation.\n\n\nLecture 31: Change of Basis and Image Compression How choosing the right basis enables compression: JPEG transforms 512√ó512 images (262,144 pixels) to Fourier basis and discards small coefficients. Three properties of good bases‚Äîfast inverse (FFT in O(n log n)), sparsity (few coefficients enough), and orthogonality (no redundancy).\n\n\nLecture 33: Left and Right Inverse; Pseudo-inverse When matrices aren‚Äôt square: full column rank matrices (\\(r = n &lt; m\\)) have left inverses \\((A^T A)^{-1} A^T\\), full row rank matrices (\\(r = m &lt; n\\)) have right inverses \\(A^T (AA^T)^{-1}\\), and the pseudo-inverse \\(A^+ = V \\Sigma^+ U^T\\) generalizes both using SVD‚Äîinverting non-zero singular values and transposing the shape."
  },
  {
    "objectID": "Math/MIT18.06/lectures.html#all-lectures",
    "href": "Math/MIT18.06/lectures.html#all-lectures",
    "title": "MIT 18.06SC Linear Algebra",
    "section": "",
    "text": "My journey through MIT‚Äôs Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.\n\n\n\n\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation.\n\n\nFrom Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series The beautiful mathematical journey from Taylor expansions to Euler‚Äôs formula and ultimately to Fourier series, revealing deep connections between polynomials, exponentials, and trigonometric functions.\n\n\n\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations Two powerful perspectives: row picture vs column picture.\n\n\nLecture 2: Elimination with Matrices The systematic algorithm that transforms linear systems into upper triangular form.\n\n\nLecture 3: Matrix Multiplication and Inverse Five different perspectives on matrix multiplication.\n\n\nLecture 4: LU Decomposition Factoring matrices into Lower √ó Upper triangular form.\n\n\nLecture 5.1: Permutation Matrices Permutation matrices reorder rows and columns.\n\n\nLecture 5.2: Transpose The transpose operation switches rows to columns.\n\n\nLecture 5.3: Vector Spaces Vector spaces and subspaces: closed under addition and scalar multiplication.\n\n\nLecture 6: Column Space and Null Space Column space determines which \\(b\\) make \\(Ax = b\\) solvable.\n\n\nLecture 7: Solving Ax=0 Systematic algorithm to find null space using pivot/free variables and RREF.\n\n\nLecture 8: Solving Ax=b Complete solution is particular solution plus null space.\n\n\nLecture 9: Independence, Basis, and Dimension Linear independence, basis, and dimension. Rank-nullity theorem.\n\n\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix.\n\n\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.\n\n\nLecture 12: Graphs, Networks, and Incidence Matrices Applying linear algebra to graph theory: incidence matrices, Kirchhoff‚Äôs laws, and Euler‚Äôs formula.\n\n\nLecture 13: Quiz 1 Review Practice problems reviewing key concepts: subspaces, rank, null spaces, and the four fundamental subspaces.\n\n\n\n\n\n\n\n\nLecture 14: Orthogonal Vectors and Subspaces Orthogonal vectors, orthogonal subspaces, and the fundamental relationships between the four subspaces.\n\n\nLecture 15: Projection onto Subspaces Projecting onto lines and subspaces, the geometry of least squares, and deriving normal equations.\n\n\nLecture 16: Projection Matrices and Least Squares Projection matrices, least squares fitting, and the connection between linear algebra and calculus optimization.\n\n\nLecture 17: Orthogonal Matrices and Gram-Schmidt Orthonormal matrices, the Gram-Schmidt process for orthogonalization, and QR factorization.\n\n\nLecture 18: Properties of Determinants Ten fundamental properties that completely define the determinant and reveal when matrices are invertible.\n\n\nLecture 19: Determinant Formulas and Cofactors Three computational methods for determinants: pivots, the big formula, and cofactor expansion.\n\n\nLecture 20: Inverse & Volume The inverse matrix formula using cofactors, Cramer‚Äôs rule for solving linear systems, and the geometric interpretation of determinants as volume.\n\n\nLecture 21: Eigenvalues and Eigenvectors The directions that matrices can only scale, not rotate: \\(Ax = \\lambda x\\).\n\n\nLecture 22: Diagonalization and Powers of A Computing matrix powers efficiently and solving Fibonacci with eigenvalues.\n\n\nLecture 23: Differential Equations and exp(At) Connecting linear algebra to differential equations: how eigenvalues determine stability and matrix exponentials solve systems.\n\n\nLecture 24: Markov Matrices and Fourier Series Two powerful applications of eigenvalues: Markov matrices for modeling state transitions and Fourier series for decomposing functions into orthogonal components.\n\n\n\n\n\n\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 29: Singular Value Decomposition The most important matrix factorization: SVD provides orthonormal bases for all four fundamental subspaces, revealing how any matrix maps its row space to its column space through scaling by singular values.\n\n\nLecture 30: Linear Transformations and Their Matrices The fundamental connection between abstract linear transformations and concrete matrices: every linear transformation can be represented as matrix multiplication, and the choice of basis determines the matrix representation.\n\n\nLecture 31: Change of Basis and Image Compression How choosing the right basis enables compression: JPEG transforms 512√ó512 images (262,144 pixels) to Fourier basis and discards small coefficients. Three properties of good bases‚Äîfast inverse (FFT in O(n log n)), sparsity (few coefficients enough), and orthogonality (no redundancy).\n\n\nLecture 33: Left and Right Inverse; Pseudo-inverse When matrices aren‚Äôt square: full column rank matrices (\\(r = n &lt; m\\)) have left inverses \\((A^T A)^{-1} A^T\\), full row rank matrices (\\(r = m &lt; n\\)) have right inverses \\(A^T (AA^T)^{-1}\\), and the pseudo-inverse \\(A^+ = V \\Sigma^+ U^T\\) generalizes both using SVD‚Äîinverting non-zero singular values and transposing the shape."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#overview",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#overview",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Overview",
    "text": "Overview\nFor any \\(m \\times n\\) matrix \\(A\\), there are four fundamental subspaces that completely characterize its structure:\n\n\n\n\n\n\n\n\n\n\nSubspace\nNotation\nLives in\nDimension\nSolves\n\n\n\n\nColumn Space\n\\(C(A)\\)\n\\(\\mathbb{R}^m\\)\n\\(r\\)\n\\(Ax = b\\)\n\n\nNull Space\n\\(N(A)\\)\n\\(\\mathbb{R}^n\\)\n\\(n - r\\)\n\\(Ax = \\mathbf{0}\\)\n\n\nRow Space\n\\(C(A^T)\\)\n\\(\\mathbb{R}^n\\)\n\\(r\\)\n\\(A^T y = c\\)\n\n\nLeft Null Space\n\\(N(A^T)\\)\n\\(\\mathbb{R}^m\\)\n\\(m - r\\)\n\\(A^T y = \\mathbf{0}\\)\n\n\n\nwhere \\(r = \\text{rank}(A)\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#column-space-ca",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#column-space-ca",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Column Space: \\(C(A)\\)",
    "text": "Column Space: \\(C(A)\\)\nDefinition: The span of all columns of \\(A\\).\nProperties:\n\nLives in \\(\\mathbb{R}^m\\) (same dimension as the rows)\nDimension = \\(\\text{rank}(A) = r\\)\nRepresents all possible outputs of \\(Ax\\)\n\nLinear combination interpretation:\n\\[\nx_1 A_{:,1} + x_2 A_{:,2} + \\cdots + x_n A_{:,n} = b\n\\]\nKey question: For which \\(b\\) does \\(Ax = b\\) have a solution?\nAnswer: When \\(b \\in C(A)\\) (when \\(b\\) is in the column space)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#null-space-na",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#null-space-na",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Null Space: \\(N(A)\\)",
    "text": "Null Space: \\(N(A)\\)\nDefinition: All vectors \\(x\\) such that \\(Ax = \\mathbf{0}\\).\nProperties:\n\nLives in \\(\\mathbb{R}^n\\) (same dimension as the columns)\nDimension = \\(n - r\\) (number of free variables)\nContains all solutions to the homogeneous system \\(Ax = \\mathbf{0}\\)\n\nInterpretation: Vectors that get ‚Äúkilled‚Äù by the matrix \\(A\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#row-space-cat",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#row-space-cat",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Row Space: \\(C(A^T)\\)",
    "text": "Row Space: \\(C(A^T)\\)\nDefinition: The span of all rows of \\(A\\), equivalently the column space of \\(A^T\\).\nProperties:\n\nLives in \\(\\mathbb{R}^n\\) (same dimension as the columns)\nDimension = \\(\\text{rank}(A) = r\\) (same as column space)\nRepresents all possible outputs of \\(A^T y\\)\n\n\n\n\n\n\n\nTipKey Insight\n\n\n\nRow rank equals column rank (both equal \\(r\\))."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#left-null-space-nat",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#left-null-space-nat",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Left Null Space: \\(N(A^T)\\)",
    "text": "Left Null Space: \\(N(A^T)\\)\nDefinition: All vectors \\(y\\) such that \\(A^T y = \\mathbf{0}\\).\nProperties:\n\nLives in \\(\\mathbb{R}^m\\) (same dimension as the rows)\nDimension = \\(m - r\\)\nAlso called the left null space of \\(A\\)\n\nWhy ‚Äúleft null space‚Äù?\nStarting from \\(A^T y = \\mathbf{0}\\):\n\\[\n\\begin{aligned}\nA^T y &= \\mathbf{0} \\\\\n(A^T y)^T &= \\mathbf{0}^T = [\\mathbf{0}] \\\\\ny^T A &= [\\mathbf{0}]\n\\end{aligned}\n\\]\nSince \\(y^T\\) is \\(1 \\times m\\) and appears on the left of \\(A\\), this is called the left null space."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#rref-and-the-four-subspaces",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#rref-and-the-four-subspaces",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "RREF and the Four Subspaces",
    "text": "RREF and the Four Subspaces\n\nExample Matrix\n\\[\nA = \\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n5 & 6 & 7 & 8 \\\\\n2 & 4 & 6 & 8\n\\end{bmatrix}\n\\]\n\n\nRow Echelon Form (REF)\n\\[\n\\text{REF} = \\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n0 & -4 & -8 & -12 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\n\n\nReduced Row Echelon Form (RREF)\nStep 1: Normalize pivot rows\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n0 & 1 & 2 & 3 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nStep 2: Eliminate above pivots\n\\[\n\\text{RREF} = \\begin{bmatrix}\n1 & 0 & -1 & -2 \\\\\n0 & 1 & 2 & 3 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\n\n\nAnalysis\n\nRank: \\(r = 2\\) (two pivot columns)\nFree variables: \\(n - r = 4 - 2 = 2\\) (columns 3 and 4)\nDimension of \\(N(A)\\): \\(n - r = 2\\)\nDimension of \\(C(A)\\): \\(r = 2\\)\nDimension of \\(C(A^T)\\): \\(r = 2\\)\nDimension of \\(N(A^T)\\): \\(m - r = 3 - 2 = 1\\)\n\n\n\n\n\n\n\nNoteKey Observation\n\n\n\nRREF is the unique simplest form of a matrix obtained through row operations that preserve the solution space."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#visualization",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#visualization",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Visualization",
    "text": "Visualization\n\n\n\nFour Fundamental Subspaces"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#dimension-relationships",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#dimension-relationships",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Dimension Relationships",
    "text": "Dimension Relationships\nFor an \\(m \\times n\\) matrix \\(A\\) with rank \\(r\\):\n\nIn \\(\\mathbb{R}^n\\):\n\n\\(\\dim(C(A^T)) + \\dim(N(A)) = r + (n - r) = n\\)\nRow space and null space partition \\(\\mathbb{R}^n\\)\n\n\n\nIn \\(\\mathbb{R}^m\\):\n\n\\(\\dim(C(A)) + \\dim(N(A^T)) = r + (m - r) = m\\)\nColumn space and left null space partition \\(\\mathbb{R}^m\\)\n\n\n\n\n\n\n\nImportantKey Insight\n\n\n\nThe four subspaces come in complementary pairs that completely partition their ambient spaces."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture10-four-subspaces.html#summary",
    "title": "MIT 18.06SC Lecture 10: Four Fundamental Subspaces",
    "section": "Summary",
    "text": "Summary\nThe four fundamental subspaces:\n\nColumn space \\(C(A)\\): where outputs \\(Ax\\) live\nNull space \\(N(A)\\): inputs that map to zero\nRow space \\(C(A^T)\\): perpendicular complement to null space in \\(\\mathbb{R}^n\\)\nLeft null space \\(N(A^T)\\): perpendicular complement to column space in \\(\\mathbb{R}^m\\)\n\nDimension formula:\n\n\\(\\dim(C(A)) = \\dim(C(A^T)) = r\\)\n\\(\\dim(N(A)) = n - r\\)\n\\(\\dim(N(A^T)) = m - r\\)\n\nTotal check: \\(r + (n-r) = n\\) and \\(r + (m-r) = m\\)\n\nSource: MIT 18.06SC Linear Algebra, Lecture 10"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture2-elimination.html",
    "href": "Math/MIT18.06/mit1806-lecture2-elimination.html",
    "title": "MIT 18.06SC Lecture 2: Elimination with Matrices",
    "section": "",
    "text": "This recap of MIT 18.06SC Lecture 2 explores Gaussian elimination‚Äîthe systematic algorithm that transforms any linear system into an easily solvable upper triangular form.\nüìì For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture2-elimination.html#from-linear-systems-to-upper-triangular-form",
    "href": "Math/MIT18.06/mit1806-lecture2-elimination.html#from-linear-systems-to-upper-triangular-form",
    "title": "MIT 18.06SC Lecture 2: Elimination with Matrices",
    "section": "From Linear Systems to Upper Triangular Form",
    "text": "From Linear Systems to Upper Triangular Form\nHow do we actually solve a system of linear equations like this?\n\\[\\begin{cases}\n2u + v + w = 5 \\\\\n4u - 6v = -2 \\\\\n-2u + 7v + 2w = 9\n\\end{cases}\\]\nThe answer is Gaussian elimination‚Äîa systematic process that transforms the coefficient matrix into an upper triangular form, where solutions can be read off directly through back substitution. This fundamental algorithm underpins much of numerical linear algebra and is essential for understanding how computers solve linear systems.\n\nQuick Reference: Understanding Elimination\nFor context on the mathematical foundations of Gaussian elimination, see the Lecture 2 summary.\nThe Two-Step Process:\n\nForward Elimination: Transform \\(A\\) into upper triangular \\(U\\) using row operations\n\nFor each pivot position, eliminate all entries below it\nRow operation: \\(\\text{Row}_i \\leftarrow \\text{Row}_i - m_{ij} \\cdot \\text{Row}_j\\) where \\(m_{ij} = a_{ij}/\\text{pivot}\\)\n\nBack Substitution: Solve \\(Ux = c\\) from bottom to top\n\nStart with the last equation (only one unknown)\nWork upward, substituting known values\n\n\nKey insight: Each elimination step can be represented as multiplication by an elimination matrix \\(E_{ij}\\), connecting row operations to matrix multiplication."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture2-elimination.html#exercise-implement-gaussian-elimination-from-scratch",
    "href": "Math/MIT18.06/mit1806-lecture2-elimination.html#exercise-implement-gaussian-elimination-from-scratch",
    "title": "MIT 18.06SC Lecture 2: Elimination with Matrices",
    "section": "üî¨ Exercise: Implement Gaussian Elimination from Scratch",
    "text": "üî¨ Exercise: Implement Gaussian Elimination from Scratch\nLet‚Äôs implement the complete Gaussian elimination algorithm and verify it works correctly.\n\n\nShow code\nimport numpy as np\nfrom IPython.display import display, Markdown\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\ndef matrix_to_latex(matrix, precision=2):\n    \"\"\"Convert numpy matrix to LaTeX format.\"\"\"\n    if len(matrix.shape) == 1:\n        # Vector\n        elements = \" \\\\\\\\ \".join([f\"{x:.{precision}f}\" for x in matrix])\n        return f\"$$\\\\begin{{bmatrix}}{elements}\\\\end{{bmatrix}}$$\"\n    else:\n        # Matrix\n        rows = []\n        for row in matrix:\n            row_str = \" & \".join([f\"{x:.{precision}f}\" for x in row])\n            rows.append(row_str)\n        matrix_str = \" \\\\\\\\ \".join(rows)\n        return f\"$$\\\\begin{{bmatrix}}{matrix_str}\\\\end{{bmatrix}}$$\"\n\nprint(\"‚úì Setup complete\")\n\n\n‚úì Setup complete\n\n\n\nAlgorithm Overview\nThe algorithm consists of two phases:\n\nForward Elimination: Transform \\(A\\) into upper triangular form \\(U\\)\n\nFor each column \\(j\\) from 0 to \\(n-1\\):\n\nPivot = \\(A[j, j]\\)\nFor each row \\(i\\) below pivot (\\(i &gt; j\\)):\n\nCompute multiplier: \\(m_{ij} = A[i, j] / \\text{pivot}\\)\nRow operation: \\(A[i, :] = A[i, :] - m_{ij} \\cdot A[j, :]\\)\nUpdate RHS: \\(b[i] = b[i] - m_{ij} \\cdot b[j]\\)\n\n\n\nBack Substitution: Solve \\(Ux = c\\) from bottom to top\n\nStart from last row: \\(x[n-1] = c[n-1] / U[n-1, n-1]\\)\nFor each row \\(i\\) from \\(n-2\\) down to \\(0\\):\n\n\\(x[i] = (c[i] - \\sum_{j=i+1}^{n-1} U[i,j] \\cdot x[j]) / U[i,i]\\)\n\n\n\n\n\nImplementation\n\n\nShow code\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using Gaussian elimination.\n\n    Parameters:\n    -----------\n    A : numpy.ndarray, shape (n, n)\n        Coefficient matrix\n    b : numpy.ndarray, shape (n,)\n        Right-hand side vector\n\n    Returns:\n    --------\n    x : numpy.ndarray, shape (n,)\n        Solution vector\n    \"\"\"\n    A_copy = A.copy()\n    b_copy = b.copy()\n\n    rows, cols = A.shape\n    assert rows == cols, \"Matrix must be square\"\n\n    for i in range(rows - 1):\n        multiplier = A_copy[i + 1:, i] / A_copy[i, i]\n        for index, m in enumerate(multiplier):\n            A_copy[i + 1 + index] = A_copy[i + 1 + index] - m * A_copy[i]\n            assert abs(A_copy[i + 1 + index, i]) &lt; 1e-10, \"Upper triangular matrix expected\"\n            b_copy[i + 1 + index] = b_copy[i + 1 + index] - m * b_copy[i]\n\n    U = A_copy\n    print(\"Upper triangular matrix U:\")\n    display(Markdown(matrix_to_latex(U)))\n\n    # Now our matrix is upper triangular\n    # Solve for x from bottom to top\n    x = np.zeros(rows)\n    for i in range(rows - 1, -1, -1):\n        x[i] = (b_copy[i] - np.dot(A_copy[i][i + 1:], x[i + 1:])) / A_copy[i][i]\n\n    return x\n\nprint(\"‚úì Function defined\")\n\n\n‚úì Function defined\n\n\n\n\nTest on Lecture Example\nTest on the system from the lecture:\n\\[\\begin{cases}\n2u + v + w = 5 \\\\\n4u - 6v = -2 \\\\\n-2u + 7v + 2w = 9\n\\end{cases}\\]\nExpected solution: \\(u = 1, v = 1, w = 2\\)\n\n\nShow code\n# Define the system from lecture\nA = np.array([\n    [2, 1, 1],\n    [4, -6, 0],\n    [-2, 7, 2]\n], dtype=float)\n\nb = np.array([5, -2, 9], dtype=float)\n\nprint(\"System to solve:\")\nprint(\"A =\")\nprint(A)\nprint(\"\\nb =\")\nprint(b)\n\n# Solve using our implementation\nx_our = gaussian_elimination(A, b)\n\n# Solve using NumPy\nx_numpy = np.linalg.solve(A, b)\n\n# Compare results\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Results:\")\nprint(\"=\" * 60)\nprint(f\"Our implementation:      x = {x_our}\")\nprint(f\"NumPy (np.linalg.solve): x = {x_numpy}\")\nprint(\"=\" * 60)\n\n# Verify solution\nresidual_our = np.linalg.norm(A @ x_our - b)\nresidual_numpy = np.linalg.norm(A @ x_numpy - b)\n\nprint(f\"\\nResidual (our):   ||Ax - b|| = {residual_our:.2e}\")\nprint(f\"Residual (NumPy): ||Ax - b|| = {residual_numpy:.2e}\")\n\n# Check difference\ndifference = np.linalg.norm(x_our - x_numpy)\nprint(f\"\\nDifference: ||x_our - x_numpy|| = {difference:.2e}\")\n\nif difference &lt; 1e-10:\n    print(\"\\n‚úÖ Solutions match! Implementation is correct.\")\nelse:\n    print(\"\\n‚ùå Solutions differ. Check implementation.\")\n\n\nSystem to solve:\nA =\n[[ 2.  1.  1.]\n [ 4. -6.  0.]\n [-2.  7.  2.]]\n\nb =\n[ 5. -2.  9.]\nUpper triangular matrix U:\n\n\n\\[\\begin{bmatrix}2.00 & 1.00 & 1.00 \\\\ 0.00 & -8.00 & -2.00 \\\\ 0.00 & 0.00 & 1.00\\end{bmatrix}\\]\n\n\n\n============================================================\nResults:\n============================================================\nOur implementation:      x = [1. 1. 2.]\nNumPy (np.linalg.solve): x = [1. 1. 2.]\n============================================================\n\nResidual (our):   ||Ax - b|| = 0.00e+00\nResidual (NumPy): ||Ax - b|| = 0.00e+00\n\nDifference: ||x_our - x_numpy|| = 0.00e+00\n\n‚úÖ Solutions match! Implementation is correct.\n\n\n\n\nVisualize Upper Triangular Matrix\nLet‚Äôs test on a larger 10√ó10 system to see the upper triangular structure more clearly.\n\n\nShow code\n# Create a 10x10 random system\nnp.random.seed(123)\nA_large = np.random.randn(10, 10)\nb_large = np.random.randn(10)\n\nprint(\"Original 10√ó10 matrix A:\")\ndisplay(Markdown(matrix_to_latex(A_large)))\n\nprint(\"\\n\" + \"=\" * 70)\n\n# Solve using our implementation\nx_large = gaussian_elimination(A_large, b_large)\n\nprint(\"=\" * 70)\nprint(\"\\n‚úì Upper triangular structure achieved!\")\n\n\nOriginal 10√ó10 matrix A:\n\n\n\\[\\begin{bmatrix}-1.09 & 1.00 & 0.28 & -1.51 & -0.58 & 1.65 & -2.43 & -0.43 & 1.27 & -0.87 \\\\ -0.68 & -0.09 & 1.49 & -0.64 & -0.44 & -0.43 & 2.21 & 2.19 & 1.00 & 0.39 \\\\ 0.74 & 1.49 & -0.94 & 1.18 & -1.25 & -0.64 & 0.91 & -1.43 & -0.14 & -0.86 \\\\ -0.26 & -2.80 & -1.77 & -0.70 & 0.93 & -0.17 & 0.00 & 0.69 & -0.88 & 0.28 \\\\ -0.81 & -1.73 & -0.39 & 0.57 & 0.34 & -0.01 & 2.39 & 0.41 & 0.98 & 2.24 \\\\ -1.29 & -1.04 & 1.74 & -0.80 & 0.03 & 1.07 & 0.89 & 1.75 & 1.50 & 1.07 \\\\ -0.77 & 0.79 & 0.31 & -1.33 & 1.42 & 0.81 & 0.05 & -0.23 & -1.20 & 0.20 \\\\ 0.47 & -0.83 & 1.16 & -1.10 & -2.12 & 1.04 & -0.40 & -0.13 & -0.84 & -1.61 \\\\ 1.26 & -0.69 & 1.66 & 0.81 & -0.31 & -1.09 & -0.73 & -1.21 & 2.09 & 0.16 \\\\ 1.15 & -1.27 & 0.18 & 1.18 & -0.34 & 1.03 & -1.08 & -1.36 & 0.38 & -0.38\\end{bmatrix}\\]\n\n\n\n======================================================================\nUpper triangular matrix U:\n\n\n\\[\\begin{bmatrix}-1.09 & 1.00 & 0.28 & -1.51 & -0.58 & 1.65 & -2.43 & -0.43 & 1.27 & -0.87 \\\\ -0.00 & -0.72 & 1.31 & 0.30 & -0.08 & -1.47 & 3.72 & 2.46 & 0.21 & 0.93 \\\\ -0.00 & 0.00 & 3.22 & 1.07 & -1.89 & -3.94 & 10.50 & 5.69 & 1.36 & 1.35 \\\\ -0.00 & 0.00 & 0.00 & 0.82 & -2.93 & -3.41 & 8.91 & 3.46 & 1.04 & -0.34 \\\\ 0.00 & 0.00 & 0.00 & 0.00 & 6.41 & 7.26 & -17.36 & -8.55 & -1.51 & 2.79 \\\\ 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 1.15 & -1.79 & -0.91 & -0.09 & -0.48 \\\\ 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 3.20 & 0.39 & -1.78 & -0.72 \\\\ -0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -2.27 & 0.17 & 2.74 \\\\ 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 1.73 & -2.90 \\\\ -0.00 & -0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 3.24\\end{bmatrix}\\]\n\n\n======================================================================\n\n‚úì Upper triangular structure achieved!\n\n\n\nThis implementation demonstrates the fundamental algorithm for solving linear systems, connecting the geometric view from Lecture 1 with the algebraic machinery of row operations and matrix elimination."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#projection-of-a-vector-onto-a-line",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#projection-of-a-vector-onto-a-line",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "1. Projection of a Vector onto a Line",
    "text": "1. Projection of a Vector onto a Line\n\n\n\nProjection onto a Line\n\n\nSetup: Project vector \\(b\\) onto vector \\(a\\).\nKey equations:\n\\[\n\\begin{aligned}\np &= xa \\\\\na^T(b - xa) &= 0 \\\\\na^T b &= xa^T a \\\\\nx &= \\frac{a^T b}{a^T a}\n\\end{aligned}\n\\]\nComponents:\n\n\\(p\\): Projection of \\(b\\) onto \\(a\\)\n\\(e\\): Error vector\n\\(e \\perp a\\) (error is perpendicular to \\(a\\))\n\nKey relationships:\n\\[\ne = b - p\n\\]\n\\[\np + e = b\n\\]\n\\[\np = b - e\n\\]\nProjection formula:\n\\[\np = a\\frac{a^T b}{a^T a}\n\\]\nProperties: - If \\(b\\) is doubled, then \\(p\\) is doubled - If \\(a\\) is doubled, then \\(p\\) does not change at all"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#projection-matrix",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#projection-matrix",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "2. Projection Matrix",
    "text": "2. Projection Matrix\nMatrix form:\n\\[\np = Pb\n\\]\nwhere\n\\[\nP = \\frac{aa^T}{a^T a}\n\\]\nAnalysis:\nThe projection matrix \\(P = \\frac{aa^T}{a^T a}\\) has these properties:\n\nThe numerator \\(aa^T\\) is \\((n \\times 1)(1 \\times n) = n \\times n\\)\nThe denominator \\(a^T a\\) is a scalar\nTherefore \\(P\\) is \\(n \\times n\\)\nColumn space of \\(P\\) is the line through \\(a\\)\nRank is 1\nSymmetric: \\(P^T = P\\)\nIdempotent: \\(P^2 = P\\) (if I project twice, the result is the same)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#why-projection",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#why-projection",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "3. Why Projection?",
    "text": "3. Why Projection?\nProblem: The equation \\(Ax = b\\) may have 0 solutions when \\(m &gt; n\\).\nSolution: Solve \\(A\\hat{x} = p\\) instead, where \\(p\\) is the projection of \\(b\\) onto the column space.\n\n\n\nProjection in Higher Dimensions\n\n\nSetup:\nThe error \\(e = b - p\\) is perpendicular to the plane (spanned by \\(a_1\\) and \\(a_2\\)).\n\\[\nA = \\begin{bmatrix}| & | \\\\ a_1 & a_2 \\\\ | & |\\end{bmatrix}\n\\]\n\\[\np = \\hat{x}_1 a_1 + \\hat{x}_2 a_2 = A\\hat{x}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#finding-hatx",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#finding-hatx",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "4. Finding \\(\\hat{x}\\)",
    "text": "4. Finding \\(\\hat{x}\\)\nKey idea: \\(b - A\\hat{x} \\perp C(A)\\) (error is perpendicular to column space)\nDerivation: Error \\(e\\) is perpendicular to \\(a_1\\) and \\(a_2\\):\n\\[\n\\begin{aligned}\na_1^T(b - A\\hat{x}) &= 0 \\text{ and } a_2^T(b - A\\hat{x}) = 0 \\\\\n\\begin{bmatrix}a_1^T \\\\ a_2^T\\end{bmatrix}(b - A\\hat{x}) &= \\begin{bmatrix}0 \\\\ 0\\end{bmatrix} \\\\\nA^T(b - A\\hat{x}) &= \\mathbf{0}\n\\end{aligned}\n\\]\nKey relationships:\nThe error \\(e = b - A\\hat{x}\\) is in the left null space:\n\\[\ne \\in N(A^T)\n\\]\n\\[\ne \\perp C(A)\n\\]\nThis means the error is perpendicular to the column space of \\(A\\).\nNormal equations:\n\\[\nA^T b = A^T A\\hat{x}\n\\]\nSolution:\n\\[\n\\hat{x} = (A^T A)^{-1} A^T b\n\\]\nProjection:\n\\[\np = A\\hat{x} = A(A^T A)^{-1} A^T b\n\\]\nProjection matrix:\n\\[\nP = A(A^T A)^{-1} A^T\n\\]\nSpecial case: If \\(A\\) is invertible, then \\(P = I\\) (identity matrix).\nProperties in high dimensions: - \\(P^T = P\\) (symmetric) - \\(P^2 = P\\) (idempotent)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#least-squares-application",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#least-squares-application",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "5. Least Squares Application",
    "text": "5. Least Squares Application\n\n\n\nLeast Squares\n\n\nProblem: Fit a line \\(y = c + dt\\) to data points \\((1,1)\\), \\((2,2)\\), and \\((3,3)\\).\nSetup:\n\\[\nA = \\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3\n\\end{bmatrix}, \\quad\nb = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}, \\quad\nx = \\begin{bmatrix}\nc \\\\\nd\n\\end{bmatrix}\n\\]\nEquation system: \\(Ax = b\\) - Row 1: \\(c + 1 \\cdot d = 1\\) - Row 2: \\(c + 2 \\cdot d = 2\\) - Row 3: \\(c + 3 \\cdot d = 3\\)\nIssue: Cannot find exact solution for \\(x\\) because: - \\(A\\) is not invertible (not square: \\(3 \\times 2\\)) - System is overdetermined (3 equations, 2 unknowns)\nSolution: Use projection to find \\(\\hat{x}\\) that minimizes \\(\\|Ax - b\\|^2\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#geometric-interpretation",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#geometric-interpretation",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "6. Geometric Interpretation",
    "text": "6. Geometric Interpretation\n3 Lines in 3D: If we have 3 lines in 3-dimensional space, then \\(A\\) will be square.\nIf \\(A\\) is invertible (full rank): - \\(Ax = b\\) has an exact solution - The solution geometrically means: find the plane spanned by column 1 and column 2, then find where column 3 crosses it\n\nSource: MIT 18.06SC Linear Algebra, Lecture 15"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#overview",
    "href": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#overview",
    "title": "MIT 18.06SC Lecture 14: Orthogonal Vectors and Subspaces",
    "section": "Overview",
    "text": "Overview\nThis lecture introduces orthogonality ‚Äî one of the most important geometric concepts in linear algebra:\n\nOrthogonal vectors (perpendicular direction)\nOrthogonal subspaces (every vector in one is perpendicular to every vector in the other)\nThe fundamental orthogonal relationships: row space ‚ä• null space, column space ‚ä• left null space\nPreview of least squares and the normal equations\n\nReference: Lecture 10: Four Fundamental Subspaces"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#orthogonal-vectors",
    "href": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#orthogonal-vectors",
    "title": "MIT 18.06SC Lecture 14: Orthogonal Vectors and Subspaces",
    "section": "Orthogonal Vectors",
    "text": "Orthogonal Vectors\nDefinition: In \\(n\\)-dimensional space, two vectors are orthogonal if the angle between them is \\(90¬∞\\).\nMathematical condition:\n\\[\nx^T y = 0\n\\]\nInterpretation: The dot product (inner product) of orthogonal vectors equals zero.\n\nPythagorean Theorem in Vector Spaces\n\n\n\nPythagorean Theorem\n\n\nClassical form:\n\\[\n\\|a\\|^2 + \\|b\\|^2 = \\|c\\|^2\n\\]\nVector space form: For orthogonal vectors \\(x\\) and \\(y\\) (where \\(x^T y = 0\\)):\n\\[\n\\|x\\|^2 + \\|y\\|^2 = \\|x + y\\|^2\n\\]\nProof:\n\\[\n\\begin{aligned}\n\\|x + y\\|^2 &= (x + y)^T(x + y) \\\\\n&= x^T x + x^T y + y^T x + y^T y \\\\\n&= \\|x\\|^2 + 2(x^T y) + \\|y\\|^2 \\\\\n&= \\|x\\|^2 + \\|y\\|^2\n\\end{aligned}\n\\]\nThe cross terms vanish because \\(x^T y = 0\\) (orthogonality condition).\n\n\n\n\n\n\nNoteVector Norm Squared\n\n\n\n\\[\n\\|x\\|^2 = x^T \\cdot x = \\sum_{i=1}^n x_i^2\n\\]\nThe squared length (magnitude) of a vector equals the sum of squares of its components."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#orthogonal-subspaces",
    "href": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#orthogonal-subspaces",
    "title": "MIT 18.06SC Lecture 14: Orthogonal Vectors and Subspaces",
    "section": "Orthogonal Subspaces",
    "text": "Orthogonal Subspaces\nDefinition: Subspace \\(S\\) is orthogonal to subspace \\(T\\) if every vector in \\(S\\) is orthogonal to every vector in \\(T\\).\nMathematical statement:\n\\[\nS \\perp T \\iff \\text{for all } s \\in S \\text{ and } t \\in T, \\quad s^T t = 0\n\\]\n\nExample: Are Wall and Floor Orthogonal Subspaces?\n\n\n\nFloor and Wall\n\n\nQuestion: In 3D space, is the wall (a 2D subspace) orthogonal to the floor (another 2D subspace)?\nAnswer: No, for two reasons:\nReason 1 (Intersection): - Their intersection is a line, not just the origin - Vectors along this line are in both subspaces - A vector cannot be orthogonal to itself (unless it‚Äôs zero) - Therefore, not all vectors in one are orthogonal to all vectors in the other\nReason 2 (Dimension): - Wall has dimension 2 - Floor has dimension 2 - The whole space has dimension 3 - For orthogonal subspaces: \\(\\dim(S) + \\dim(T) \\leq \\dim(\\text{space})\\) - Here: \\(2 + 2 = 4 &gt; 3\\) (impossible!)\n\n\n\n\n\n\nImportantKey Insight\n\n\n\nOrthogonal subspaces can only intersect at the origin."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#row-space-and-null-space",
    "href": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#row-space-and-null-space",
    "title": "MIT 18.06SC Lecture 14: Orthogonal Vectors and Subspaces",
    "section": "Row Space and Null Space",
    "text": "Row Space and Null Space\n\n\n\nSubspace Orthogonality\n\n\nTheorem: The null space \\(N(A)\\) contains all vectors perpendicular to the row space \\(C(A^T)\\).\nProof: For any \\(x \\in N(A)\\) and any row \\(r_i\\) of \\(A\\):\n\\[\nAx = \\mathbf{0} \\implies r_i^T x = 0 \\text{ for all rows } r_i\n\\]\nTherefore \\(x\\) is orthogonal to every row, hence orthogonal to the entire row space.\nDirect sum decomposition:\n\\[\n\\mathbb{R}^n = C(A^T) \\oplus N(A)\n\\]\nInterpretation: - Every vector in \\(\\mathbb{R}^n\\) can be uniquely decomposed into a row space component and a null space component - These two subspaces are orthogonal complements - \\(\\dim(C(A^T)) + \\dim(N(A)) = n\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#column-space-and-left-null-space",
    "href": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#column-space-and-left-null-space",
    "title": "MIT 18.06SC Lecture 14: Orthogonal Vectors and Subspaces",
    "section": "Column Space and Left Null Space",
    "text": "Column Space and Left Null Space\nTheorem: The left null space \\(N(A^T)\\) contains all vectors perpendicular to the column space \\(C(A)\\).\nProof: For any \\(y \\in N(A^T)\\):\n\\[\nA^T y = \\mathbf{0} \\implies y^T A = \\mathbf{0}^T\n\\]\nThis means \\(y\\) is orthogonal to every column of \\(A\\).\nDirect sum decomposition:\n\\[\n\\mathbb{R}^m = C(A) \\oplus N(A^T)\n\\]\nInterpretation: - Every vector in \\(\\mathbb{R}^m\\) can be uniquely decomposed into a column space component and a left null space component - These two subspaces are orthogonal complements - \\(\\dim(C(A)) + \\dim(N(A^T)) = m\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#orthogonality-and-the-least-squares-problem",
    "href": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#orthogonality-and-the-least-squares-problem",
    "title": "MIT 18.06SC Lecture 14: Orthogonal Vectors and Subspaces",
    "section": "Orthogonality and the Least Squares Problem",
    "text": "Orthogonality and the Least Squares Problem\n\nWhen There‚Äôs No Exact Solution\nProblem: For \\(Ax = b\\) where \\(m &gt; n\\), there‚Äôs typically no exact solution.\nSolution: Find the best approximate solution \\(\\hat{x}\\) that minimizes \\(\\|Ax - b\\|^2\\).\n\n\nNormal Equations\nApproximate solution:\n\\[\nA^T A\\hat{x} = A^T b\n\\]\nNote: The derivation of \\(\\hat{x}\\) and why this works will be introduced in Lecture 15.\n\n\nProperties of \\(A^T A\\)\nNull space relationship:\n\\[\nN(A^T A) = N(A)\n\\]\nProof: - If \\(Ax = \\mathbf{0}\\), then \\(A^T Ax = \\mathbf{0}\\) - Conversely, if \\(A^T Ax = \\mathbf{0}\\), then \\(x^T A^T Ax = \\|Ax\\|^2 = 0 \\implies Ax = \\mathbf{0}\\)\nRank relationship:\n\\[\n\\operatorname{rank}(A^T A) = \\operatorname{rank}(A)\n\\]\nInvertibility: \\(A^T A\\) is invertible when \\(\\operatorname{rank}(A) = n\\) (full column rank).\nWhen full column rank:\n\\[\n\\hat{x} = (A^T A)^{-1} A^T b\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#summary-of-orthogonal-complements",
    "href": "Math/MIT18.06/mit1806-lecture14-orthogonality.html#summary-of-orthogonal-complements",
    "title": "MIT 18.06SC Lecture 14: Orthogonal Vectors and Subspaces",
    "section": "Summary of Orthogonal Complements",
    "text": "Summary of Orthogonal Complements\n\n\n\nSpace in \\(\\mathbb{R}^n\\)\nOrthogonal Complement\nTotal Dimension\n\n\n\n\nRow space \\(C(A^T)\\)\nNull space \\(N(A)\\)\n\\(r + (n-r) = n\\)\n\n\n\n\n\n\nSpace in \\(\\mathbb{R}^m\\)\nOrthogonal Complement\nTotal Dimension\n\n\n\n\nColumn space \\(C(A)\\)\nLeft null space \\(N(A^T)\\)\n\\(r + (m-r) = m\\)\n\n\n\nKey relationships:\n\n\\(C(A^T) \\perp N(A)\\) and they span \\(\\mathbb{R}^n\\)\n\\(C(A) \\perp N(A^T)\\) and they span \\(\\mathbb{R}^m\\)\nDimensions add up to the ambient space dimension\nOrthogonal subspaces intersect only at the origin\n\n\nSource: MIT 18.06SC Linear Algebra, Lecture 14"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture17-gram-schmidt.html#orthonormal-vectors",
    "href": "Math/MIT18.06/mit1806-lecture17-gram-schmidt.html#orthonormal-vectors",
    "title": "MIT 18.06 Lecture 17: Orthogonal Matrices and Gram-Schmidt",
    "section": "Orthonormal Vectors",
    "text": "Orthonormal Vectors\nOrthonormal vectors are perpendicular to each other and have unit length.\n\\[\nq_i^\\top q_j=\\begin{cases}0 & \\text{if }i\\ne j,\\\\1&\\text{if }i=j\\end{cases}\n\\]\nThis means:\n\n\\(q_i \\perp q_j\\) if \\(i\\neq j\\) (orthogonal)\n\\(\\|q_i\\|^2=1 \\: \\forall i \\in \\{1,2...n\\}\\) (normalized)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture17-gram-schmidt.html#orthonormal-matrices",
    "href": "Math/MIT18.06/mit1806-lecture17-gram-schmidt.html#orthonormal-matrices",
    "title": "MIT 18.06 Lecture 17: Orthogonal Matrices and Gram-Schmidt",
    "section": "Orthonormal Matrices",
    "text": "Orthonormal Matrices\nA matrix \\(Q\\) with orthonormal columns satisfies \\(Q^\\top Q=I_n\\).\n\\[\nQ^\\top Q=I_n\n\\]\nIf \\(Q\\) is square, then \\(Q^\\top=Q^{-1}\\) (the transpose equals the inverse).\n\nExamples of Orthonormal Matrices\nPermutation Matrices:\n\\[\nQ=\\begin{bmatrix}0&0&1\\\\1&0&0\\\\0&1&0\\end{bmatrix}\n\\]\nRotation Matrix:\n\\[\nQ=\\begin{bmatrix}\\cos\\theta&-\\sin\\theta\\\\ \\sin\\theta&\\cos\\theta \\end{bmatrix}\n\\]\nHadamard Matrix:\n\\[\nQ=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1&1\\\\1&-1\\end{bmatrix}\n\\]\n\\[\nQ=\\frac{1}{2}\\begin{bmatrix}1&1&1&1\\\\1&-1&1&-1\\\\1&1&-1&-1\\\\1&-1&-1&1\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture17-gram-schmidt.html#projection-onto-orthonormal-matrices",
    "href": "Math/MIT18.06/mit1806-lecture17-gram-schmidt.html#projection-onto-orthonormal-matrices",
    "title": "MIT 18.06 Lecture 17: Orthogonal Matrices and Gram-Schmidt",
    "section": "Projection onto Orthonormal Matrices",
    "text": "Projection onto Orthonormal Matrices\nWhen \\(Q\\) has orthonormal columns, the projection matrix simplifies significantly.\n\\[\nP=Q(Q^\\top Q)^{-1}Q^\\top=QQ^\\top\n\\]\nIf \\(Q\\) is square: \\(P=I\\)\nProof of the 2 key properties:\n\nSymmetric: \\(QQ^\\top\\) is symmetric\nIdempotent:\n\n\\[\n(QQ^\\top)(QQ^\\top)=QIQ^\\top=QQ^\\top\n\\]\nLeast squares simplification:\nGeneral form:\n\\[\nA^\\top A\\hat{x}=A^\\top b\n\\]\nIf \\(A\\) is an orthonormal matrix \\(Q\\), the solution becomes trivial:\n\\[\n\\begin{aligned}\nQ^\\top Q \\hat{x} &= Q^\\top b \\\\\n\\hat{x} &= Q^\\top b \\\\\n\\hat{x}_i &= q_i^\\top b\n\\end{aligned}\n\\]\nEach component is simply the projection of \\(b\\) onto the corresponding column of \\(Q\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture17-gram-schmidt.html#gram-schmidt-process",
    "href": "Math/MIT18.06/mit1806-lecture17-gram-schmidt.html#gram-schmidt-process",
    "title": "MIT 18.06 Lecture 17: Orthogonal Matrices and Gram-Schmidt",
    "section": "Gram-Schmidt Process",
    "text": "Gram-Schmidt Process\nThe Gram-Schmidt process converts independent vectors into orthonormal vectors.\nGiven 2 independent vectors \\(a\\) and \\(b\\):\n\n\\(q_1=\\frac{A}{\\|A\\|}\\)\n\\(q_2=\\frac{B}{\\|B\\|}\\)\n\n\n2D Case\n\n\n\nGram-Schmidt in 2D\n\n\nSet \\(a\\) as \\(A\\) (no change needed for the first vector).\nWe then remove the projection of \\(b\\) onto \\(a\\) from \\(b\\), using the error component as \\(B\\).\nThis ensures \\(A \\perp B\\).\n\\[\nB=b-\\frac{A^\\top b}{A^\\top A}A\n\\]\nProof:\n\\[\nA^\\top B=A^\\top b-A^\\top A\\frac{A^\\top b}{A^\\top A}=0\n\\]\n\n\n3D Case\nAt this point, we have orthogonal vectors \\(A\\) and \\(B\\), and we find \\(C\\) by removing its projections onto both \\(A\\) and \\(B\\).\n\\[\nq_3=\\frac{C}{\\|C\\|}\n\\]\n\\[\nC=c-\\frac{A^\\top c}{A^\\top A}A-\\frac{B^\\top c}{B^\\top B}B\n\\]\n\\[\nC \\perp A \\text{ and } C\\perp B\n\\]\n\n\n\nGram-Schmidt in 3D"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture17-gram-schmidt.html#real-example",
    "href": "Math/MIT18.06/mit1806-lecture17-gram-schmidt.html#real-example",
    "title": "MIT 18.06 Lecture 17: Orthogonal Matrices and Gram-Schmidt",
    "section": "Real Example",
    "text": "Real Example\nGiven:\n\n\\(a=\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\\)\n\\(b=\\begin{bmatrix}1\\\\0\\\\2\\end{bmatrix}\\)\n\nThe original matrix \\(M\\) is:\n\\[\nM=\\begin{bmatrix}1&1\\\\1&0\\\\1&2\\end{bmatrix}\n\\]\nStep 1: Use \\(a\\) as \\(A\\)\nStep 2: Calculate \\(B\\):\n\\[\n\\begin{aligned}\nB &= b-\\frac{A^\\top b}{A^\\top A}A \\\\\n&= b-\\frac{3}{3}A \\\\\n&= \\begin{bmatrix}0\\\\-1\\\\1\\end{bmatrix}\n\\end{aligned}\n\\]\nStep 3: Normalize to get \\(Q\\)\n\\(q_1\\):\n\\[\n\\frac{1}{\\sqrt{3}}\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}=\\begin{bmatrix}\\frac{1}{\\sqrt{3}}\\\\\\frac{1}{\\sqrt{3}}\\\\\\frac{1}{\\sqrt{3}}\\end{bmatrix}\n\\]\n\\(q_2\\):\n\\[\n\\frac{1}{\\sqrt{2}}\\begin{bmatrix}0\\\\-1\\\\1\\end{bmatrix}=\\begin{bmatrix}0\\\\\\frac{-1}{\\sqrt{2}}\\\\\\frac{1}{\\sqrt{2}}\\end{bmatrix}\n\\]\nResult:\n\\[\nQ=\\begin{bmatrix}\\frac{1}{\\sqrt{3}}&0\\\\\\frac{1}{\\sqrt{3}}&\\frac{-1}{\\sqrt{2}}\\\\\\frac{1}{\\sqrt{3}}&\\frac{1}{\\sqrt{2}}\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture17-gram-schmidt.html#qr-factorization",
    "href": "Math/MIT18.06/mit1806-lecture17-gram-schmidt.html#qr-factorization",
    "title": "MIT 18.06 Lecture 17: Orthogonal Matrices and Gram-Schmidt",
    "section": "QR Factorization",
    "text": "QR Factorization\n\\(Q\\) spans the same column space as \\(M\\), so we can write \\(M=QR\\).\n\n\\(Q\\) is \\(m \\times n\\) (orthonormal columns)\n\\(R\\) is \\(n \\times n\\) (upper triangular)\n\nBecause \\(Q\\) is orthonormal:\n\\[\nQ^\\top M=R\n\\]\n\\[\nr_{ij}=q_i^\\top m_j\n\\]\nWhy is R upper triangular?\n\\[\n\\begin{bmatrix}|&|\\\\a_1&a_2\\\\|&|\\end{bmatrix}=\\begin{bmatrix}q_1&q_2\\end{bmatrix}\\begin{bmatrix}a_1^\\top q_1&a_2^\\top q_1\\\\0 &a_2^\\top q_2\\end{bmatrix}\n\\]\nBecause \\(q_1 \\perp q_2\\), we have \\(q_2^\\top a_1=0\\) in the lower left position, making \\(R\\) upper triangular.\n\nSource: MIT 18.06SC Linear Algebra, Lecture 17"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-2-transpose.html",
    "href": "Math/MIT18.06/mit1806-lecture5-2-transpose.html",
    "title": "MIT 18.06SC Lecture 5.2: Transpose",
    "section": "",
    "text": "My lecture notes\nThe transpose operation switches rows to columns. This post covers the transpose portion of Lecture 5."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-2-transpose.html#context",
    "href": "Math/MIT18.06/mit1806-lecture5-2-transpose.html#context",
    "title": "MIT 18.06SC Lecture 5.2: Transpose",
    "section": "",
    "text": "My lecture notes\nThe transpose operation switches rows to columns. This post covers the transpose portion of Lecture 5."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-2-transpose.html#definition",
    "href": "Math/MIT18.06/mit1806-lecture5-2-transpose.html#definition",
    "title": "MIT 18.06SC Lecture 5.2: Transpose",
    "section": "Definition",
    "text": "Definition\nTranspose is the operation that switches rows to columns - each \\(A_{i,j}\\) becomes \\(A_{j,i}\\)\nFor a matrix \\(A\\), the transpose \\(A^T\\) satisfies: \\[\n(A^T)_{i,j} = A_{j,i}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-2-transpose.html#symmetric-matrices",
    "href": "Math/MIT18.06/mit1806-lecture5-2-transpose.html#symmetric-matrices",
    "title": "MIT 18.06SC Lecture 5.2: Transpose",
    "section": "Symmetric Matrices",
    "text": "Symmetric Matrices\nA symmetric matrix satisfies \\(A = A^T\\)\n\nProperty: \\(RR^T\\) is Symmetric\nFor any matrix \\(R\\), the product \\(RR^T\\) is always symmetric.\nProof: \\[\n(RR^T)^T = (R^T)^T R^T = RR^T\n\\]\nTherefore, \\(RR^T\\) is symmetric.\n\nSource: MIT 18.06SC Linear Algebra, Lecture 5"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture22-diagonalization-powers.html",
    "href": "Math/MIT18.06/mit1806-lecture22-diagonalization-powers.html",
    "title": "MIT 18.06 Lecture 22: Diagonalization and Powers of A",
    "section": "",
    "text": "This lecture explores one of the most powerful applications of eigenvalues and eigenvectors: diagonalization. By expressing a matrix in terms of its eigenvalues and eigenvectors, we can efficiently compute matrix powers and solve recurrence relations like the Fibonacci sequence."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture22-diagonalization-powers.html#diagonalizing-a-matrix",
    "href": "Math/MIT18.06/mit1806-lecture22-diagonalization-powers.html#diagonalizing-a-matrix",
    "title": "MIT 18.06 Lecture 22: Diagonalization and Powers of A",
    "section": "Diagonalizing a Matrix",
    "text": "Diagonalizing a Matrix\nA matrix \\(A\\) is diagonalizable if it can be written as:\n\\[\nS^{-1}AS = \\Lambda\n\\]\nwhere \\(\\Lambda\\) is a diagonal matrix containing the eigenvalues of \\(A\\).\nKey definition: \\(S\\) is the matrix whose columns are the \\(n\\) eigenvectors of \\(A\\):\n\\[\nS = \\begin{bmatrix}\n| & | & | & | \\\\\nx_1 & x_2 & \\ldots & x_n \\\\\n| & | & | & |\n\\end{bmatrix}\n\\]\n\nWhy Does Diagonalization Work?\nWhen we compute \\(AS\\), we multiply \\(A\\) by each eigenvector column:\n\\[\nAS = \\begin{bmatrix}\n| & | & | & | \\\\\n\\lambda_1 x_1 & \\lambda_2 x_2 & \\ldots & \\lambda_n x_n \\\\\n| & | & | & |\n\\end{bmatrix}\n\\]\nSince \\(Ax_i = \\lambda_i x_i\\) for each eigenvector, multiplying by \\(A\\) simply scales each column by its corresponding eigenvalue.\nNow, multiplying both sides by \\(S^{-1}\\) from the left:\n\\[\nS^{-1}AS = \\begin{bmatrix}\n\\lambda_1 & 0 & \\ldots & 0 \\\\\n0 & \\lambda_2 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\ldots & \\lambda_n\n\\end{bmatrix} = \\Lambda\n\\]\nThis gives us a beautiful representation of \\(A\\):\n\\[\nSS^{-1}AS = S\\Lambda = IAS = AS\n\\]\n\\[\nASS^{-1} = S\\Lambda S^{-1}\n\\]\n\\[\nA = S\\Lambda S^{-1}\n\\]\nThis formula expresses \\(A\\) in terms of its eigenvalues (\\(\\Lambda\\)) and eigenvectors (\\(S\\))."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture22-diagonalization-powers.html#powers-of-a",
    "href": "Math/MIT18.06/mit1806-lecture22-diagonalization-powers.html#powers-of-a",
    "title": "MIT 18.06 Lecture 22: Diagonalization and Powers of A",
    "section": "Powers of A",
    "text": "Powers of A\n\nA Squared\nIf \\(Ax = \\lambda x\\), then what happens when we apply \\(A\\) again?\n\\[\nA^2 x = A(Ax) = A(\\lambda x) = \\lambda Ax = \\lambda^2 x\n\\]\nConclusion:\n\nThe eigenvalues of \\(A^2\\) are the squares of \\(A\\)‚Äôs eigenvalues\nThe eigenvectors of \\(A^2\\) are the same as \\(A\\)‚Äôs eigenvectors\n\nUsing diagonalization:\n\\[\nA^2 = S\\Lambda S^{-1} \\cdot S\\Lambda S^{-1} = S\\Lambda^2 S^{-1}\n\\]\nNotice how the middle terms \\(S^{-1}S\\) cancel to give the identity matrix, leaving us with \\(\\Lambda^2\\).\n\n\nThe General Power Formula\nBy the same reasoning:\n\\[\nA^k = S\\Lambda^k S^{-1}\n\\]\nwhere \\(\\Lambda^k\\) is simply the diagonal matrix with each eigenvalue raised to the \\(k\\)-th power:\n\\[\n\\Lambda^k = \\begin{bmatrix}\n\\lambda_1^k & 0 & \\ldots & 0 \\\\\n0 & \\lambda_2^k & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\ldots & \\lambda_n^k\n\\end{bmatrix}\n\\]\nKey properties:\n\nThe eigenvalues of \\(A^k\\) are the \\(k\\)-th powers of the eigenvalues of \\(A\\)\nThe eigenvectors remain unchanged\n\nThis formula transforms a difficult problem (computing matrix powers) into a simple one (raising numbers to powers).\n\n\nImportant Theorems\nTheorem 1 (Convergence to zero): \\(A^k \\to 0\\) as \\(k \\to \\infty\\) if and only if all \\(|\\lambda_i| &lt; 1\\).\nThis is because \\(\\lambda_i^k \\to 0\\) when \\(|\\lambda_i| &lt; 1\\), making all entries in \\(\\Lambda^k\\) approach zero.\nTheorem 2 (Diagonalizability with distinct eigenvalues): If all eigenvalues \\(\\lambda_i\\) are distinct (no repeated eigenvalues), then \\(A\\) is guaranteed to have \\(n\\) independent eigenvectors and is diagonalizable.\nTheorem 3 (Repeated eigenvalues): If there are repeated eigenvalues, \\(A\\) may or may not have \\(n\\) independent eigenvectors.\nExamples:\n\nThe identity matrix \\(I_n\\) has eigenvalue 1 repeated \\(n\\) times, but it has \\(n\\) independent eigenvectors (any vector is an eigenvector)\nTriangular matrices with repeated eigenvalues often lack sufficient eigenvectors\n\n\n\nExample: Non-diagonalizable Matrix\nConsider:\n\\[\nA = \\begin{bmatrix}2 & 1 \\\\ 0 & 2\\end{bmatrix}\n\\]\nFinding eigenvalues:\n\\[\n|A - \\lambda I| = \\begin{vmatrix}2 - \\lambda & 1 \\\\ 0 & 2 - \\lambda\\end{vmatrix} = (2 - \\lambda)^2 = 0\n\\]\n\\[\n\\lambda_1 = \\lambda_2 = 2\n\\]\nFinding eigenvectors: The null space of \\(A - \\lambda I = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}\\) is only one-dimensional.\nThe only eigenvector is \\(\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\\).\nSince we have only one independent eigenvector for a 2√ó2 matrix, \\(A\\) is not diagonalizable."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture22-diagonalization-powers.html#solving-difference-equations-u_k-ak-u_0",
    "href": "Math/MIT18.06/mit1806-lecture22-diagonalization-powers.html#solving-difference-equations-u_k-ak-u_0",
    "title": "MIT 18.06 Lecture 22: Diagonalization and Powers of A",
    "section": "Solving Difference Equations: \\(u_k = A^k u_0\\)",
    "text": "Solving Difference Equations: \\(u_k = A^k u_0\\)\nMany problems in science and engineering involve iterating a linear transformation:\n\n\\(u_1 = Au_0\\)\n\\(u_2 = Au_1 = A^2u_0\\)\n\\(u_k = A^k u_0\\)\n\n\nSolution Strategy\nThe key insight is to decompose \\(u_0\\) in terms of eigenvectors:\n\\[\nu_0 = c_1 x_1 + c_2 x_2 + \\ldots + c_n x_n = SC\n\\]\nwhere \\(C = \\begin{bmatrix}c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n\\end{bmatrix}\\) are the coefficients.\nApplying \\(A\\) to each eigenvector:\n\\[\nAu_0 = \\lambda_1 c_1 x_1 + \\lambda_2 c_2 x_2 + \\ldots + \\lambda_n c_n x_n\n\\]\nApplying \\(A\\) a total of \\(k\\) times:\n\\[\nA^k u_0 = \\lambda_1^k c_1 x_1 + \\lambda_2^k c_2 x_2 + \\ldots + \\lambda_n^k c_n x_n = S\\Lambda^k C\n\\]\nThis shows that each eigenvector component grows (or decays) exponentially at a rate determined by its eigenvalue."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture22-diagonalization-powers.html#fibonacci-example",
    "href": "Math/MIT18.06/mit1806-lecture22-diagonalization-powers.html#fibonacci-example",
    "title": "MIT 18.06 Lecture 22: Diagonalization and Powers of A",
    "section": "Fibonacci Example",
    "text": "Fibonacci Example\n\nThe Question\nWhat is \\(F_{100}\\) in the Fibonacci sequence \\(0, 1, 1, 2, 3, 5, 8, 13, \\ldots\\)?\nComputing this directly would require 100 additions, but eigenvalues give us a closed-form solution.\n\n\nThe Trick: Convert to Matrix Form\nThe Fibonacci recurrence relation is:\n\\[\nF_{k+2} = F_{k+1} + F_k\n\\]\nWe can convert this scalar recurrence into a vector equation. Define:\n\\[\nu_k = \\begin{bmatrix}F_{k+1} \\\\ F_k\\end{bmatrix}\n\\]\nThen:\n\\[\nu_{k+1} = \\begin{bmatrix}F_{k+2} \\\\ F_{k+1}\\end{bmatrix} = \\begin{bmatrix}F_{k+1} + F_k \\\\ F_{k+1}\\end{bmatrix} = \\begin{bmatrix}1 & 1 \\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix}F_{k+1} \\\\ F_k\\end{bmatrix} = Au_k\n\\]\nSo the Fibonacci sequence is generated by repeatedly multiplying by matrix \\(A = \\begin{bmatrix}1 & 1 \\\\ 1 & 0\\end{bmatrix}\\).\n\n\nFinding Eigenvalues and Eigenvectors\n\\[\n|A - \\lambda I| = \\begin{vmatrix}1 - \\lambda & 1 \\\\ 1 & -\\lambda\\end{vmatrix} = \\lambda^2 - \\lambda - 1 = 0\n\\]\nFrom Vieta‚Äôs formulas for a quadratic equation:\n\\[\n\\lambda_1 + \\lambda_2 = 1\n\\]\n\\[\n\\lambda_1 \\lambda_2 = -1\n\\]\nUsing the quadratic formula for \\(ax^2 + bx + c = 0\\):\n\\[\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n\\]\nWe get:\n\\[\n\\lambda = \\frac{1 \\pm \\sqrt{1 - 4(1)(-1)}}{2} = \\frac{1 \\pm \\sqrt{5}}{2}\n\\]\n\\[\n\\lambda_1 = \\frac{1 + \\sqrt{5}}{2} \\approx 1.618 \\quad \\text{(the golden ratio!)}\n\\]\n\\[\n\\lambda_2 = \\frac{1 - \\sqrt{5}}{2} \\approx -0.618\n\\]\n\n\nThe Solution\nSince \\(u_k = A^k u_0\\), we can write:\n\\[\nu_k = c_1 \\lambda_1^k x_1 + c_2 \\lambda_2^k x_2\n\\]\nFor large \\(k\\):\n\\[\nF_{100} \\approx c_1 \\left(\\frac{1 + \\sqrt{5}}{2}\\right)^{100}\n\\]\nBecause \\(|\\lambda_2| &lt; 1\\), the term \\(\\lambda_2^{100}\\) becomes negligibly small. The Fibonacci numbers grow exponentially at a rate determined by the golden ratio!\nKey insight: The eigenvalues tell us the long-term behavior. The dominant eigenvalue \\(\\lambda_1 \\approx 1.618\\) determines how fast the Fibonacci sequence grows, while the smaller eigenvalue \\(|\\lambda_2| &lt; 1\\) contributes a rapidly decaying oscillation that becomes negligible for large \\(k\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#overview",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#overview",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Overview",
    "text": "Overview\nThis lecture explores new vector spaces beyond \\(\\mathbb{R}^n\\):\n\nMatrix spaces as vector spaces with their own subspaces\nRank-1 matrices and their fundamental structure\nDimension formulas for intersections and sums of subspaces\nApplications to differential equations and constraints"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#matrix-subspaces",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#matrix-subspaces",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Matrix Subspaces",
    "text": "Matrix Subspaces\n\n\n\n\n\n\nNoteBackground: Vector Spaces and Subspaces\n\n\n\nFor a review of vector space fundamentals (what makes something a subspace, closure under addition and scalar multiplication), see Lecture 5.3: Vector Spaces. Matrix subspaces follow the same rules‚Äîthey just happen to be spaces of matrices rather than vectors in \\(\\mathbb{R}^n\\).\n\n\n\nSpace of All 3√ó3 Matrices\nNotation: \\(M\\) = all 3√ó3 matrices\nDimension: \\(\\dim(M) = 9\\)\nEach entry is a free variable, so a 3√ó3 matrix space has dimension \\(3 \\times 3 = 9\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#important-matrix-subspaces",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#important-matrix-subspaces",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Important Matrix Subspaces",
    "text": "Important Matrix Subspaces\n\nSymmetric Matrices: \\(S\\)\nDefinition: A matrix \\(A\\) is symmetric if \\(A = A^T\\)\nProperties:\n\nSum of two symmetric matrices is symmetric (closed under addition)\nScalar multiple of a symmetric matrix is symmetric (closed under scalar multiplication)\nTherefore \\(S\\) is a subspace\n\nDimension: \\(\\dim(S) = 6\\)\nFree variables (independent entries):\n\n\\((1,1)\\) - first diagonal entry\n\\((2,2)\\) - second diagonal entry\n\\((3,3)\\) - third diagonal entry\n\\((1,2) = (2,1)\\) - upper/lower symmetric pair\n\\((1,3) = (3,1)\\) - upper/lower symmetric pair\n\\((2,3) = (3,2)\\) - upper/lower symmetric pair\n\nExample: \\[\n\\begin{bmatrix}\na & b & c \\\\\nb & d & e \\\\\nc & e & f\n\\end{bmatrix}\n\\] has 6 free parameters: \\(a, b, c, d, e, f\\).\n\n\nUpper Triangular Matrices: \\(U\\)\nDefinition: A matrix \\(A\\) is upper triangular if all entries below the diagonal are zero\nDimension: \\(\\dim(U) = 6\\)\nFree variables:\n\n3 diagonal entries: \\((1,1), (2,2), (3,3)\\)\n3 above-diagonal entries: \\((1,2), (1,3), (2,3)\\)\n\nExample: \\[\n\\begin{bmatrix}\na & b & c \\\\\n0 & d & e \\\\\n0 & 0 & f\n\\end{bmatrix}\n\\]\n\n\nDiagonal Matrices: \\(D = S \\cap U\\)\nDefinition: The intersection of symmetric and upper triangular matrices\nDimension: \\(\\dim(D) = 3\\)\nFree variables:\n\n\\((1,1)\\)\n\\((2,2)\\)\n\\((3,3)\\)\n\nExample: \\[\n\\begin{bmatrix}\na & 0 & 0 \\\\\n0 & b & 0 \\\\\n0 & 0 & c\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nTipKey Insight\n\n\n\nA matrix that is both symmetric and upper triangular must be diagonal."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#dimension-formula-for-subspaces",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#dimension-formula-for-subspaces",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Dimension Formula for Subspaces",
    "text": "Dimension Formula for Subspaces\n\nSum of Subspaces: \\(S + U\\)\nDefinition: \\(S + U\\) contains all matrices that can be written as the sum of a symmetric matrix and an upper triangular matrix.\nFor 3√ó3 matrices: \\[\nS + U = M\n\\]\nEvery 3√ó3 matrix can be decomposed into symmetric and upper triangular parts.\n\n\nDimension Formula\nGeneral formula: \\[\n\\dim(S) + \\dim(U) = \\dim(S \\cap U) + \\dim(S + U)\n\\]\nFor our example: \\[\n6 + 6 = 3 + 9\n\\]\n\n\n\n\n\n\nNoteInclusion-Exclusion Analogy\n\n\n\nThis is analogous to the inclusion-exclusion principle:\n\nThe sum of dimensions counts the intersection twice\nSo we must subtract it once to get the dimension of the union/sum"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#example-differential-equations-as-vector-spaces",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#example-differential-equations-as-vector-spaces",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Example: Differential Equations as Vector Spaces",
    "text": "Example: Differential Equations as Vector Spaces\n\nSecond-Order Homogeneous Differential Equation\nEquation: \\[\n\\frac{d^2y}{dx^2} + y = 0\n\\]\nor equivalently: \\[\ny'' + y = 0\n\\]\nSolutions: The solution space is spanned by: \\[\ny = c_1 \\cos(x) + c_2 \\sin(x)\n\\]\nDimension: 2 (two free parameters: \\(c_1\\) and \\(c_2\\))\n\n\n\n\n\n\nImportantKey Principle\n\n\n\nThe set of all solutions to a linear homogeneous differential equation forms a vector space. The dimension equals the order of the differential equation."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#rank-1-matrices",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#rank-1-matrices",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Rank-1 Matrices",
    "text": "Rank-1 Matrices\n\nStructure of Rank-1 Matrices\nDefinition: A matrix has rank 1 if it can be written as the outer product of a column vector and a row vector.\nGeneral form: \\[\nA = u v^T\n\\]\nwhere:\n\n\\(u\\) is a column vector (in \\(\\mathbb{R}^m\\))\n\\(v^T\\) is a row vector (in \\(\\mathbb{R}^n\\))\n\n\n\nExample\n\\[\nA = \\begin{bmatrix}\n1 & 4 & 5 \\\\\n2 & 8 & 10\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 4 & 5\n\\end{bmatrix}\n\\]\nVerification: \\[\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 4 & 5\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 \\cdot 1 & 1 \\cdot 4 & 1 \\cdot 5 \\\\\n2 \\cdot 1 & 2 \\cdot 4 & 2 \\cdot 5\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 4 & 5 \\\\\n2 & 8 & 10\n\\end{bmatrix}\n\\]\nKey observation: All rows are multiples of each other (row 2 = 2 √ó row 1).\n\n\nRank-1 Matrices Do Not Form a Subspace\nClaim: The set of all rank-1 matrices is not a subspace.\nReason: Rank-1 matrices are not closed under addition.\nCounterexample: \\[\nA_1 = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}, \\quad\nA_2 = \\begin{bmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]\nBoth have rank 1, but: \\[\nA_1 + A_2 = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]\nhas rank 2.\n\n\n\n\n\n\nWarningGeneral Principle\n\n\n\nIf \\(A\\) has rank \\(n\\) and \\(B\\) has rank \\(m\\), then \\(A + B\\) can have rank anywhere from \\(|n - m|\\) to \\(n + m\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#null-space-example",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#null-space-example",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Null Space Example",
    "text": "Null Space Example\n\nProblem Setup\nGiven: \\(S\\) is the set of all vectors \\(v\\) in \\(\\mathbb{R}^4\\) such that: \\[\nv_1 + v_2 + v_3 + v_4 = 0\n\\]\nQuestion: What is the dimension of \\(S\\)?\n\n\nSolution Using Null Space\nDefine the matrix: \\[\nA = \\begin{bmatrix}\n1 & 1 & 1 & 1\n\\end{bmatrix}\n\\]\nThen \\(S\\) is the null space of \\(A\\): \\[\nS = N(A) = \\left\\{\nv = \\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\nv_3 \\\\\nv_4\n\\end{bmatrix}\n: Av = 0\n\\right\\}\n\\]\nDimension calculation:\n\n\\(\\text{rank}(A) = 1\\) (one independent row)\n\\(n = 4\\) (number of columns)\nBy the rank-nullity theorem: \\[\n\\dim(N(A)) = n - r = 4 - 1 = 3\n\\]\n\nInterpretation: Three degrees of freedom. If we choose any values for \\(v_1, v_2, v_3\\), then \\(v_4\\) is determined by \\(v_4 = -(v_1 + v_2 + v_3)\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture11-matrix-spaces.html#summary",
    "title": "MIT 18.06SC Lecture 11: Matrix Spaces, Rank-1, and Graphs",
    "section": "Summary",
    "text": "Summary\nKey concepts:\n\nMatrix spaces are vector spaces with dimension equal to the number of independent entries\nSymmetric matrices (3√ó3): dimension 6\nUpper triangular matrices (3√ó3): dimension 6\nDiagonal matrices (3√ó3): dimension 3\nDimension formula: \\(\\dim(S) + \\dim(U) = \\dim(S \\cap U) + \\dim(S + U)\\)\nRank-1 matrices: \\(A = uv^T\\) (outer product structure)\nRank-1 matrices are not a subspace: sum of two rank-1 matrices can have rank 2\n\n\nSource: MIT 18.06SC Linear Algebra, Lecture 11"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nPermutation matrices reorder rows and columns using a simple structure of 0s and 1s. This post covers the permutation matrices portion of Lecture 5."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html#context",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html#context",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nPermutation matrices reorder rows and columns using a simple structure of 0s and 1s. This post covers the permutation matrices portion of Lecture 5."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html#what-is-a-permutation-matrix",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html#what-is-a-permutation-matrix",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "What is a Permutation Matrix?",
    "text": "What is a Permutation Matrix?\nA matrix is a permutation matrix if: - Each row has exactly one 1, all other entries are 0 - Each column has exactly one 1, all other entries are 0\n\nExample: Row Swapping\n\n\nShow code\nimport numpy as np\nfrom IPython.display import display, Markdown\n\ndef matrix_to_latex(mat, name=\"\"):\n    \"\"\"Convert numpy matrix to LaTeX bmatrix format\"\"\"\n    rows = []\n    for row in mat:\n        rows.append(\" & \".join(map(str, row)))\n    latex = r\"\\begin{bmatrix}\" + \" \\\\\\\\ \".join(rows) + r\"\\end{bmatrix}\"\n    if name:\n        latex = f\"{name} = {latex}\"\n    return f\"$${latex}$$\"\n\n# Create a random 3√ó4 matrix\nA = np.random.randint(0, 10, (3, 4))\ndisplay(Markdown(\"**Matrix A:**\"))\ndisplay(Markdown(matrix_to_latex(A)))\n\n# Create a permutation matrix that swaps rows 0 and 1\nP = np.zeros((3, 3), dtype=int)\nP[0, 1] = 1\nP[1, 0] = 1\nP[2, 2] = 1\ndisplay(Markdown(\"**Permutation matrix P:**\"))\ndisplay(Markdown(matrix_to_latex(P)))\n\n# Apply permutation: PA swaps rows 0 and 1 of A\ndisplay(Markdown(\"**P @ A (rows 0 and 1 swapped):**\"))\ndisplay(Markdown(matrix_to_latex(P @ A)))\n\n\nMatrix A:\n\n\n\\[\\begin{bmatrix}2 & 9 & 1 & 3 \\\\ 0 & 0 & 4 & 9 \\\\ 5 & 1 & 2 & 5\\end{bmatrix}\\]\n\n\nPermutation matrix P:\n\n\n\\[\\begin{bmatrix}0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\n\n\nP @ A (rows 0 and 1 swapped):\n\n\n\\[\\begin{bmatrix}0 & 0 & 4 & 9 \\\\ 2 & 9 & 1 & 3 \\\\ 5 & 1 & 2 & 5\\end{bmatrix}\\]\n\n\nKey insight: Left multiplication by \\(P\\) reorders the rows of \\(A\\) according to the pattern encoded in \\(P\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html#counting-permutation-matrices",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html#counting-permutation-matrices",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Counting Permutation Matrices",
    "text": "Counting Permutation Matrices\nAn \\(n \\times n\\) matrix has exactly \\(n!\\) permutation matrices.\nExamples: - \\(n=3\\): \\(3! = 6\\) permutation matrices - \\(n=4\\): \\(4! = 24\\) permutation matrices - \\(n=5\\): \\(5! = 120\\) permutation matrices\n\nGenerating All Permutations\n\n\nShow code\nfrom itertools import permutations\n\n# Generate all permutations for n=3\nn = 3\nperms = list(permutations(range(n)))\ndisplay(Markdown(f\"There are **{len(perms)}** permutation matrices for n={n}\"))\ndisplay(Markdown(\"\"))\n\n# Display all 6 permutation matrices\nfor i, perm in enumerate(perms):\n    display(Markdown(f\"**Permutation {i+1}:** {perm}\"))\n    P = np.zeros((n, n), dtype=int)\n    P[np.arange(n), perm] = 1\n    display(Markdown(matrix_to_latex(P)))\n    display(Markdown(\"\"))\n\n\nThere are 6 permutation matrices for n=3\n\n\n\n\n\nPermutation 1: (0, 1, 2)\n\n\n\\[\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 2: (0, 2, 1)\n\n\n\\[\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 3: (1, 0, 2)\n\n\n\\[\\begin{bmatrix}0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 4: (1, 2, 0)\n\n\n\\[\\begin{bmatrix}0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 5: (2, 0, 1)\n\n\n\\[\\begin{bmatrix}0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 6: (2, 1, 0)\n\n\n\\[\\begin{bmatrix}0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0\\end{bmatrix}\\]\n\n\n\n\n\n\n\nNotable Permutation Matrices for \\(n=3\\)\nIdentity (no permutation): \\[\nI = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\\]\nSingle swap (rows 0 and 1): \\[\nP = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\\]\nCyclic permutation: \\[\nP = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html#key-properties-of-permutation-matrices",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html#key-properties-of-permutation-matrices",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Key Properties of Permutation Matrices",
    "text": "Key Properties of Permutation Matrices\nPermutation matrices satisfy three properties:\n\n\\(P^{-1} = P^T\\) (inverse equals transpose)\n\\(PP^T = P^TP = I\\) (orthogonal matrix)\n\\(\\det(P) = \\pm 1\\) (determinant is \\(\\pm 1\\))"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html#proof-1-p-1-pt",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html#proof-1-p-1-pt",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Proof 1: \\(P^{-1} = P^T\\)",
    "text": "Proof 1: \\(P^{-1} = P^T\\)\n\nStrategy\nSince \\(PP^{-1} = I\\) by definition of inverse, we only need to prove that \\(PP^T = I\\). Then we can conclude \\(P^{-1} = P^T\\).\n\n\nProof of \\(PP^T = I\\)\n\n\nShow code\n# Example permutation matrix (swaps rows 0 and 1)\nP = np.array([[0, 1, 0],\n              [1, 0, 0],\n              [0, 0, 1]])\n\n# Compute transpose\nP_T = P.T\n\n# Verify PP^T = I\nresult = P @ P_T\ndisplay(Markdown(\"**P @ P^T =**\"))\ndisplay(Markdown(matrix_to_latex(result)))\n\n\nP @ P^T =\n\n\n\\[\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\n\n\nIntuition:\nThe \\(j\\)-th column of \\(P^T\\) is the \\(j\\)-th row of \\(P\\).\nWhen computing \\((PP^T)_{ij}\\) (the \\((i,j)\\) entry): - \\((PP^T)_{ij} = \\text{(row } i \\text{ of } P) \\cdot \\text{(column } j \\text{ of } P^T\\text{)}\\) - \\(= \\text{(row } i \\text{ of } P) \\cdot \\text{(row } j \\text{ of } P\\text{)}\\)\nTwo cases: - When \\(i = j\\): Dot product of a row with itself = 1 (each row has exactly one 1) - When \\(i \\neq j\\): Dot product of different rows = 0 (the 1s are in different positions)\nTherefore, \\(PP^T = I\\).\n\n\nVerification by Row\n\n\nShow code\n# Verify by computing each row of P times P^T\nfor i in range(3):\n    result_row = P[i] @ P_T\n    display(Markdown(f\"**Row {i} of P times P^T:** $[{' \\\\ '.join(map(str, result_row))}]$\"))\ndisplay(Markdown(\"\"))\ndisplay(Markdown(\"Each row gives one row of the identity matrix!\"))\n\n\nRow 0 of P times P^T: \\([1 \\ 0 \\ 0]\\)\n\n\nRow 1 of P times P^T: \\([0 \\ 1 \\ 0]\\)\n\n\nRow 2 of P times P^T: \\([0 \\ 0 \\ 1]\\)\n\n\n\n\n\nEach row gives one row of the identity matrix!\n\n\n\n\nConclusion: \\(P^{-1} = P^T\\)\nProof:\n\\[\n\\begin{align}\nPP^T &= I \\quad \\text{(proved above)} \\\\\nPP^{-1} &= I \\quad \\text{(definition of inverse)} \\\\\n\\therefore P^{-1} &= P^T\n\\end{align}\n\\]\nPractical implications: - Computing \\(P^{-1}\\) is as simple as transposing \\(P\\) - Transposition is \\(O(n^2)\\), much faster than general matrix inversion \\(O(n^3)\\) - If \\(PA\\) permutes rows, then \\(P^T(PA) = A\\) undoes the permutation"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html#proof-2-detp-pm-1",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html#proof-2-detp-pm-1",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Proof 2: \\(\\det(P) = \\pm 1\\)",
    "text": "Proof 2: \\(\\det(P) = \\pm 1\\)\n\nProperties of Determinants\nProperties used: 1. \\(\\det(AB) = \\det(A)\\det(B)\\) (product property) 2. \\(\\det(A^T) = \\det(A)\\) (transpose property)\n\n\nProof\n\\[\n\\begin{align}\n\\det(PP^T) &= \\det(I) = 1 \\\\\n\\det(PP^T) &= \\det(P)\\det(P^T) \\quad \\text{(product property)} \\\\\n&= \\det(P)\\det(P) \\quad \\text{(transpose property)} \\\\\n&= [\\det(P)]^2 \\\\\n\\therefore [\\det(P)]^2 &= 1 \\\\\n\\det(P) &= \\pm 1\n\\end{align}\n\\]\n\n\nInterpretation: Even vs.¬†Odd Permutations\n\n\\(\\det(P) = +1\\): Even permutation (even number of row swaps)\n\\(\\det(P) = -1\\): Odd permutation (odd number of row swaps)\n\nExamples:\n\n\nShow code\n# Identity matrix (0 swaps)\nI = np.eye(3, dtype=int)\ndisplay(Markdown(f\"$\\\\det(I) = {np.linalg.det(I):.0f}$ (even: 0 swaps)\"))\n\n# Single swap (1 swap)\nP_swap = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\ndisplay(Markdown(f\"$\\\\det(P_{{swap}}) = {np.linalg.det(P_swap):.0f}$ (odd: 1 swap)\"))\n\n# Cyclic permutation (2 swaps)\nP_cycle = np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\ndisplay(Markdown(f\"$\\\\det(P_{{cycle}}) = {np.linalg.det(P_cycle):.0f}$ (even: 2 swaps)\"))\n\n\n\\(\\det(I) = 1\\) (even: 0 swaps)\n\n\n\\(\\det(P_{swap}) = -1\\) (odd: 1 swap)\n\n\n\\(\\det(P_{cycle}) = 1\\) (even: 2 swaps)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-permutations.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture5-permutations.html#summary",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Summary",
    "text": "Summary\nPermutation matrices:\n\nStructure: One 1 per row and column, rest are 0s\nCount: \\(n!\\) permutation matrices for \\(n \\times n\\) matrices\nInverse: \\(P^{-1} = P^T\\) (orthogonal property)\nDeterminant: \\(\\det(P) = \\pm 1\\) (sign indicates even/odd permutation)\nApplications: Critical for LU decomposition, numerical stability, and efficient computation\n\n\nSource: MIT 18.06SC Linear Algebra, Lecture 5"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture4-part2-conjugate-quasiconvex.html#examples",
    "href": "Math/EE364A/ee364a-lecture4-part2-conjugate-quasiconvex.html#examples",
    "title": "EE 364A (Convex Optimization): Lecture 4.2 - Conjugate & Quasiconvex Functions",
    "section": "Examples",
    "text": "Examples\n\nExample 1: Negative Logarithm\nFor \\(f(x) = -\\log(x)\\):\n\\[\nf^*(y) = \\sup_{x &gt; 0} (xy + \\log x) =\n\\begin{cases}\n-1 - \\log(-y) & y &lt; 0 \\\\\n\\infty & \\text{otherwise}\n\\end{cases}\n\\]\n\n\nExample 2: Strictly Convex Quadratic\nFor \\(f(x) = \\frac{1}{2}x^\\top Q x\\) with \\(Q \\in S_{++}^n\\) (strictly positive definite):\n\\[\nf^*(y) = \\sup_{x} \\left( y^\\top x - \\frac{1}{2}x^\\top Q x \\right) = \\frac{1}{2}y^\\top Q^{-1}y\n\\]\nThe conjugate is also quadratic, with the inverse of the Hessian."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture4-part2-conjugate-quasiconvex.html#examples-1",
    "href": "Math/EE364A/ee364a-lecture4-part2-conjugate-quasiconvex.html#examples-1",
    "title": "EE 364A (Convex Optimization): Lecture 4.2 - Conjugate & Quasiconvex Functions",
    "section": "Examples",
    "text": "Examples\n\n\\(\\sqrt{|x|}\\) is quasiconvex on \\(\\mathbb{R}\\)\n\\(\\text{ceil}(x) = \\inf\\{z \\in \\mathbb{Z} \\mid z \\geq x\\}\\) is quasilinear\n\\(\\log(x)\\) is quasilinear on \\(\\mathbb{R}_{++}\\)\n\\(f(x_1, x_2) = x_1 x_2\\) is quasiconcave on \\(\\mathbb{R}_{++}^2\\)\nLinear-fractional function: \\[\nf(x) = \\frac{a^\\top x + b}{c^\\top x + d}, \\quad \\text{dom } f = \\{x \\mid c^\\top x + d &gt; 0\\}\n\\] is quasilinear\nDistance ratio: \\[\nf(x) = \\frac{\\|x - a\\|_2}{\\|x - b\\|_2}, \\quad \\text{dom } f = \\{x \\mid \\|x - a\\|_2 \\leq \\|x - b\\|_2\\}\n\\] is quasiconvex"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture4-part2-conjugate-quasiconvex.html#modified-jensen-inequality",
    "href": "Math/EE364A/ee364a-lecture4-part2-conjugate-quasiconvex.html#modified-jensen-inequality",
    "title": "EE 364A (Convex Optimization): Lecture 4.2 - Conjugate & Quasiconvex Functions",
    "section": "Modified Jensen Inequality",
    "text": "Modified Jensen Inequality\nFor quasiconvex functions, Jensen‚Äôs inequality becomes:\n\\[\nf(\\theta x + (1 - \\theta)y) \\leq \\max\\{f(x), f(y)\\} \\quad \\text{for } 0 \\leq \\theta \\leq 1\n\\]\nThis says that a quasiconvex function along a line segment is bounded by the maximum of the endpoint values, rather than a weighted average (as in the standard Jensen inequality for convex functions)."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#mathematical-definition",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#mathematical-definition",
    "title": "EE 364A (Convex Optimization): Lecture 1 - Introduction",
    "section": "Mathematical Definition",
    "text": "Mathematical Definition\nConstraint Optimization Problems\nMinimize \\(f_0(x)\\), subject to \\(f_i(x) \\leq b_i\\), \\(i=1,2,...,m\\)\n\n\\(x=(x_1,...,x_n)\\): optimization variable\n\\(f_0: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\): objective function\n\\(f_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), \\(i=1,2,...,m\\): constraint functions\n\nSolution or optimal point \\(x^*\\) has the smallest value of \\(f_0\\) among all vectors that satisfy the constraints."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#examples",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#examples",
    "title": "EE 364A (Convex Optimization): Lecture 1 - Introduction",
    "section": "Examples",
    "text": "Examples\n\nProfit Optimization\n\nVariables: amount invested in different assets\nConstraints: budget, max/min per asset, minimum return\nObjective: overall risk or return variance\n\n\n\nDevice Sizing in Electronic Circuits\n\nVariables: device width and lengths\nConstraints: manufacturing limits, timing requirements, maximum area\nObjective: power consumption\n\n\n\nData Fitting\n\nVariables: model parameters\nConstraints: prior information, parameter limits\nObjective: measure of misfit or prediction error, plus regularization term"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#solving-optimization-problems",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#solving-optimization-problems",
    "title": "EE 364A (Convex Optimization): Lecture 1 - Introduction",
    "section": "Solving Optimization Problems",
    "text": "Solving Optimization Problems\n\nGeneral Optimization Problems\n\nDifficult to solve\nSome compromise: very long computation time, or not always finding the solution\n\n\n\nExceptions\n\nLeast square problems\nLinear programming\nConvex optimization"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#least-squares",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#least-squares",
    "title": "EE 364A (Convex Optimization): Lecture 1 - Introduction",
    "section": "Least Squares",
    "text": "Least Squares\nMinimize \\(\\|Ax-b\\|_2^2\\)\n\nAnalytical solution: \\(x^* = (A^\\top A)^{-1}A^\\top b\\)\nComputation time: proportional to \\(n^2k\\) (where \\(A \\in \\mathbb{R}^{k \\times n}\\))"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#linear-programming",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#linear-programming",
    "title": "EE 364A (Convex Optimization): Lecture 1 - Introduction",
    "section": "Linear Programming",
    "text": "Linear Programming\nMinimize \\(c^\\top x\\)\nSubject to \\(a_i^\\top x \\leq b_i\\), \\(i=1,2,...,m\\)\n\nNo analytical solution\nReliable and efficient algorithms, software available\nComputation time: proportional to \\(n^2m\\) (if \\(m \\geq n\\))"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#convex-optimization",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#convex-optimization",
    "title": "EE 364A (Convex Optimization): Lecture 1 - Introduction",
    "section": "Convex Optimization",
    "text": "Convex Optimization\nMinimize \\(f_0(x)\\)\nSubject to \\(f_i(x) \\leq b_i\\)\nConvexity requirement: objective and constraint functions are convex:\n\\[f(\\alpha x + \\beta y) \\leq \\alpha f(x) + \\beta f(y)\\]\nwhere \\(\\alpha &gt; 0\\), \\(\\beta &gt; 0\\), \\(\\alpha + \\beta = 1\\)\n\nLeast squares and linear programming are special cases\nNo analytical solution\nComputation time: proportional to \\(\\max(n^3, n^2m, F)\\), where \\(F\\) is the cost of evaluating \\(f_i\\)‚Äôs and their first and second derivatives\n\n Figure: Comparing least squares, linear programming, and convex optimization in terms of complexity and solution methods. Least squares has analytical solutions, linear programming has polynomial-time algorithms, and convex optimization generalizes both while maintaining tractability."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#example-lamp-illumination-optimization",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#example-lamp-illumination-optimization",
    "title": "EE 364A (Convex Optimization): Lecture 1 - Introduction",
    "section": "Example: Lamp Illumination Optimization",
    "text": "Example: Lamp Illumination Optimization\nProblem: There are \\(m\\) lamps illuminating \\(n\\) patches. The goal is to choose lamp powers so that all patches have illumination \\(I_k\\) close to a desired value \\(I_{\\text{des}}\\), by minimizing the maximum log-illumination error, subject to power constraints.\n Figure: Lamp illumination optimization problem. Given m lamps and n patches, find lamp powers that achieve uniform illumination across all patches while respecting power constraints. This is a convex optimization problem that can be solved efficiently."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#key-insight",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#key-insight",
    "title": "EE 364A (Convex Optimization): Lecture 1 - Introduction",
    "section": "Key Insight",
    "text": "Key Insight\nConvex optimization bridges the gap between tractable special cases and general nonlinear programming. Least squares problems have closed-form solutions but are limited in expressiveness. General optimization is flexible but computationally intractable. Convex optimization occupies a sweet spot: it generalizes least squares and linear programming while maintaining polynomial-time solvability through interior-point methods. This makes convex optimization the foundation for practical applications in machine learning, control systems, signal processing, and engineering design."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html",
    "href": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html",
    "title": "EE 364A (Convex Optimization): Lecture 5.1 - Log-Concave and Log-Convex Functions",
    "section": "",
    "text": "Convex Optimization Textbook - Chapter 3.5 (page 118)"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html#proof",
    "href": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html#proof",
    "title": "EE 364A (Convex Optimization): Lecture 5.1 - Log-Concave and Log-Convex Functions",
    "section": "Proof",
    "text": "Proof\n\nTo prove: \\[\n\\log(f(\\theta x+(1-\\theta)y))\\ge \\theta \\log f(x)+(1-\\theta)\\log f(y)\n\\]\nTake exponential on both sides: \\[\n\\begin{aligned}\n\\exp(\\log(f(\\theta x+(1-\\theta)y))) &\\ge \\exp(\\theta \\log f(x)+(1-\\theta)\\log f(y))\\\\\nf(\\theta x+(1-\\theta)y) &\\ge \\exp(\\theta \\log f(x)) \\cdot \\exp((1-\\theta)\\log f(y))\\\\\n&\\ge f(x)^\\theta \\cdot f(y)^{1-\\theta}\n\\end{aligned}\n\\]\nFinal inequality: \\[\nf(\\theta x+(1-\\theta)y)\\ge  f(x)^\\theta\\cdot  f(y)^{1-\\theta}\n\\]\n\n\n\n\nGeometric interpretation of log-concavity"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html#examples",
    "href": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html#examples",
    "title": "EE 364A (Convex Optimization): Lecture 5.1 - Log-Concave and Log-Convex Functions",
    "section": "Examples",
    "text": "Examples\n\nPowers: \\(x^a\\) on \\(\\mathbb{R}_{++}\\) is log-convex for \\(a \\le 0\\), log-concave for \\(a \\ge 0\\)\n\n\\(f(x)=\\log(x^a)=a\\log(x)\\)\n\\(f'(x)=a\\cdot \\frac{1}{x}\\)\n\\(f''(x)=a\\cdot -\\frac{1}{x^2}\\)\n\nThus, the sign of \\(a\\) determines if \\(\\log(x^a)\\) is convex or concave.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction \\(f(x)\\)\nDomain\nSecond Derivative / Curvature\nConvex / Concave\n\\(\\log f(x)\\)\nLog-Convex / Log-Concave\nKey Notes\n\n\n\n\n\\(x^a\\)\n\\(\\mathbb{R}_{++}\\)\n\\(a(a-1)x^{a-2}\\)\ndepends on \\(a\\)\n\\(a\\log x\\)\n\\(a\\ge 0\\): log-concave\\(a\\le 0\\): log-convex\nClassic Boyd example\n\n\n\\(e^{ax}\\)\n\\(\\mathbb{R}\\)\n\\(a^2 e^{ax}\\)\nconvex (all \\(a\\))\n\\(ax\\)\nboth\nExponential is always convex\n\n\n\\(\\log x\\)\n\\(\\mathbb{R}_{++}\\)\n\\(-1/x^2\\)\nconcave\n\\(\\log\\log x\\)\n‚Äî\nFundamental concave function\n\n\nGaussian pdf \\(\\phi(x)\\)\n\\(\\mathbb{R}\\)\n‚Äî\n‚ùå\n\\(-x^2/2 + c\\)\nlog-concave\nCore likelihood model\n\n\nGaussian CDF \\(\\Phi(x)\\)\n\\(\\mathbb{R}\\)\n\\(-x\\phi(x)\\)\n‚ùå (S-shaped)\n\\(\\log\\Phi(x)\\)\nlog-concave\nProbit models\n\n\nLogistic \\(\\sigma(x)\\)\n\\(\\mathbb{R}\\)\nchanges sign\n‚ùå\nconcave\nlog-concave\nLogistic regression\n\n\n\\(\\\\|x\\\\|_2\\)\n\\(\\mathbb{R}^n\\)\n‚Äî\nconvex\n‚Äî\n‚Äî\nAll norms are convex\n\n\n\\(\\log\\sum_i e^{x_i}\\)\n\\(\\mathbb{R}^n\\)\nHessian \\(\\succeq 0\\)\nconvex\n‚Äî\n‚Äî\nSoftmax / log-partition\n\n\n\\(\\prod_i x_i\\)\n\\(\\mathbb{R}_{++}^n\\)\n‚Äî\n‚ùå\n\\(\\sum_i \\log x_i\\)\nlog-concave\nGeometric programming"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html#properties-of-log-concave-functions",
    "href": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html#properties-of-log-concave-functions",
    "title": "EE 364A (Convex Optimization): Lecture 5.1 - Log-Concave and Log-Convex Functions",
    "section": "Properties of log-concave functions",
    "text": "Properties of log-concave functions\n\nTwice differentiable function \\(f\\) with convex domain is log-concave iff \\(f(x)\\nabla^2f(x) \\preceq \\nabla f(x)\\nabla f(x)^\\top\\) for all \\(x \\in \\mathrm{dom}\\, f\\)\nProduct of log-concave functions is log-concave\nSum of log-concave functions is not always log-concave\nIf \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is log-concave, then \\(g(x)=\\int f(x,y) dy\\) is log-concave\n\n\n\n\nProperties of log-concave functions"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html#review-basic-operations-from-lecture-3",
    "href": "Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html#review-basic-operations-from-lecture-3",
    "title": "EE 364A (Convex Optimization): Lecture 4.1 - Operations Preserving Convexity",
    "section": "Review: Basic Operations from Lecture 3",
    "text": "Review: Basic Operations from Lecture 3\nWe‚Äôve already seen some fundamental operations in Lecture 3:\n\nNonnegative weighted sum: If \\(f_1, f_2\\) are convex, then \\(f_1 + f_2\\) is convex\nNonnegative scaling: If \\(f\\) is convex and \\(\\alpha &gt; 0\\), then \\(\\alpha f\\) is convex\nComposition with affine mapping: If \\(f\\) is convex, then \\(g(x) = f(Ax + b)\\) is convex\n\nNow we‚Äôll extend this toolkit with more powerful operations that preserve convexity."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html#pointwise-maximum",
    "href": "Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html#pointwise-maximum",
    "title": "EE 364A (Convex Optimization): Lecture 4.1 - Operations Preserving Convexity",
    "section": "Pointwise Maximum",
    "text": "Pointwise Maximum\nif \\(f_1,...f_n\\) are convex, then \\(f(x)=\\max(f_1(x),...f_n(x))\\) is convex\n\n\n\nPointwise maximum visualization\n\n\nExamples:\n\nPiecewise-linear function: \\(f(x)=\\max_{i=1,...m}(a_i^\\top x+b_i)\\) is convex\nSum of \\(r\\) largest components of \\(x \\in R^n\\): \\[\nf(x)=x_{[1]}+x_{[2]}+...x_{[r]}\n\\] is convex"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html#pointwise-supremum",
    "href": "Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html#pointwise-supremum",
    "title": "EE 364A (Convex Optimization): Lecture 4.1 - Operations Preserving Convexity",
    "section": "Pointwise Supremum",
    "text": "Pointwise Supremum\nif \\(f(x,y)\\) is convex in \\(x\\) for each \\(y \\in A\\), then \\[\ng(x)=\\sup_{y \\in A}f(x,y)\n\\] is convex.\nExamples:\n\nSupport function of a set C: \\(S_C(x)=\\sup_{y\\in C}y^\\top x\\)\nDistance to farthest point in a set C: \\(f(x)=\\sup_{y\\in C}||y-x||\\)\nMaximum eigenvalue of symmetric matrix for \\(X \\in S^n\\), \\(\\lambda_{\\max}(X)=\\sup_{||y||_2=1}y^\\top Xy\\)"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html#composition-with-scalar-functions",
    "href": "Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html#composition-with-scalar-functions",
    "title": "EE 364A (Convex Optimization): Lecture 4.1 - Operations Preserving Convexity",
    "section": "Composition with Scalar Functions",
    "text": "Composition with Scalar Functions\nComposition of g: \\(R^n \\to R\\), and h : \\(R \\to R\\)\n\\(f(x)=h(g(x))\\)\nis convex if\n\ng is convex, h is convex, \\(\\tilde{h}\\) nondecreasing\ng is concave, h is convex, \\(\\tilde{h}\\) nonincreasing\n\n\nProof\n\\(f'(x)=h'(g(x))\\cdot g'(x)\\)\nFrom the general 1st order derivative theorem of \\(f_n(x)=f_a(x)\\cdot f_b(x)\\) is \\(f_n'(x)=f_a'(x)\\cdot f_b(x)+f_a(x)\\cdot f_b'(x)\\)\nSo the second order derivative of \\(f(x)\\) is:\n\\(f''(x)=h''(g(x))\\cdot g'(x)^2+h'(g(x))g''(x)\\)\n\n\n\nComposition with scalar functions analysis\n\n\nExamples:\n\n\\(\\exp(g(z))\\) is convex if \\(g(z)\\) is convex\n\\(\\frac{1}{g(z)}\\) is convex if \\(g(z)\\) is concave and positive"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html#vector-composition-general-rule",
    "href": "Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html#vector-composition-general-rule",
    "title": "EE 364A (Convex Optimization): Lecture 4.1 - Operations Preserving Convexity",
    "section": "Vector Composition (General Rule)",
    "text": "Vector Composition (General Rule)\nComposition of g: \\(R^n \\to R^k\\) and h: \\(R^k \\to R\\)\n\\(f(x)=h(g(x))=h(g_1(x),g_2(x),...,g_k(x))\\)\nis convex if\n\ncase 1: \\(g_i\\) is convex, h is convex, \\(\\tilde{h}\\) nondecreasing in each argument\ncase 2: \\(g_i\\) is concave, h is convex, \\(\\tilde{h}\\) nonincreasing in each argument\n\n\nProof\n\\[\nf''(x)=g'(x)^\\top \\nabla^2h(g(x))g'(x)+\\nabla h(g(x))^\\top g''(x)\n\\]\n\n\n\nVector composition analysis\n\n\nExamples:\n\n\\(\\sum_{i=1}^m\\log g_i(x)\\) is concave if \\(g_i(x)\\) is concave and positive\n\\(\\log\\sum_{i=1}^m\\exp g_i(x)\\) is convex if \\(g_i(x)\\) is convex"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html#minimization",
    "href": "Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html#minimization",
    "title": "EE 364A (Convex Optimization): Lecture 4.1 - Operations Preserving Convexity",
    "section": "Minimization",
    "text": "Minimization\nif \\(f(x,y)\\) is convex in \\((x,y)\\) and \\(C\\) is a convex set, then \\[\ng(x)=\\inf_{y \\in C}f(x,y)\n\\] is convex.\n\n\n\nMinimization preserves convexity\n\n\n\nExample 1: Quadratic function\n\\[\nf(x,y)\n= \\begin{bmatrix} x \\\\ y \\end{bmatrix}^\\top\n  \\begin{bmatrix} A & B \\\\ B^\\top & C \\end{bmatrix}\n  \\begin{bmatrix} x \\\\ y \\end{bmatrix},\n\\quad\nQ = \\begin{bmatrix}A&B\\\\B^\\top&C\\end{bmatrix}\\succeq 0,\\; C\\succ 0\n\\]\nMinimizing over y gives \\(g(x)=\\inf_yf(x,y)\\)\nThe gradient with respect to y is:\n\\[\n\\nabla_y f(x,y) = 2B^\\top x + 2C y\n\\]\nSetting the gradient to zero: \\(2B^\\top x + 2C y = 0 \\Rightarrow y^*=-C^{-1}B^\\top x\\)\n\\[\nf(x,y^\\star)=x^\\top(A-BC^{-1}B^\\top)x\n\\]\nFrom Schur complement, \\(A-BC^{-1}B^\\top \\succeq 0\\)\nFrom the preservation rule, \\(g(x)\\) is convex.\n\n\nExample 2: Distance to a set\nDistance to a set: \\(\\mathrm{dist}(x,S)=\\inf_{y\\in S}||x-y||\\) is convex if S is convex"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html#perspective",
    "href": "Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html#perspective",
    "title": "EE 364A (Convex Optimization): Lecture 4.1 - Operations Preserving Convexity",
    "section": "Perspective",
    "text": "Perspective\nThe perspective function of a function \\(f: R^n \\to R\\) is the function \\(g: R^n \\times R \\to R\\)\n\\[\ng(x,t)=tf(x/t), \\quad \\mathrm{dom}\\, g=\\{(x,t) |x/t\\in \\mathrm{dom}\\,f,t&gt; 0 \\}\n\\]\n\\(g\\) is convex if \\(f\\) is convex.\n\n\n\nPerspective function visualization\n\n\nExamples:\n\n\\(f(x)=x^\\top x\\) is convex; hence \\(g(x,t)=x^\\top x/t\\) is convex for \\(t&gt; 0\\)\n\\(f(x)=-\\log x\\) is convex, so \\(g(x,t)=t\\log t- t \\log x\\) is convex on \\(R_{++}^2\\)\nif f is convex, then \\(g(x)=(c^\\top x+b)f((Ax+b)/(c^\\top x+d))\\) is convex on \\[\n\\{x \\mid c^\\top x + b &gt; 0,\\ (Ax+b)/(c^\\top x + d) \\in \\mathrm{dom}\\, f \\}\n\\]\n\n\n\n\nPerspective examples"
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html",
    "href": "Math/reflections/mit1806-invertibility-connections.html",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "",
    "text": "In linear algebra, concepts like invertibility, null space, linear independence, rank, and pivots often seem like separate topics. However, they are deeply interconnected - different views of the same mathematical reality. This post synthesizes these fundamental concepts and reveals how they all tell the same story about whether a linear transformation preserves information."
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html#introduction",
    "href": "Math/reflections/mit1806-invertibility-connections.html#introduction",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "",
    "text": "In linear algebra, concepts like invertibility, null space, linear independence, rank, and pivots often seem like separate topics. However, they are deeply interconnected - different views of the same mathematical reality. This post synthesizes these fundamental concepts and reveals how they all tell the same story about whether a linear transformation preserves information."
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html#invertibility-the-foundation",
    "href": "Math/reflections/mit1806-invertibility-connections.html#invertibility-the-foundation",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "Invertibility: The Foundation",
    "text": "Invertibility: The Foundation\nReference: Lecture 3: Matrix Multiplication and Inverse\n\nDefinition\nA square matrix \\(A\\) is invertible if there exists a matrix \\(A^{-1}\\) such that:\n\\[\nAA^{-1} = A^{-1}A = I_n\n\\]\n\n\nProperties of Invertible Matrices\nAn invertible matrix \\(A\\) must satisfy:\n\nSquare: \\(m = n\\) (same number of rows and columns)\nFull rank: \\(\\text{rank}(A) = n\\)\nAll pivot variables: \\(r = n\\) pivot positions\nNo free variables: \\(n - r = 0\\) free variables\nIndependent rows: All rows are linearly independent\n\n\n\nSolving Systems with Invertible Matrices\nWhen \\(A\\) is invertible, solving \\(Ax = b\\) becomes straightforward:\n\\[\n\\begin{aligned}\nAx &= b \\\\\nA^{-1}Ax &= A^{-1}b \\\\\nx &= A^{-1}b\n\\end{aligned}\n\\]\nThe solution is unique and exists for every \\(b\\)."
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html#null-space-a-direct-test-for-invertibility",
    "href": "Math/reflections/mit1806-invertibility-connections.html#null-space-a-direct-test-for-invertibility",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "Null Space: A Direct Test for Invertibility",
    "text": "Null Space: A Direct Test for Invertibility\nReference: Lecture 7: Solving Ax=0\n\nThe Connection\nThe null space \\(N(A)\\) provides a direct test for invertibility:\n\nIf \\(N(A) = \\{\\mathbf{0}\\}\\): Matrix is invertible\nIf \\(N(A) \\neq \\{\\mathbf{0}\\}\\): Matrix is NOT invertible\n\n\n\nWhy Non-Trivial Null Space Prevents Invertibility\nProof by contradiction:\nSuppose \\(N(A) \\neq \\{\\mathbf{0}\\}\\) and \\(A^{-1}\\) exists. Let \\(v_1 \\in N(A)\\) with \\(v_1 \\neq \\mathbf{0}\\).\nThen:\n\\[\n\\begin{aligned}\nAv_1 &= \\mathbf{0} \\quad \\text{(by definition of null space)} \\\\\nA^{-1}(Av_1) &= A^{-1}\\mathbf{0} \\quad \\text{(multiply both sides by } A^{-1}\\text{)} \\\\\n(A^{-1}A)v_1 &= \\mathbf{0} \\\\\nI_n v_1 &= \\mathbf{0} \\\\\nv_1 &= \\mathbf{0}\n\\end{aligned}\n\\]\nContradiction! We assumed \\(v_1 \\neq \\mathbf{0}\\), but our logic forces \\(v_1 = \\mathbf{0}\\).\nTherefore, \\(A^{-1}\\) cannot exist when \\(N(A)\\) contains non-zero vectors.\n\n\n\n\n\n\nTipKey Insight\n\n\n\nA non-trivial null space means information is lost in the transformation \\(A\\), making it impossible to uniquely reverse."
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html#linear-independence-the-geometric-view",
    "href": "Math/reflections/mit1806-invertibility-connections.html#linear-independence-the-geometric-view",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "Linear Independence: The Geometric View",
    "text": "Linear Independence: The Geometric View\nReference: Lecture 8: Solving Ax=b\n\nConnection to Invertibility\nIf the rows (or columns) of a matrix are not all linearly independent, the matrix cannot be invertible.\nFor a square matrix:\n\nIndependent rows/columns ‚ü∫ \\(\\text{rank}(A) = n\\)\nDependent rows/columns ‚ü∫ \\(\\text{rank}(A) &lt; n\\)\n\n\n\nWhy Dependence Prevents Invertibility\nWhen rows are dependent:\n\nRank: \\(r &lt; n\\) (not full rank)\nPivots: Only \\(r &lt; n\\) pivot positions\nFree variables: \\(n - r &gt; 0\\) free variables exist\n\n\nTwo Perspectives on Dependence\n1. Algebraic Perspective\nWhen free variables exist:\n\\[\nR_{\\text{rref}} = [I_r \\mid F]\n\\]\nwhere \\(F\\) is the \\((n-r)\\)-dimensional free variable matrix.\nThe null space \\(N(A)\\) has dimension \\(n - r &gt; 0\\), containing infinitely many vectors. By our earlier proof, this means \\(A\\) is not invertible.\n2. Geometric Perspective\nIf rows are dependent, the transformation \\(A\\) collapses the \\(n\\)-dimensional space down to an \\(r\\)-dimensional subspace (where \\(r &lt; n\\)).\n\nInformation loss: The transformation maps multiple distinct inputs to the same output\nCannot invert: We cannot uniquely recover the original \\(n\\)-dimensional vector from its \\(r\\)-dimensional image\nMissing dimensions: The \\((n-r)\\) dimensions of information are permanently lost\n\n\n\n\n\n\n\nNoteExample: Dimensional Collapse\n\n\n\nA \\(3 \\times 3\\) matrix with rank 2 maps all of \\(\\mathbb{R}^3\\) onto a 2-dimensional plane. Infinitely many points in 3D space map to each point on the plane. There‚Äôs no way to uniquely invert this mapping."
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html#the-big-picture-everything-is-connected",
    "href": "Math/reflections/mit1806-invertibility-connections.html#the-big-picture-everything-is-connected",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "The Big Picture: Everything is Connected",
    "text": "The Big Picture: Everything is Connected\nAll these concepts are different views of the same mathematical reality:\n\n\n\n\n\n\n\n\nPerspective\nInvertible (\\(A^{-1}\\) exists)\nNot Invertible (\\(A^{-1}\\) doesn‚Äôt exist)\n\n\n\n\nNull Space\n\\(N(A) = \\{\\mathbf{0}\\}\\)\n\\(N(A) \\neq \\{\\mathbf{0}\\}\\)\n\n\nRank\n\\(\\text{rank}(A) = n\\) (full rank)\n\\(\\text{rank}(A) &lt; n\\) (rank deficient)\n\n\nPivots\n\\(n\\) pivots (all columns)\n\\(r &lt; n\\) pivots\n\n\nFree Variables\n0 free variables\n\\(n - r &gt; 0\\) free variables\n\n\nIndependence\nRows/columns independent\nRows/columns dependent\n\n\nDimension\n\\(\\dim(N(A)) = 0\\)\n\\(\\dim(N(A)) = n - r &gt; 0\\)\n\n\nSolutions to \\(Ax = 0\\)\nOnly \\(x = \\mathbf{0}\\)\nInfinitely many solutions\n\n\nDeterminant\n\\(\\det(A) \\neq 0\\)\n\\(\\det(A) = 0\\)"
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html#fundamental-theorem-of-invertible-matrices",
    "href": "Math/reflections/mit1806-invertibility-connections.html#fundamental-theorem-of-invertible-matrices",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "Fundamental Theorem of Invertible Matrices",
    "text": "Fundamental Theorem of Invertible Matrices\nFor a square \\(n \\times n\\) matrix \\(A\\), the following are equivalent (all true or all false):\n\n\\(A\\) is invertible\n\\(A^{-1}\\) exists\n\\(\\text{rank}(A) = n\\)\n\\(N(A) = \\{\\mathbf{0}\\}\\)\nColumns of \\(A\\) are linearly independent\nRows of \\(A\\) are linearly independent\n\\(\\det(A) \\neq 0\\)\n\\(Ax = 0\\) has only the trivial solution\n\\(Ax = b\\) has a unique solution for every \\(b\\)\n\\(A\\) has \\(n\\) pivot positions\n\n\n\n\n\n\n\nImportantThe Core Insight\n\n\n\nAll these conditions are testing whether the linear transformation preserves information. If any information is lost (through dimensional collapse, non-trivial null space, or linear dependence), the transformation cannot be inverted."
  },
  {
    "objectID": "Math/reflections/mit1806-invertibility-connections.html#related-posts",
    "href": "Math/reflections/mit1806-invertibility-connections.html#related-posts",
    "title": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots",
    "section": "Related Posts",
    "text": "Related Posts\n\nLecture 3: Matrix Multiplication and Inverse\nLecture 7: Solving Ax=0 - Finding the Null Space\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nLecture 9: Independence, Basis, and Dimension"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ickma.dev",
    "section": "",
    "text": "A growing collection of structured study notes and visual explanations ‚Äî written for clarity, reproducibility, and long-term memory."
  },
  {
    "objectID": "index.html#latest-updates",
    "href": "index.html#latest-updates",
    "title": "ickma.dev",
    "section": "Latest Updates",
    "text": "Latest Updates\n\n‚àá Deep Learning Book 45 chapters\nMy notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\nChapter 10.7: The Challenge of Long-Term Dependencies The fundamental challenge of long-term dependencies in RNNs is training difficulty: gradients propagated across many time steps either vanish exponentially (common) or explode (rare but severe). Eigenvalue analysis shows how powers of the transition matrix govern this instability.\n\n\nChapter 10.6: Recursive Neural Network Recursive neural networks compute over tree structures rather than linear chains, applying shared composition functions at internal nodes to build hierarchical representations bottom-up. This reduces computation depth from O(œÑ) to O(log œÑ), but requires external tree structure specification.\n\n\nChapter 10.5: Deep Recurrent Networks Three architectural patterns for adding depth to RNNs: hierarchical hidden states (vertical stacking), deep transition RNNs (MLPs replace transformations), and deep transition with skip connections (residual paths for gradient flow).\n\n\nChapter 10.4: Encoder-Decoder Sequence-to-Sequence Architecture The seq2seq architecture handles variable-length input and output sequences by compressing the input into a fixed context vector C, then decoding it step-by-step. This enables machine translation, summarization, and dialogue generation where input and output lengths differ.\n\n\n\nSee all Deep Learning chapters ‚Üí\n\n\n\nüìê MIT 18.06SC Linear Algebra 36 lectures\nMy journey through MIT‚Äôs Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\n\nSee all MIT 18.06SC lectures ‚Üí\n\n\n\nüìê MIT 18.065: Linear Algebra Applications 2 lectures\nMy notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning‚Äîexploring how linear algebra powers modern applications.\n\n\nLecture 9: Four Ways to Solve Least Squares Problems Four equivalent methods for solving \\(Ax = b\\) when \\(A\\) has no inverse: pseudo-inverse, normal equations, algebraic minimization, and geometric projection‚Äîall converging to the same optimal solution.\n\n\nLecture 8: Norms of Vectors and Matrices Understanding vector p-norms (\\(\\|v\\|_p\\)) and when they satisfy the triangle inequality (only for \\(p \\geq 1\\)). The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù creates non-convex unit balls for strong sparsity.\n\n\n\nSee all MIT 18.065 lectures ‚Üí\n\n\n\nüìê Stanford EE 364A: Convex Optimization 7 lectures\nMy notes from Stanford EE 364A: Convex Optimization‚Äîtheory and applications of optimization problems.\n\n\nLecture 5.2: Monotonicity with Generalized Inequalities K-nondecreasing functions satisfy \\(x \\preceq_K y \\Rightarrow f(x)\\le f(y)\\). Gradient condition: \\(\\nabla f(x) \\succeq_{K^*} 0\\) (dual inequality). Matrix monotone examples: \\(\\mathrm{tr}(WX)\\), \\(\\mathrm{det}X\\). K-convexity extends convexity to generalized inequalities with dual characterization and composition theorems.\n\n\nLecture 5.1: Log-Concave and Log-Convex Functions Log-concave functions satisfy \\(f(\\theta x+(1-\\theta)y)\\ge f(x)^\\theta f(y)^{1-\\theta}\\). Powers \\(x^a\\) are log-concave for \\(a \\ge 0\\) and log-convex for \\(a \\le 0\\). Key properties: products preserve log-concavity, but sums do not. Integration of log-concave functions preserves log-concavity.\n\n\nLecture 4 Part 2: Conjugate and Quasiconvex Functions Conjugate functions \\(f^*(y) = \\sup_x (y^\\top x - f(x))\\) are always convex, with examples including negative logarithm and quadratic functions. Quasiconvex functions have convex sublevel sets with modified Jensen inequality \\(f(\\theta x + (1-\\theta)y) \\leq \\max\\{f(x), f(y)\\}\\). Examples include linear-fractional functions and distance ratios.\n\n\nLecture 4 Part 1: Operations preserving Convexity Pointwise maximum and supremum (support function, distance to farthest point, maximum eigenvalue), composition with scalar functions (exponential, reciprocal), vector composition (log-sum-exp), minimization over convex sets (Schur complement, distance to set), and perspective functions with examples.\n\n\n\nSee all EE 364A lectures ‚Üí"
  },
  {
    "objectID": "index.html#more-topics",
    "href": "index.html#more-topics",
    "title": "ickma.dev",
    "section": "More Topics",
    "text": "More Topics\n\n\nMachine Learning\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\nAlgorithms\n\nDP Regex"
  },
  {
    "objectID": "Algorithm/index.html",
    "href": "Algorithm/index.html",
    "title": "Algorithm Topics",
    "section": "",
    "text": "DP: Regular Expression Matching"
  },
  {
    "objectID": "Algorithm/index.html#dynamic-programming",
    "href": "Algorithm/index.html#dynamic-programming",
    "title": "Algorithm Topics",
    "section": "",
    "text": "DP: Regular Expression Matching"
  },
  {
    "objectID": "index-backup.html",
    "href": "index-backup.html",
    "title": "ickma.dev",
    "section": "",
    "text": "My learning notes and thoughts on math and machine learning.\nCurrently reading the Deep Learning book.\n\n\n\n\nHow ReLU solves problems that linear models cannot handle.\n\n\n\nThe mathematical connection between probabilistic models and loss functions.\n\n\n\nExploring activation functions and their impact on neural network learning.\n\n\n\nHow depth enables hierarchical feature reuse and exponential expressiveness with fewer parameters.\n\n\n\nThe algorithm that makes training deep networks computationally feasible through efficient gradient computation.\n\n\n\nEssential second-order calculus concepts needed before Chapter 7 on optimization algorithms.\n\n\n\nHow L2 regularization shrinks weights based on Hessian eigenvalues, preserving important directions while penalizing less sensitive ones.\n\n\n\nL1 regularization uses soft thresholding to push small weights to exactly zero, creating sparse solutions that perform feature selection.\n\n\n\nRegularization as constrained optimization: penalty form vs Lagrangian with KKT conditions and min-max dual training.\n\n\n\nWhy regularization is mathematically necessary when solving under-constrained linear systems, and how it ensures invertibility.\n\n\n\nHow transforming existing data can improve generalization and combat overfitting when training data is limited.\n\n\n\nMathematical derivation showing how adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\n\n\n\n\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation in linear transformations.\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations\nTwo powerful perspectives that reveal the hidden beauty of linear systems: row picture vs column picture.\nLecture 2: Elimination with Matrices\nThe systematic algorithm that transforms linear systems into upper triangular form for easy solution.\nLecture 3: Matrix Multiplication and Inverse\nFive different perspectives on matrix multiplication, from element-wise computation to rank-1 decomposition, plus understanding when matrices can‚Äôt be inverted.\nLecture 4: LU Decomposition\nFactoring matrices into Lower √ó Upper triangular form: the foundation of efficient numerical linear algebra and solving multiple systems with the same matrix.\nLecture 5.1: Permutation Matrices\nPermutation matrices reorder rows and columns using a simple structure of 0s and 1s.\nLecture 5.2: Transpose\nThe transpose operation switches rows to columns, creating symmetric matrices.\nLecture 5.3: Vector Spaces\nVector spaces and subspaces: closed under addition and scalar multiplication.\nLecture 6: Column Space and Null Space\nColumn space determines which \\(b\\) make \\(Ax = b\\) solvable. Null space contains all solutions to \\(Ax = 0\\).\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nSystematic algorithm to find null space using pivot/free variables and RREF. Dimension of null space is n-r.\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nComplete solution is particular solution plus null space. Four cases based on rank: exactly determined (unique), overdetermined (0 or 1), underdetermined (infinite), and rank deficient (0 or infinite).\nLecture 9: Independence, Basis, and Dimension Linear independence prevents redundancy, basis is minimal spanning set, dimension measures space size. Rank-nullity theorem: dim(C(A)) + dim(N(A)) = n.\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix: column space, null space, row space, and left null space.\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas for subspace intersections and sums, and differential equations as vector spaces.\n\n\n\n\n\n\n\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\n\n\nDP Regex"
  },
  {
    "objectID": "index-backup.html#deep-learning-book",
    "href": "index-backup.html#deep-learning-book",
    "title": "ickma.dev",
    "section": "",
    "text": "How ReLU solves problems that linear models cannot handle.\n\n\n\nThe mathematical connection between probabilistic models and loss functions.\n\n\n\nExploring activation functions and their impact on neural network learning.\n\n\n\nHow depth enables hierarchical feature reuse and exponential expressiveness with fewer parameters.\n\n\n\nThe algorithm that makes training deep networks computationally feasible through efficient gradient computation.\n\n\n\nEssential second-order calculus concepts needed before Chapter 7 on optimization algorithms.\n\n\n\nHow L2 regularization shrinks weights based on Hessian eigenvalues, preserving important directions while penalizing less sensitive ones.\n\n\n\nL1 regularization uses soft thresholding to push small weights to exactly zero, creating sparse solutions that perform feature selection.\n\n\n\nRegularization as constrained optimization: penalty form vs Lagrangian with KKT conditions and min-max dual training.\n\n\n\nWhy regularization is mathematically necessary when solving under-constrained linear systems, and how it ensures invertibility.\n\n\n\nHow transforming existing data can improve generalization and combat overfitting when training data is limited.\n\n\n\nMathematical derivation showing how adding Gaussian noise to weights is equivalent to penalizing large gradients."
  },
  {
    "objectID": "index-backup.html#mathematics",
    "href": "index-backup.html#mathematics",
    "title": "ickma.dev",
    "section": "",
    "text": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation in linear transformations.\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations\nTwo powerful perspectives that reveal the hidden beauty of linear systems: row picture vs column picture.\nLecture 2: Elimination with Matrices\nThe systematic algorithm that transforms linear systems into upper triangular form for easy solution.\nLecture 3: Matrix Multiplication and Inverse\nFive different perspectives on matrix multiplication, from element-wise computation to rank-1 decomposition, plus understanding when matrices can‚Äôt be inverted.\nLecture 4: LU Decomposition\nFactoring matrices into Lower √ó Upper triangular form: the foundation of efficient numerical linear algebra and solving multiple systems with the same matrix.\nLecture 5.1: Permutation Matrices\nPermutation matrices reorder rows and columns using a simple structure of 0s and 1s.\nLecture 5.2: Transpose\nThe transpose operation switches rows to columns, creating symmetric matrices.\nLecture 5.3: Vector Spaces\nVector spaces and subspaces: closed under addition and scalar multiplication.\nLecture 6: Column Space and Null Space\nColumn space determines which \\(b\\) make \\(Ax = b\\) solvable. Null space contains all solutions to \\(Ax = 0\\).\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nSystematic algorithm to find null space using pivot/free variables and RREF. Dimension of null space is n-r.\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nComplete solution is particular solution plus null space. Four cases based on rank: exactly determined (unique), overdetermined (0 or 1), underdetermined (infinite), and rank deficient (0 or infinite).\nLecture 9: Independence, Basis, and Dimension Linear independence prevents redundancy, basis is minimal spanning set, dimension measures space size. Rank-nullity theorem: dim(C(A)) + dim(N(A)) = n.\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix: column space, null space, row space, and left null space.\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas for subspace intersections and sums, and differential equations as vector spaces."
  },
  {
    "objectID": "index-backup.html#more",
    "href": "index-backup.html#more",
    "title": "ickma.dev",
    "section": "",
    "text": "K-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\n\n\nDP Regex"
  },
  {
    "objectID": "Algorithm/dp_regex.html",
    "href": "Algorithm/dp_regex.html",
    "title": "DP: Regular Expression Matching",
    "section": "",
    "text": "Dynamic programming is a technique for solving problems by breaking them down into smaller sub-problems and solving each subproblem only once."
  },
  {
    "objectID": "Algorithm/dp_regex.html#example-of-regular-expression-matching",
    "href": "Algorithm/dp_regex.html#example-of-regular-expression-matching",
    "title": "DP: Regular Expression Matching",
    "section": "Example of Regular Expression Matching",
    "text": "Example of Regular Expression Matching\nA problem from Leetcode 10:\nYou are given a string s and a pattern p, implement regular expression matching with support for ‚Äò.‚Äô and ‚Äô*‚Äô where:\n‚Äò.‚Äô Matches any single character. ‚Äô*‚Äô Matches zero or more of the preceding element. The matching should cover the entire input string (not partial).\ns = \"abcabc\"    \np1 = \".*c\"    \np2 = \".*d\""
  },
  {
    "objectID": "Algorithm/dp_regex.html#dp-table",
    "href": "Algorithm/dp_regex.html#dp-table",
    "title": "DP: Regular Expression Matching",
    "section": "1. DP Table",
    "text": "1. DP Table\nLook at the following case.\n\nCase 1\np1 is valid if we have a table like this:\nwe can see that the last cell is T, so p1 is valid.\n\n\n\n\n\n.\n*\nc\n\n\n\n\n\nT\nF\nT\nF\n\n\na\nF\nT\nT\nF\n\n\nb\nF\nF\nT\nF\n\n\nc\nF\nF\nT\nT\n\n\na\nF\nF\nT\nF\n\n\nb\nF\nF\nT\nF\n\n\nc\nF\nF\nT\nT\n\n\n\nThe table is the match result of s[0:i] and p[0:j],\nso the last cell is the match result of s[0:6](the entire string) and p[0:3](the entire pattern). If the result is T, then the entire string matches the entire pattern.\n\n\nHow does each cell is calculated?\n\nthe last cell, p[:3] matches s[:6], also p[:2] matches s[:5]\n\nit is now a dp problem, the cell‚Äôs value is the match result of p[:i] and s[:j] and the match result of p[:i-1] and s[:j-1],meaning both should be T.\n\n\n\nCase 2\nNow look at an invalid case:\np2 is invalid because .* can match abcab but d cannot match c\n\n\n\n\n\n.\n*\nd\n\n\n\n\n\nT\nF\nT\nF\n\n\na\nF\nT\nT\nF\n\n\nb\nF\nF\nT\nF\n\n\nc\nF\nF\nT\nF\n\n\na\nF\nF\nT\nF\n\n\nb\nF\nF\nT\nF\n\n\nc\nF\nF\nT\nF\n\n\n\nLook at the last cell, p[:3] matches s[:6], but p[2] does not match s[5], so the last cell is F."
  },
  {
    "objectID": "Algorithm/dp_regex.html#formula-derivation",
    "href": "Algorithm/dp_regex.html#formula-derivation",
    "title": "DP: Regular Expression Matching",
    "section": "2. Formula Derivation",
    "text": "2. Formula Derivation\n\nTwo rules\n\nwe can compare single character of the string s[i] with 1 or 2 characters of the pattern p[j],p[j-2]....,\nwe can query the previous results from the DP table dp[i-1][j-1], dp[i][j-2], dp[i-1][j].\n\n\n\nThe flow\nThe diagramm below shows how can we calculate the match result of s[0...i] and p[0...j].\n\n\n\nalt\n\n\nNow the formula seems to be: \\[\n\\text{dp}[i][j] =\n\\begin{cases}\n\\text{true} & \\text{if } p[i] \\neq '*'  \\land s[i] \\text{ matches } p[j] \\land \\text{dp}[i-1][j-1] = \\text{true} \\\\\n\\text{true} & \\text{if } p[i] = '*'  \\land dp[i][j-2] = \\text{true} \\\\\n\\text{true} & \\text{if } p[i] = '*'  \\land s[i] \\text{ matches } p[j-1] \\land \\text{dp}[i-1][j-2] = \\text{true} \\\\\n\\text{true} & \\text{if } p[i] = '*'  \\land s[i] \\text{ matches } p[j-1] \\land \\text{dp}[i-1][j] = \\text{true} \\\\\n\\text{false} & \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "Algorithm/dp_regex.html#code-example",
    "href": "Algorithm/dp_regex.html#code-example",
    "title": "DP: Regular Expression Matching",
    "section": "3. Code Example",
    "text": "3. Code Example\nPlease not that in the code, when we retrieve character from the string or pattern, we need to use s[i-1] and p[j-1] instead of s[i] and p[j] as the index of the string and pattern is 0-based.\nfrom collections import defaultdict\nclass Solution:\n    def isMatch(self,s, p):\n        m, n = len(s), len(p)\n        dp = [[False] * (n + 1) for _ in range(m + 1)]\n     # DP is a table with m+1 rows and n+1 columns\n     # we retrieve dp[i][k], i is the index of s, k is the index of p\n        dp[0][0] = True\n        for j in range(2,n+1):\n            if p[j-1]=='*':\n                dp[0][j]=dp[0][j-2]\n            \n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if p[j-1] == '*':\n                    dp[i][j] = dp[i][j-2] # zero occurrence\n                    if s[i-1]==p[j-2] or p[j-2]=='.':\n                        dp[i][j]|=dp[i-1][j] or dp[i-1][j-2] # one or more occurrence\n                else:\n                    dp[i][j] = dp[i-1][j-1] and (s[i-1] == p[j-1] or p[j-1] == '.')\n        return dp[m][n]"
  },
  {
    "objectID": "Math/reflections/taylor-euler-fourier.html",
    "href": "Math/reflections/taylor-euler-fourier.html",
    "title": "From Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series",
    "section": "",
    "text": "This reflection explores the beautiful mathematical journey from Taylor expansions to Euler‚Äôs formula and ultimately to Fourier series. We‚Äôll see how these concepts naturally build upon each other, revealing deep connections between polynomials, exponentials, and trigonometric functions."
  },
  {
    "objectID": "Math/reflections/taylor-euler-fourier.html#complex-numbers-foundation",
    "href": "Math/reflections/taylor-euler-fourier.html#complex-numbers-foundation",
    "title": "From Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series",
    "section": "Complex Numbers: Foundation",
    "text": "Complex Numbers: Foundation\nComplex numbers provide the foundation for understanding Euler‚Äôs formula.\nDefinition: \\(i^2 = -1\\)\nCyclic behavior: - \\(i^1 = i\\) - \\(i^2 = -1\\) - \\(i^3 = -i\\) - \\(i^4 = 1\\) - \\(i^n = i^{n-4} \\cdot i^4 = i^{n-4}\\) (the pattern repeats every 4 powers)\nVisualization of the periodic behavior:\n\n\nYour browser does not support the video tag. \n\nChain Rule of Derivatives\nTwo fundamental rules we‚Äôll use throughout:\n\\[\n\\frac{d}{dx}(f(x) + g(x)) = f'(x) + g'(x)\n\\]\n\\[\n\\frac{d}{dx}(c \\cdot f(x)) = c \\cdot f'(x)\n\\]"
  },
  {
    "objectID": "Math/reflections/taylor-euler-fourier.html#taylor-expansion",
    "href": "Math/reflections/taylor-euler-fourier.html#taylor-expansion",
    "title": "From Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series",
    "section": "Taylor Expansion",
    "text": "Taylor Expansion\n\nThe Formula\nAny analytic function can be expressed as an infinite polynomial around a point \\(a\\):\n\\[\nf(x) = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n!}(x-a)^n\n\\]\nKey insight: A function can be written as a power series if and only if it is analytic. Analyticity means that all derivatives exist and the Taylor remainder goes to zero, so the infinite polynomial determined by the derivatives reconstructs the function exactly.\n\n\nProof: Finding the Coefficients\nWe want to approximate \\(f(x)\\) using a polynomial:\n\\[\nP(x) = c_0 + c_1(x-a) + c_2(x-a)^2 + c_3(x-a)^3 + \\cdots\n\\]\nOur goal is to match all derivatives at point \\(a\\): - \\(P(a) = f(a)\\) - \\(P'(a) = f'(a)\\) - \\(P''(a) = f''(a)\\) - \\(P^{(n)}(a) = f^{(n)}(a)\\)\nLet‚Äôs find the coefficients by taking derivatives:\n0th-order (evaluating at \\(x = a\\)):\n\\[\nP(a) = c_0 = f(a)\n\\]\n1st-order:\n\\[\nP'(x) = c_1 + 2c_2(x-a) + 3c_3(x-a)^2 + \\cdots\n\\]\n\\[\nP'(a) = c_1 = f'(a)\n\\]\n2nd-order:\n\\[\nP''(x) = 2c_2 + 6c_3(x-a) + \\cdots\n\\]\n\\[\nP''(a) = 2c_2 = f''(a)\n\\]\n\\[\nc_2 = \\frac{f''(a)}{2!}\n\\]\nnth-order:\n\\[\nP^{(n)}(a) = n! \\cdot c_n = f^{(n)}(a)\n\\]\n\\[\nc_n = \\frac{f^{(n)}(a)}{n!}\n\\]\nCrucial observation: Every lower-order derivative still contains a factor \\((x-a)\\). For any \\(m &lt; n\\):\n\\[\n\\frac{d^m}{dx^m}(x-a)^n = \\text{(some constant)} \\cdot (x-a)^{n-m}\n\\]\nAt \\(x = a\\), this becomes \\((x-a)^{n-m} = 0\\). So all lower-order derivatives vanish at \\(x = a\\), which is why each derivative gives us exactly one coefficient.\n\n\nVisualization: Approximating \\(e^x\\)\nLet‚Äôs visualize how different order Taylor polynomials approximate \\(e^x\\) around \\(x = 0\\):\nFirst-order approximation: \\(P(x) = c_0 + c_1(x-a) = 1 + x\\)\n\n\n\nFirst Order\n\n\nSecond-order approximation: \\(P(x) = c_0 + c_1(x-a) + c_2(x-a)^2 = 1 + x + \\frac{x^2}{2}\\)\n\n\n\nSecond Order\n\n\nThird-order approximation: \\(P(x) = c_0 + c_1(x-a) + c_2(x-a)^2 + c_3(x-a)^3 = 1 + x + \\frac{x^2}{2} + \\frac{x^3}{6}\\)\n\n\n\nThird Order\n\n\nFourth-order approximation: \\(P(x) = 1 + x + \\frac{x^2}{2} + \\frac{x^3}{6} + \\frac{x^4}{24}\\)\n\n\n\nFourth Order\n\n\nAs we add more terms, the polynomial approximation becomes increasingly accurate over a wider range."
  },
  {
    "objectID": "Math/reflections/taylor-euler-fourier.html#deriving-the-taylor-series-of-three-fundamental-functions",
    "href": "Math/reflections/taylor-euler-fourier.html#deriving-the-taylor-series-of-three-fundamental-functions",
    "title": "From Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series",
    "section": "Deriving the Taylor Series of Three Fundamental Functions",
    "text": "Deriving the Taylor Series of Three Fundamental Functions\n\nMaclaurin Series of \\(e^x\\)\nFor \\(f(x) = e^x\\), we have a remarkable property: every derivative equals the function itself:\n\n\\(f'(x) = e^x\\)\n\\(f''(x) = e^x\\)\n\\(f^{(n)}(x) = e^x\\)\n\nAt \\(a = 0\\) (Maclaurin series):\n\n\\(c_0 = e^0 = 1\\)\n\\(c_1 = e^0 = 1\\)\n\\(c_2 = \\frac{e^0}{2!} = \\frac{1}{2!}\\)\n\\(c_n = \\frac{e^0}{n!} = \\frac{1}{n!}\\)\n\nTaylor expansion:\n\\[\ne^x = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!}\n\\]\n\n\nMaclaurin Series of \\(\\sin x\\)\nFor \\(f(x) = \\sin x\\), the derivatives follow a cycle of period 4:\n\n\\(f'(x) = \\cos x\\)\n\\(f''(x) = -\\sin x\\)\n\\(f'''(x) = -\\cos x\\)\n\\(f^{(4)}(x) = \\sin x\\)\n\\(f^{(n+4)}(x) = f^{(n)}(x)\\)\n\nAt \\(a = 0\\):\n\n\\(c_0 = \\sin 0 = 0\\)\n\\(c_1 = \\cos 0 = 1\\)\n\\(c_2 = \\frac{-\\sin 0}{2!} = 0\\)\n\\(c_3 = \\frac{-\\cos 0}{3!} = -\\frac{1}{3!}\\)\n\\(c_4 = \\frac{\\sin 0}{4!} = 0\\)\n\nPattern: Only odd powers survive, with alternating signs.\nTaylor expansion:\n\\[\n\\boxed{\\sin x = \\sum_{k=0}^{\\infty} (-1)^k \\frac{x^{2k+1}}{(2k+1)!} = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots}\n\\]\n\n\nMaclaurin Series of \\(\\cos x\\)\nFor \\(f(x) = \\cos x\\), the derivatives also follow a cycle of period 4:\n\n\\(f'(x) = -\\sin x\\)\n\\(f''(x) = -\\cos x\\)\n\\(f'''(x) = \\sin x\\)\n\\(f^{(4)}(x) = \\cos x\\)\n\nAt \\(a = 0\\):\n\n\\(c_0 = \\cos 0 = 1\\)\n\\(c_1 = -\\sin 0 = 0\\)\n\\(c_2 = \\frac{-\\cos 0}{2!} = -\\frac{1}{2!}\\)\n\\(c_3 = \\frac{\\sin 0}{3!} = 0\\)\n\\(c_4 = \\frac{\\cos 0}{4!} = \\frac{1}{4!}\\)\n\nPattern: Only even powers survive, with alternating signs. We have \\(c_{2k} = \\frac{(-1)^k}{(2k)!}\\) and \\(c_{2k+1} = 0\\).\nTaylor expansion:\n\\[\n\\cos x = \\sum_{k=0}^{\\infty} (-1)^k \\frac{x^{2k}}{(2k)!} = 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\cdots\n\\]"
  },
  {
    "objectID": "Math/reflections/taylor-euler-fourier.html#eulers-formula-the-bridge",
    "href": "Math/reflections/taylor-euler-fourier.html#eulers-formula-the-bridge",
    "title": "From Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series",
    "section": "Euler‚Äôs Formula: The Bridge",
    "text": "Euler‚Äôs Formula: The Bridge\nNow comes one of the most beautiful formulas in mathematics:\n\\[\ne^{i\\theta} = \\cos\\theta + i\\sin\\theta\n\\]\n\nProof\nStep 1: Start with the Maclaurin series of the exponential function, which is valid for complex arguments:\n\\[\ne^z = \\sum_{n=0}^{\\infty} \\frac{z^n}{n!}\n\\]\nStep 2: Split into even and odd terms:\n\\[\ne^z = \\sum_{k=0}^{\\infty} \\frac{z^{2k}}{(2k)!} + \\sum_{k=0}^{\\infty} \\frac{z^{2k+1}}{(2k+1)!}\n\\]\nStep 3: Substitute \\(z = i\\theta\\):\n\\[\ne^{i\\theta} = \\sum_{k=0}^{\\infty} \\frac{(i\\theta)^{2k}}{(2k)!} + \\sum_{k=0}^{\\infty} \\frac{(i\\theta)^{2k+1}}{(2k+1)!}\n\\]\nStep 4: Simplify even powers:\nFor even powers: \\((i\\theta)^{2k} = i^{2k} \\theta^{2k}\\)\nSince \\(i^{2k} = (i^2)^k = (-1)^k\\), the first sum becomes:\n\\[\n\\sum_{k=0}^{\\infty} \\frac{(-1)^k \\theta^{2k}}{(2k)!} = \\cos\\theta\n\\]\nThis is exactly the Taylor series of \\(\\cos\\theta\\)!\nStep 5: Simplify odd powers:\nFor odd powers: \\((i\\theta)^{2k+1} = i^{2k+1} \\theta^{2k+1} = i \\cdot i^{2k} \\theta^{2k+1} = i(-1)^k \\theta^{2k+1}\\)\nThe second sum becomes:\n\\[\ni \\sum_{k=0}^{\\infty} \\frac{(-1)^k \\theta^{2k+1}}{(2k+1)!} = i\\sin\\theta\n\\]\nConclusion:\n\\[\ne^{i\\theta} = \\cos\\theta + i\\sin\\theta\n\\]\nThis formula reveals that exponentials and trigonometric functions are intimately connected through complex numbers."
  },
  {
    "objectID": "Math/reflections/taylor-euler-fourier.html#fourier-series-the-application",
    "href": "Math/reflections/taylor-euler-fourier.html#fourier-series-the-application",
    "title": "From Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series",
    "section": "Fourier Series: The Application",
    "text": "Fourier Series: The Application\nThe Fourier series allows us to represent any periodic function as an infinite sum of sines and cosines:\n\\[\nf(x) = \\frac{a_0}{2} + \\sum_{n=1}^{\\infty} \\left(a_n \\cos nx + b_n \\sin nx\\right)\n\\]\n\nConverting to Complex Form Using Euler‚Äôs Formula\nStep 1: Recall key properties of trigonometric functions:\n\n\\(\\cos(-x) = \\cos(x)\\) (even function)\n\\(\\sin(-x) = -\\sin(x)\\) (odd function)\n\nStep 2: From Euler‚Äôs formula and its complex conjugate:\n\\[\ne^{ix} = \\cos x + i\\sin x\n\\]\n\\[\ne^{-ix} = \\cos x - i\\sin x\n\\]\nStep 3: Solve for \\(\\cos\\) and \\(\\sin\\):\nAdding the two equations:\n\\[\n\\cos(nx) = \\frac{e^{inx} + e^{-inx}}{2}\n\\]\nSubtracting the two equations:\n\\[\n\\sin(nx) = \\frac{e^{inx} - e^{-inx}}{2i}\n\\]\nStep 4: Substitute into the Fourier series:\n\\[\na_n\\cos(nx) + b_n\\sin(nx) = a_n\\frac{e^{inx} + e^{-inx}}{2} + b_n\\frac{e^{inx} - e^{-inx}}{2i}\n\\]\nStep 5: Define complex coefficients:\nFor positive frequencies (\\(n &gt; 0\\)):\n\\[\nc_n = \\frac{a_n}{2} + \\frac{b_n}{2i} = \\frac{a_n - ib_n}{2}\n\\]\n(Note: \\(\\frac{1}{i} = -i\\) because \\(i \\cdot (-i) = -i^2 = 1\\))\nFor negative frequencies (\\(n &lt; 0\\)):\n\\[\nc_{-n} = \\frac{a_n}{2} - \\frac{b_n}{2i} = \\frac{a_n + ib_n}{2}\n\\]\nThe complex form of the Fourier series:\n\\[\nf(x) = \\sum_{n=-\\infty}^{\\infty} c_n e^{inx}\n\\]\nThis elegant form shows that periodic functions can be decomposed into complex exponentials, which are easier to manipulate mathematically than sines and cosines."
  },
  {
    "objectID": "Math/reflections/taylor-euler-fourier.html#the-beautiful-chain-of-ideas",
    "href": "Math/reflections/taylor-euler-fourier.html#the-beautiful-chain-of-ideas",
    "title": "From Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series",
    "section": "The Beautiful Chain of Ideas",
    "text": "The Beautiful Chain of Ideas\nLet‚Äôs trace the entire journey:\n\nTaylor Expansion: Any analytic function can be represented as an infinite polynomial\nExponential and Trigonometric Series: We derive the series for \\(e^x\\), \\(\\sin x\\), and \\(\\cos x\\)\nEuler‚Äôs Formula: By substituting \\(i\\theta\\) into the exponential series and using the cyclic properties of \\(i\\), we discover that \\(e^{i\\theta} = \\cos\\theta + i\\sin\\theta\\)\nFourier Series: Using Euler‚Äôs formula, we can express any periodic function as a sum of complex exponentials, revealing the fundamental frequencies that compose the function\n\nThis chain illustrates a profound truth in mathematics: seemingly different concepts (polynomials, exponentials, trigonometric functions, complex numbers) are all deeply interconnected. The bridge between them‚ÄîEuler‚Äôs formula‚Äîis not just a computational tool but a revelation of the underlying unity of mathematical structures."
  },
  {
    "objectID": "Math/reflections/taylor-euler-fourier.html#why-this-matters",
    "href": "Math/reflections/taylor-euler-fourier.html#why-this-matters",
    "title": "From Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series",
    "section": "Why This Matters",
    "text": "Why This Matters\nUnderstanding this progression is crucial for:\n\nSignal Processing: Fourier analysis decomposes signals into frequency components\nQuantum Mechanics: Wave functions are expressed using complex exponentials\nDifferential Equations: Many solutions involve exponentials and trigonometric functions\nControl Theory: System responses are analyzed in the frequency domain\n\nThe mathematical beauty lies not just in the formulas themselves, but in how naturally they flow from one another, each building on the foundation of the previous concept."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part2-monotonicity.html",
    "href": "Math/EE364A/ee364a-lecture5-part2-monotonicity.html",
    "title": "EE 364A (Convex Optimization): Lecture 5.2 - Monotonicity and Convexity with Generalized Inequalities",
    "section": "",
    "text": "Convex Optimization Textbook - Chapter 3.6 (page 122)"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part2-monotonicity.html#proper-cone",
    "href": "Math/EE364A/ee364a-lecture5-part2-monotonicity.html#proper-cone",
    "title": "EE 364A (Convex Optimization): Lecture 5.2 - Monotonicity and Convexity with Generalized Inequalities",
    "section": "Proper Cone",
    "text": "Proper Cone\nA proper cone has five properties:\n\nCone: \\(x \\in K,\\ \\alpha \\ge 0 \\Rightarrow \\alpha x \\in K\\)\nConvex: \\(x,y \\in K,\\ 0 \\le \\theta \\le 1 \\Rightarrow \\theta x + (1-\\theta)y \\in K\\)\nClosed\nPointed\nNon-empty interior\n\nA cone \\(K\\subseteq\\mathbb{R}^n\\) is proper if it is convex, closed, pointed, and has nonempty interior.\n\n\n\nProper cone visualization"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part2-monotonicity.html#examples",
    "href": "Math/EE364A/ee364a-lecture5-part2-monotonicity.html#examples",
    "title": "EE 364A (Convex Optimization): Lecture 5.2 - Monotonicity and Convexity with Generalized Inequalities",
    "section": "Examples",
    "text": "Examples\n\nMonotone Vector Functions\nA function \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) is nondecreasing with respect to \\(\\mathbb{R}_+^n\\) iff:\n\\[\nx_1\\le y_1,...,x_n \\le y_n \\Longrightarrow f(x)\\le f(y)\n\\]\n\n\nMatrix Monotone Functions\nA function \\(f:S^n \\to \\mathbb{R}\\) is called matrix monotone (increasing, decreasing) if it is monotone with respect to the positive semidefinite cone.\nExamples:\n\n\\(\\mathrm{tr}(WX) \\quad |W \\in S^n\\):\n\n\\(W \\succeq 0\\): matrix nondecreasing\n\\(W \\succ 0\\): matrix increasing\n\\(W \\preceq 0\\): matrix nonincreasing\n\\(W \\prec 0\\): matrix decreasing\n\n\\(\\mathrm{tr}(X^{-1})\\) is matrix decreasing on \\(S_{++}^n\\)\n\\(\\mathrm{det}X\\) is matrix increasing on \\(S_{++}^n\\), and matrix nondecreasing on \\(S_{+}^n\\)\n\n\n\n\nMatrix monotone function examples"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part2-monotonicity.html#examples-1",
    "href": "Math/EE364A/ee364a-lecture5-part2-monotonicity.html#examples-1",
    "title": "EE 364A (Convex Optimization): Lecture 5.2 - Monotonicity and Convexity with Generalized Inequalities",
    "section": "Examples",
    "text": "Examples\n\nMatrix Convexity\nMatrix Inequality:\nFor \\(A \\in S^n, B \\in S^n\\):\n\n\\(A \\succ B\\) if \\(A-B\\) is positive definite\n\\(A \\succeq B\\) if \\(A-B\\) is positive semidefinite\n\\(A\\prec B\\) if \\(A-B\\) is negative definite\n\\(A \\preceq B\\) if \\(A-B\\) is negative semidefinite\nOtherwise, not comparable\n\nSuppose \\(f\\) is a symmetric-valued function, e.g., \\(f:\\mathbb{R}^n \\to S^m\\). The function \\(f\\) is convex with respect to matrix inequality if:\n\\[\nf(\\theta x+(1-\\theta)y) \\preceq \\theta f(x)+(1-\\theta)f(y)\n\\]\nfor all \\(x\\) and \\(y\\), and \\(0 \\le \\theta \\le 1\\).\n\\(f\\) is strictly matrix convex if \\(f(\\theta x+(1-\\theta)y) \\prec \\theta f(x)+(1-\\theta)f(y)\\) for all \\(x \\ne y\\) and \\(0 \\le \\theta \\le 1\\).\nExamples:\n\n\\(f(X)=XX^\\top\\) where \\(X \\in \\mathbb{R}^{n \\times m}\\)\n\\(f(X)=X^p\\) where \\(X \\in S_{++}^n\\), \\(1\\le p\\le2\\) or \\(-1 \\le p\\le 0\\)\n\n\\(f\\) is matrix concave for \\(0 \\le p \\le1\\)\n\n\\(f(X)=e^X\\) is NOT matrix convex on \\(S^n\\) for \\(n \\ge 2\\)"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part2-monotonicity.html#dual-characterization-of-k-convexity",
    "href": "Math/EE364A/ee364a-lecture5-part2-monotonicity.html#dual-characterization-of-k-convexity",
    "title": "EE 364A (Convex Optimization): Lecture 5.2 - Monotonicity and Convexity with Generalized Inequalities",
    "section": "Dual characterization of K-convexity",
    "text": "Dual characterization of K-convexity\nA function \\(f:\\mathbb{R}^n \\to \\mathbb{R}^m\\) is K-convex iff \\(\\forall w \\succeq _{K^*} 0\\), \\(w^\\top f\\) is convex (in the ordinary sense).\n\\(f\\) is strictly K-convex if \\(w^\\top f\\) is strictly convex for every nonzero \\(w\\) in \\(K^*\\)."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part2-monotonicity.html#differentiable-k-convex-functions",
    "href": "Math/EE364A/ee364a-lecture5-part2-monotonicity.html#differentiable-k-convex-functions",
    "title": "EE 364A (Convex Optimization): Lecture 5.2 - Monotonicity and Convexity with Generalized Inequalities",
    "section": "Differentiable K-convex functions",
    "text": "Differentiable K-convex functions\nA differentiable function \\(f\\) is K-convex iff:\n\nIts domain is convex\nFor all \\(x,y \\in \\mathrm{dom } f\\):\n\n\\[\nf(y)\\succeq_K f(x)+Df(x)(y-x)\n\\]\nwhere \\(Df(x)\\) is the Jacobian matrix of \\(f\\) at \\(x\\)."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part2-monotonicity.html#composition-theorem",
    "href": "Math/EE364A/ee364a-lecture5-part2-monotonicity.html#composition-theorem",
    "title": "EE 364A (Convex Optimization): Lecture 5.2 - Monotonicity and Convexity with Generalized Inequalities",
    "section": "Composition Theorem",
    "text": "Composition Theorem\nMany of the results on composition can be generalized to K-convexity. For example, if:\n\n\\(g:\\mathbb{R}^n \\to \\mathbb{R}^p\\) is K-convex\n\\(h: \\mathbb{R}^p \\to \\mathbb{R}\\) is convex\n\\(\\tilde{h}\\) is K-nondecreasing (extended-value extension of \\(h\\))\n\nThe condition that \\(\\tilde h\\) is K-nondecreasing implies \\(\\mathrm{dom}\\,h - K = \\mathrm{dom}\\,h\\).\nThen \\(h\\circ g\\) is convex."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#convex-sets",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#convex-sets",
    "title": "EE 364A (Convex Optimization): Lecture 2 - Convex Sets",
    "section": "Convex Sets",
    "text": "Convex Sets\n\nAffine Combination\n\\[x = \\theta x_1 + (1-\\theta)x_2, \\quad \\theta \\in \\mathbb{R}\\]\nAffine combination is the special form of linear combination where the sum of coefficients equals 1.\nExample: Solution set \\(\\{x \\mid Ax = b\\}\\)\nFor any \\(x_1, x_2\\) which both are solutions of \\(Ax=b\\):\n\\[A(\\theta x_1 + (1-\\theta)x_2) = \\theta A x_1 + (1-\\theta) A x_2 = \\theta b + (1-\\theta)b = b\\]\n Figure: Affine combinations form a line through two points. Unlike linear combinations, affine combinations require coefficients to sum to 1, making them translation-invariant‚Äîthe solution set of Ax=b forms an affine subspace.\n\n\n\nConvex Set\nLinear segment: \\(x = \\theta x_1 + (1-\\theta)x_2, \\quad 0 \\leq \\theta \\leq 1\\)\nConvex set: A set \\(C\\) is convex if for all \\(x_1, x_2 \\in C\\) and \\(0 \\leq \\theta \\leq 1\\):\n\\[\\theta x_1 + (1-\\theta)x_2 \\in C\\]\n Figure: A convex set contains all line segments between any two points in the set. The left shape is convex (any line segment between two points stays inside), while the right shape is non-convex (some line segments exit the set).\n\n\n\nConvex Combination and Convex Hull\nConvex combination:\n\\[x = \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k\\]\nwhere \\(\\theta_1 + \\theta_2 + \\ldots + \\theta_k = 1\\) and \\(\\theta_i \\geq 0\\)\nConvex hull: The set of all convex combinations of points in \\(S\\)‚Äîthe smallest convex set containing \\(S\\).\n Figure: The convex hull of a finite set of points forms a convex polygon (or polyhedron in higher dimensions) that encloses all the points. It‚Äôs like stretching a rubber band around the outermost points.\n\n\n\nConvex Cone\n\\[x = \\theta_1 x_1 + \\theta_2 x_2, \\quad \\theta_1 \\geq 0, \\theta_2 \\geq 0\\]\n Figure: A convex cone is closed under positive linear combinations. If you take any two vectors in the cone and scale them by non-negative coefficients, their sum remains in the cone. The cone extends infinitely from the origin."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#hyperplanes-and-halfspaces",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#hyperplanes-and-halfspaces",
    "title": "EE 364A (Convex Optimization): Lecture 2 - Convex Sets",
    "section": "Hyperplanes and Halfspaces",
    "text": "Hyperplanes and Halfspaces\n\nHyperplane\nSet of the form \\(a^\\top x = b\\) where \\(a \\neq 0\\)\n\n\\(a\\) is the normal vector\nHyperplanes are both affine and convex\n\n\n\nHalfspaces\n\\[a^\\top x \\leq b\\]\n\nHalfspaces are convex\n\n Figure: A hyperplane (a^x = b) divides space into two halfspaces: a^x ‚â§ b and a^x ‚â• b. The normal vector a points perpendicular to the hyperplane, and b controls the offset from the origin."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#euclidean-balls-and-ellipsoids",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#euclidean-balls-and-ellipsoids",
    "title": "EE 364A (Convex Optimization): Lecture 2 - Convex Sets",
    "section": "Euclidean Balls and Ellipsoids",
    "text": "Euclidean Balls and Ellipsoids\n\nEuclidean Ball\n\\[B(x_c, r) = \\{x \\mid \\|x - x_c\\|_2 \\leq r\\} = \\{x_c + ru \\mid \\|u\\|_2 \\leq 1\\}\\]\n\n\nEllipsoid\n\\[\\{x \\mid (x - x_c)^\\top P^{-1}(x - x_c) \\leq 1\\}\\]\nwhere \\(P \\in \\mathbb{S}_{++}^n\\) (P is positive definite symmetric)\nAlternative representation: \\(x_c + Au\\) where \\(A\\) is an invertible square matrix\n Figure: Euclidean balls have uniform radius in all directions, while ellipsoids stretch the ball along different axes. The matrix P in the ellipsoid formula determines the shape and orientation of the stretching."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#norm-balls-and-norm-cones",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#norm-balls-and-norm-cones",
    "title": "EE 364A (Convex Optimization): Lecture 2 - Convex Sets",
    "section": "Norm Balls and Norm Cones",
    "text": "Norm Balls and Norm Cones\n\nNorm Ball\nA norm function \\(\\|\\cdot\\|\\) satisfies:\n\n\\(\\|x\\| \\geq 0\\), and \\(\\|x\\| = 0\\) only if \\(x = 0\\)\n\\(\\|tx\\| = |t| \\cdot \\|x\\|\\) for \\(t \\in \\mathbb{R}\\)\n\\(\\|x + y\\| \\leq \\|x\\| + \\|y\\|\\) (triangle inequality)\n\nNorm ball: \\(\\{x \\mid \\|x - x_c\\| \\leq r\\}\\)\n Figure: Different norms create different ball shapes. L1 norm creates a diamond, L2 norm creates a circle, and L‚àû norm creates a square. All satisfy the norm axioms but measure distance differently.\n\n\n\nNorm Cone\n\\[\\{(x, t) \\mid \\|x\\| \\leq t\\}\\]\nThe Euclidean norm cone is called the second-order cone.\n Figure: A norm cone extends infinitely upward, containing all points (x,t) where the norm of x is bounded by t. The second-order cone (using L2 norm) is fundamental in conic optimization and has special properties for efficient optimization."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#polyhedra",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#polyhedra",
    "title": "EE 364A (Convex Optimization): Lecture 2 - Convex Sets",
    "section": "Polyhedra",
    "text": "Polyhedra\nDefined by inequality and equality constraints:\n\\[Ax \\preceq b, \\quad Cx = d\\]\nwhere \\(A \\in \\mathbb{R}^{m \\times n}\\) and \\(C \\in \\mathbb{R}^{p \\times n}\\)\nHere \\(Ax \\preceq b\\) denotes componentwise inequality, i.e., \\((Ax)_i \\leq b_i\\) for all \\(i\\).\n Figure: A polyhedron is the intersection of finitely many halfspaces and hyperplanes. Linear programming optimizes over polyhedra, making them fundamental to optimization theory. Every vertex represents a basic feasible solution."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#positive-semidefinite-cone",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#positive-semidefinite-cone",
    "title": "EE 364A (Convex Optimization): Lecture 2 - Convex Sets",
    "section": "Positive Semidefinite Cone",
    "text": "Positive Semidefinite Cone\nLet \\(\\mathbb{S}^n\\) denote the set of \\(n \\times n\\) symmetric matrices.\n\n\\(\\mathbb{S}_+^n = \\{X \\in \\mathbb{S}^n \\mid X \\succeq 0\\}\\) ‚Äî positive semidefinite\n\n\\(\\mathbb{S}_+^n\\) is a convex cone\n\n\\(\\mathbb{S}_{++}^n = \\{X \\in \\mathbb{S}^n \\mid X \\succ 0\\}\\) ‚Äî positive definite\n\n Figure: The positive semidefinite cone contains all symmetric matrices with non-negative eigenvalues. It‚Äôs a convex cone in the space of symmetric matrices, fundamental to semidefinite programming (SDP)."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#preserving-convexity",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#preserving-convexity",
    "title": "EE 364A (Convex Optimization): Lecture 2 - Convex Sets",
    "section": "Preserving Convexity",
    "text": "Preserving Convexity\n\nTesting Convexity\nTo determine if a set \\(C\\) is convex, check whether every convex combination of any two points in \\(C\\) is also in \\(C\\):\n\\[x_1, x_2 \\in C, \\quad 0 \\leq \\theta \\leq 1 \\implies \\theta x_1 + (1-\\theta)x_2 \\in C\\]\n\n\nOperations That Preserve Convexity\n1. Intersection: The intersection of convex sets is convex.\n Figure: Intersecting convex sets always produces a convex set. This property is crucial because it allows us to build complex convex sets by intersecting simpler ones, like defining polyhedra as intersections of halfspaces.\n2. Affine functions: \\(f(x) = Ax + b\\)\nIf \\(C\\) is convex, then both \\(f(C)\\) and \\(f^{-1}(C)\\) are convex.\n Figure: Affine transformations (scaling, rotation, translation) preserve convexity. If you take a convex set and apply an affine function, the image is still convex. Similarly, the pre-image of a convex set under an affine function is convex.\n3. Perspective functions: \\(P: \\mathbb{R}^{n+1} \\to \\mathbb{R}^n\\)\n\\[P(x, t) = x/t, \\quad \\text{dom}\\,P = \\{(x, t) \\mid t &gt; 0\\}\\]\n4. Linear-fractional functions:\n\\[f(x) = \\frac{Ax + b}{c^\\top x + d}, \\quad \\text{dom}\\,f = \\{x \\mid c^\\top x + d &gt; 0\\}\\]\nExample: \\(f(x) = \\frac{1}{x_1 + x_2 + 1}x\\)"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#generalized-inequalities",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#generalized-inequalities",
    "title": "EE 364A (Convex Optimization): Lecture 2 - Convex Sets",
    "section": "Generalized Inequalities",
    "text": "Generalized Inequalities\nA convex cone \\(K \\subseteq \\mathbb{R}^n\\) is a proper cone if:\n\n\\(K\\) is closed (contains its boundary)\n\\(K\\) is solid (has nonempty interior)\n\\(K\\) is pointed (contains no line)\n\n\nExamples of Proper Cones\n\nNonnegative orthant: \\(K = \\mathbb{R}_+^n = \\{x \\in \\mathbb{R}^n \\mid x_i \\geq 0, \\, i=1,\\ldots,n\\}\\)\nPositive semidefinite cone: \\(\\mathbb{S}_+^n\\)\nNonnegative polynomials on [0,1]: \\[K = \\{x \\in \\mathbb{R}^n \\mid x_1 + x_2t + x_3t^2 + \\ldots + x_nt^{n-1} \\geq 0 \\text{ for } t \\in [0,1]\\}\\]\n\n\n\nGeneralized Inequality Notation\n\\[x \\preceq_K y \\Longleftrightarrow y - x \\in K\\] \\[x \\prec_K y \\Longleftrightarrow y - x \\in \\text{int}\\,K\\]\n\n\nExamples\n\nComponentwise inequality (\\(K = \\mathbb{R}_+^n\\)): \\[x \\preceq_{\\mathbb{R}_+^n} y \\Longleftrightarrow x_i \\leq y_i \\text{ for } i=1,2,\\ldots,n\\]\nMatrix inequality (\\(K = \\mathbb{S}_+^n\\)): \\[X \\preceq_{\\mathbb{S}_+^n} Y \\Longleftrightarrow Y - X \\text{ is positive semidefinite}\\]\n\n\n\nProperties Similar to \\(\\leq\\) in \\(\\mathbb{R}\\)\n\\[x \\preceq_K y, \\, u \\preceq_K v \\implies x + u \\preceq_K y + v\\]"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#minimum-and-minimal-elements",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#minimum-and-minimal-elements",
    "title": "EE 364A (Convex Optimization): Lecture 2 - Convex Sets",
    "section": "Minimum and Minimal Elements",
    "text": "Minimum and Minimal Elements\nImportant: \\(\\preceq_K\\) is not a total ordering‚Äîwe can have \\(x \\not\\preceq_K y\\) and \\(y \\not\\preceq_K x\\) simultaneously.\n\nMinimum: \\(x\\) is the minimum element of \\(S\\) with respect to \\(\\preceq_K\\) if: \\[y \\in S \\implies x \\preceq_K y\\]\nMinimal: \\(x\\) is a minimal element of \\(S\\) with respect to \\(\\preceq_K\\) if: \\[y \\in S, \\, y \\preceq_K x \\implies y = x\\]\n\n Figure: The minimum element is unique and comparable to all other elements (if it exists). Minimal elements are not dominated by any other element in the set, but there can be multiple minimal elements that are incomparable to each other."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#key-insight",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#key-insight",
    "title": "EE 364A (Convex Optimization): Lecture 2 - Convex Sets",
    "section": "Key Insight",
    "text": "Key Insight\nMathematical foundations of convex optimization rest on convex sets and generalized inequalities. Convex sets (balls, cones, polyhedra, semidefinite cones) form the geometric foundation. Affine functions, intersections, and perspective operations preserve convexity, allowing us to construct complex convex sets from simpler building blocks. Generalized inequalities (\\(\\preceq_K\\)) extend scalar ordering to vectors and matrices, enabling matrix inequalities (semidefinite constraints) and componentwise constraints. While generalized orderings lack total comparability, they retain key properties like additivity, making them powerful tools for formulating optimization problems over cones. These mathematical structures‚Äîconvex sets, proper cones, and their preservation under operations‚Äîform the bedrock of convex optimization theory."
  },
  {
    "objectID": "Math/EE364A/lectures.html",
    "href": "Math/EE364A/lectures.html",
    "title": "Stanford EE 364A: Convex Optimization",
    "section": "",
    "text": "My notes from Stanford EE 364A: Convex Optimization. This course covers the theory and applications of convex optimization, including least squares, linear programming, and convex optimization problems.\n\n\n\nLecture 1: Introduction to Convex Optimization Introduction to constraint optimization problems: least squares (\\(\\|Ax-b\\|_2^2\\)), linear programming (\\(c^\\top x\\) with linear constraints), and convex optimization (convex objective and constraints). Convex optimization generalizes both while maintaining polynomial-time solvability: \\(O(\\max(n^3, n^2m, F))\\) where \\(F\\) is derivative computation cost.\n\n\nLecture 2: Convex Sets Convex sets (affine combinations, convex combinations, convex hull, convex cone), hyperplanes and halfspaces, Euclidean balls and ellipsoids, norm balls and norm cones (including second-order cone), polyhedra, positive semidefinite cone, operations preserving convexity (intersection, affine functions, perspective, linear-fractional), generalized inequalities (proper cones, componentwise and matrix inequalities), and minimum vs minimal elements.\n\n\nLecture 3: Convex Functions Separating and supporting hyperplane theorems, dual cones and generalized inequalities, convex function definition, first-order condition (tangent underestimates), second-order condition (Hessian PSD), extended value extension, restriction to a line, epigraph and sublevel sets, Jensen‚Äôs inequality, and operations preserving convexity (nonnegative sums, affine composition, pointwise maximum).\n\n\nLecture 4 Part 1: Operations preserving Convexity Pointwise maximum and supremum (support function, distance to farthest point, maximum eigenvalue), composition with scalar functions (exponential, reciprocal), vector composition (log-sum-exp), minimization over convex sets (Schur complement, distance to set), and perspective functions with examples.\n\n\nLecture 4 Part 2: Conjugate and Quasiconvex Functions Conjugate functions \\(f^*(y) = \\sup_x (y^\\top x - f(x))\\) are always convex, with examples including negative logarithm and quadratic functions. Quasiconvex functions have convex sublevel sets with modified Jensen inequality \\(f(\\theta x + (1-\\theta)y) \\leq \\max\\{f(x), f(y)\\}\\). Examples include linear-fractional functions and distance ratios.\n\n\nLecture 5 Part 1: Log-Concave and Log-Convex Functions Log-concave functions satisfy \\(f(\\theta x+(1-\\theta)y)\\ge f(x)^\\theta f(y)^{1-\\theta}\\). Powers \\(x^a\\) are log-concave for \\(a \\ge 0\\) and log-convex for \\(a \\le 0\\). Key properties: products preserve log-concavity, but sums do not. Integration of log-concave functions preserves log-concavity.\n\n\nLecture 5 Part 2: Monotonicity with Generalized Inequalities K-nondecreasing functions satisfy \\(x \\preceq_K y \\Rightarrow f(x)\\le f(y)\\). Gradient condition: \\(\\nabla f(x) \\succeq_{K^*} 0\\) (dual inequality). Matrix monotone examples: \\(\\mathrm{tr}(WX)\\), \\(\\mathrm{det}X\\). K-convexity extends convexity to generalized inequalities with dual characterization and composition theorems."
  },
  {
    "objectID": "Math/EE364A/lectures.html#all-lectures",
    "href": "Math/EE364A/lectures.html#all-lectures",
    "title": "Stanford EE 364A: Convex Optimization",
    "section": "",
    "text": "My notes from Stanford EE 364A: Convex Optimization. This course covers the theory and applications of convex optimization, including least squares, linear programming, and convex optimization problems.\n\n\n\nLecture 1: Introduction to Convex Optimization Introduction to constraint optimization problems: least squares (\\(\\|Ax-b\\|_2^2\\)), linear programming (\\(c^\\top x\\) with linear constraints), and convex optimization (convex objective and constraints). Convex optimization generalizes both while maintaining polynomial-time solvability: \\(O(\\max(n^3, n^2m, F))\\) where \\(F\\) is derivative computation cost.\n\n\nLecture 2: Convex Sets Convex sets (affine combinations, convex combinations, convex hull, convex cone), hyperplanes and halfspaces, Euclidean balls and ellipsoids, norm balls and norm cones (including second-order cone), polyhedra, positive semidefinite cone, operations preserving convexity (intersection, affine functions, perspective, linear-fractional), generalized inequalities (proper cones, componentwise and matrix inequalities), and minimum vs minimal elements.\n\n\nLecture 3: Convex Functions Separating and supporting hyperplane theorems, dual cones and generalized inequalities, convex function definition, first-order condition (tangent underestimates), second-order condition (Hessian PSD), extended value extension, restriction to a line, epigraph and sublevel sets, Jensen‚Äôs inequality, and operations preserving convexity (nonnegative sums, affine composition, pointwise maximum).\n\n\nLecture 4 Part 1: Operations preserving Convexity Pointwise maximum and supremum (support function, distance to farthest point, maximum eigenvalue), composition with scalar functions (exponential, reciprocal), vector composition (log-sum-exp), minimization over convex sets (Schur complement, distance to set), and perspective functions with examples.\n\n\nLecture 4 Part 2: Conjugate and Quasiconvex Functions Conjugate functions \\(f^*(y) = \\sup_x (y^\\top x - f(x))\\) are always convex, with examples including negative logarithm and quadratic functions. Quasiconvex functions have convex sublevel sets with modified Jensen inequality \\(f(\\theta x + (1-\\theta)y) \\leq \\max\\{f(x), f(y)\\}\\). Examples include linear-fractional functions and distance ratios.\n\n\nLecture 5 Part 1: Log-Concave and Log-Convex Functions Log-concave functions satisfy \\(f(\\theta x+(1-\\theta)y)\\ge f(x)^\\theta f(y)^{1-\\theta}\\). Powers \\(x^a\\) are log-concave for \\(a \\ge 0\\) and log-convex for \\(a \\le 0\\). Key properties: products preserve log-concavity, but sums do not. Integration of log-concave functions preserves log-concavity.\n\n\nLecture 5 Part 2: Monotonicity with Generalized Inequalities K-nondecreasing functions satisfy \\(x \\preceq_K y \\Rightarrow f(x)\\le f(y)\\). Gradient condition: \\(\\nabla f(x) \\succeq_{K^*} 0\\) (dual inequality). Matrix monotone examples: \\(\\mathrm{tr}(WX)\\), \\(\\mathrm{det}X\\). K-convexity extends convexity to generalized inequalities with dual characterization and composition theorems."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#separating-hyperplane-theorem",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#separating-hyperplane-theorem",
    "title": "EE 364A (Convex Optimization): Lecture 3 - Convex Functions",
    "section": "Separating Hyperplane Theorem",
    "text": "Separating Hyperplane Theorem\nIf C and D are nonempty disjoint convex sets, then there exists \\(a \\neq 0, b\\) such that:\n\\[\na^\\top x \\leq b \\quad \\text{for } x \\in C, \\quad a^\\top x \\geq b \\quad \\text{for } x \\in D\n\\]\nThe hyperplane \\(a^\\top x = b\\) separates C and D.\n\n\n\nSeparating hyperplane between two convex sets"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#supporting-hyperplane-theorem",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#supporting-hyperplane-theorem",
    "title": "EE 364A (Convex Optimization): Lecture 3 - Convex Functions",
    "section": "Supporting Hyperplane Theorem",
    "text": "Supporting Hyperplane Theorem\nSupporting hyperplane to set C (doesn‚Äôt have to be a convex set) at boundary point \\(x_0\\):\n\\[\n\\{x \\mid a^\\top x = a^\\top x_0\\}\n\\]\nwhere \\(a\\) is the normal vector that determines the hyperplane tangent to C at point \\(x_0\\), with \\(a \\neq 0\\) and \\(a^\\top x \\leq a^\\top x_0\\) for \\(x \\in C\\).\n\n\n\nSupporting hyperplane at boundary point\n\n\nIf C is convex, then there is a supporting hyperplane at every point of its boundary.\n\n\n\nSupporting hyperplanes exist at every boundary point for convex sets"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#dual-cones-and-generalized-inequalities",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#dual-cones-and-generalized-inequalities",
    "title": "EE 364A (Convex Optimization): Lecture 3 - Convex Functions",
    "section": "Dual Cones and Generalized Inequalities",
    "text": "Dual Cones and Generalized Inequalities\nDual cone of a cone K:\n\\[\nK^* = \\{y \\mid y^\\top x \\geq 0 \\quad \\forall x \\in K\\}\n\\]\nThe inner product of any vector in \\(K^*\\) and K is nonnegative.\n\n\n\nDual cone visualization\n\n\n\nExamples\n\n\\(K = \\{0\\}\\): the dual cone is \\(K^* = \\mathbb{R}^n\\)\nK is a ray: the dual cone is a half space\nSelf-dual cones:\n\n\\(K = \\mathbb{R}_+^n\\): dual cone \\(K^* = K\\)\n\\(K = S_+^n\\) (positive semidefinite cone)\n\\(K = \\{(x,t) \\mid \\|x\\|_2 \\leq t\\}\\) (second-order cone)\n\n\\(K = \\{(x,t) \\mid \\|x\\|_\\infty \\leq t\\}\\): dual cone \\(K^* = \\{(y,\\tau) \\mid \\|y\\|_1 \\leq \\tau\\}\\)\n\nThe dual cone of a proper cone is proper."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#minimum-and-minimal-elements-via-dual-inequalities",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#minimum-and-minimal-elements-via-dual-inequalities",
    "title": "EE 364A (Convex Optimization): Lecture 3 - Convex Functions",
    "section": "Minimum and Minimal Elements via Dual Inequalities",
    "text": "Minimum and Minimal Elements via Dual Inequalities\n\nMinimum Element\n\\(x\\) is the minimum element of S if and only if for all \\(\\lambda \\succ_{K^*} 0\\), \\(x\\) is the unique minimizer of \\(\\lambda^\\top z\\) over S.\n\n\nMinimal Element\nIf \\(x\\) minimizes \\(\\lambda^\\top z\\) over S for some \\(\\lambda \\succ_{K^*} 0\\), then \\(x\\) is minimal.\nIf \\(x\\) is a minimal element of a convex set S, then there exists a nonzero \\(\\lambda \\succ_{K^*} 0\\) such that \\(x\\) minimizes \\(\\lambda^\\top z\\) over S."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#examples-on-mathbbr",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#examples-on-mathbbr",
    "title": "EE 364A (Convex Optimization): Lecture 3 - Convex Functions",
    "section": "Examples on \\(\\mathbb{R}\\)",
    "text": "Examples on \\(\\mathbb{R}\\)\nConvex:\n\nAffine: \\(ax + b\\) for any \\(a, b \\in \\mathbb{R}\\)\nExponential: \\(e^{ax}\\) for any \\(a \\in \\mathbb{R}\\)\nPowers: \\(x^a\\) on \\(\\mathbb{R}_{++}\\) for \\(a \\geq 1\\) or \\(a \\leq 0\\)\nPowers of absolute value: \\(|x|^p\\) on \\(\\mathbb{R}\\) for \\(p \\geq 1\\)\nNegative entropy: \\(x \\log x\\) on \\(\\mathbb{R}_{++}\\)\n\nConcave:\n\nAffine: \\(ax + b\\)\nPowers: \\(x^a\\) on \\(\\mathbb{R}_{++}\\) for \\(0 \\leq a \\leq 1\\)\nLogarithm: \\(\\log x\\) on \\(\\mathbb{R}_{++}\\)"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#examples-on-mathbbrn-and-mathbbrm-times-n",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#examples-on-mathbbrn-and-mathbbrm-times-n",
    "title": "EE 364A (Convex Optimization): Lecture 3 - Convex Functions",
    "section": "Examples on \\(\\mathbb{R}^n\\) and \\(\\mathbb{R}^{m \\times n}\\)",
    "text": "Examples on \\(\\mathbb{R}^n\\) and \\(\\mathbb{R}^{m \\times n}\\)\nOn \\(\\mathbb{R}^n\\):\n\nAffine function: \\(a^\\top x + b\\)\nNorms: \\(\\|x\\|_p = \\sqrt[p]{\\sum_i |x_i|^p}\\); \\(\\|x\\|_\\infty = \\max_k |x_k|\\)\n\nOn \\(\\mathbb{R}^{m \\times n}\\):\n\nAffine function: \\(f(X) = \\text{tr}(A^\\top X) + b = \\sum_{i,j} A_{ij} X_{ij} + b\\)\nSpectral norm: \\(f(X) = \\|X\\|_2 = \\sigma_{\\max}(X)\\)"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#restriction-of-a-convex-function-to-a-line",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#restriction-of-a-convex-function-to-a-line",
    "title": "EE 364A (Convex Optimization): Lecture 3 - Convex Functions",
    "section": "Restriction of a Convex Function to a Line",
    "text": "Restriction of a Convex Function to a Line\n\\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is convex if and only if the function \\(g: \\mathbb{R} \\rightarrow \\mathbb{R}\\):\n\\[\ng(t) = f(x + tv), \\quad \\text{dom } g = \\{t \\mid x + tv \\in \\text{dom } f\\}\n\\]\nis convex for any \\(x \\in \\text{dom } f\\), \\(v \\in \\mathbb{R}^n\\).\nThis allows checking convexity of \\(f\\) by checking convexity of functions of one variable.\n\nExample\n\\(f: S^n \\rightarrow \\mathbb{R}\\) with \\(f(X) = \\log \\det X\\), \\(\\text{dom } f = S_{++}^n\\)\n\\[\ng(t) = \\log \\det(X + tV) = \\log \\det X + \\log \\det(I + tX^{-1/2}VX^{-1/2}) = \\log \\det X + \\sum_{i=1}^n \\log(1 + t\\lambda_i)\n\\]\nwhere \\(\\lambda_i\\) are the eigenvalues of \\(X^{-1/2}VX^{-1/2}\\). Since \\(g\\) is concave, \\(f\\) is concave."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#extended-value-extension",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#extended-value-extension",
    "title": "EE 364A (Convex Optimization): Lecture 3 - Convex Functions",
    "section": "Extended Value Extension",
    "text": "Extended Value Extension\n\\[\n\\tilde{f}(x) = \\begin{cases} f(x) & x \\in \\text{dom } f \\\\ +\\infty & x \\notin \\text{dom } f \\end{cases}\n\\]\nThis extends the output domain of \\(f\\) to \\(\\mathbb{R} \\cup \\{+\\infty\\}\\).\nConvexity of \\(\\tilde{f}(\\theta x + (1-\\theta)y) \\leq \\theta \\tilde{f}(x) + (1-\\theta)\\tilde{f}(y)\\) still holds because:\n\nFor \\(x, y \\in \\text{dom } f\\): \\(f(x)\\) is convex, and \\(\\theta x + (1-\\theta)y \\in \\text{dom } f\\) (domain of convex function is convex)\nFor \\(x \\notin \\text{dom } f\\) or \\(y \\notin \\text{dom } f\\): \\(\\tilde{f}(\\theta x + (1-\\theta)y) \\leq +\\infty\\)\n\n\n\n\nExtended value extension"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#first-order-condition",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#first-order-condition",
    "title": "EE 364A (Convex Optimization): Lecture 3 - Convex Functions",
    "section": "First-Order Condition",
    "text": "First-Order Condition\n\\(f\\) is differentiable if \\(\\text{dom } f\\) is open and the gradient exists at each \\(x \\in \\text{dom } f\\):\n\\[\n\\nabla f(x) = \\left(\\frac{\\partial f(x)}{\\partial x_1}, \\frac{\\partial f(x)}{\\partial x_2}, \\ldots, \\frac{\\partial f(x)}{\\partial x_n}\\right)\n\\]\nFirst-order condition: Differentiable \\(f\\) with convex domain is convex if and only if:\n\\[\nf(y) \\geq f(x) + \\nabla f(x)^\\top (y - x) \\quad \\forall x, y \\in \\text{dom } f\n\\]\n\n\n\nFirst-order condition: tangent always underestimates\n\n\nKey insight: For a convex function, the tangent (first-order linear approximation) always underestimates the function."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#second-order-conditions",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#second-order-conditions",
    "title": "EE 364A (Convex Optimization): Lecture 3 - Convex Functions",
    "section": "Second-Order Conditions",
    "text": "Second-Order Conditions\n\\(f\\) is twice differentiable if \\(\\text{dom } f\\) is open and the Hessian \\(\\nabla^2 f(x) \\in S^n\\) exists at each \\(x \\in \\text{dom } f\\):\n\\[\n\\nabla^2 f(x)_{ij} = \\frac{\\partial^2 f(x)}{\\partial x_i \\partial x_j}, \\quad i, j = 1, \\ldots, n\n\\]\nSecond-order conditions: For twice differentiable \\(f\\) with convex domain:\n\n\\(f\\) is convex if and only if \\(\\nabla^2 f(x) \\succeq 0\\) for all \\(x \\in \\text{dom } f\\)\nIf \\(\\nabla^2 f(x) \\succ 0\\), then \\(f\\) is strictly convex\n\n\n\n\nSecond-order condition visualization\n\n\n\nExamples\nQuadratic function: \\(f(x) = \\frac{1}{2}x^\\top P x + q^\\top x + r\\) (with \\(P \\in S^n\\))\n\\[\n\\nabla f(x) = Px + q, \\quad \\nabla^2 f(x) = P\n\\]\nIf \\(P \\succeq 0\\), then \\(f(x)\\) is convex.\nLeast-squares objective: \\(f(x) = \\|Ax - b\\|_2^2\\)\n\\[\n\\nabla f(x) = 2A^\\top(Ax - b), \\quad \\nabla^2 f(x) = 2A^\\top A\n\\]\nConvex for any \\(A\\), because \\(A^\\top A\\) is always positive semidefinite.\nQuadratic-over-linear: \\(f(x, y) = x^2/y\\) is convex if \\(y &gt; 0\\)\n\\[\n\\nabla^2 f(x, y) = \\frac{2}{y^3} \\begin{bmatrix} y \\\\ -x \\end{bmatrix} \\begin{bmatrix} y & -x \\end{bmatrix} \\succeq 0\n\\]\nLog-sum-exp: \\(f(x) = \\log \\sum_{k=1}^n \\exp x_k\\) is convex\nLet \\(T = \\sum_{k=1}^n \\exp x_k\\) and \\(z_k = \\exp x_k\\):\n\\[\nH = \\nabla^2 f(x) = \\frac{1}{T}\\text{diag}(z) - \\frac{1}{T^2}zz^\\top\n\\]\nTo verify H is PSD, check \\(v^\\top H v \\geq 0\\) for all \\(v\\):\n\\[\nv^\\top \\nabla^2 f(x) v = \\frac{\\sum_k z_k v_k^2}{\\sum_k z_k} - \\frac{(\\sum_k v_k z_k)^2}{(\\sum_k z_k)^2}\n\\]\nBy Cauchy-Schwarz inequality: \\((\\sum_k v_k z_k)^2 \\leq (\\sum_k z_k v_k^2)(\\sum_k z_k)\\), so \\(v^\\top \\nabla^2 f(x) v \\geq 0\\).\nGeometric mean: \\(f(x) = \\sqrt[n]{\\prod_{k=1}^n x_k}\\) on \\(\\mathbb{R}_{++}^n\\) is concave."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#epigraph-and-sub-level-sets",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#epigraph-and-sub-level-sets",
    "title": "EE 364A (Convex Optimization): Lecture 3 - Convex Functions",
    "section": "Epigraph and Sub-level Sets",
    "text": "Epigraph and Sub-level Sets\n\nSub-level Sets\n\\(\\alpha\\)-sublevel set of \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\):\n\\[\nC_\\alpha = \\{x \\in \\text{dom } f \\mid f(x) \\leq \\alpha\\}\n\\]\nSub-level sets of convex functions are convex (converse is false).\n\n\nEpigraph\nEpigraph of \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\):\n\\[\n\\text{epi } f = \\{(x, t) \\in \\mathbb{R}^{n+1} \\mid f(x) \\leq t, \\; x \\in \\text{dom } f\\}\n\\]\n\\(f\\) is convex if and only if \\(\\text{epi } f\\) is a convex set.\n\n\n\nEpigraph of a convex function"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#jensens-inequality",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#jensens-inequality",
    "title": "EE 364A (Convex Optimization): Lecture 3 - Convex Functions",
    "section": "Jensen‚Äôs Inequality",
    "text": "Jensen‚Äôs Inequality\nIf \\(f\\) is convex:\n\\[\nf(\\mathbb{E}[z]) \\leq \\mathbb{E}[f(z)]\n\\]\nfor any random variable \\(z\\).\nThe function of the expectation is less than or equal to the expectation of the function.\nThe basic convexity inequality is a special case with discrete distribution:\n\n\\(\\text{prob}(z = x) = \\theta\\)\n\\(\\text{prob}(z = y) = 1 - \\theta\\)\n\n\n\n\nJensen‚Äôs inequality visualization"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#operations-that-preserve-convexity",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#operations-that-preserve-convexity",
    "title": "EE 364A (Convex Optimization): Lecture 3 - Convex Functions",
    "section": "Operations that Preserve Convexity",
    "text": "Operations that Preserve Convexity\nWays to establish convexity:\n\nVerify definition directly\nFor twice differentiable functions, show \\(\\nabla^2 f(x) \\succeq 0\\)\nCombine simple convex functions using operations that preserve convexity:\n\nNonnegative weighted sum: \\(f_1 + f_2\\) is convex if \\(f_1, f_2\\) are convex\nNonnegative multiplication: \\(\\alpha f\\) is convex if \\(f\\) is convex and \\(\\alpha &gt; 0\\)\nComposition with affine function: \\(f(Ax + b)\\) is convex if \\(f\\) is convex\nPointwise maximum and supremum\nComposition\nMinimization\nPerspective\n\nExamples\nLog barrier for linear inequalities: \\[\nf(x) = -\\sum_{i=1}^m \\log(b_i - a_i^\\top x), \\quad \\text{dom } f = \\{x \\mid a_i^\\top x &lt; b_i, \\; i = 1, \\ldots, m\\}\n\\]\nNorm of affine function: \\[\nf(x) = \\|Ax + b\\|\n\\]"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#key-insights",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#key-insights",
    "title": "EE 364A (Convex Optimization): Lecture 3 - Convex Functions",
    "section": "Key Insights",
    "text": "Key Insights\nConvex functions are fundamental to optimization. The first-order condition shows that local information (gradient) provides a global lower bound. The second-order condition connects convexity to positive semidefiniteness of the Hessian. Jensen‚Äôs inequality generalizes the basic convexity definition to expectations, with applications throughout probability and machine learning. Operations preserving convexity allow building complex convex functions from simpler ones."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strang‚Äôs fourth lecture introduces one of the most important matrix factorizations: LU decomposition, which factors any invertible matrix \\(A\\) into the product of a Lower triangular matrix and an Upper triangular matrix. This factorization is the foundation of efficient numerical linear algebra."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#context",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#context",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strang‚Äôs fourth lecture introduces one of the most important matrix factorizations: LU decomposition, which factors any invertible matrix \\(A\\) into the product of a Lower triangular matrix and an Upper triangular matrix. This factorization is the foundation of efficient numerical linear algebra."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#what-is-lu-decomposition",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#what-is-lu-decomposition",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "What is LU Decomposition?",
    "text": "What is LU Decomposition?\nGoal: Factor any invertible matrix \\(A\\) as the product of: - \\(L\\) = Lower triangular matrix (with 1‚Äôs on diagonal) - \\(U\\) = Upper triangular matrix (the result of elimination)\n\\[\nA = LU\n\\]\n\nWhy is this useful?\n\nEfficient solving: \\(Ax = b\\) becomes two simpler triangular solves:\nStep 1 - Forward substitution: Solve \\(Lc = b\\) for \\(c\\)\nStep 2 - Back substitution: Solve \\(Ux = c\\) for \\(x\\)\nHow this works:\nSince \\(A = LU\\), we have \\(Ax = LUx = b\\). Let \\(Ux = c\\), then: \\[\nLUx = Lc = b\n\\]\nForward substitution (solving \\(Lc = b\\)):\nSince \\(L\\) is lower triangular with 1‚Äôs on the diagonal, we can solve for \\(c\\) step by step: \\[\n\\begin{aligned}\nc_1 &= b_1 \\\\\nc_2 &= b_2 - m_{21}c_1 \\\\\nc_3 &= b_3 - m_{31}c_1 - m_{32}c_2 \\\\\n&\\vdots\n\\end{aligned}\n\\]\nEach \\(c_i\\) depends only on previously computed values, so we solve forward from \\(c_1\\) to \\(c_n\\).\nBack substitution (solving \\(Ux = c\\)):\nSince \\(U\\) is upper triangular, we solve backward from \\(x_n\\) to \\(x_1\\): \\[\n\\begin{aligned}\nx_n &= \\frac{c_n}{u_{nn}} \\\\\nx_{n-1} &= \\frac{c_{n-1} - u_{n-1,n}x_n}{u_{n-1,n-1}} \\\\\n&\\vdots\n\\end{aligned}\n\\]\nResult: We‚Äôve solved \\(Ax = b\\) without ever explicitly computing \\(A^{-1}\\)!\nReusable factorization: When \\(A\\) is fixed but \\(b\\) changes, we can reuse \\(L\\) and \\(U\\)\n\nFactorization: \\(O(n^3)\\) operations (done once)\nEach solve: \\(O(n^2)\\) operations\nHuge savings for multiple right-hand sides!\n\nFoundation of numerical computing: Used in MATLAB, NumPy, and all scientific computing libraries"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#how-elimination-creates-u",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#how-elimination-creates-u",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "How Elimination Creates U",
    "text": "How Elimination Creates U\n\nThe Elimination Process\nStarting with \\(A\\), we apply elimination matrices \\(E_{21}, E_{31}, E_{32}, \\ldots\\) to get upper triangular \\(U\\):\n\\[\nE_{32} E_{31} E_{21} A = U\n\\]\nExample (3√ó3 case):\n\\[\nA = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\n\\]\nStep 1: Eliminate below first pivot (rows 2 and 3)\n\\[\nE_{21} = \\begin{bmatrix} 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad\nE_{31} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix}\n\\]\nStep 2: Eliminate below second pivot (row 3)\n\\[\nE_{32} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -1 & 1 \\end{bmatrix}\n\\]\n\n\nStructure of Elimination Matrices\nAn elimination matrix \\(E_{ij}\\) eliminates the entry at position \\((i,j)\\) by subtracting a multiple of row \\(j\\) from row \\(i\\).\nGeneral form: \\[\nE_{ij} = I - m_{ij} \\mathbf{e}_i \\mathbf{e}_j^T\n\\]\nwhere: - \\(m_{ij}\\) = multiplier = \\(\\frac{A_{ij}}{\\text{pivot at } (j,j)}\\) - \\(\\mathbf{e}_i\\) = \\(i\\)-th standard basis vector - The \\((i,j)\\) entry of \\(E_{ij}\\) is \\(-m_{ij}\\)\nKey properties: 1. Lower triangular (operates below diagonal) 2. Determinant = 1 (doesn‚Äôt change volume) 3. Easy to invert: \\(E_{ij}^{-1} = I + m_{ij} \\mathbf{e}_i \\mathbf{e}_j^T\\) (just flip the sign!)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#inverting-to-get-l-the-key-insight",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#inverting-to-get-l-the-key-insight",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Inverting to Get L: The Key Insight",
    "text": "Inverting to Get L: The Key Insight\nFrom elimination, we have:\n\\[\nE_{32} E_{31} E_{21} A = U\n\\]\nMultiply both sides by the inverses (in reverse order):\n\\[\nA = E_{21}^{-1} E_{31}^{-1} E_{32}^{-1} U = LU\n\\]\nwhere: \\[\nL = E_{21}^{-1} E_{31}^{-1} E_{32}^{-1}\n\\]\n\nThe Beautiful Result\nWhen elimination matrices are multiplied in the right order, their inverses combine beautifully:\n\\[\nL = \\begin{bmatrix}\n1 & 0 & 0 & \\cdots \\\\\nm_{21} & 1 & 0 & \\cdots \\\\\nm_{31} & m_{32} & 1 & \\cdots \\\\\n\\vdots & \\vdots & \\ddots & \\ddots\n\\end{bmatrix}\n\\]\nThe multipliers \\(m_{ij}\\) (used during elimination) directly fill in the entries of \\(L\\) below the diagonal!\nNo extra computation needed ‚Äî just save the multipliers as you eliminate."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#computational-complexity",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#computational-complexity",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Computational Complexity",
    "text": "Computational Complexity\n\nOperation Counts\nFor an \\(n \\times n\\) matrix:\n\n\n\nStep\nOperations\nOrder\n\n\n\n\nElimination (find U)\n\\(\\frac{n^3}{3} + O(n^2)\\)\n\\(O(n^3)\\)\n\n\nForward substitution \\((Lc = b)\\)\n\\(\\frac{n^2}{2}\\)\n\\(O(n^2)\\)\n\n\nBack substitution \\((Ux = c)\\)\n\\(\\frac{n^2}{2}\\)\n\\(O(n^2)\\)\n\n\n\n\n\nWhy \\(\\frac{n^3}{3}\\)?\nAt step \\(k\\), we update an \\((n-k) \\times (n-k)\\) submatrix:\n\\[\n\\text{Total operations} = \\sum_{k=1}^{n-1} (n-k)^2 \\approx \\int_0^n x^2 \\, dx = \\frac{n^3}{3}\n\\]\n\n\nWhen is LU Worth It?\nSingle solve: \\(Ax = b\\) costs \\(O(n^3)\\) either way\nMultiple solves: If solving \\(Ax = b_1, Ax = b_2, \\ldots, Ax = b_m\\): - Without LU: \\(m \\times O(n^3)\\) - With LU: \\(O(n^3)\\) (once) + \\(m \\times O(n^2)\\) ‚úÖ\nHuge savings when \\(A\\) is fixed but \\(b\\) changes!"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#hands-on-exercises",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#hands-on-exercises",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Hands-On Exercises",
    "text": "Hands-On Exercises\nLet‚Äôs practice LU decomposition with concrete examples.\n\n\nShow code\nimport numpy as np\n\nprint(\"‚úì Libraries imported successfully\")\n\n\n‚úì Libraries imported successfully\n\n\n\nExercise 1: Manual LU Decomposition (2√ó2)\nCompute the LU decomposition of \\(A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}\\) by hand.\nSteps: 1. Perform elimination to get \\(U\\) 2. Record the multiplier \\(m_{21}\\) to build \\(L\\) 3. Verify \\(A = LU\\)\n\n\nShow code\nfrom IPython.display import display, Markdown, Latex\n\n# Original matrix\nA = np.array([[2, 3],\n              [4, 7]])\n\ndisplay(Markdown(\"**Original matrix A:**\"))\ndisplay(Latex(r\"$$A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}$$\"))\n\n# Compute multiplier m21\nm21 = 4/2  # row2[0] / row1[0]\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 1: Compute multiplier**\"))\ndisplay(Latex(f\"$$m_{{21}} = \\\\frac{{4}}{{2}} = {m21}$$\"))\n\n# Build L matrix\nL = np.array([[1, 0],\n              [m21, 1]])\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 2: Build L matrix**\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 \\\\ m_{21} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}$$\"))\n\n# Build U matrix (result after elimination)\n# After: row2 = row2 - m21*row1\n# [2, 3]        [2, 3]\n# [4, 7]  --&gt;   [0, 1]  (because 7 - 2*3 = 1)\nU = np.array([[2, 3],\n              [0, 1]])\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 3: Build U matrix (after elimination)**\"))\ndisplay(Markdown(\"Row 2 ‚Üí Row 2 - 2 √ó Row 1\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Verification: $A = LU$**\"))\ndisplay(Latex(r\"$$LU = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix} = A \\quad \\checkmark$$\"))\n\n\nOriginal matrix A:\n\n\n\\[A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}\\]\n\n\n\n\n\nStep 1: Compute multiplier\n\n\n\\[m_{21} = \\frac{4}{2} = 2.0\\]\n\n\n\n\n\nStep 2: Build L matrix\n\n\n\\[L = \\begin{bmatrix} 1 & 0 \\\\ m_{21} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nStep 3: Build U matrix (after elimination)\n\n\nRow 2 ‚Üí Row 2 - 2 √ó Row 1\n\n\n\\[U = \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(A = LU\\)\n\n\n\\[LU = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix} = A \\quad \\checkmark\\]\n\n\nKey observation: The multiplier \\(m_{21} = 2\\) goes directly into position \\((2,1)\\) of \\(L\\)!\n\n\nExercise 2: LU Decomposition (3√ó3)\nPerform LU decomposition on:\n\\[\nA = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\n\\]\nGoal: Find \\(L\\) and \\(U\\) such that \\(A = LU\\)\n\n\nShow code\nfrom IPython.display import display, Markdown, Latex\n\nA = np.array([[2, 1, 1],\n              [4, -6, 0],\n              [-2, 7, 2]])\n\ndisplay(Markdown(\"**Original matrix A:**\"))\ndisplay(Latex(r\"$$A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Step 1: Eliminate column 1\"))\n\n# Calculate multipliers for column 1\nm21 = A[1, 0] / A[0, 0]  # 4/2 = 2\nm31 = A[2, 0] / A[0, 0]  # -2/2 = -1\n\ndisplay(Markdown(\"**Multipliers:**\"))\ndisplay(Latex(f\"$$m_{{21}} = \\\\frac{{4}}{{2}} = {m21}, \\\\quad m_{{31}} = \\\\frac{{-2}}{{2}} = {m31}$$\"))\n\n# Create A1 after first elimination\nA1 = A.copy().astype(float)\nA1[1] = A1[1] - m21 * A1[0]  # row2 - 2*row1\nA1[2] = A1[2] - m31 * A1[0]  # row3 - (-1)*row1\n\ndisplay(Markdown(\"**After eliminating column 1:**\"))\ndisplay(Markdown(\"- Row 2 ‚Üí Row 2 - 2 √ó Row 1\"))\ndisplay(Markdown(\"- Row 3 ‚Üí Row 3 - (-1) √ó Row 1\"))\ndisplay(Latex(r\"$$A^{(1)} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 8 & 3 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Step 2: Eliminate column 2\"))\n\n# Calculate multiplier for column 2\nm32 = A1[2, 1] / A1[1, 1]  # 8/(-8) = -1\n\ndisplay(Markdown(\"**Multiplier:**\"))\ndisplay(Latex(f\"$$m_{{32}} = \\\\frac{{8}}{{-8}} = {m32}$$\"))\n\n# Create U (final upper triangular)\nU = A1.copy()\nU[2] = U[2] - m32 * U[1]  # row3 - (-1)*row2\n\ndisplay(Markdown(\"**After eliminating column 2:**\"))\ndisplay(Markdown(\"- Row 3 ‚Üí Row 3 - (-1) √ó Row 2\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Build L from multipliers\"))\n\n# Build L from multipliers\nL = np.array([[1, 0, 0],\n              [m21, 1, 0],\n              [m31, m32, 1]])\n\ndisplay(Markdown(\"The multipliers directly fill in $L$:\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 & 0 \\\\ m_{21} & 1 & 0 \\\\ m_{31} & m_{32} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Verification: $A = LU$\"))\n\ndisplay(Latex(r\"$$LU = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix} = A \\quad \\checkmark$$\"))\n\n\nOriginal matrix A:\n\n\n\\[A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\\]\n\n\n\n\n\nStep 1: Eliminate column 1\n\n\nMultipliers:\n\n\n\\[m_{21} = \\frac{4}{2} = 2.0, \\quad m_{31} = \\frac{-2}{2} = -1.0\\]\n\n\nAfter eliminating column 1:\n\n\n\nRow 2 ‚Üí Row 2 - 2 √ó Row 1\n\n\n\n\nRow 3 ‚Üí Row 3 - (-1) √ó Row 1\n\n\n\n\\[A^{(1)} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 8 & 3 \\end{bmatrix}\\]\n\n\n\n\n\nStep 2: Eliminate column 2\n\n\nMultiplier:\n\n\n\\[m_{32} = \\frac{8}{-8} = -1.0\\]\n\n\nAfter eliminating column 2:\n\n\n\nRow 3 ‚Üí Row 3 - (-1) √ó Row 2\n\n\n\n\\[U = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nBuild L from multipliers\n\n\nThe multipliers directly fill in \\(L\\):\n\n\n\\[L = \\begin{bmatrix} 1 & 0 & 0 \\\\ m_{21} & 1 & 0 \\\\ m_{31} & m_{32} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(A = LU\\)\n\n\n\\[LU = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix} = A \\quad \\checkmark\\]\n\n\nKey observation: All three multipliers \\((m_{21}, m_{31}, m_{32})\\) go directly into their corresponding positions in \\(L\\):\n\\[\nL = \\begin{bmatrix}\n1 & 0 & 0 \\\\\nm_{21} & 1 & 0 \\\\\nm_{31} & m_{32} & 1\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n-1 & -1 & 1\n\\end{bmatrix}\n\\]\n\nNote: In practice, numerical libraries like SciPy provide scipy.linalg.lu() which computes LU decomposition efficiently and includes automatic row permutation (pivoting) for numerical stability.\n\n\nShow code\nfrom scipy.linalg import lu\nfrom IPython.display import display, Markdown, Latex\n\nA = np.array([[2, 1, 1],\n              [4, -6, 0],\n              [-2, 7, 2]], dtype=float)\n\n# SciPy returns P, L, U where PA = LU (P is permutation matrix)\nP, L_scipy, U_scipy = lu(A)\n\ndisplay(Markdown(\"**SciPy's LU decomposition:**\"))\ndisplay(Markdown(\"SciPy returns $P$, $L$, $U$ where $PA = LU$ ($P$ is a permutation matrix)\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Permutation matrix P:**\"))\ndisplay(Latex(r\"$$P = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\ndisplay(Markdown(\"(This swaps rows 1 and 2 for numerical stability)\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Lower triangular L:**\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0.5 & 1 & 0 \\\\ -0.5 & 1 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Upper triangular U:**\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 4 & -6 & 0 \\\\ 0 & 4 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Verification: $PA = LU$**\"))\n\n# Note: If P = I (identity), then our manual L and U should match\nif np.allclose(P, np.eye(3)):\n    display(Markdown(\"‚úì No row swaps needed! Our manual $L$ and $U$ match SciPy.\"))\nelse:\n    display(Markdown(\"‚ö† **Row swaps were performed** (pivot strategy for numerical stability).\"))\n    display(Markdown(\"SciPy chose the largest pivot to minimize rounding errors.\"))\n    display(Markdown(\"Our manual decomposition is valid but uses a different pivot order.\"))\n\n\nSciPy‚Äôs LU decomposition:\n\n\nSciPy returns \\(P\\), \\(L\\), \\(U\\) where \\(PA = LU\\) (\\(P\\) is a permutation matrix)\n\n\n\n\n\nPermutation matrix P:\n\n\n\\[P = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n(This swaps rows 1 and 2 for numerical stability)\n\n\n\n\n\nLower triangular L:\n\n\n\\[L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0.5 & 1 & 0 \\\\ -0.5 & 1 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nUpper triangular U:\n\n\n\\[U = \\begin{bmatrix} 4 & -6 & 0 \\\\ 0 & 4 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(PA = LU\\)\n\n\n‚ö† Row swaps were performed (pivot strategy for numerical stability).\n\n\nSciPy chose the largest pivot to minimize rounding errors.\n\n\nOur manual decomposition is valid but uses a different pivot order."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html",
    "href": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html",
    "title": "MIT 18.06SC Lecture 8: Solving Ax=b - Complete Solution to Linear Systems",
    "section": "",
    "text": "My lecture notes\nThis lecture presents the complete solution structure for \\(Ax = b\\) and classifies all linear systems into four cases based on rank: exactly determined (unique solution), overdetermined (0 or 1 solution), underdetermined (infinite solutions), and rank deficient (0 or infinite solutions)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#context",
    "href": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#context",
    "title": "MIT 18.06SC Lecture 8: Solving Ax=b - Complete Solution to Linear Systems",
    "section": "",
    "text": "My lecture notes\nThis lecture presents the complete solution structure for \\(Ax = b\\) and classifies all linear systems into four cases based on rank: exactly determined (unique solution), overdetermined (0 or 1 solution), underdetermined (infinite solutions), and rank deficient (0 or infinite solutions)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#complete-solution-structure",
    "href": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#complete-solution-structure",
    "title": "MIT 18.06SC Lecture 8: Solving Ax=b - Complete Solution to Linear Systems",
    "section": "Complete Solution Structure",
    "text": "Complete Solution Structure\nThe general solution to \\(Ax = b\\) (when it exists) has the form: \\[\nx = x_p + x_n\n\\]\nwhere: - \\(x_p\\) is a particular solution: any specific solution satisfying \\(Ax_p = b\\) - \\(x_n\\) is any vector from the null space: \\(Ax_n = 0\\)\n\nWhy This Works\n\\[\n\\begin{aligned}\nAx_p &= b \\\\\nAx_n &= 0 \\\\\n\\hline\nA(x_p + x_n) &= Ax_p + Ax_n = b + 0 = b\n\\end{aligned}\n\\]\nKey insight: The complete solution is the particular solution plus the entire null space."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#finding-a-particular-solution",
    "href": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#finding-a-particular-solution",
    "title": "MIT 18.06SC Lecture 8: Solving Ax=b - Complete Solution to Linear Systems",
    "section": "Finding a Particular Solution",
    "text": "Finding a Particular Solution\n\nAlgorithm\n\nRow reduce \\([A \\mid b]\\) to reduced row echelon form\nSet all free variables to 0\nSolve for the pivot variables from the reduced equations\nThis gives \\(x_p\\)\n\nIf row reduction produces a row like \\([0 \\, 0 \\, \\cdots \\, 0 \\mid c]\\) where \\(c \\neq 0\\), then no solution exists."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#rank-and-solution-existence",
    "href": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#rank-and-solution-existence",
    "title": "MIT 18.06SC Lecture 8: Solving Ax=b - Complete Solution to Linear Systems",
    "section": "Rank and Solution Existence",
    "text": "Rank and Solution Existence\nFor an \\(m \\times n\\) matrix \\(A\\) with rank \\(r\\):\n\\[\nr \\leq \\min(m, n)\n\\]\nThis is because: - \\(r \\leq m\\) (rank cannot exceed number of rows) - \\(r \\leq n\\) (rank cannot exceed number of columns)\n\nSolvability Condition\nThe system \\(Ax = b\\) has a solution if and only if \\(b\\) is in the column space of \\(A\\): \\[\nb \\in C(A)\n\\]\nEquivalently, the system is solvable when \\(\\text{rank}(A) = \\text{rank}([A \\mid b])\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#four-cases-based-on-rank",
    "href": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#four-cases-based-on-rank",
    "title": "MIT 18.06SC Lecture 8: Solving Ax=b - Complete Solution to Linear Systems",
    "section": "Four Cases Based on Rank",
    "text": "Four Cases Based on Rank\n\nCase 1: Exactly Determined (Square, Full Rank)\nConditions: \\(r = n = m\\) (square matrix with full rank)\nProperties: - Matrix is invertible - RREF: \\(R = I_n\\) (identity matrix) - No free variables - Null space: \\(N(A) = \\{\\vec{0}\\}\\) (only zero vector)\nSolutions: - Unique solution for every \\(b\\): \\(x = A^{-1}b\\) - Number of solutions = 1 (always)\nExample: \\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad r = 2 = m = n\n\\]\n\n\nCase 2: Overdetermined (Tall, Full Column Rank)\nConditions: \\(m &gt; n\\) and \\(r = n\\) (more equations than unknowns)\nProperties: - Matrix is tall and skinny - RREF: \\(R = \\begin{bmatrix} I_n \\\\ 0 \\end{bmatrix}\\) (identity on top, zeros below) - No free variables (\\(n - r = 0\\)) - Null space: \\(N(A) = \\{\\vec{0}\\}\\) (only zero vector)\nSolutions: - 0 or 1 solution (depends on whether \\(b \\in C(A)\\)) - If solution exists, it is unique (no free variables) - Most \\(b\\) vectors have no solution (overconstrained system)\nExample: \\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}, \\quad r = 2, \\, m = 3, \\, n = 2\n\\]\nInterpretation: More constraints than degrees of freedom; typically no exact solution (leads to least squares in practice).\n\n\nCase 3: Underdetermined (Wide, Full Row Rank)\nConditions: \\(m &lt; n\\) and \\(r = m\\) (fewer equations than unknowns)\nProperties: - Matrix is short and wide - RREF: \\(R = [I_m \\mid F]\\) (identity on left, free columns on right) - Has free variables (\\(n - r = n - m &gt; 0\\)) - Non-trivial null space: \\(\\dim(N(A)) = n - m\\)\nSolutions: - Infinitely many solutions for every \\(b\\) (when \\(r = m\\)) - Solution exists because \\(C(A) = \\mathbb{R}^m\\) (full row rank) - Complete solution: \\(x = x_p + x_n\\) where \\(x_n\\) ranges over \\((n-m)\\)-dimensional null space\nExample: \\[\nA = \\begin{bmatrix} 1 & 2 & 3 & 4 \\\\ 0 & 0 & 1 & 2 \\end{bmatrix}, \\quad r = 2, \\, m = 2, \\, n = 4\n\\]\nInterpretation: More degrees of freedom than constraints; infinitely many ways to satisfy the equations.\n\n\nCase 4: Rank Deficient (Neither Full Column nor Full Row Rank)\nConditions: \\(r &lt; m\\) and \\(r &lt; n\\) (rank deficient)\nProperties: - Has free variables: \\(n - r &gt; 0\\) - Column space is proper subspace: \\(C(A) \\subsetneq \\mathbb{R}^m\\)\nSolutions: - 0 or infinitely many solutions - If \\(b \\in C(A)\\): infinitely many solutions (due to non-trivial null space) - If \\(b \\notin C(A)\\): no solution\nExample: \\[\nA = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\\\ 0 & 0 & 0 \\end{bmatrix}, \\quad r = 1, \\, m = 3, \\, n = 3\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#summary-table",
    "href": "Math/MIT18.06/mit1806-lecture8-solving-ax-b.html#summary-table",
    "title": "MIT 18.06SC Lecture 8: Solving Ax=b - Complete Solution to Linear Systems",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\n\n\nCase\nRank Condition\nMatrix Shape\n# Free Vars\n# Solutions\n\n\n\n\nExactly determined\n\\(r = n = m\\)\nSquare, invertible\n0\n1 (always)\n\n\nOverdetermined\n\\(r = n &lt; m\\)\nTall, full column rank\n0\n0 or 1\n\n\nUnderdetermined\n\\(r = m &lt; n\\)\nWide, full row rank\n\\(n - m\\)\n\\(\\infty\\) (always)\n\n\nRank deficient\n\\(r &lt; \\min(m,n)\\)\nGeneral\n\\(n - r &gt; 0\\)\n0 or \\(\\infty\\)\n\n\n\nKey principle: - Number of free variables = \\(n - r\\) - If \\(r = n\\): 0 or 1 solution (unique if exists) - If \\(r &lt; n\\): 0 or \\(\\infty\\) solutions (null space is non-trivial)\n\nSource: MIT 18.06SC Linear Algebra, Lecture 8"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "This is for MIT 18.06SC Lecture 1, covering how to understand linear systems from two perspectives: geometry (row picture) and algebra (column picture)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#the-example-system",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#the-example-system",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "The Example System",
    "text": "The Example System\nLet‚Äôs work with this concrete example:\n\\[\\begin{align}\nx + 2y &= 5 \\\\\n3x + 4y &= 6\n\\end{align}\\]\nIn matrix form: \\[\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nWe can interpret this system in two completely different ways."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#row-picture-geometry",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#row-picture-geometry",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Row Picture (Geometry)",
    "text": "Row Picture (Geometry)\nIn the row picture, each equation represents a geometric object: - In 2D: each equation is a line - In 3D: each equation is a plane\n- In higher dimensions: each equation is a hyperplane\nThe solution is where all these objects intersect.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the equations in the form y = mx + c\n# Line 1: x + 2y = 5  =&gt;  y = -1/2*x + 5/2\n# Line 2: 3x + 4y = 6  =&gt;  y = -3/4*x + 3/2\nx = np.linspace(-10, 10, 100)\ny1 = -1/2 * x + 5/2\ny2 = -3/4 * x + 3/2\n\n# Solve for intersection point\nA = np.array([[1, 2], [3, 4]])\nb = np.array([5, 6])\nsolution = np.linalg.solve(A, b)\n\n# Plot both lines and intersection\nplt.figure(figsize=(8, 6))\nplt.plot(x, y1, 'b-', label='Line 1: x + 2y = 5', linewidth=2)\nplt.plot(x, y2, 'r-', label='Line 2: 3x + 4y = 6', linewidth=2)\nplt.scatter(solution[0], solution[1], color='green', s=100, zorder=5, \n           label=f'Solution: ({solution[0]:.1f}, {solution[1]:.1f})', edgecolor='white', linewidth=2)\n\nplt.xlim(-8, 8)\nplt.ylim(-1, 8)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Row Picture: Where Lines Meet')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Solution: x = {solution[0]:.3f}, y = {solution[1]:.3f}\")\nprint(f\"Verification: {A @ solution} equals {b}\")\n\n\n\n\n\n\n\n\n\nSolution: x = -4.000, y = 4.500\nVerification: [5. 6.] equals [5 6]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#column-picture-algebra",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#column-picture-algebra",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Column Picture (Algebra)",
    "text": "Column Picture (Algebra)\nThe column picture reframes the same system as a question about vector combinations:\n\\[x \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} + y \\begin{bmatrix}2 \\\\ 4\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nInstead of asking ‚Äúwhere do lines intersect?‚Äù, we ask: ‚ÄúCan we combine these vectors to reach our target?‚Äù\n\n\nCode\n# Define column vectors and target vector\na1 = np.array([1, 3])\na2 = np.array([2, 4])\nb = np.array([5, 6])\n\n# Solve for coefficients\nA = np.column_stack([a1, a2])\nsolution = np.linalg.solve(A, b)\nx, y = solution[0], solution[1]\n\nprint(f\"Question: Can we write b as a linear combination of a‚ÇÅ and a‚ÇÇ?\")\nprint(f\"Answer: {x:.3f} √ó a‚ÇÅ + {y:.3f} √ó a‚ÇÇ = b\")\nprint(f\"Verification: {x*a1} + {y*a2} = {x*a1 + y*a2}\")\n\n# Visualize the vector construction\nplt.figure(figsize=(8, 6))\n\n# Step 1: Draw x*a1 (scaled version)\nplt.arrow(0, 0, x*a1[0], x*a1[1], head_width=0.2, head_length=0.2, \n         fc='blue', ec='blue', linewidth=3,\n         label=f'{x:.2f} √ó a‚ÇÅ')\n\n# Step 2: Draw y*a2 starting from the tip of x*a1\nplt.arrow(x*a1[0], x*a1[1], y*a2[0], y*a2[1], head_width=0.2, head_length=0.2, \n         fc='green', ec='green', linewidth=3,\n         label=f'{y:.2f} √ó a‚ÇÇ')\n\n# Show final result vector b\nplt.arrow(0, 0, b[0], b[1], head_width=0.25, head_length=0.25, \n         fc='red', ec='red', linewidth=4, alpha=0.8,\n         label=f'b = [{b[0]}, {b[1]}]')\n\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.xlim(-1, 6)\nplt.ylim(-12, 7)\nplt.xlabel('x-component')\nplt.ylabel('y-component')\nplt.title('Column Picture: Vector Combination')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\n\n\nQuestion: Can we write b as a linear combination of a‚ÇÅ and a‚ÇÇ?\nAnswer: -4.000 √ó a‚ÇÅ + 4.500 √ó a‚ÇÇ = b\nVerification: [ -4. -12.] + [ 9. 18.] = [5. 6.]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Three Types of Linear Systems",
    "text": "Three Types of Linear Systems\nLinear systems can have three possible outcomes:\n\nUnique solution - Lines intersect at one point\nNo solution - Lines are parallel (don‚Äôt intersect)\nInfinitely many solutions - Lines are the same (overlap completely)\n\n\n\nCode\n# Case (a): Unique solution - non-parallel vectors\nprint(\"üéØ Case (a) - Unique Solution:\")\nA_a = np.array([[1, 2], [3, 4]])\nb_a = np.array([5, 6])\nsolution_a = np.linalg.solve(A_a, b_a)\ndet_a = np.linalg.det(A_a)\nprint(f\"   Solution: {solution_a}\")\nprint(f\"   Matrix determinant: {det_a:.3f} ‚â† 0 ‚Üí linearly independent columns\")\nprint(f\"   Column space: ENTIRE 2D plane (any point reachable)\")\n\n# Case (b): No solution - parallel vectors, b not in span\nprint(f\"\\n‚ùå Case (b) - No Solution:\")\nA_b = np.array([[1, 2], [2, 4]])  # Columns are parallel\nb_b = np.array([5, 6])            # b not in span\ndet_b = np.linalg.det(A_b)\nprint(f\"   Matrix determinant: {det_b:.3f} = 0 ‚Üí linearly dependent columns\")\nprint(f\"   Column space: 1D line only (most points unreachable)\")\nprint(f\"   Target b = {b_b} is NOT on the line ‚Üí No solution exists\")\n\n# Case (c): Infinitely many solutions - parallel vectors, b in span\nprint(f\"\\n‚ôæÔ∏è  Case (c) - Infinitely Many Solutions:\")\nA_c = np.array([[1, 2], [2, 4]])  # Same parallel columns\nb_c = np.array([3, 6])            # b = 3 * [1, 2], so b is in span\ndet_c = np.linalg.det(A_c)\nprint(f\"   Matrix determinant: {det_c:.3f} = 0 ‚Üí linearly dependent columns\")\nprint(f\"   Column space: 1D line only\")\nprint(f\"   Target b = {b_c} IS on the line ‚Üí Infinite solutions exist\")\n\n# Find one particular solution using pseudoinverse\nsolution_c = np.linalg.pinv(A_c) @ b_c\nprint(f\"   One particular solution: {solution_c}\")\nprint(f\"   Other solutions: {solution_c} + t√ó[2, -1] for any real number t\")\n\n\nüéØ Case (a) - Unique Solution:\n   Solution: [-4.   4.5]\n   Matrix determinant: -2.000 ‚â† 0 ‚Üí linearly independent columns\n   Column space: ENTIRE 2D plane (any point reachable)\n\n‚ùå Case (b) - No Solution:\n   Matrix determinant: 0.000 = 0 ‚Üí linearly dependent columns\n   Column space: 1D line only (most points unreachable)\n   Target b = [5 6] is NOT on the line ‚Üí No solution exists\n\n‚ôæÔ∏è  Case (c) - Infinitely Many Solutions:\n   Matrix determinant: 0.000 = 0 ‚Üí linearly dependent columns\n   Column space: 1D line only\n   Target b = [3 6] IS on the line ‚Üí Infinite solutions exist\n   One particular solution: [0.6 1.2]\n   Other solutions: [0.6 1.2] + t√ó[2, -1] for any real number t\n\n\n\n\nCode\n# Visualize all three cases\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Case (a): Unique solution\nax = axes[0]\nax.fill_between([-1, 6], [-1, -1], [7, 7], color='lightblue', alpha=0.2, \n                label='Column space = ENTIRE plane')\n\n# Draw vectors\nax.arrow(0, 0, A_a[0,0], A_a[1,0], head_width=0.15, head_length=0.15,\n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,3]')\nax.arrow(0, 0, A_a[0,1], A_a[1,1], head_width=0.15, head_length=0.15,\n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4]')\nax.arrow(0, 0, b_a[0], b_a[1], head_width=0.2, head_length=0.2,\n         fc='red', ec='red', linewidth=3, label='b = [5,6]')\n\nax.set_title('Unique Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (b): No solution\nax = axes[1]\nt = np.linspace(-2, 5, 100)\nspan_x, span_y = t * A_b[0,0], t * A_b[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6, \n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_b[0,0], A_b[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,2]')\nax.arrow(0, 0, A_b[0,1], A_b[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4] = 2√óa‚ÇÅ')\nax.arrow(0, 0, b_b[0], b_b[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [5,6] (off line)')\n\nax.set_title('No Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (c): Infinitely many solutions\nax = axes[2]\nt = np.linspace(-1, 4, 100)\nspan_x, span_y = t * A_c[0,0], t * A_c[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6,\n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_c[0,0], A_c[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,2]')\nax.arrow(0, 0, A_c[0,1], A_c[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4] = 2√óa‚ÇÅ')\nax.arrow(0, 0, b_c[0], b_c[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [3,6] (on line)')\n\nax.set_title('Infinite Solutions')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key insight: Solution depends on whether target vector b lies in the column space\")\n\n\n\n\n\n\n\n\n\nKey insight: Solution depends on whether target vector b lies in the column space\n\n\n\nThis covers the core geometric foundations from MIT 18.06SC Lecture 1: understanding linear systems through both row and column perspectives."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html",
    "href": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html",
    "title": "Lecture 30: Linear Transformations and Their Matrices",
    "section": "",
    "text": "This lecture explores the fundamental connection between linear transformations and matrices:\n\nDefinition and properties of linear transformations\nExamples of linear and non-linear transformations\nHow to represent transformations as matrices using basis\nConstructing the matrix representation from basis vectors\nExamples: projection, rotation, reflection, differentiation"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#overview",
    "href": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#overview",
    "title": "Lecture 30: Linear Transformations and Their Matrices",
    "section": "",
    "text": "This lecture explores the fundamental connection between linear transformations and matrices:\n\nDefinition and properties of linear transformations\nExamples of linear and non-linear transformations\nHow to represent transformations as matrices using basis\nConstructing the matrix representation from basis vectors\nExamples: projection, rotation, reflection, differentiation"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#linear-transformations-definition",
    "href": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#linear-transformations-definition",
    "title": "Lecture 30: Linear Transformations and Their Matrices",
    "section": "1. Linear Transformations: Definition",
    "text": "1. Linear Transformations: Definition\nA transformation \\(T: V \\to W\\) between vector spaces is linear if it satisfies two properties:\n\nLinearity Rules\n\nAdditivity: \\(T(v + w) = T(v) + T(w)\\)\nHomogeneity: \\(T(cv) = c T(v)\\) for any scalar \\(c\\)\n\nCombined form: \\(T(cv + dw) = cT(v) + dT(w)\\)\nThese two properties together ensure that the transformation preserves the vector space structure.\n\n\nImportant Consequence: \\(T(0) = 0\\)\nIn any linear transformation, \\(T(0)\\) must always equal \\(0\\).\nProof: Using homogeneity with \\(c = 0\\):\n\\[\nT(0 \\cdot v) = 0 \\cdot T(v) = 0\n\\]\nSince \\(0 \\cdot v = 0\\) for any vector \\(v\\), we have \\(T(0) = 0\\).\nTest for non-linearity: If \\(T(0) \\neq 0\\), then \\(T\\) is immediately not linear."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#example-1-projection-linear",
    "href": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#example-1-projection-linear",
    "title": "Lecture 30: Linear Transformations and Their Matrices",
    "section": "2. Example 1: Projection (Linear)",
    "text": "2. Example 1: Projection (Linear)\n\nDefinition\n\\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) where \\(T(v)\\) projects vector \\(v\\) onto a line.\n Figure: Projection onto a line - a linear transformation that maps each vector to its closest point on the target line.\n\n\nVerification of Linearity\nAdditivity: The projection of \\(v + w\\) equals the sum of projections:\n\\[\nT(v + w) = T(v) + T(w)\n\\]\nHomogeneity: Scaling a vector scales its projection:\n\\[\nT(cv) = c T(v)\n\\]\nBoth properties are satisfied geometrically: projecting a scaled or summed vector gives the same result as scaling or summing the projections."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#example-2-translationshift-not-linear",
    "href": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#example-2-translationshift-not-linear",
    "title": "Lecture 30: Linear Transformations and Their Matrices",
    "section": "3. Example 2: Translation/Shift (NOT Linear)",
    "text": "3. Example 2: Translation/Shift (NOT Linear)\n\nDefinition\n\\[\nT(v) = v + v_0\n\\]\nwhere \\(v_0\\) is a fixed non-zero vector.\n\n\nWhy Not Linear?\nCheck homogeneity with \\(c = 2\\):\n\\[\nT(2v) = 2v + v_0\n\\]\nBut:\n\\[\n2T(v) = 2(v + v_0) = 2v + 2v_0\n\\]\nSince \\(T(2v) \\neq 2T(v)\\), the transformation is not linear.\nAlternative check: \\(T(0) = 0 + v_0 = v_0 \\neq 0\\), which violates the requirement that \\(T(0) = 0\\).\nKey insight: Any transformation that ‚Äúshifts‚Äù the origin is not linear."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#example-3-lengthnorm-not-linear",
    "href": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#example-3-lengthnorm-not-linear",
    "title": "Lecture 30: Linear Transformations and Their Matrices",
    "section": "4. Example 3: Length/Norm (NOT Linear)",
    "text": "4. Example 3: Length/Norm (NOT Linear)\n\nDefinition\n\\[\nT(v) = \\|v\\|\n\\]\n(the length of vector \\(v\\))\n\n\nWhy Not Linear?\nConsider \\(v \\neq 0\\) and \\(c = -1\\):\n\\[\nT(-v) = \\|-v\\| = \\|v\\| = T(v)\n\\]\nBut:\n\\[\n-T(v) = -\\|v\\| \\neq \\|v\\|\n\\]\nSince \\(T(-v) \\neq -T(v)\\), the transformation is not linear.\nIntuition: Length is always non-negative, so it cannot satisfy \\(T(cv) = cT(v)\\) for negative \\(c\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#example-4-rotation-linear",
    "href": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#example-4-rotation-linear",
    "title": "Lecture 30: Linear Transformations and Their Matrices",
    "section": "5. Example 4: Rotation (Linear)",
    "text": "5. Example 4: Rotation (Linear)\n\nDefinition\n\\(T(v)\\) rotates vector \\(v\\) by a fixed angle (e.g., \\(45¬∞\\)).\n Figure: Rotation by 45¬∞ - a linear transformation that preserves lengths and angles between vectors.\n\n\nVerification of Linearity\nRotation preserves vector addition and scalar multiplication:\n\nAdditivity: Rotating \\(v + w\\) is the same as adding the rotated vectors\nHomogeneity: Rotating \\(cv\\) gives \\(c\\) times the rotated vector\n\nGeometrically, rotation preserves the parallelogram law and scaling."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#example-5-matrix-multiplication-linear",
    "href": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#example-5-matrix-multiplication-linear",
    "title": "Lecture 30: Linear Transformations and Their Matrices",
    "section": "6. Example 5: Matrix Multiplication (Linear)",
    "text": "6. Example 5: Matrix Multiplication (Linear)\n\nDefinition\n\\[\nT(v) = Av\n\\]\nwhere \\(A\\) is a fixed matrix.\n\n\nProof of Linearity\nHomogeneity:\n\\[\nA(cv) = c(Av)\n\\]\n(by properties of matrix multiplication)\nAdditivity:\n\\[\nA(v + w) = Av + Aw\n\\]\n(distributive property)\nTherefore, every matrix multiplication defines a linear transformation.\n\n\nExample: Reflection Across \\(x\\)-axis\n\\[\nA = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}\n\\]\nThis matrix reflects vectors across the \\(x\\)-axis (flips the \\(y\\)-coordinate).\n Figure: Reflection across the x-axis - multiplying by this diagonal matrix keeps x unchanged and flips the sign of y.\nAction:\n\\[\n\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} x \\\\ -y \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#linear-transformations-between-different-dimensions",
    "href": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#linear-transformations-between-different-dimensions",
    "title": "Lecture 30: Linear Transformations and Their Matrices",
    "section": "7. Linear Transformations Between Different Dimensions",
    "text": "7. Linear Transformations Between Different Dimensions\n\nExample: \\(\\mathbb{R}^3 \\to \\mathbb{R}^2\\)\n\\[\nT(v) = Av\n\\]\nwhere \\(A\\) is a \\(2 \\times 3\\) matrix.\nInterpretation: The transformation maps 3D vectors to 2D vectors (a projection from 3D space onto a plane).\nKey fact: The dimensions of the input and output spaces determine the size of the matrix."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#basis-and-coordinates",
    "href": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#basis-and-coordinates",
    "title": "Lecture 30: Linear Transformations and Their Matrices",
    "section": "8. Basis and Coordinates",
    "text": "8. Basis and Coordinates\n\nThe Power of Basis\nFor a linear transformation \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\), if we know the transformation of a basis:\n\\[\nT(v_1), T(v_2), \\ldots, T(v_n)\n\\]\nthen we can compute \\(T(v)\\) for any vector \\(v\\).\nWhy? Every vector \\(v\\) can be written as a linear combination:\n\\[\nv = c_1 v_1 + c_2 v_2 + \\cdots + c_n v_n\n\\]\nBy linearity:\n\\[\nT(v) = c_1 T(v_1) + c_2 T(v_2) + \\cdots + c_n T(v_n)\n\\]\n\n\nCoordinates\nCoordinates come from a choice of basis.\nIf \\(v = c_1 v_1 + \\cdots + c_n v_n\\), then the coordinate vector of \\(v\\) in the basis \\(\\{v_1, \\ldots, v_n\\}\\) is:\n\\[\n[v]_{\\{v\\}} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#projection-in-the-cleanest-basis",
    "href": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#projection-in-the-cleanest-basis",
    "title": "Lecture 30: Linear Transformations and Their Matrices",
    "section": "9. Projection in the Cleanest Basis",
    "text": "9. Projection in the Cleanest Basis\n\nChoosing the Right Basis\nFor a projection onto a line in the direction of unit vector \\(x\\):\nStandard basis \\(\\{e_1, e_2\\}\\): The matrix is complicated (involves dot products).\nNatural basis \\(\\{x, x^{\\perp}\\}\\) where \\(x^{\\perp}\\) is perpendicular to \\(x\\):\n\n\\(T(x) = x\\) (projection keeps vectors in the direction \\(x\\) unchanged)\n\\(T(x^{\\perp}) = 0\\) (perpendicular components vanish)\n\nMatrix in this basis:\n\\[\nA_{\\{x, x^{\\perp}\\}} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}\n\\]\nThis is the cleanest possible representation of the projection!\nKey insight: The choice of basis dramatically affects how simple the matrix looks."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#constructing-the-matrix-of-a-linear-transformation",
    "href": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#constructing-the-matrix-of-a-linear-transformation",
    "title": "Lecture 30: Linear Transformations and Their Matrices",
    "section": "10. Constructing the Matrix of a Linear Transformation",
    "text": "10. Constructing the Matrix of a Linear Transformation\n\nGeneral Setup\nGiven: - Input basis: \\(v_1, v_2, \\ldots, v_n\\) (basis for \\(\\mathbb{R}^n\\)) - Output basis: \\(w_1, w_2, \\ldots, w_m\\) (basis for \\(\\mathbb{R}^m\\)) - Linear transformation: \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\)\nGoal: Find the \\(m \\times n\\) matrix \\(A\\) such that \\(T\\) acts as multiplication by \\(A\\) in these coordinates.\n\n\nConstructing the Columns\nFor each input basis vector \\(v_i\\), compute \\(T(v_i)\\) and express it in the output basis:\n\\[\nT(v_i) = a_{1i} w_1 + a_{2i} w_2 + \\cdots + a_{mi} w_m\n\\]\nThe coefficients \\([a_{1i}, a_{2i}, \\ldots, a_{mi}]^{\\top}\\) form the \\(i\\)-th column of \\(A\\).\nMatrix:\n\\[\nA = \\begin{bmatrix}\n| & | & & | \\\\\n[T(v_1)]_{\\{w\\}} & [T(v_2)]_{\\{w\\}} & \\cdots & [T(v_n)]_{\\{w\\}} \\\\\n| & | & & |\n\\end{bmatrix}\n\\]\nwhere \\([T(v_i)]_{\\{w\\}}\\) denotes the coordinate vector of \\(T(v_i)\\) in the basis \\(\\{w_1, \\ldots, w_m\\}\\).\n Figure: The matrix of a linear transformation is built column-by-column from the transformed basis vectors, expressed in the output basis coordinates."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#example-differentiation-as-a-linear-transformation",
    "href": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#example-differentiation-as-a-linear-transformation",
    "title": "Lecture 30: Linear Transformations and Their Matrices",
    "section": "11. Example: Differentiation as a Linear Transformation",
    "text": "11. Example: Differentiation as a Linear Transformation\n\nSetup\nInput space: Polynomials of degree \\(\\leq 2\\)\n\\[\np(x) = c_1 + c_2 x + c_3 x^2\n\\]\nOutput space: Polynomials of degree \\(\\leq 1\\)\n\\[\nq(x) = d_1 + d_2 x\n\\]\nTransformation: \\(T = \\frac{d}{dx}\\) (differentiation)\n\n\nChoosing Bases\nInput basis: \\(\\{1, x, x^2\\}\\)\nOutput basis: \\(\\{1, x\\}\\)\n\n\nComputing the Matrix\nApply \\(T\\) to each input basis vector:\n\n\\(T(1) = 0 = 0 \\cdot 1 + 0 \\cdot x\\)\n\\(T(x) = 1 = 1 \\cdot 1 + 0 \\cdot x\\)\n\\(T(x^2) = 2x = 0 \\cdot 1 + 2 \\cdot x\\)\n\nMatrix:\n\\[\nA = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 2\n\\end{bmatrix}\n\\]\n\n\nVerification\nFor \\(p(x) = c_1 + c_2 x + c_3 x^2\\):\n\\[\nA \\begin{bmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{bmatrix} = \\begin{bmatrix} c_2 \\\\ 2c_3 \\end{bmatrix}\n\\]\nThis represents:\n\\[\n\\frac{d}{dx}(c_1 + c_2 x + c_3 x^2) = c_2 + 2c_3 x\n\\]\nExactly correct!\n Figure: The derivative operator as a linear transformation - represented by a 2√ó3 matrix mapping polynomials of degree ‚â§2 to polynomials of degree ‚â§1."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture30-linear-transformations.html#summary",
    "title": "Lecture 30: Linear Transformations and Their Matrices",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\nConcept\nKey Idea\n\n\n\n\nLinear Transformation\n\\(T(cv + dw) = cT(v) + dT(w)\\)\n\n\n\\(T(0) = 0\\)\nRequired for all linear transformations\n\n\nLinear Examples\nProjection, rotation, reflection, matrix multiplication, differentiation\n\n\nNon-Linear Examples\nTranslation/shift, length/norm\n\n\nMatrix Representation\nColumns are \\(T(v_i)\\) expressed in output basis\n\n\nBasis Choice\nRight basis makes the matrix simple (e.g., projection is diagonal)\n\n\nDimensions\n\\(m \\times n\\) matrix for \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\)\n\n\n\nFundamental theorem: Every linear transformation can be represented as matrix multiplication, and every matrix multiplication defines a linear transformation. The choice of basis determines what the matrix looks like."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html",
    "href": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html",
    "title": "MIT 18.06SC Lecture 7: Solving Ax=0 - Pivot Variables and Special Solutions",
    "section": "",
    "text": "My lecture notes\nThis lecture develops a systematic algorithm to find all solutions to \\(Ax = 0\\) using pivot variables, free variables, and special solutions. The null space structure is revealed through reduced row echelon form."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#context",
    "href": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#context",
    "title": "MIT 18.06SC Lecture 7: Solving Ax=0 - Pivot Variables and Special Solutions",
    "section": "",
    "text": "My lecture notes\nThis lecture develops a systematic algorithm to find all solutions to \\(Ax = 0\\) using pivot variables, free variables, and special solutions. The null space structure is revealed through reduced row echelon form."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#homogeneous-linear-system",
    "href": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#homogeneous-linear-system",
    "title": "MIT 18.06SC Lecture 7: Solving Ax=0 - Pivot Variables and Special Solutions",
    "section": "Homogeneous Linear System",
    "text": "Homogeneous Linear System\nWe consider the homogeneous linear system: \\[\nAx = 0\n\\]\nwhere \\(A\\) is an \\(m \\times n\\) matrix with rank \\(r\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#pivot-variables-and-free-variables",
    "href": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#pivot-variables-and-free-variables",
    "title": "MIT 18.06SC Lecture 7: Solving Ax=0 - Pivot Variables and Special Solutions",
    "section": "Pivot Variables and Free Variables",
    "text": "Pivot Variables and Free Variables\nAfter performing row reduction on \\(A\\), we identify:\n\nPivot variables: Variables corresponding to columns with pivot positions (leading entries in row echelon form)\nFree variables: Variables corresponding to columns without pivot positions\nNumber of free variables: \\(n - r\\) (total variables minus rank)\n\n\nRelationship to Solutions\n\nIf rank \\(r = n\\): No free variables\n\nOnly the trivial solution \\(x = \\vec{0}\\) exists\nThe null space contains only the zero vector\n\nIf rank \\(r &lt; n\\): There are \\(n - r\\) free variables\n\nInfinitely many non-trivial solutions exist\nThe null space is a \\((n-r)\\)-dimensional subspace\n\n\nKey insight: Free variables are necessary for non-trivial solutions. Each free variable adds one dimension to the null space."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#special-solutions",
    "href": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#special-solutions",
    "title": "MIT 18.06SC Lecture 7: Solving Ax=0 - Pivot Variables and Special Solutions",
    "section": "Special Solutions",
    "text": "Special Solutions\nSpecial solutions form a basis for the null space \\(N(A)\\). To find them:\n\nAlgorithm\n\nIdentify the free variables (there are \\(n - r\\) of them)\nFor each free variable:\n\nSet that free variable to 1\nSet all other free variables to 0\nSolve for the pivot variables\nThis gives one special solution\n\n\nThe null space is the span of all special solutions.\n\n\nExample\nConsider the row echelon form matrix: \\[\nR = \\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n0 & 0 & 2 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nFrom this, we have: - Pivot columns: 1 and 3 (variables \\(x_1\\) and \\(x_3\\)) - Free columns: 2 and 4 (variables \\(x_2\\) and \\(x_4\\)) - Equations: \\(x_1 + 2x_2 + 3x_3 + 4x_4 = 0\\) and \\(2x_3 + x_4 = 0\\)\nSpecial solution 1: Set \\(x_2 = 1\\), \\(x_4 = 0\\) - From equation 2: \\(2x_3 = 0 \\Rightarrow x_3 = 0\\) - From equation 1: \\(x_1 + 2(1) + 3(0) + 4(0) = 0 \\Rightarrow x_1 = -2\\) - Special solution: \\(\\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\\)\nSpecial solution 2: Set \\(x_2 = 0\\), \\(x_4 = 1\\) - From equation 2: \\(2x_3 + 1 = 0 \\Rightarrow x_3 = -\\frac{1}{2}\\) - From equation 1: \\(x_1 + 2(0) + 3(-\\frac{1}{2}) + 4(1) = 0 \\Rightarrow x_1 = -\\frac{5}{2}\\) - Special solution: \\(\\begin{bmatrix} -\\frac{5}{2} \\\\ 0 \\\\ -\\frac{1}{2} \\\\ 1 \\end{bmatrix}\\)\nComplete null space: \\[\nN(A) = c_1 \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + c_2 \\begin{bmatrix} -\\frac{5}{2} \\\\ 0 \\\\ -\\frac{1}{2} \\\\ 1 \\end{bmatrix}\n\\]\nfor any scalars \\(c_1, c_2 \\in \\mathbb{R}\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#rref-reduced-row-echelon-form",
    "href": "Math/MIT18.06/mit1806-lecture7-solving-ax-0.html#rref-reduced-row-echelon-form",
    "title": "MIT 18.06SC Lecture 7: Solving Ax=0 - Pivot Variables and Special Solutions",
    "section": "RREF (Reduced Row Echelon Form)",
    "text": "RREF (Reduced Row Echelon Form)\nThe reduced row echelon form provides the clearest view of the null space structure.\n\nProperties of RREF\n\nEach pivot column contains exactly one 1 (the pivot) and all other entries are 0\nThe pivot is the only non-zero entry in its row among pivot columns\nMakes it easy to read off special solutions directly\n\n\n\nBlock Structure\nFor a matrix with \\(r\\) pivot columns and \\(n-r\\) free columns, the RREF has the form: \\[\nR_{\\text{rref}} = \\begin{bmatrix}\nI_r & F\n\\end{bmatrix}\n\\]\nwhere: - \\(I_r\\) is the \\(r \\times r\\) identity matrix (pivot columns) - \\(F\\) is an \\(r \\times (n-r)\\) matrix (free variable coefficients)\n\n\nExample\nFor our example, RREF would be: \\[\nR_{\\text{rref}} = \\begin{bmatrix}\n1 & 2 & 0 & \\frac{5}{2} \\\\\n0 & 0 & 1 & \\frac{1}{2} \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n2 & \\frac{5}{2} \\\\\n0 & \\frac{1}{2}\n\\end{bmatrix}\n\\]\nHere \\(I_r = I_2\\) and \\(F = \\begin{bmatrix} 2 & \\frac{5}{2} \\\\ 0 & \\frac{1}{2} \\end{bmatrix}\\).\n\n\nDirect Formula for Null Space\nFrom the RREF block structure \\([I_r \\mid F]\\), the null space matrix (whose columns are special solutions) is: \\[\nN(A) = \\begin{bmatrix}\n-F \\\\\nI_{n-r}\n\\end{bmatrix}\n\\]\nThis \\(n \\times (n-r)\\) matrix contains all special solutions as its columns.\n\n\nDimensions Summary\n\n\n\nMatrix\nShape\nDescription\n\n\n\n\n\\(A\\)\n\\(m \\times n\\)\nOriginal matrix\n\n\n\\(I_r\\)\n\\(r \\times r\\)\nIdentity block in RREF\n\n\n\\(F\\)\n\\(r \\times (n-r)\\)\nFree variable coefficients\n\n\n\\(N(A)\\)\n\\(n \\times (n-r)\\)\nNull space basis matrix\n\n\n\nThe null space \\(N(A)\\) is an \\((n-r)\\)-dimensional subspace of \\(\\mathbb{R}^n\\).\n\nSource: MIT 18.06SC Linear Algebra, Lecture 7"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html",
    "href": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html",
    "title": "MIT 18.06 Lecture 27: Positive Definite Matrices and Minima",
    "section": "",
    "text": "This lecture deepens our understanding of positive definite matrices by connecting them to multivariable calculus, optimization, and geometry. We explore how these matrices guarantee the existence of unique minima and how their eigenvalues shape the geometry of quadratic forms."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#review-tests-for-positive-definiteness",
    "href": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#review-tests-for-positive-definiteness",
    "title": "MIT 18.06 Lecture 27: Positive Definite Matrices and Minima",
    "section": "Review: Tests for Positive Definiteness",
    "text": "Review: Tests for Positive Definiteness\nA symmetric matrix \\(A\\) is positive definite if any of the following equivalent conditions hold:\n\n1. Energy Test\n\\[\nx^{\\top}Ax &gt; 0 \\quad \\text{for all } x \\neq 0\n\\]\n\n\n2. Eigenvalue Test\nAll eigenvalues are positive: \\(\\lambda_i &gt; 0\\) for all \\(i\\)\n\n\n3. Pivot Test\nAll pivots are positive (from Gaussian elimination)\n\n\n4. Determinant Test\nAll leading principal minors have positive determinants: - \\(\\det(A_1) &gt; 0\\) - \\(\\det(A_2) &gt; 0\\) - \\(\\vdots\\) - \\(\\det(A_n) &gt; 0\\)\nwhere \\(A_k\\) is the upper-left \\(k \\times k\\) submatrix."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#example-22-positive-definite-matrix",
    "href": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#example-22-positive-definite-matrix",
    "title": "MIT 18.06 Lecture 27: Positive Definite Matrices and Minima",
    "section": "Example: 2√ó2 Positive Definite Matrix",
    "text": "Example: 2√ó2 Positive Definite Matrix\nConsider: \\[\nA = \\begin{bmatrix}a & b \\\\ b & c\\end{bmatrix}\n\\]\nTests for positive definiteness:\n\nDeterminants:\n\n\\(a &gt; 0\\) (first principal minor)\n\\(ac - b^2 &gt; 0\\) (full determinant)\n\nPivots:\n\nFirst pivot: \\(a &gt; 0\\)\nSecond pivot: \\(\\frac{ac - b^2}{a} &gt; 0\\)\n\nEigenvalues: Both \\(\\lambda_1 &gt; 0\\) and \\(\\lambda_2 &gt; 0\\)\nEnergy: \\(x^{\\top}Ax &gt; 0\\) for all \\(x \\neq 0\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#positive-semidefinite-matrices",
    "href": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#positive-semidefinite-matrices",
    "title": "MIT 18.06 Lecture 27: Positive Definite Matrices and Minima",
    "section": "Positive Semidefinite Matrices",
    "text": "Positive Semidefinite Matrices\nA matrix is positive semidefinite if \\(x^{\\top}Ax \\geq 0\\) for all \\(x\\) (allowing zero).\n\nExample\n\\[\nA = \\begin{bmatrix}2 & 6 \\\\ 6 & 18\\end{bmatrix}\n\\]\nCheck the tests: - Determinant: \\((2 \\times 18) - (6 \\times 6) = 36 - 36 = 0\\) - Eigenvalues: \\(\\lambda_1 = 0\\), \\(\\lambda_2 = 20\\) - Pivots: \\(2, 0\\)\nSince one eigenvalue is zero and the determinant is zero, this matrix is positive semidefinite (not positive definite).\n\n\nQuadratic Form\n\\[\nx^{\\top}Ax = \\begin{bmatrix}x_1 & x_2\\end{bmatrix}\\begin{bmatrix}2 & 6 \\\\ 6 & 18\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}\n\\]\n\\[\n= 2x_1^2 + 12x_1x_2 + 18x_2^2 = 2(x_1 + 3x_2)^2 \\geq 0\n\\]\nKey observation: The quadratic form is a perfect square, so it‚Äôs always non-negative but can be zero (when \\(x_1 = -3x_2\\)).\n\n\n\nSemi-Positive Definite\n\n\nGeometric interpretation: The level curves form a parabolic cylinder‚Äîthere‚Äôs a whole line of minima along \\(x_1 + 3x_2 = 0\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#indefinite-matrices",
    "href": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#indefinite-matrices",
    "title": "MIT 18.06 Lecture 27: Positive Definite Matrices and Minima",
    "section": "Indefinite Matrices",
    "text": "Indefinite Matrices\nA matrix is indefinite if \\(x^{\\top}Ax\\) can be both positive and negative depending on \\(x\\).\n\nExample\n\\[\nA = \\begin{bmatrix}2 & 6 \\\\ 6 & 7\\end{bmatrix}\n\\]\nQuadratic form: \\[\nf(x, y) = 2x^2 + 12xy + 7y^2\n\\]\nFinding negative values: - Try \\(x = -1, y = 1\\): \\(f(-1, 1) = 2 - 12 + 7 = -3 &lt; 0\\) - Try \\(x = 1, y = -1\\): \\(f(1, -1) = 2 - 12 + 7 = -3 &lt; 0\\)\nSince the quadratic form can be negative, the matrix is indefinite.\n\n\nFactorizing the Quadratic Form\nWe can complete the square: \\[\nf(x, y) = 2x^2 + 12xy + 7y^2 = 2(x + 3y)^2 - 11y^2\n\\]\nInterpretation: - The term \\(2(x + 3y)^2\\) is positive - The term \\(-11y^2\\) is negative - The function has a saddle point at the origin\n\n\n\nIndefinite\n\n\nGeometric interpretation: The surface is a hyperbolic paraboloid (saddle). At the critical point, first-order derivatives are zero, but the function has no minimum‚Äîit goes up in some directions and down in others."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#positive-definite-example",
    "href": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#positive-definite-example",
    "title": "MIT 18.06 Lecture 27: Positive Definite Matrices and Minima",
    "section": "Positive Definite Example",
    "text": "Positive Definite Example\nConsider: \\[\nA = \\begin{bmatrix}2 & 2 \\\\ 2 & 20\\end{bmatrix}\n\\]\nQuadratic form: \\[\nf(x, y) = 2x^2 + 4xy + 20y^2\n\\]\n\nFinding the Minimum\nCritical point: At the minimum, all partial derivatives must be zero: \\[\n\\frac{\\partial f}{\\partial x} = 4x + 4y = 0\n\\] \\[\n\\frac{\\partial f}{\\partial y} = 4x + 40y = 0\n\\]\nThe only solution is \\((x, y) = (0, 0)\\).\nSecond derivative test: Computing the second derivatives: \\[\n\\frac{\\partial^2 f}{\\partial x^2} = 4, \\quad \\frac{\\partial^2 f}{\\partial x \\partial y} = 4, \\quad \\frac{\\partial^2 f}{\\partial y^2} = 40\n\\]\nThe Hessian matrix is: \\[\nH = \\begin{bmatrix}4 & 4 \\\\ 4 & 40\\end{bmatrix} = 2A\n\\]\nSince \\(H = 2A\\) is positive definite (all eigenvalues positive, or equivalently \\(A\\) is positive definite), this confirms \\((0, 0)\\) is a minimum.\n\n\nFactorizing the Quadratic Form\nComplete the square: \\[\nf(x, y) = 2x^2 + 4xy + 20y^2 = 2(x + y)^2 + 18y^2\n\\]\nKey insight: Both terms are squares with positive coefficients, so \\(f(x, y) \\geq 0\\) with equality only at \\((0, 0)\\).\n\n\n\nPositive Definite\n\n\nGeometric interpretation: The level curves are ellipses centered at the origin. The function forms a ‚Äúbowl‚Äù with a unique minimum at the origin.\n\n\nConnection to Elimination\nPerforming Gaussian elimination: \\[\nA = \\begin{bmatrix}2 & 2 \\\\ 2 & 20\\end{bmatrix} \\to U = \\begin{bmatrix}2 & 2 \\\\ 0 & 18\\end{bmatrix}\n\\]\nPivots: \\([2, 18]\\)\nRemarkable fact: These pivots appear as the coefficients in the factored form \\(f(x, y) = 2(x + y)^2 + 18y^2\\). This is no coincidence‚Äîthe pivots measure the curvature in each direction after eliminating previous variables."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#calculus-and-the-hessian-matrix",
    "href": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#calculus-and-the-hessian-matrix",
    "title": "MIT 18.06 Lecture 27: Positive Definite Matrices and Minima",
    "section": "Calculus and the Hessian Matrix",
    "text": "Calculus and the Hessian Matrix\nIn multivariable calculus, to determine if a critical point is a minimum, we check:\n\nFirst-order conditions (critical point): \\[\n\\frac{\\partial f}{\\partial x} = 0, \\quad \\frac{\\partial f}{\\partial y} = 0\n\\]\nSecond-order conditions (minimum test): \\[\n\\frac{\\partial^2 f}{\\partial x^2} &gt; 0, \\quad \\frac{\\partial^2 f}{\\partial y^2} &gt; 0\n\\]\nMore precisely, the Hessian matrix must be positive definite.\n\n\nThe Hessian Matrix\nFor a function \\(f(x, y)\\), the Hessian is: \\[\nH = \\begin{bmatrix}f_{xx} & f_{xy} \\\\ f_{yx} & f_{yy}\\end{bmatrix}\n\\]\nKey properties: - The Hessian is symmetric because \\(f_{xy} = f_{yx}\\) (assuming continuous second derivatives) - The Hessian must be positive definite for a minimum - For quadratic forms \\(f(x) = x^{\\top}Ax\\), the Hessian is exactly \\(2A\\)\n\n\nSummary Table\n\n\n\n\n\n\n\n\nMatrix Type\nGeometry\nMinima\n\n\n\n\nPositive Definite\nBowl\nSingle unique minimum\n\n\nPositive Semidefinite\nBowl with flat line\nInfinitely many minima along a line/plane\n\n\nIndefinite\nSaddle\nNo minimum (saddle point)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#example-2",
    "href": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#example-2",
    "title": "MIT 18.06 Lecture 27: Positive Definite Matrices and Minima",
    "section": "3√ó3 Example",
    "text": "3√ó3 Example\nConsider the tridiagonal matrix: \\[\nA = \\begin{bmatrix}2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2\\end{bmatrix}\n\\]\n\nTests for Positive Definiteness\n\nLeading principal determinants:\n\n\\(\\det([2]) = 2 &gt; 0\\)\n\\(\\det\\begin{bmatrix}2 & -1 \\\\ -1 & 2\\end{bmatrix} = 4 - 1 = 3 &gt; 0\\)\n\\(\\det(A) = 4 &gt; 0\\)\n\nPivots: \\(2, \\frac{3}{2}, \\frac{4}{3}\\) (all positive)\nEigenvalues: \\(2 - \\sqrt{2}, 2, 2 + \\sqrt{2}\\) (all positive)\n\nAll tests confirm: \\(A\\) is positive definite.\n\n\nQuadratic Form\n\\[\nx^{\\top}Ax = 2x_1^2 + 2x_2^2 + 2x_3^2 - 2x_1x_2 - 2x_2x_3\n\\]\nThis can be factored as: \\[\nx^{\\top}Ax = (x_1 - x_2)^2 + (x_2 - x_3)^2 + x_1^2 + x_3^2\n\\]\nAll terms are squares with positive coefficients, confirming positive definiteness.\n\n\nGeometry: Ellipsoids\nThe level surface \\(x^{\\top}Ax = 1\\) is an ellipsoid in 3D space.\n\n\n\nGeneral Ellipsoid\n\n\nPrincipal Axis Theorem: The spectral decomposition \\[\nA = Q\\Lambda Q^{\\top}\n\\]\nreveals the geometry: - Eigenvectors (columns of \\(Q\\)): Principal axes of the ellipsoid - Eigenvalues: Determine the lengths of these axes\nFor our matrix with eigenvalues \\(2 - \\sqrt{2}, 2, 2 + \\sqrt{2}\\): - Major axis: Length proportional to \\(\\frac{1}{\\sqrt{2 - \\sqrt{2}}}\\) (longest) - Middle axis: Length proportional to \\(\\frac{1}{\\sqrt{2}}\\) - Minor axis: Length proportional to \\(\\frac{1}{\\sqrt{2 + \\sqrt{2}}}\\) (shortest)\nKey insight: Larger eigenvalues correspond to directions of greater curvature (tighter curves), resulting in shorter axes on the ellipsoid. Smaller eigenvalues correspond to flatter directions with longer axes."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html#summary",
    "title": "MIT 18.06 Lecture 27: Positive Definite Matrices and Minima",
    "section": "Summary",
    "text": "Summary\nThis lecture connects positive definite matrices to optimization and geometry:\n\nFour equivalent tests: Energy, eigenvalues, pivots, and determinants all characterize positive definiteness\nSemidefinite vs definite: Semidefinite allows zero eigenvalues and has infinitely many minima\nIndefinite matrices: Mixed signs in eigenvalues lead to saddle points\nFactorization: Completing the square reveals the structure and connects to pivots\nHessian matrix: Second derivatives form a matrix that must be positive definite for a minimum\nGeometric interpretation:\n\nPositive definite ‚Üí ellipsoid level curves ‚Üí unique minimum\nSemidefinite ‚Üí parabolic cylinder ‚Üí line of minima\nIndefinite ‚Üí hyperbolic paraboloid ‚Üí saddle point\n\nPrincipal Axis Theorem: Eigenvalues and eigenvectors determine the shape and orientation of ellipsoids\n\nPositive definite matrices are fundamental in optimization, machine learning (loss functions), statistics (covariance matrices), and physics (energy functions)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html",
    "href": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html",
    "title": "Lecture 31: Change of Basis and Image Compression",
    "section": "",
    "text": "This lecture explores how change of basis enables image compression:\n\nImage representation as high-dimensional vectors\nJPEG compression using Fourier basis\nWavelet basis as an alternative\nProperties of good bases for compression\nChange of basis formulas and similar matrices"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#overview",
    "href": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#overview",
    "title": "Lecture 31: Change of Basis and Image Compression",
    "section": "",
    "text": "This lecture explores how change of basis enables image compression:\n\nImage representation as high-dimensional vectors\nJPEG compression using Fourier basis\nWavelet basis as an alternative\nProperties of good bases for compression\nChange of basis formulas and similar matrices"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#lossy-compression",
    "href": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#lossy-compression",
    "title": "Lecture 31: Change of Basis and Image Compression",
    "section": "1. Lossy Compression",
    "text": "1. Lossy Compression\nA gray \\(512 \\times 512\\) image contains 262,144 pixels.\nWe use \\(x_i\\) to represent a pixel, \\(0 \\leq x_i \\leq 255\\) (8 bits, \\(2^8 = 256\\)).\nThe entire image can be represented as a vector:\n\\[\nx \\in \\mathbb{R}^n, \\quad n = 512^2 = 262,144\n\\]\n Figure: Image representation as a vector - each pixel becomes a component, transforming the 2D image into a point in high-dimensional space.\nChallenge: Storing 262,144 numbers requires significant memory. Can we represent the same image with fewer numbers by choosing a better basis?"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#jpeg-compression",
    "href": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#jpeg-compression",
    "title": "Lecture 31: Change of Basis and Image Compression",
    "section": "2. JPEG Compression",
    "text": "2. JPEG Compression\nJPEG (Joint Photographic Experts Group) is fundamentally about changing the basis.\n\nStandard Basis\nThe standard basis in \\(\\mathbb{R}^n\\) consists of unit vectors:\n\\[\n\\begin{bmatrix}1\\\\0\\\\0\\\\\\vdots\\end{bmatrix}, \\quad \\begin{bmatrix}0\\\\1\\\\0\\\\\\vdots\\end{bmatrix}, \\quad \\ldots, \\quad \\begin{bmatrix}0\\\\0\\\\\\vdots\\\\1\\end{bmatrix}\n\\]\nEach basis vector represents a single pixel.\n\n\nBetter Basis\nA better basis might have vectors that capture image structure:\n\\[\n\\begin{bmatrix}1\\\\1\\\\1\\\\1\\\\\\vdots\\\\1\\end{bmatrix}, \\quad \\begin{bmatrix}1\\\\1\\\\-1\\\\-1\\\\\\vdots\\\\-1\\end{bmatrix}, \\quad \\begin{bmatrix}1\\\\-1\\\\1\\\\-1\\\\\\vdots\\\\-1\\end{bmatrix}, \\quad \\ldots\n\\]\nThese vectors capture patterns like: - Constant intensities (first vector) - Low-frequency variations (second vector) - Higher-frequency patterns (remaining vectors)\n\n\nFourier Basis\nJPEG uses the Fourier basis for compression:\nSteps:\n\nBreak down the image into \\(n\\) blocks of size \\(8 \\times 8\\) (64 pixels per block)\nBuild the Fourier matrix \\(F_{64}\\):\n\n\\[\nF_{64} = \\begin{bmatrix}\n1 & 1 & \\cdots & 1 \\\\\n1 & w & \\cdots & w^{63} \\\\\n1 & w^2 & w^4 & w^{126} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & w^{63} & \\cdots & w^{63 \\times 63}\n\\end{bmatrix}\n\\]\nwhere \\(w = e^{2\\pi i/64}\\) is the 64th root of unity.\n\nCompute coefficients in the Fourier basis:\n\n\\[\nc = Fx\n\\]\nwhere \\(x\\) is the 64-element pixel vector for one block.\n\nCompression: Thresholding\n\nSet small coefficients to zero:\n\\[\n\\hat{c} = \\text{threshold}(c)\n\\]\nThis produces a sparse matrix where most entries are zero.\n Figure: JPEG compression pipeline - the image is transformed to the Fourier basis, small coefficients are discarded (thresholding), and the image is reconstructed from the remaining coefficients.\n\nReconstruction:\n\n\\[\n\\hat{x} = \\sum_{i} \\hat{c}_i v_i\n\\]\nwhere \\(v_i\\) are the Fourier basis vectors.\nKey idea: Most energy in natural images concentrates in low-frequency components, so we can discard high-frequency coefficients with minimal perceptual loss."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#wavelets",
    "href": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#wavelets",
    "title": "Lecture 31: Change of Basis and Image Compression",
    "section": "3. Wavelets",
    "text": "3. Wavelets\nWavelets provide an alternative basis for image compression with some advantages over Fourier.\n\nThe Wavelet Basis\nFor \\(n = 8\\), the wavelet basis \\(W\\) consists of vectors:\n\\[\nW = \\begin{bmatrix}\n1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1\n\\end{bmatrix}, \\quad\n\\begin{bmatrix}\n1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ -1 \\\\ -1 \\\\ -1 \\\\ -1\n\\end{bmatrix}, \\quad\n\\begin{bmatrix}\n1 \\\\ 1 \\\\ -1 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{bmatrix}, \\quad\n\\begin{bmatrix}\n0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ -1 \\\\ -1\n\\end{bmatrix}, \\quad\n\\begin{bmatrix}\n1 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{bmatrix}, \\quad \\ldots\n\\]\n\n\nRepresentation in Wavelet Basis\nAn 8-pixel vector from the standard basis can be represented as:\n\\[\np = c_1 w_1 + c_2 w_2 + \\cdots + c_8 w_8\n\\]\nIn matrix form:\n\\[\np = W \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_8 \\end{bmatrix} = Wc\n\\]\n\n\nSolving for Coefficients\n\\[\np = Wc \\implies c = W^{-1}p\n\\]\nAdvantage: Wavelets capture both spatial and frequency information, making them effective for images with edges and localized features."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#properties-of-a-good-basis",
    "href": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#properties-of-a-good-basis",
    "title": "Lecture 31: Change of Basis and Image Compression",
    "section": "4. Properties of a Good Basis",
    "text": "4. Properties of a Good Basis\nWhat makes a basis ‚Äúgood‚Äù for compression?\n\n1. Fast Inverse\nA good basis has a nice, fast inverse.\nThe matrix \\(W^{-1}\\) should be: - Easy to compute (ideally \\(O(n \\log n)\\) or better) - Numerically stable\nExamples: - Fourier basis: Fast Fourier Transform (FFT) computes \\(F^{-1}\\) in \\(O(n \\log n)\\) - Wavelet basis: Fast Wavelet Transform also \\(O(n \\log n)\\)\n\n\n2. Few Is Enough\nA few basis vectors are enough to reproduce the signal.\nMost coefficients \\(c_i\\) should be small or zero, so we can: - Keep only the largest coefficients - Discard small coefficients with minimal error - Achieve high compression ratios\nThis is called sparsity in the transformed domain.\n\n\n3. Orthogonal (No Redundant Information)\nBasis vectors should be orthogonal.\nOrthogonality ensures: - No redundancy between basis vectors - Each coefficient captures independent information - Simple formula: \\(W^{-1} = W^T\\) (up to normalization)\nFor compression, we want \\(W^T W = I\\) (orthonormal basis)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#change-of-basis",
    "href": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#change-of-basis",
    "title": "Lecture 31: Change of Basis and Image Compression",
    "section": "5. Change of Basis",
    "text": "5. Change of Basis\n\nGeneral Formula\nLet \\(W\\) be the matrix whose columns are the new basis vectors.\nLet \\(x\\) be a vector in the old (standard) basis.\nLet \\(c\\) be the coordinates in the new basis.\nRelationship:\n\\[\nx = Wc\n\\]\nTo convert from old to new basis:\n\\[\nc = W^{-1}x\n\\]\nIf \\(W\\) is orthonormal, this simplifies to:\n\\[\nc = W^T x\n\\]\nExample: For pixel vector \\(p\\) in standard basis and wavelet coefficients \\(c\\):\n\\[\np = Wc \\quad \\text{and} \\quad c = W^{-1}p\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#similar-matrices-and-transformations",
    "href": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#similar-matrices-and-transformations",
    "title": "Lecture 31: Change of Basis and Image Compression",
    "section": "6. Similar Matrices and Transformations",
    "text": "6. Similar Matrices and Transformations\n\nSetup\nSay we have a linear transformation \\(T\\).\nThen we can represent \\(T\\) with different matrices depending on the basis:\n\nWith respect to basis \\(v_1, v_2, \\ldots, v_n\\), we have matrix \\(A\\)\nWith respect to basis \\(w_1, w_2, \\ldots, w_n\\), we have matrix \\(B\\)\n\nRelationship: \\(A\\) and \\(B\\) are similar matrices:\n\\[\nA \\sim B \\quad \\text{means} \\quad B = M^{-1}AM\n\\]\nwhere \\(M\\) is the change of basis matrix.\n\n\nConstructing Matrix \\(A\\)\nAfter we know matrix \\(A\\), we know:\n\\[\nT(v_1), T(v_2), \\ldots, T(v_n)\n\\]\nBecause for any vector:\n\\[\nx = c_1 v_1 + c_2 v_2 + \\cdots + c_n v_n\n\\]\nBy linearity:\n\\[\nT(x) = c_1 T(v_1) + c_2 T(v_2) + \\cdots + c_n T(v_n)\n\\]\nIf we know all the transformation results:\n\\[\n\\begin{align}\nT(v_1) &= a_{11}v_1 + a_{21}v_2 + \\cdots + a_{n1}v_n \\\\\nT(v_2) &= a_{12}v_1 + a_{22}v_2 + \\cdots + a_{n2}v_n \\\\\n&\\vdots \\\\\nT(v_n) &= a_{1n}v_1 + a_{2n}v_2 + \\cdots + a_{nn}v_n\n\\end{align}\n\\]\nWe can form the matrix:\n\\[\nA = \\begin{bmatrix}\n| & | & & | \\\\\na_1 & a_2 & \\cdots & a_n \\\\\n| & | & & |\n\\end{bmatrix}\n\\]\nwhere \\(a_i\\) is the coefficient vector for \\(T(v_i)\\).\n\n\nSpecial Case: Eigenvector Basis\nIf all \\(v_i\\) happen to be eigenvectors:\n\\[\nT(v_i) = \\lambda_i v_i\n\\]\nThen \\(A\\) is a diagonal matrix of eigenvalues:\n\\[\nA = \\begin{bmatrix}\n\\lambda_1 & 0 & \\cdots & 0 \\\\\n0 & \\lambda_2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\lambda_n\n\\end{bmatrix}\n\\]\nApplication to compression: If we can find eigenvectors of the image covariance matrix, we can use PCA (Principal Component Analysis) for compression. This seems to be the perfect way of compression, but given the computational cost, in practice we don‚Äôt use this approach for large images."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture31-change-of-basis.html#summary",
    "title": "Lecture 31: Change of Basis and Image Compression",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\nConcept\nKey Idea\n\n\n\n\nImage as Vector\n\\(512 \\times 512\\) image = vector in \\(\\mathbb{R}^{262,144}\\)\n\n\nJPEG Compression\nChange to Fourier basis, threshold small coefficients\n\n\nWavelet Basis\nAlternative basis capturing spatial + frequency information\n\n\nGood Basis Properties\nFast inverse, sparsity (few coefficients enough), orthogonal\n\n\nChange of Basis\n\\(x = Wc\\) where \\(W\\) has new basis vectors as columns\n\n\nSimilar Matrices\n\\(B = M^{-1}AM\\) represents same transformation in different basis\n\n\nEigenvector Basis\nGives diagonal matrix (ideal but computationally expensive)\n\n\n\nKey insight: The effectiveness of compression depends on choosing a basis where the signal is sparse (most coefficients are small). Different bases work best for different types of data‚ÄîFourier for smooth images, wavelets for images with edges, eigenvectors (PCA) for data-specific optimal compression."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture16-least-squares.html#projection-matrix",
    "href": "Math/MIT18.06/mit1806-lecture16-least-squares.html#projection-matrix",
    "title": "MIT 18.06 Lecture 16: Projection Matrices and Least Squares",
    "section": "Projection Matrix",
    "text": "Projection Matrix\nThe projection matrix \\(P\\) projects any vector \\(b\\) onto the column space of \\(A\\).\n\\[\nP=A(A^\\top A)^{-1}A^\\top\n\\]\nKey properties:\n\nIf \\(b\\) is in the column space, \\(Pb=b\\)\nIf \\(b\\) is perpendicular to the column space, \\(Pb=0\\)\n\n\n\n\nProjections Decomposition\n\n\n\\[\np+e=b\n\\]\n\\[\n\\begin{aligned}\nPb &= p \\\\\ne &= (I-P)b\n\\end{aligned}\n\\]\nOrthogonal decomposition:\n\n\\(p\\) is in the column space\n\\(e\\) is in the left null space\n\nThis decomposition shows that any vector \\(b\\) can be split into its projection \\(p\\) and error \\(e\\), which are orthogonal."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture16-least-squares.html#least-squares",
    "href": "Math/MIT18.06/mit1806-lecture16-least-squares.html#least-squares",
    "title": "MIT 18.06 Lecture 16: Projection Matrices and Least Squares",
    "section": "Least Squares",
    "text": "Least Squares\nWhen \\(Ax=b\\) has no solution, least squares finds the \\(x\\) that minimizes the error \\(\\|Ax-b\\|\\).\n\n\n\nLeast Squares Fitting\n\n\nSetup:\n\n\\(b_1, b_2, b_3\\) are not on the same line, so we cannot solve \\(Ax=b\\) exactly\n\\(p_1, p_2, p_3\\) are on the same line - the best fit line\n\nObjective:\n\\[\n\\arg\\min\\|Ax-b\\|^2 = e_1^2+e_2^2+e_3^2\n\\]\nExample: Fit a line \\(C+Dt\\) to points \\((1,1)\\), \\((2,2)\\), \\((3,2)\\):\n\\[\n\\begin{aligned}\nC+D &= 1 \\\\\nC+2D &= 2 \\\\\nC+3D &= 2\n\\end{aligned}\n\\]\nThis is the equation \\(Ax=b\\):\n\\[\n\\begin{bmatrix}1&1\\\\1&2\\\\1&3\\end{bmatrix} \\begin{bmatrix}C\\\\D\\end{bmatrix}=\\begin{bmatrix}1\\\\2\\\\2\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture16-least-squares.html#finding-the-best-fit",
    "href": "Math/MIT18.06/mit1806-lecture16-least-squares.html#finding-the-best-fit",
    "title": "MIT 18.06 Lecture 16: Projection Matrices and Least Squares",
    "section": "Finding the Best Fit",
    "text": "Finding the Best Fit\nGoals:\n\nFind \\(\\hat{x}=\\begin{bmatrix}\\hat{C}\\\\\\hat{D}\\end{bmatrix}\\), the best fit line (not the perfect line)\nFind the projection \\(P\\)\n\nNormal equation:\n\\[\nA^\\top A\\hat{x}=A^\\top b\n\\]\nThis is called the normal equation because \\(A^\\top(A\\hat{x}-b)=0\\) means the error is normal (perpendicular) to the column space.\nComputation:\n\\(A^\\top A\\):\n\\[\n\\begin{bmatrix}1&1&1\\\\1&2&3\\end{bmatrix}\\begin{bmatrix}1&1\\\\1&2\\\\1&3\\end{bmatrix} =\\begin{bmatrix}3&6\\\\6&14\\end{bmatrix}\n\\]\n\\(A^\\top b\\):\n\\[\n\\begin{bmatrix}1&1&1\\\\1&2&3\\end{bmatrix}\\begin{bmatrix}1\\\\2\\\\2\\end{bmatrix}=\\begin{bmatrix}5\\\\11\\end{bmatrix}\n\\]\nSystem:\n\\[\n\\begin{aligned}\n3C+6D &= 5 \\\\\n6C+14D &= 11\n\\end{aligned}\n\\]\nSolution: \\(C=\\frac{2}{3}\\), \\(D=\\frac{1}{2}\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture16-least-squares.html#calculating-projections-and-errors",
    "href": "Math/MIT18.06/mit1806-lecture16-least-squares.html#calculating-projections-and-errors",
    "title": "MIT 18.06 Lecture 16: Projection Matrices and Least Squares",
    "section": "Calculating Projections and Errors",
    "text": "Calculating Projections and Errors\nProjections:\n\n\\(p_1=\\frac{2}{3}+\\frac{1}{2}=\\frac{7}{6}\\)\n\\(p_2=\\frac{2}{3}+1=\\frac{5}{3}\\)\n\\(p_3=\\frac{2}{3}+\\frac{3}{2}=\\frac{13}{6}\\)\n\nErrors:\n\n\\(e_1=1-\\frac{7}{6}=-\\frac{1}{6}\\)\n\\(e_2=2-\\frac{5}{3}=\\frac{1}{3}\\)\n\\(e_3=2-\\frac{13}{6}=-\\frac{1}{6}\\)\n\nVerify perpendicularity:\n\n\\(p=\\begin{bmatrix}\\frac{7}{6}\\\\\\frac{5}{3}\\\\\\frac{13}{6}\\end{bmatrix}\\)\n\\(e=\\begin{bmatrix}-\\frac{1}{6}\\\\\\frac{1}{3}\\\\-\\frac{1}{6}\\end{bmatrix}\\)\n\n\n\n\nPerpendicular Vectors"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture16-least-squares.html#preview-orthonormal-matrices",
    "href": "Math/MIT18.06/mit1806-lecture16-least-squares.html#preview-orthonormal-matrices",
    "title": "MIT 18.06 Lecture 16: Projection Matrices and Least Squares",
    "section": "Preview: Orthonormal Matrices",
    "text": "Preview: Orthonormal Matrices\nThis is a preview for the next lecture.\nThe vectors \\([0,0,1]\\), \\([0,1,0]\\), \\([1,0,0]\\) are orthonormal unit vectors (perpendicular to each other with unit length).\nIf we arrange them as columns in matrix \\(Q\\):\n\\[\nQ^\\top Q=I_n\n\\]\nAnother orthonormal matrix: The rotation matrix:\n\\[\n\\begin{bmatrix}\\cos\\theta&-\\sin\\theta\\\\\\sin\\theta &\\cos\\theta\\end{bmatrix}\n\\]\nProof that columns are unit vectors:\n\n\\((\\cos\\theta)^2+(\\sin\\theta)^2=1\\)\n\\((-\\sin\\theta)^2+(\\cos\\theta)^2=1\\)\n\nProof of perpendicularity:\n\n\\(q_1^\\top q_2 = \\cos\\theta(-\\sin\\theta) + \\sin\\theta(\\cos\\theta) = 0\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture16-least-squares.html#connection-to-calculus",
    "href": "Math/MIT18.06/mit1806-lecture16-least-squares.html#connection-to-calculus",
    "title": "MIT 18.06 Lecture 16: Projection Matrices and Least Squares",
    "section": "Connection to Calculus",
    "text": "Connection to Calculus\nFrom the error minimization perspective, we have the error function:\n\\[\n(C+D-1)^2+(C+2D-2)^2+(C+3D-2)^2\n\\]\n\n\n\nCalculus Perspective\n\n\nWe can find the minimum by setting the gradient to zero for this positive definite function.\nThe professor briefly introduced this approach to show the connection between linear algebra and calculus.\n\nSource: MIT 18.06SC Linear Algebra, Lecture 16"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture20-cramers-rule.html#inverse-matrix-formula",
    "href": "Math/MIT18.06/mit1806-lecture20-cramers-rule.html#inverse-matrix-formula",
    "title": "MIT 18.06SC Lecture 20: Cramer‚Äôs Rule, Inverse Matrix and Volume",
    "section": "Inverse Matrix Formula",
    "text": "Inverse Matrix Formula\nThe inverse of a matrix can be computed using cofactors and determinants, providing an explicit formula that reveals the geometric structure behind matrix inversion.\n\n2√ó2 Matrices\n\\[\n\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}^{-1}=\\frac{1}{ad-bc}\\begin{bmatrix}d&-b\\\\-c&a\\end{bmatrix}\n\\]\nThe general formula for the inverse is:\n\\[\nA^{-1}=\\frac{1}{\\det A}C^\\top\n\\]\nwhere \\(C\\) is the cofactor matrix.\n\n\nWhy Does This Formula Work?\nWe need to prove that \\(AC^\\top=(\\det A)I\\).\n\\[\n\\begin{bmatrix}a_{11}&\\cdots&a_{1n}\\\\.&.&.\\\\a_{n1}&\\cdots&a_{nn}\\end{bmatrix}\n\\begin{bmatrix}C_{11}&\\cdots&C_{n1}\\\\.&.&.\\\\C_{1n}&\\cdots&C_{nn}\\end{bmatrix}\n\\]\nProof:\nDiagonal entries: From the cofactor formula, we have:\n\\[\na_{11}C_{11}+a_{12}C_{12}+\\cdots+a_{1n}C_{1n}=\\det A\n\\]\nSo the diagonal entries of \\(AC^\\top\\) are \\(\\det A\\).\nOff-diagonal entries: Row \\(i\\) of \\(A\\) times column \\(j\\) of \\(C^\\top\\) (where \\(i \\neq j\\)) is always 0.\nProof of off-diagonal being zero:\n\nConsider matrix \\(A=\\begin{bmatrix}A_1\\\\A_2\\\\A_3\\end{bmatrix}\\) and cofactor matrix \\(C=\\begin{bmatrix}C_1\\\\C_2\\\\C_3\\end{bmatrix}\\)\nIf we compute \\(A_1 \\cdot C_2^\\top\\), construct a modified matrix:\n\n\\[\nA_s=\\begin{bmatrix}A_1\\\\A_1\\\\A_3\\end{bmatrix}\n\\]\n\nThen \\(|A_s|\\) equals \\(A_1 \\cdot C_2^\\top\\), because \\(C_2\\) is the cofactor computed by removing row 2, and now row 2 is replaced by \\(A_1\\)\nSince \\(A_s\\) has two identical rows, \\(|A_s|=0\\)\nTherefore, all off-diagonal entries are 0\n\nThis proves \\(AC^\\top=(\\det A)I\\), which gives us \\(A^{-1}=\\frac{1}{\\det A}C^\\top\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture20-cramers-rule.html#cramers-rule",
    "href": "Math/MIT18.06/mit1806-lecture20-cramers-rule.html#cramers-rule",
    "title": "MIT 18.06SC Lecture 20: Cramer‚Äôs Rule, Inverse Matrix and Volume",
    "section": "Cramer‚Äôs Rule",
    "text": "Cramer‚Äôs Rule\nStarting from:\n\\[\nAx=b\\\\\nx=A^{-1}b=\\frac{1}{\\det A}C^\\top b\n\\]\nCramer‚Äôs Rule states:\n\\[\nx_j=\\frac{\\det B_j}{\\det A}\n\\]\nwhere \\(B_j\\) is matrix \\(A\\) with column \\(j\\) replaced by vector \\(b\\).\nWhile this formula is mathematically elegant, it requires computing determinants for \\(n+1\\) matrices. In practice, elimination remains the more efficient method for solving \\(Ax=b\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture20-cramers-rule.html#determinant-as-volume",
    "href": "Math/MIT18.06/mit1806-lecture20-cramers-rule.html#determinant-as-volume",
    "title": "MIT 18.06SC Lecture 20: Cramer‚Äôs Rule, Inverse Matrix and Volume",
    "section": "Determinant as Volume",
    "text": "Determinant as Volume\n\\(|\\det A|\\) equals the volume of a box (parallelepiped).\nThe absolute value of the determinant of an \\(n \\times n\\) matrix equals the n-dimensional volume of the parallelotope spanned by its column vectors.\n\n\n\nDeterminant as Volume\n\n\n\nSign of the Determinant\nThe sign is determined by the orientation (handedness) of the box:\n\nPositive for right-handed orientation\nNegative for left-handed orientation\n\n\n\nIdentity Matrix\n\\[\nA=I\n\\]\nThe identity matrix spans a unit cube with edge length 1, so the volume is 1. Correspondingly, \\(\\det I=1\\).\n\n\nOrthogonal Matrix\nFor an orthogonal matrix \\(Q\\):\n\\[\n|\\det Q|=1\n\\]\nProof:\n\\[\n|QQ^\\top|=|I|=1\\\\\n|QQ^\\top|=|Q||Q^\\top|=|Q|^2=1\n\\]\nBased on the determinant property \\(|AB|=|A||B|\\), we have \\(|Q|^2=1\\), so \\(|Q|=\\pm1\\).\nThis means orthogonal transformations preserve volume.\n\n\nArea of Parallelogram\n\n\n\nDeterminant Area Parallelogram\n\n\nFor a 2D parallelogram formed by vectors \\(\\begin{bmatrix}a\\\\c\\end{bmatrix}\\) and \\(\\begin{bmatrix}b\\\\d\\end{bmatrix}\\):\n\\[\n\\text{Area} = |ad-bc| = \\left|\\begin{vmatrix}a&b\\\\c&d\\end{vmatrix}\\right|\n\\]\n\n\nTriangle Area\nThe area of a triangle is half the area of the parallelogram:\n\\[\n\\text{Triangle Area} = \\left|\\frac{1}{2}(ad-bc)\\right|\n\\]\nWhen we only have the coordinates of the 3 vertices of the triangle \\((x_1, y_1)\\), \\((x_2, y_2)\\), \\((x_3, y_3)\\), we have 3 vectors in a 2D world. We can lift the dimension by using:\n\\[\n\\text{Triangle Area} = \\frac{1}{2}\\left|\\begin{vmatrix}x_1&y_1&1\\\\x_2&y_2&1\\\\x_3&y_3&1\\end{vmatrix}\\right|\n\\]\nThis projects the triangle into 3D space without changing its area (all points remain in the same plane with \\(z=\\) constant).\n\n\nScaling Property\nDoubling an edge doubles the volume:\n\\[\n\\begin{vmatrix}ta&tb\\\\c&d\\end{vmatrix}=t\\begin{vmatrix}a&b\\\\c&d\\end{vmatrix}\n\\]\nIf we multiply row 1 by \\(t\\) and keep all other rows unchanged, then \\(|A'|=t|A|\\).\nThis reflects the geometric fact that scaling one edge of a parallelotope scales its volume proportionally.\n\nSource: MIT 18.06SC Linear Algebra, Lecture 20"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture18-determinants.html",
    "href": "Math/MIT18.06/mit1806-lecture18-determinants.html",
    "title": "MIT 18.06 Lecture 18: Properties of Determinants",
    "section": "",
    "text": "Every square matrix has a number called the determinant.\n\\[\n\\det A=|A|\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture18-determinants.html#the-ten-properties",
    "href": "Math/MIT18.06/mit1806-lecture18-determinants.html#the-ten-properties",
    "title": "MIT 18.06 Lecture 18: Properties of Determinants",
    "section": "The Ten Properties",
    "text": "The Ten Properties\n\n1. Identity Matrix\n\\[\n\\det I=1\n\\]\n\n\n2. Row Exchange\nExchanging rows changes the sign of the determinant.\nDeterminant of permutation matrices is: - 1, when number of exchanges is even - -1, when number of exchanges is odd\n\\[\n\\begin{vmatrix}1&0\\\\0&1\\end{vmatrix}=1\n\\]\n\\[\n\\begin{vmatrix}0&1\\\\1&0\\end{vmatrix}=-1\n\\]\n\n\n3. Linearity in Each Row\nThe determinant is linear in each row separately.\n\\[\n\\begin{vmatrix}a&b\\\\c&d\\end{vmatrix}=ad-bc\n\\]\n\\[\n\\begin{vmatrix}ta&tb\\\\c&d\\end{vmatrix}= t\\begin{vmatrix}a&b\\\\c&d\\end{vmatrix} \\tag{3a}\n\\]\n\\[\n\\begin{vmatrix}a+a'&b+b'\\\\c&d\\end{vmatrix}=\\begin{vmatrix}a&b\\\\c&d\\end{vmatrix}+ \\begin{vmatrix}a'&b'\\\\c&d\\end{vmatrix} \\tag{3b}\n\\]\n\n\n4. Equal Rows\nTwo equal rows lead to determinant 0.\nProof: If two rows are equal, exchanging them doesn‚Äôt change the matrix. But by property 2, exchanging rows changes the sign of the determinant. Therefore \\(\\det A = -\\det A\\), which means \\(\\det A = 0\\).\n\n\n5. Row Operations\nSubtracting \\(l \\times \\text{row}_i\\) from \\(\\text{row}_k\\) doesn‚Äôt change the determinant.\n\\[\n\\begin{vmatrix}a&b\\\\c-la&d-lb\\end{vmatrix}=\\begin{vmatrix}a&b\\\\c&d\\end{vmatrix}+\\begin{vmatrix}a&b\\\\-la&-lb\\end{vmatrix}=\\\\\n\\begin{vmatrix}a&b\\\\c&d\\end{vmatrix}-l\\begin{vmatrix}a&b\\\\a&b\\end{vmatrix}\n\\]\nThe last term is zero by property 4 (equal rows), so the determinant remains unchanged.\n\n\n6. Zero Rows\nRows of zeros lead to determinant 0.\n\\[\n5\\begin{vmatrix}0&0\\\\c&d\\end{vmatrix}=\\begin{vmatrix}5 \\times 0&5 \\times 0\\\\c&d\\end{vmatrix}= \\begin{vmatrix}0&0\\\\c&d\\end{vmatrix}\n\\]\nIf \\(5 \\times n=n\\), then \\(n=0\\).\n\n\n7. Triangular Matrix\nThe determinant of a triangular matrix is the product of the diagonal elements (pivots).\n\\[\n|U|= \\begin{vmatrix}d_1&*&*&*\\\\0&d_2&*&*\\\\0&0&d_3&*\\\\0&0&0&d_4\\end{vmatrix}=d_1 \\times d_2 \\times ... \\times d_n\n\\]\nFrom property 5, we can change an upper triangular matrix to a diagonal matrix with the same determinant. Then using property 3a, we can factor out each diagonal element:\n\\[\n|U|=d_1 \\times d_2 \\times ... \\times d_n\\begin{vmatrix}1&0&0&0\\\\0&1&0&0\\\\0&0&\\ddots&0\\\\0&0&0&1\\end{vmatrix}=d_1 \\times d_2 \\times ... \\times d_n\n\\]\n\n\n8. Singular Matrix\nThe determinant is 0 exactly when A is singular.\nWhen the determinant is not 0, A is invertible.\n\n\n9. Product of Matrices\n\\[\n\\det AB=(\\det A)(\\det B)\n\\]\nThis property leads to:\n\\[\n\\det A^{-1}=\\frac{1}{\\det A}\n\\]\n\\[\n\\det A^2=(\\det A)^2\n\\]\n\\[\n\\det 2A=2^n (\\det A)\n\\]\n\n\n10. Transpose\n\\[\n\\det A^\\top=\\det A\n\\]\nProof:\n\\[\n|A|=|LU|\n\\]\n\\[\n|A^\\top|=|U^\\top L^\\top|\n\\]\nBecause both \\(L\\) and \\(U\\) are triangular matrices, the determinant is just the product of diagonal elements, so the transpose doesn‚Äôt change the result.\n\nSource: MIT 18.06SC Linear Algebra, Lecture 18"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#dimension-of-subspaces",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#dimension-of-subspaces",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "1. Dimension of Subspaces",
    "text": "1. Dimension of Subspaces\nProblem: Suppose \\(u, v, w\\) are non-zero vectors in \\(\\mathbb{R}^7\\). They span a subspace of \\(\\mathbb{R}^7\\). What are the possible dimensions?\nAnswer: 1, 2, or 3\nExplanation:\n\nMinimum dimension: 1 (if all vectors are scalar multiples of each other)\nMaximum dimension: 3 (if all three vectors are linearly independent)\nMiddle case: 2 (if exactly two are independent)\nCannot be 0 (all vectors are non-zero)\nCannot exceed 3 (only three vectors available)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#null-space-dimensions",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#null-space-dimensions",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "2. Null Space Dimensions",
    "text": "2. Null Space Dimensions\nProblem: We have \\(A\\), a \\(5 \\times 3\\) matrix with rank \\(r = 3\\). What is the null space?\nAnswer: \\(N(A) = \\{\\mathbf{0}\\}\\) (just the zero vector)\nCalculation:\n\\[\n\\dim(N(A)) = n - r = 3 - 3 = 0\n\\]\nInterpretation: Since the rank equals the number of columns, all columns are independent. The only solution to \\(Ax = \\mathbf{0}\\) is \\(x = \\mathbf{0}\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#echelon-forms-with-block-matrices",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#echelon-forms-with-block-matrices",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "3. Echelon Forms with Block Matrices",
    "text": "3. Echelon Forms with Block Matrices\n\nProblem 3a: Matrix B\nGiven:\n\\[\nB = \\begin{bmatrix}\nu \\\\\n2u\n\\end{bmatrix}\n\\]\nwhere \\(u\\) is a row vector.\nEchelon form:\n\\[\n\\begin{bmatrix}\nu \\\\\n\\mathbf{0}\n\\end{bmatrix}\n\\]\nExplanation: Row 2 is a multiple of row 1, so elimination makes row 2 become zero.\n\n\n\nProblem 3b: Matrix C\nGiven:\n\\[\nC = \\begin{bmatrix}\nu & u \\\\\nu & 0\n\\end{bmatrix}\n\\]\nwhere \\(u\\) is a \\(5 \\times 3\\) matrix with rank 3.\nElimination steps:\n\\[\n\\begin{bmatrix}\nu & u \\\\\nu & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\nu & u \\\\\n0 & -u\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\nu & 0 \\\\\n0 & -u\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\nu & 0 \\\\\n0 & u\n\\end{bmatrix}\n\\]\nStep-by-step:\n\nSubtract row 1 from row 2: eliminates bottom-left block\nSubtract column 1 from column 2: eliminates top-right block\nMultiply row 2 by -1: normalize\n\nRank of C:\n\n\\(u\\) is \\(5 \\times 3\\) with rank 3\n\\(C\\) is \\(10 \\times 6\\) (two \\(5 \\times 3\\) blocks side by side)\nBoth \\(u\\) blocks contribute full rank\nRank of C = 6\n\nDimension of left null space:\n\\[\n\\dim(N(C^T)) = m - r = 10 - 6 = 4\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#complete-solution-to-linear-systems",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#complete-solution-to-linear-systems",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "4. Complete Solution to Linear Systems",
    "text": "4. Complete Solution to Linear Systems\nProblem: Given\n\\[\nAx = \\begin{bmatrix}\n2 \\\\\n4 \\\\\n2\n\\end{bmatrix}, \\quad\nx = \\begin{bmatrix}\n2 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n+ c\\begin{bmatrix}\n1 \\\\\n1 \\\\\n0\n\\end{bmatrix}\n+ d\\begin{bmatrix}\n0 \\\\\n0 \\\\\n1\n\\end{bmatrix}\n\\]\n\nPart a: Dimension of Row Space\nAnswer: 1\nReasoning:\n\nThe null space has dimension 2 (two free variables: \\(c\\) and \\(d\\))\n\\(\\dim(N(A)) = n - r = 2\\)\nSince \\(n = 3\\) (three columns), \\(r = 3 - 2 = 1\\)\nRow space dimension = rank = 1\n\n\n\n\nPart b: Find Matrix A\nStrategy: Use the particular solution and null space vectors.\nColumn 1:\n\\[\nA\\begin{bmatrix}\n2 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n= \\begin{bmatrix}\n2 \\\\\n4 \\\\\n2\n\\end{bmatrix}\n\\implies \\text{Column 1} = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n1\n\\end{bmatrix}\n\\]\nColumn 3:\n\\[\nA\\begin{bmatrix}\n0 \\\\\n0 \\\\\n1\n\\end{bmatrix}\n= \\mathbf{0}\n\\implies \\text{Column 3} = \\begin{bmatrix}\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n\\]\nColumn 2:\n\\[\nA\\begin{bmatrix}\n1 \\\\\n1 \\\\\n0\n\\end{bmatrix}\n= \\mathbf{0}\n\\implies \\text{Column 1} + \\text{Column 2} = \\mathbf{0}\n\\]\nTherefore, Column 2 = \\(-\\)Column 1:\n\\[\n\\text{Column 2} = \\begin{bmatrix}\n-1 \\\\\n-2 \\\\\n-1\n\\end{bmatrix}\n\\]\nMatrix A:\n\\[\nA = \\begin{bmatrix}\n1 & -1 & 0 \\\\\n2 & -2 & 0 \\\\\n1 & -1 & 0\n\\end{bmatrix}\n\\]\n\n\n\nPart c: Which b Can Be Solved?\nAnswer: \\(b\\) must be a multiple of \\(\\begin{bmatrix}1 \\\\ 2 \\\\ 1\\end{bmatrix}\\)\nReasoning:\n\nColumn space of \\(A\\) is spanned by \\(\\begin{bmatrix}1 \\\\ 2 \\\\ 1\\end{bmatrix}\\)\nAll columns are multiples of this vector\n\\(Ax = b\\) has a solution only if \\(b \\in C(A)\\)\n\nTherefore: \\(b = c\\begin{bmatrix}1 \\\\ 2 \\\\ 1\\end{bmatrix}\\) for some scalar \\(c\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#properties-of-null-spaces",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#properties-of-null-spaces",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "5. Properties of Null Spaces",
    "text": "5. Properties of Null Spaces\nProblem: If the null space is \\(\\{\\mathbf{0}\\}\\) and \\(A\\) is square, what is the null space of \\(A^T\\)?\nAnswer: \\(N(A^T) = \\{\\mathbf{0}\\}\\)\nProof:\n\n\\(A\\) is \\(n \\times n\\) (square)\n\\(N(A) = \\{\\mathbf{0}\\}\\) means \\(\\dim(N(A)) = 0\\)\nTherefore: \\(\\text{rank}(A) = n - 0 = n\\)\nSince \\(A\\) is square with full rank, \\(A\\) is invertible\nFor \\(A^T\\): \\(\\dim(N(A^T)) = m - r = n - n = 0\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#subspaces-of-matrix-spaces",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#subspaces-of-matrix-spaces",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "6. Subspaces of Matrix Spaces",
    "text": "6. Subspaces of Matrix Spaces\nProblem: Consider the space of all \\(5 \\times 5\\) matrices (dimension 25). Do the invertible \\(5 \\times 5\\) matrices form a subspace?\nAnswer: No\nReason: Not closed under addition.\nCounterexample:\n\\[\nA = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}, \\quad\nB = \\begin{bmatrix}\n-1 & 0 \\\\\n0 & -1\n\\end{bmatrix}\n\\]\nBoth \\(A\\) and \\(B\\) are invertible, but:\n\\[\nA + B = \\begin{bmatrix}\n0 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\\]\nis not invertible."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#nilpotent-matrices",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#nilpotent-matrices",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "7. Nilpotent Matrices",
    "text": "7. Nilpotent Matrices\nProblem: If \\(B^2 = 0\\), must \\(B = \\mathbf{0}\\)?\nAnswer: False\nCounterexample:\n\\[\nB = \\begin{bmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n\\]\nVerification:\n\\[\nB^2 = \\begin{bmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n= \\begin{bmatrix}\n0 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\\]\nExplanation: Rows times columns can all be zero even when the matrix itself is non-zero."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#solvability-of-square-systems",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#solvability-of-square-systems",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "8. Solvability of Square Systems",
    "text": "8. Solvability of Square Systems\nProblem: If an \\(n \\times n\\) matrix has rank \\(n\\), does \\(Ax = b\\) always have a solution?\nAnswer: Yes\nProof:\n\nRank \\(n\\) for an \\(n \\times n\\) matrix means \\(A\\) is invertible\nTherefore: \\(x = A^{-1}b\\) always exists\nEvery \\(b \\in \\mathbb{R}^n\\) is in the column space"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#null-space-from-matrix-factorization",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#null-space-from-matrix-factorization",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "9. Null Space from Matrix Factorization",
    "text": "9. Null Space from Matrix Factorization\nProblem: Given\n\\[\nB = \\begin{bmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n1 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 & -1 & 2 \\\\\n0 & 1 & 1 & -1 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nWithout multiplication, find the basis for \\(N(B)\\).\n\nAnalysis\nDimensions:\n\nLeft matrix: \\(3 \\times 3\\), full rank (invertible)\nRight matrix: \\(3 \\times 4\\), rank 2\n\\(B\\): \\(3 \\times 4\\)\n\nKey insight: \\(\\operatorname{rank}(AB) \\leq \\min(\\operatorname{rank}(A), \\operatorname{rank}(B))\\)\nTherefore: \\(\\operatorname{rank}(B) = 2\\)\nNull space dimension:\n\\[\n\\dim(N(B)) = n - r = 4 - 2 = 2\n\\]\nFinding basis: Since the left matrix is invertible, \\(N(B) = N(\\text{right matrix})\\)\nFrom the right matrix in RREF:\n\nPivot columns: 1, 2\nFree variables: columns 3, 4\n\nBasis for N(B):\n\\[\nc_1\\begin{bmatrix}\n1 \\\\\n-1 \\\\\n1 \\\\\n0\n\\end{bmatrix}\n+ c_2\\begin{bmatrix}\n-2 \\\\\n1 \\\\\n0 \\\\\n1\n\\end{bmatrix}\n\\]\nVerification:\n\nColumn 3 = \\(1 \\cdot\\)Column 1 + \\((-1) \\cdot\\)Column 2\nColumn 4 = \\((-2) \\cdot\\)Column 1 + \\(1 \\cdot\\)Column 2\n\n\n\n\nSolving Bx = b\nProblem: Solve \\(Bx = \\begin{bmatrix}1 \\\\ 0 \\\\ 1\\end{bmatrix}\\)\nAnalysis:\n\\[\nB_{\\text{col1}} = [1, 0, 1]^T \\\\\nB_{\\text{col2}} = [1, 1, 0]^T \\\\\nB_{\\text{col3}} = [0, 1, -1]^T \\\\\nB_{\\text{col4}} = [1, -1, 2]^T\n\\]\nParticular solution: \\(x_p = \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix}\\)\nComplete solution:\n\\[\nx = \\begin{bmatrix}\n1 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n+ c_1\\begin{bmatrix}\n1 \\\\\n-1 \\\\\n1 \\\\\n0\n\\end{bmatrix}\n+ c_2\\begin{bmatrix}\n-2 \\\\\n1 \\\\\n0 \\\\\n1\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#conceptual-questions",
    "href": "Math/MIT18.06/mit1806-lecture13-quiz-review.html#conceptual-questions",
    "title": "Lecture 13: Quiz 1 Review",
    "section": "10. Conceptual Questions",
    "text": "10. Conceptual Questions\n\nQuestion 1: Do A and -A Share the Same Four Fundamental Subspaces?\nAnswer: Yes\nProof:\n\n\\(C(A) = C(-A)\\) (columns are just scaled by -1)\n\\(N(A) = N(-A)\\) (if \\(Ax = \\mathbf{0}\\), then \\((-A)x = \\mathbf{0}\\))\n\\(C(A^T) = C((-A)^T)\\) (row space argument)\n\\(N(A^T) = N((-A)^T)\\) (left null space argument)\n\n\n\n\nQuestion 2: If m = n, Are Row Space and Column Space the Same?\nAnswer: No, not in general\nWhen true: Only when \\(A\\) is symmetric (\\(A = A^T\\))\nCounterexample:\n\\[\nA = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\\]\n\nColumn space: span of \\(\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\\)\nRow space: span of \\(\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\\)\n\n(In this case they happen to be the same, but that‚Äôs coincidental)\nBetter counterexample:\n\\[\nA = \\begin{bmatrix}\n1 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n\\]\n\nColumn space: \\(\\{c\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\\}\\) (in \\(\\mathbb{R}^2\\))\nRow space: \\(\\{c\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}\\}\\) (in \\(\\mathbb{R}^2\\))\n\nDifferent vectors!\n\n\n\nQuestion 3: If A and B Have the Same Four Subspaces, Is A a Multiple of B?\nAnswer: No\nCounterexample: If \\(A\\) and \\(B\\) are both full rank \\(n \\times n\\) matrices:\n\nThey both have column space = \\(\\mathbb{R}^n\\)\nThey both have null space = \\(\\{\\mathbf{0}\\}\\)\nThey both have row space = \\(\\mathbb{R}^n\\)\nThey both have left null space = \\(\\{\\mathbf{0}\\}\\)\n\nBut \\(A\\) and \\(B\\) can be completely different matrices (e.g., different invertible matrices).\n\n\n\nQuestion 4: If I Change Two Rows of A, Which Subspaces Stay the Same?\nAnswer:\n\n\\(N(A)\\) (null space)\n\\(C(A^T)\\) (row space)\n\nExplanation:\n\nRow operations preserve the null space\nRow operations preserve the row space (just produce different linear combinations)\nColumn space and left null space will generally change\n\n\n\n\nQuestion 5: Can Vector [1, 2, 3] Be in Both Null Space and Row Space?\nAnswer: No (assuming non-zero matrix)\nReason:\n\nIf \\(v\\) is in the null space: \\(Av = \\mathbf{0}\\)\nIf \\(v\\) is also a row of \\(A\\): then \\(Av\\) includes the dot product \\(v \\cdot v = \\|v\\|^2 &gt; 0\\)\nThis is a contradiction\n\nKey insight: Null space and row space are orthogonal complements in \\(\\mathbb{R}^n\\).\n\nSource: MIT 18.06SC Linear Algebra, Lecture 13"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#overview",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#overview",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "Overview",
    "text": "Overview\nThis lecture connects linear algebra to graph theory and electrical networks:\n\nIncidence matrices: Representing graphs with matrices\nFour fundamental subspaces: Applied to graphs (loops and potential differences)\nOhm‚Äôs law: Relating currents to potential differences\nKirchhoff‚Äôs laws: Current conservation and voltage laws\nEuler‚Äôs formula: Relationship between nodes, edges, and loops"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#graph-representation-with-incidence-matrix",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#graph-representation-with-incidence-matrix",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "1. Graph Representation with Incidence Matrix",
    "text": "1. Graph Representation with Incidence Matrix\n\nExample Graph\n\n\n\nGraph with 4 nodes and 5 edges\n\n\nGraph structure:\n\n4 nodes (vertices)\n5 edges\n\n\n\nIncidence Matrix\n\\[\nA = \\begin{bmatrix}\n-1 & 1 & 0 & 0 \\\\\n0 & -1 & 1 & 0 \\\\\n-1 & 0 & 1 & 0 \\\\\n0 & 0 & -1 & 1 \\\\\n-1 & 0 & 0 & 1\n\\end{bmatrix}\n\\]\nDimensions: \\(A\\) is \\(5 \\times 4\\) (m √ó n matrix)\n\n\\(m = 5\\): number of edges (rows)\n\\(n = 4\\): number of nodes (columns)\n\nStructure:\n\nEach row represents an edge\nEach edge has exactly one \\(-1\\) (starting node) and one \\(+1\\) (ending node)\nThe rest are zeros\n\nExample: First row \\([-1, 1, 0, 0]\\) represents an edge from node 1 to node 2."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#null-space-of-a-constant-potentials",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#null-space-of-a-constant-potentials",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "2. Null Space of \\(A\\): Constant Potentials",
    "text": "2. Null Space of \\(A\\): Constant Potentials\n\nFinding \\(N(A)\\)\nTo find \\(N(A)\\), we solve \\(Ax = \\mathbf{0}\\):\n\\[\n\\begin{bmatrix}\n-1 & 1 & 0 & 0 \\\\\n0 & -1 & 1 & 0 \\\\\n-1 & 0 & 1 & 0 \\\\\n0 & 0 & -1 & 1 \\\\\n-1 & 0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4\n\\end{bmatrix}=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n\\]\nThis gives us the system:\n\nRow 1: \\(-x_1 + x_2 = 0\\) ‚Üí \\(x_1 = x_2\\)\nRow 2: \\(-x_2 + x_3 = 0\\) ‚Üí \\(x_2 = x_3\\)\nRow 4: \\(-x_3 + x_4 = 0\\) ‚Üí \\(x_3 = x_4\\)\n\nFrom these equations: \\(x_1 = x_2 = x_3 = x_4\\)\nSolution: \\[\nN(A) = c \\begin{bmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n1\n\\end{bmatrix}\n\\]\nDimension: \\(\\dim(N(A)) = 1\\)\nInterpretation:\n\nThe null space represents constant potentials across all nodes\nIf all nodes have the same potential, there is no voltage difference across any edge\nThis corresponds to setting all nodes to the same ‚Äúground level‚Äù\n\nRank calculation: \\[\n\\text{rank}(A) = n - \\dim(N(A)) = 4 - 1 = 3\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#left-null-space-of-a-loops",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#left-null-space-of-a-loops",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "3. Left Null Space of \\(A\\): Loops",
    "text": "3. Left Null Space of \\(A\\): Loops\n\nTranspose Matrix\n\\[\nA^T = \\begin{bmatrix}\n-1 & 0 & -1 & 0 & -1 \\\\\n1 & -1 & 0 & 0 & 0 \\\\\n0 & 1 & 1 & -1 & 0 \\\\\n0 & 0 & 0 & 1 & 1\n\\end{bmatrix}\n\\]\nDimensions: \\(4 \\times 5\\) (n √ó m matrix)\nStructure:\n\nEach column represents an edge\nEach row represents a node\nRow \\(i\\) shows: which edges flow out of node \\(i\\) (-1) and which flow in (+1)\n\nNode balance:\n\nNode 1: 3 edges out (edges 1, 3, 5)\nNode 2: 1 edge in (edge 1), 1 edge out (edge 2)\nNode 3: 2 edges in (edges 2, 3), 1 edge out (edge 4)\nNode 4: 2 edges in (edges 4, 5)\n\nKey property: Each column sums to \\(-1 + 1 = 0\\).\n\n\nFinding \\(N(A^T)\\)\nDimension:\n\\[\n\\dim(N(A^T)) = m - r = 5 - 3 = 2\n\\]\nSystem of equations: \\(A^T y = \\mathbf{0}\\) gives:\n\\[\n\\begin{aligned}\ny_1 + y_3 + y_5 &= 0 \\\\\ny_1 - y_2 &= 0 \\\\\ny_2 + y_3 - y_4 &= 0 \\\\\ny_4 + y_5 &= 0\n\\end{aligned}\n\\]\nSolution process:\n\nSet \\(y_1 = 1\\), then \\(y_2 = 1\\) (from equation 2)\nSet \\(y_3 = -1\\) to satisfy equation 1 (with \\(y_5 = 0\\))\nThis gives the first loop: edges 1, 2, 3\n\nFor the second loop:\n\nSet \\(y_4 = 1\\), then \\(y_5 = -1\\) (from equation 4)\nSet \\(y_1 = y_2 = y_3 = 0\\)\nThis gives the second loop: edges 4, 5\n\nBasis for \\(N(A^T)\\):\n\\[\nN(A^T) = c_1 \\begin{bmatrix}\n1 \\\\\n1 \\\\\n-1 \\\\\n0 \\\\\n0\n\\end{bmatrix} + c_2 \\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n1 \\\\\n-1\n\\end{bmatrix}\n\\]\nInterpretation:\n\nLoop 1: edges 1 ‚Üí 2 ‚Üí 3 form a cycle\nLoop 2: edges 4 ‚Üí 5 form a cycle\nThe \\(-1\\) entries indicate edges traversed in the opposite direction"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#currents-and-potentials",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#currents-and-potentials",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "4. Currents and Potentials",
    "text": "4. Currents and Potentials\n\nDefinitions\nPotential (\\(u_i\\)):\n\nAssociated with nodes\nRepresents ‚Äúvoltage‚Äù or ‚Äúenergy level‚Äù at each node\n\nCurrent (\\(x_i\\)):\n\nAssociated with edges\nRepresents flow of charge or material along edges"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#ohms-law",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#ohms-law",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "5. Ohm‚Äôs Law",
    "text": "5. Ohm‚Äôs Law\nStatement: The current on an edge is proportional to the potential drop across that edge.\nMathematical form:\n\\[\nx_{ij} = u_i - u_j\n\\]\nwhere:\n\n\\(x_{ij}\\): current on edge from node \\(i\\) to node \\(j\\)\n\\(u_i\\), \\(u_j\\): potentials at nodes \\(i\\) and \\(j\\)\n\nMatrix form:\n\\[\nx = A u\n\\]\nInterpretation:\n\n\\(Au\\) maps node potentials to edge currents\nEach current is the difference in potential between the edge‚Äôs endpoints"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#kirchhoffs-current-law",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#kirchhoffs-current-law",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "6. Kirchhoff‚Äôs Current Law",
    "text": "6. Kirchhoff‚Äôs Current Law\nStatement: The total current flowing into a node equals the total current flowing out.\nMathematical form:\n\\[\nA^T y = \\mathbf{0}\n\\]\nwhere \\(y\\) is the vector of edge currents.\nPhysical interpretation: ‚ÄúIn equals out‚Äù ‚Äî charge conservation at each node.\nConnection to left null space: Kirchhoff‚Äôs current law defines exactly the left null space of \\(A\\), which represents the valid current patterns (loops) in the graph."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#graph-properties",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#graph-properties",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "7. Graph Properties",
    "text": "7. Graph Properties\n\nTree\nDefinition: A graph without loops.\nProperties:\n\nA tree on \\(n\\) nodes has exactly \\(n - 1\\) edges\n\\(\\text{rank}(A) = n - 1\\) for a tree\n\\(\\dim(N(A^T)) = 0\\) (no loops)\n\n\n\nLoops\nNumber of loops:\n\\[\n\\text{number of loops} = \\dim(N(A^T)) = m - r\n\\]\nwhere:\n\n\\(m\\) = number of edges\n\\(r\\) = rank of \\(A\\) = (number of nodes - 1)\n\nFor our example:\n\nNumber of loops = \\(5 - 3 = 2\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#eulers-formula",
    "href": "Math/MIT18.06/mit1806-lecture12-graphs-networks.html#eulers-formula",
    "title": "Lecture 12: Graphs, Networks, and Incidence Matrices",
    "section": "8. Euler‚Äôs Formula",
    "text": "8. Euler‚Äôs Formula\nFormula:\n\\[\n(\\text{nodes}) - (\\text{edges}) + (\\text{loops}) = 1\n\\]\nDerivation using rank-nullity:\n\\[\n\\begin{aligned}\n\\text{loops} &= m - r \\\\\n&= m - (n - 1) \\\\\n&= m - n + 1\n\\end{aligned}\n\\]\nRearranging:\n\\[\nn - m + (\\text{loops}) = 1\n\\]\nFor our example:\n\\[\n4 - 5 + 2 = 1 \\quad \\checkmark\n\\]\nInterpretation: This fundamental relationship connects graph topology to linear algebra through the rank-nullity theorem.\n\nKey concepts:\n\nIncidence matrix \\(A\\): Rows = edges, columns = nodes; entries are \\(-1\\), \\(0\\), \\(+1\\)\n\\(N(A)\\): Constant potentials (dimension = 1)\n\\(N(A^T)\\): Loops in the graph (dimension = number of loops)\nRank: \\(\\text{rank}(A) = n - 1\\) (number of nodes - 1)\nOhm‚Äôs law: \\(x = A^T u\\) (currents from potentials)\nKirchhoff‚Äôs law: \\(A^T y = \\mathbf{0}\\) (current conservation)\nEuler‚Äôs formula: (nodes) - (edges) + (loops) = 1\n\nPhysical applications:\n\nElectrical networks and circuits\nFlow networks and transportation\nCommunication networks\nStructural engineering (trusses)\n\n\nSource: MIT 18.06SC Linear Algebra, Lecture 12"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture29-svd.html",
    "href": "Math/MIT18.06/mit1806-lecture29-svd.html",
    "title": "Lecture 29: Singular Value Decomposition",
    "section": "",
    "text": "This lecture introduces the Singular Value Decomposition (SVD), one of the most important matrix factorizations:\n\nGeometric interpretation: how \\(A\\) maps the row space to the column space\nThe fundamental SVD equation: \\(A = U\\Sigma V^{\\top}\\)\nComputing SVD through \\(A^{\\top}A\\) and \\(AA^{\\top}\\)\nFull rank example with complete computation\nSingular (rank-deficient) example\nGeometric meaning: orthonormal bases for all four fundamental subspaces\n\n\n Figure: The Singular Value Decomposition provides orthonormal bases for all four fundamental subspaces, revealing how a matrix transforms space."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture29-svd.html#overview",
    "href": "Math/MIT18.06/mit1806-lecture29-svd.html#overview",
    "title": "Lecture 29: Singular Value Decomposition",
    "section": "",
    "text": "This lecture introduces the Singular Value Decomposition (SVD), one of the most important matrix factorizations:\n\nGeometric interpretation: how \\(A\\) maps the row space to the column space\nThe fundamental SVD equation: \\(A = U\\Sigma V^{\\top}\\)\nComputing SVD through \\(A^{\\top}A\\) and \\(AA^{\\top}\\)\nFull rank example with complete computation\nSingular (rank-deficient) example\nGeometric meaning: orthonormal bases for all four fundamental subspaces\n\n\n Figure: The Singular Value Decomposition provides orthonormal bases for all four fundamental subspaces, revealing how a matrix transforms space."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture29-svd.html#understanding-ax-what-does-a-matrix-see-and-do",
    "href": "Math/MIT18.06/mit1806-lecture29-svd.html#understanding-ax-what-does-a-matrix-see-and-do",
    "title": "Lecture 29: Singular Value Decomposition",
    "section": "1. Understanding \\(Ax\\): What Does a Matrix See and Do?",
    "text": "1. Understanding \\(Ax\\): What Does a Matrix See and Do?\n\nWhat Can \\(A\\) See? (Input Side)\nWhen computing \\(Ax\\), the matrix \\(A\\) only ‚Äúsees‚Äù the component of \\(x\\) that lies in the row space of \\(A\\).\nKey insight: Any component of \\(x\\) in the nullspace of \\(A\\) is invisible to \\(A\\) and gets mapped to zero.\nMathematically: - Decompose \\(x = x_{\\text{row}} + x_{\\text{null}}\\) where: - \\(x_{\\text{row}} \\in \\text{Row}(A)\\) - \\(x_{\\text{null}} \\in \\text{Null}(A)\\) - Then \\(Ax = A(x_{\\text{row}} + x_{\\text{null}}) = Ax_{\\text{row}} + 0 = Ax_{\\text{row}}\\)\n\n\nWhere Does \\(Ax\\) Go? (Output Side)\nThe result \\(Ax\\) always lies in the column space of \\(A\\), because \\(Ax\\) is a linear combination of the columns of \\(A\\):\n\\[\nAx = x_1 c_1 + x_2 c_2 + \\cdots + x_n c_n\n\\]\nwhere \\(c_i\\) are the columns of \\(A\\).\n\n\nThe Complete Picture\nThe full action of \\(A\\): 1. Extract the projection of \\(x\\) onto the row space 2. Map that projection into the column space\nThis is precisely what SVD makes explicit!\n Figure: Matrix \\(A\\) maps vectors from its row space to its column space. Components in the nullspace vanish."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture29-svd.html#the-svd-equation-av-usigma",
    "href": "Math/MIT18.06/mit1806-lecture29-svd.html#the-svd-equation-av-usigma",
    "title": "Lecture 29: Singular Value Decomposition",
    "section": "2. The SVD Equation: \\(AV = U\\Sigma\\)",
    "text": "2. The SVD Equation: \\(AV = U\\Sigma\\)\n\nFundamental Relationship\nThe singular value decomposition expresses how \\(A\\) maps orthonormal vectors in its row space to scaled orthonormal vectors in its column space:\n\\[\n\\sigma_i u_i = A v_i\n\\]\nfor \\(i = 1, 2, \\ldots, r\\) where \\(r = \\text{rank}(A)\\).\n\n\nMatrix Form\n\\[\nA \\begin{bmatrix} v_1 & v_2 & \\cdots & v_r \\end{bmatrix} = \\begin{bmatrix} u_1 & u_2 & \\cdots & u_r \\end{bmatrix} \\begin{bmatrix} \\sigma_1 & & \\\\ & \\sigma_2 & \\\\ & & \\ddots \\end{bmatrix}\n\\]\nKey properties: - \\(r\\): rank of \\(A\\) - \\(V = [v_1, v_2, \\ldots, v_r]\\): orthonormal basis for row space - \\(U = [u_1, u_2, \\ldots, u_r]\\): orthonormal basis for column space - \\(\\Sigma\\): diagonal matrix of singular values \\(\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r &gt; 0\\)\n\n\nFull SVD Form\nFor an \\(m \\times n\\) matrix \\(A\\) of rank \\(r\\):\n\\[\nA = U \\Sigma V^{\\top}\n\\]\nwhere: - \\(U\\): \\(m \\times m\\) orthogonal matrix (left singular vectors) - \\(\\Sigma\\): \\(m \\times n\\) diagonal matrix (singular values) - \\(V\\): \\(n \\times n\\) orthogonal matrix (right singular vectors)\nSince \\(V\\) is orthogonal: \\(V^{-1} = V^{\\top}\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture29-svd.html#computing-svd-via-atopa-and-aatop",
    "href": "Math/MIT18.06/mit1806-lecture29-svd.html#computing-svd-via-atopa-and-aatop",
    "title": "Lecture 29: Singular Value Decomposition",
    "section": "3. Computing SVD via \\(A^{\\top}A\\) and \\(AA^{\\top}\\)",
    "text": "3. Computing SVD via \\(A^{\\top}A\\) and \\(AA^{\\top}\\)\n\nStrategy\nThe key insight is that: - \\(A^{\\top}A\\) gives us \\(V\\) and \\(\\sigma^2\\) values - \\(AA^{\\top}\\) gives us \\(U\\) and \\(\\sigma^2\\) values\n\n\nDerivation for \\(A^{\\top}A\\)\n\\[\nA^{\\top}A = (U\\Sigma V^{\\top})^{\\top} (U\\Sigma V^{\\top}) = V\\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top}\n\\]\nSince \\(U^{\\top}U = I\\) (orthonormal):\n\\[\nA^{\\top}A = V \\Sigma^{\\top} \\Sigma V^{\\top} = V \\begin{bmatrix} \\sigma_1^2 & & \\\\ & \\sigma_2^2 & \\\\ & & \\ddots \\end{bmatrix} V^{\\top}\n\\]\nConclusion: \\(A^{\\top}A\\) is symmetric with: - Eigenvectors: columns of \\(V\\) (right singular vectors) - Eigenvalues: \\(\\sigma_i^2\\) (squared singular values)\n\n\nDerivation for \\(AA^{\\top}\\)\nSimilarly:\n\\[\nAA^{\\top} = U\\Sigma V^{\\top} V \\Sigma^{\\top} U^{\\top} = U \\Sigma \\Sigma^{\\top} U^{\\top} = U \\begin{bmatrix} \\sigma_1^2 & & \\\\ & \\sigma_2^2 & \\\\ & & \\ddots \\end{bmatrix} U^{\\top}\n\\]\nConclusion: \\(AA^{\\top}\\) is symmetric with: - Eigenvectors: columns of \\(U\\) (left singular vectors) - Eigenvalues: \\(\\sigma_i^2\\) (same squared singular values)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture29-svd.html#example-1-full-rank-matrix",
    "href": "Math/MIT18.06/mit1806-lecture29-svd.html#example-1-full-rank-matrix",
    "title": "Lecture 29: Singular Value Decomposition",
    "section": "4. Example 1: Full Rank Matrix",
    "text": "4. Example 1: Full Rank Matrix\n\nProblem\n\\[\nA = \\begin{bmatrix} 4 & 4 \\\\ -3 & 3 \\end{bmatrix}\n\\]\nGoal: Find \\(V, \\Sigma, U\\) such that \\(A = U\\Sigma V^{\\top}\\).\n\n\nStep 1: Compute \\(A^{\\top}A\\)\n\\[\nA^{\\top}A = \\begin{bmatrix} 4 & -3 \\\\ 4 & 3 \\end{bmatrix} \\begin{bmatrix} 4 & 4 \\\\ -3 & 3 \\end{bmatrix} = \\begin{bmatrix} 25 & 7 \\\\ 7 & 25 \\end{bmatrix}\n\\]\n\n\nStep 2: Find Eigenvalues and Eigenvectors of \\(A^{\\top}A\\)\nEigenvalue 1: \\(\\lambda_1 = 32\\)\n\\[\n\\begin{bmatrix} 25 & 7 \\\\ 7 & 25 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 32 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\\]\nEigenvalue 2: \\(\\lambda_2 = 18\\)\n\\[\n\\begin{bmatrix} 25 & 7 \\\\ 7 & 25 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = 18 \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n\\]\n\n\nStep 3: Form \\(\\Sigma\\) and \\(V\\)\nSingular values:\n\\[\n\\sigma_1 = \\sqrt{32} = 4\\sqrt{2}, \\quad \\sigma_2 = \\sqrt{18} = 3\\sqrt{2}\n\\]\n\\[\n\\Sigma = \\begin{bmatrix} \\sqrt{32} & 0 \\\\ 0 & \\sqrt{18} \\end{bmatrix} = \\begin{bmatrix} 4\\sqrt{2} & 0 \\\\ 0 & 3\\sqrt{2} \\end{bmatrix}\n\\]\nRight singular vectors (normalize eigenvectors):\n\\[\nV = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\end{bmatrix}\n\\]\n\n\nStep 4: Compute \\(AA^{\\top}\\)\n\\[\nAA^{\\top} = \\begin{bmatrix} 4 & 4 \\\\ -3 & 3 \\end{bmatrix} \\begin{bmatrix} 4 & -3 \\\\ 4 & 3 \\end{bmatrix} = \\begin{bmatrix} 32 & 0 \\\\ 0 & 18 \\end{bmatrix}\n\\]\nNote: \\(AA^{\\top}\\) is already diagonal!\n\n\nStep 5: Find Eigenvectors of \\(AA^{\\top}\\)\nEigenvalue 1: \\(\\lambda_1 = 32\\)\n\\[\n\\begin{bmatrix} 32 & 0 \\\\ 0 & 18 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 32 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n\\]\nEigenvalue 2: \\(\\lambda_2 = 18\\)\n\\[\n\\begin{bmatrix} 32 & 0 \\\\ 0 & 18 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = 18 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n\\]\nWait, there‚Äôs a sign issue! Let me check with \\(Av_i = \\sigma_i u_i\\):\nFor consistency, we should have:\n\\[\nu_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad u_2 = \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}\n\\]\n(The sign of \\(u_2\\) must match \\(Av_2 = \\sigma_2 u_2\\).)\nLeft singular vectors:\n\\[\nU = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}\n\\]\n\n\nFinal SVD\n\\[\nA = U\\Sigma V^{\\top} = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\begin{bmatrix} 4\\sqrt{2} & 0 \\\\ 0 & 3\\sqrt{2} \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture29-svd.html#example-2-singular-rank-1-matrix",
    "href": "Math/MIT18.06/mit1806-lecture29-svd.html#example-2-singular-rank-1-matrix",
    "title": "Lecture 29: Singular Value Decomposition",
    "section": "5. Example 2: Singular (Rank 1) Matrix",
    "text": "5. Example 2: Singular (Rank 1) Matrix\n Figure: A rank-1 matrix maps the entire 2D input space onto a 1D line (the column space).\n\nProblem\n\\[\nA = \\begin{bmatrix} 4 & 3 \\\\ 8 & 6 \\end{bmatrix}\n\\]\nObservations: - Rank: 1 (second row is \\(2 \\times\\) first row) - Row space: all multiples of \\([4, 3]\\) - Column space: all multiples of \\([4, 8]\\) (or equivalently \\([1, 2]\\))\n\n\nStep 1: Compute \\(v_1\\) (Normalize Row Space)\nThe row space is spanned by \\([4, 3]\\). Normalize:\n\\[\nv_1 = \\frac{1}{\\sqrt{16 + 9}} \\begin{bmatrix} 4 \\\\ 3 \\end{bmatrix} = \\frac{1}{5} \\begin{bmatrix} 4 \\\\ 3 \\end{bmatrix}\n\\]\n\n\nStep 2: Compute \\(u_1\\) (Normalize Column Space)\nThe column space is spanned by \\([4, 8]\\) or equivalently \\([1, 2]\\). Normalize:\n\\[\nu_1 = \\frac{1}{\\sqrt{1 + 4}} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\frac{1}{\\sqrt{5}} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n\\]\n\n\nStep 3: Compute \\(\\sigma_1\\) via \\(AA^{\\top}\\)\n\\[\nAA^{\\top} = \\begin{bmatrix} 4 & 3 \\\\ 8 & 6 \\end{bmatrix} \\begin{bmatrix} 4 & 8 \\\\ 3 & 6 \\end{bmatrix} = \\begin{bmatrix} 25 & 50 \\\\ 50 & 100 \\end{bmatrix}\n\\]\nEigenvalues: Solve \\(\\det(AA^{\\top} - \\lambda I) = 0\\):\n\\[\n(25 - \\lambda)(100 - \\lambda) - 2500 = \\lambda^2 - 125\\lambda = 0\n\\]\nSo \\(\\lambda_1 = 125, \\lambda_2 = 0\\).\nSingular value:\n\\[\n\\sigma_1 = \\sqrt{125} = 5\\sqrt{5}\n\\]\n\n\nStep 4: Complete \\(V\\) and \\(U\\) (Nullspace Vectors)\nFor \\(V\\): Need \\(v_2 \\perp v_1\\), so:\n\\[\nv_2 = \\frac{1}{5} \\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix}\n\\]\nFor \\(U\\): Need \\(u_2 \\perp u_1\\), so:\n\\[\nu_2 = \\frac{1}{\\sqrt{5}} \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\n\\]\n\n\nFinal SVD\n\\[\nA = U\\Sigma V^{\\top} = \\frac{1}{\\sqrt{5}} \\begin{bmatrix} 1 & 2 \\\\ 2 & -1 \\end{bmatrix} \\begin{bmatrix} 5\\sqrt{5} & 0 \\\\ 0 & 0 \\end{bmatrix} \\frac{1}{5} \\begin{bmatrix} 4 & 3 \\\\ 3 & -4 \\end{bmatrix}\n\\]\nNote: \\(\\Sigma\\) has a zero on the diagonal because \\(\\text{rank}(A) = 1\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture29-svd.html#what-does-svd-tell-us-geometric-interpretation",
    "href": "Math/MIT18.06/mit1806-lecture29-svd.html#what-does-svd-tell-us-geometric-interpretation",
    "title": "Lecture 29: Singular Value Decomposition",
    "section": "6. What Does SVD Tell Us? Geometric Interpretation",
    "text": "6. What Does SVD Tell Us? Geometric Interpretation\nThe SVD provides orthonormal bases for all four fundamental subspaces:\n\nRow Space and Column Space (Active Part)\n\n\\(v_1, v_2, \\ldots, v_r\\): orthonormal basis for row space of \\(A\\)\n\\(u_1, u_2, \\ldots, u_r\\): orthonormal basis for column space of \\(A\\)\n\nThe matrix \\(A\\) maps each \\(v_i\\) to \\(\\sigma_i u_i\\):\n\\[\nAv_i = \\sigma_i u_i\n\\]\nThe singular values \\(\\sigma_i\\) measure how much \\(A\\) stretches along each direction.\n\n\nNullspaces (Inactive Part)\n\n\\(v_{r+1}, \\ldots, v_n\\): orthonormal basis for nullspace of \\(A\\)\n\\(u_{r+1}, \\ldots, u_m\\): orthonormal basis for left nullspace (nullspace of \\(A^{\\top}\\))\n\nThese vectors get mapped to zero:\n\\[\nAv_i = 0 \\quad \\text{for } i &gt; r\n\\]\n\n\nComplete Picture\n\\[\n\\underbrace{\\begin{bmatrix} | & | & & | & | & & | \\\\ u_1 & u_2 & \\cdots & u_r & u_{r+1} & \\cdots & u_m \\\\ | & | & & | & | & & | \\end{bmatrix}}_{U \\text{ (column space)} \\oplus \\text{(left null)}} \\underbrace{\\begin{bmatrix} \\sigma_1 & & & & \\\\ & \\ddots & & & \\\\ & & \\sigma_r & & \\\\ & & & 0 & \\\\ & & & & \\ddots \\end{bmatrix}}_{\\Sigma} \\underbrace{\\begin{bmatrix} - & v_1^{\\top} & - \\\\ & \\vdots & \\\\ - & v_r^{\\top} & - \\\\ - & v_{r+1}^{\\top} & - \\\\ & \\vdots & \\\\ - & v_n^{\\top} & - \\end{bmatrix}}_{V^{\\top} \\text{ (row space)} \\oplus \\text{(null)}}\n\\]\nKey insight: SVD gives the best orthonormal coordinates for understanding how \\(A\\) transforms space: - Input coordinates: \\(v_i\\) (rows of \\(V^{\\top}\\)) - Output coordinates: \\(u_i\\) (columns of \\(U\\)) - Scaling factors: \\(\\sigma_i\\) (diagonal of \\(\\Sigma\\))"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture29-svd.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture29-svd.html#summary",
    "title": "Lecture 29: Singular Value Decomposition",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\nConcept\nDescription\n\n\n\n\nSVD Formula\n\\(A = U\\Sigma V^{\\top}\\)\n\n\n\\(U\\)\n\\(m \\times m\\) orthogonal matrix; columns are left singular vectors (basis for column space ‚äï left nullspace)\n\n\n\\(\\Sigma\\)\n\\(m \\times n\\) diagonal matrix; diagonal entries are singular values \\(\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r &gt; 0\\)\n\n\n\\(V\\)\n\\(n \\times n\\) orthogonal matrix; columns are right singular vectors (basis for row space ‚äï nullspace)\n\n\nComputing \\(V, \\sigma^2\\)\nEigenvectors and eigenvalues of \\(A^{\\top}A\\)\n\n\nComputing \\(U, \\sigma^2\\)\nEigenvectors and eigenvalues of \\(AA^{\\top}\\)\n\n\nRank\nNumber of nonzero singular values\n\n\nGeometric meaning\n\\(A\\) maps orthonormal basis \\(v_i\\) to scaled orthonormal basis \\(\\sigma_i u_i\\)\n\n\n\nWhy SVD is important: - Unique (up to sign and ordering of singular values) - Always exists (for any matrix, even non-square or singular) - Numerically stable computation - Applications: dimensionality reduction (PCA), image compression, least squares, pseudoinverse, matrix approximation"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-3-spaces.html",
    "href": "Math/MIT18.06/mit1806-lecture5-3-spaces.html",
    "title": "MIT 18.06SC Lecture 5.3: Vector Spaces",
    "section": "",
    "text": "My lecture notes\nVector spaces and subspaces are fundamental structures in linear algebra. This post covers the vector spaces portion of Lecture 5."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#context",
    "href": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#context",
    "title": "MIT 18.06SC Lecture 5.3: Vector Spaces",
    "section": "",
    "text": "My lecture notes\nVector spaces and subspaces are fundamental structures in linear algebra. This post covers the vector spaces portion of Lecture 5."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#definition",
    "href": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#definition",
    "title": "MIT 18.06SC Lecture 5.3: Vector Spaces",
    "section": "Definition",
    "text": "Definition\nThe space \\(\\mathbb{R}^n\\) has exactly n dimensions.\nExample: \\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\] is a matrix in \\(\\mathbb{R}^{2 \\times 2}\\) (2√ó2 real matrices)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#subspaces",
    "href": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#subspaces",
    "title": "MIT 18.06SC Lecture 5.3: Vector Spaces",
    "section": "Subspaces",
    "text": "Subspaces\nA subspace is a subset of \\(\\mathbb{R}^n\\) that is closed under addition and scalar multiplication.\n\nExamples of Subspaces\nIn \\(\\mathbb{R}^2\\): - A line through the origin creates a subspace\nIn \\(\\mathbb{R}^3\\): - A line through the origin creates a line subspace - A plane through the origin creates a plane subspace\nIn \\(\\mathbb{R}^4\\): - Think: How can we create subspaces in \\(\\mathbb{R}^4\\)?"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#properties-of-subspaces",
    "href": "Math/MIT18.06/mit1806-lecture5-3-spaces.html#properties-of-subspaces",
    "title": "MIT 18.06SC Lecture 5.3: Vector Spaces",
    "section": "Properties of Subspaces",
    "text": "Properties of Subspaces\nA subspace must satisfy two properties:\n\n1. Closed Under Addition\nIf vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) are in the subspace, then \\(\\vec{v} + \\vec{w}\\) is also in the subspace.\nExample: For \\(\\mathbb{R}^2\\) line \\(y=2x\\): - \\([1,2]\\) and \\([2,4]\\) are in the line - \\([1,2] + [2,4] = [3,6]\\) is still in the line ‚úì\n\n\n2. Closed Under Scalar Multiplication\nIf vector \\(\\vec{v}\\) is in the subspace and \\(c\\) is any scalar, then \\(c\\vec{v}\\) is also in the subspace.\nExample: For \\(\\mathbb{R}^2\\) line \\(y=2x\\): - \\([1,2]\\) is in the line - \\(1.5 \\times [1,2] = [1.5, 3]\\) is still in the line ‚úì\n\nSource: MIT 18.06SC Linear Algebra, Lecture 5"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture24-markov-fourier.html",
    "href": "Math/MIT18.06/mit1806-lecture24-markov-fourier.html",
    "title": "MIT 18.06 Lecture 24: Markov Matrices and Fourier Series",
    "section": "",
    "text": "This lecture explores two important applications of eigenvalues and eigenvectors: Markov matrices for modeling state transitions, and Fourier series for decomposing functions into orthogonal components."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture24-markov-fourier.html#markov-matrices",
    "href": "Math/MIT18.06/mit1806-lecture24-markov-fourier.html#markov-matrices",
    "title": "MIT 18.06 Lecture 24: Markov Matrices and Fourier Series",
    "section": "Markov Matrices",
    "text": "Markov Matrices\nA Markov matrix is a special type of matrix that describes state transitions in a probabilistic system.\nExample:\n\\[\nA = \\begin{bmatrix}0.2 & 0.01 & 0.3 \\\\ 0.7 & 0.99 & 0.3 \\\\ 0.1 & 0 & 0.4\\end{bmatrix}\n\\]\n\nTwo Key Properties\n\nAll entries ‚â• 0: Every element represents a probability\nAll columns sum to 1: Each column represents a complete probability distribution\n\nThese properties ensure that if we start with a probability distribution (a vector whose entries sum to 1), applying the Markov matrix preserves this property.\n\n\nSteady State\nThe steady state is given by \\(c_1 x_1\\), where \\(x_1\\) is the eigenvector corresponding to eigenvalue \\(\\lambda = 1\\).\nImportant property: \\(x_1 \\geq 0\\) (all entries are non-negative), which makes sense for a probability distribution.\n\n\nWhy Œª = 1 is Always an Eigenvalue\nProof: We need to show that \\(A - I\\) is singular.\n\\[\nA - I = \\begin{bmatrix}-0.8 & 0.01 & 0.3 \\\\ 0.7 & -0.01 & 0.3 \\\\ 0.1 & 0 & -0.6\\end{bmatrix}\n\\]\nObserve that \\(\\text{row}_1 + \\text{row}_2 + \\text{row}_3 = \\mathbf{0}\\), which means:\n\\[\n\\text{row}_1 + \\text{row}_2 = -\\text{row}_3\n\\]\nThe rows are linearly dependent because \\((1, 1, 1)\\) is in the left null space \\(N(A^{\\top})\\).\nKey insight: \\((1, 1, 1)\\) is an eigenvector of \\(A^{\\top}\\) with eigenvalue 1. Since Markov matrices and their transposes share eigenvalues, \\(\\lambda = 1\\) is also an eigenvalue of \\(A\\).\n\n\nFinding the Eigenvector \\(x_1\\)\nFrom the eigenvalue equation:\n\\[\nAx = \\lambda x\n\\]\n\\[\nAx - \\lambda x = (A - \\lambda I)x = 0\n\\]\nWe can find \\(x_1\\) by solving \\((A - I)x = \\mathbf{0}\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture24-markov-fourier.html#application-population-migration",
    "href": "Math/MIT18.06/mit1806-lecture24-markov-fourier.html#application-population-migration",
    "title": "MIT 18.06 Lecture 24: Markov Matrices and Fourier Series",
    "section": "Application: Population Migration",
    "text": "Application: Population Migration\n\nExample: California and Massachusetts\nWe model population movement between California and Massachusetts using a Markov matrix:\n\\[\n\\begin{bmatrix}u_{\\text{cal}} \\\\ u_{\\text{mass}}\\end{bmatrix}_{t=k+1} = \\begin{bmatrix}0.9 & 0.2 \\\\ 0.1 & 0.8\\end{bmatrix} \\begin{bmatrix}u_{\\text{cal}} \\\\ u_{\\text{mass}}\\end{bmatrix}_{t=k}\n\\]\nReading the transition matrix: - 90% of people in California stay, 10% move to Massachusetts - 80% of people in Massachusetts stay, 20% move to California\n\n\nEvolution Over Time\nInitial state: All 1000 people in Massachusetts\n\\[\n\\begin{bmatrix}u_{\\text{cal}} \\\\ u_{\\text{mass}}\\end{bmatrix}_{t=0} = \\begin{bmatrix}0 \\\\ 1000\\end{bmatrix}\n\\]\nAfter 1 time step:\n\\[\n\\begin{bmatrix}u_{\\text{cal}} \\\\ u_{\\text{mass}}\\end{bmatrix}_{t=1} = \\begin{bmatrix}200 \\\\ 800\\end{bmatrix}\n\\]\nAfter 2 time steps:\n\\[\n\\begin{bmatrix}u_{\\text{cal}} \\\\ u_{\\text{mass}}\\end{bmatrix}_{t=2} = \\begin{bmatrix}340 \\\\ 660\\end{bmatrix}\n\\]\nObservation: The total population remains 1000 at every step, confirming conservation of probability.\n\n\nFinding the Steady State\nEigenvalues:\n\\[\n\\lambda_1 = 1, \\quad \\lambda_2 = 0.7\n\\]\nEigenvector 1 (for \\(\\lambda_1 = 1\\)):\n\\[\n\\left(\\begin{bmatrix}0.9 & 0.2 \\\\ 0.1 & 0.8\\end{bmatrix} - I\\right)x_1 = \\begin{bmatrix}-0.1 & 0.2 \\\\ 0.1 & -0.2\\end{bmatrix}x_1 = 0\n\\]\n\\[\nx_1 = \\begin{bmatrix}2 \\\\ 1\\end{bmatrix}\n\\]\nInterpretation: The steady state has \\(\\frac{2}{3}\\) of people in California and \\(\\frac{1}{3}\\) in Massachusetts. Once this state is reached, the number of people moving into California equals the number moving out, maintaining equilibrium.\nEigenvector 2 (for \\(\\lambda_2 = 0.7\\)):\n\\[\n\\left(\\begin{bmatrix}0.9 & 0.2 \\\\ 0.1 & 0.8\\end{bmatrix} - 0.7I\\right)x_2 = \\begin{bmatrix}0.2 & 0.2 \\\\ 0.1 & 0.1\\end{bmatrix}x_2 = 0\n\\]\n\\[\nx_2 = \\begin{bmatrix}-1 \\\\ 1\\end{bmatrix}\n\\]\n\n\nGeneral Formula\nThe population at time \\(k\\) is:\n\\[\nu_k = c_1 \\begin{bmatrix}2 \\\\ 1\\end{bmatrix} + c_2 (0.7)^k \\begin{bmatrix}-1 \\\\ 1\\end{bmatrix}\n\\]\nAs \\(k \\to \\infty\\), the term \\((0.7)^k \\to 0\\), leaving only the steady state:\n\\[\nu_{\\infty} = c_1 \\begin{bmatrix}2 \\\\ 1\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture24-markov-fourier.html#projection-onto-orthonormal-basis",
    "href": "Math/MIT18.06/mit1806-lecture24-markov-fourier.html#projection-onto-orthonormal-basis",
    "title": "MIT 18.06 Lecture 24: Markov Matrices and Fourier Series",
    "section": "Projection onto Orthonormal Basis",
    "text": "Projection onto Orthonormal Basis\nAny vector \\(v\\) can be projected onto an orthonormal basis \\(\\{q_1, q_2, \\ldots, q_n\\}\\) (where each \\(q_i\\) has unit length and is perpendicular to all others):\n\\[\nv = x_1 q_1 + x_2 q_2 + \\cdots + x_n q_n\n\\]\n\nFinding the Coefficients\nTo find \\(x_1\\), take the inner product with \\(q_1\\):\n\\[\nq_1^{\\top} v = x_1 q_1^{\\top} q_1 + x_2 q_2^{\\top} q_1 + \\cdots + x_n q_n^{\\top} q_1\n\\]\nSince the basis is orthonormal: - \\(q_i^{\\top} q_j = 0\\) for \\(i \\neq j\\) (orthogonality) - \\(q_i^{\\top} q_i = 1\\) (unit length)\nThis leaves:\n\\[\nq_1^{\\top} v = x_1\n\\]\nNote: If the basis is only orthogonal (not normalized), we would have \\(q_1^{\\top} v = x_1 \\|q_1\\|^2\\), requiring:\n\\[\nx_1 = \\frac{q_1^{\\top} v}{q_1^{\\top} q_1}\n\\]\n\n\nMatrix Form\nWriting this for all coefficients with an orthonormal basis:\n\\[\n\\begin{bmatrix}| & \\cdots & | \\\\ q_1 & \\cdots & q_n \\\\ | & \\cdots & |\\end{bmatrix} \\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix} = v\n\\]\n\\[\nQx = v\n\\]\nFor an orthonormal matrix (columns are orthonormal vectors), \\(Q^{-1} = Q^{\\top}\\), so:\n\\[\nx = Q^{\\top} v\n\\]\nThis shows that finding coefficients in an orthonormal basis is computationally simple‚Äîjust take inner products with each basis vector."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture24-markov-fourier.html#fourier-series",
    "href": "Math/MIT18.06/mit1806-lecture24-markov-fourier.html#fourier-series",
    "title": "MIT 18.06 Lecture 24: Markov Matrices and Fourier Series",
    "section": "Fourier Series",
    "text": "Fourier Series\nFourier series extends the idea of orthogonal decomposition to functions:\n\\[\nf(x) = a_0 + a_1 \\cos x + b_1 \\sin x + a_2 \\cos 2x + b_2 \\sin 2x + \\cdots\n\\]\n\nOrthogonal Functions\nTwo functions \\(f(x)\\) and \\(g(x)\\) are orthogonal on an interval \\([a, b]\\) if their inner product is zero:\n\\[\n\\langle f, g \\rangle = \\int_a^b f(x)g(x)\\,dx = 0\n\\]\nThis generalizes the concept of ‚Äútwo vectors having zero dot product‚Äù to functions. Here, the integral plays the role of the dot product.\n\n\nWhy Are cos x and cos 2x Orthogonal?\nWe can verify:\n\\[\n\\int_{-\\pi}^{\\pi} \\cos x \\cos 2x\\,dx = 0\n\\]\nProof: Use the product-to-sum identity:\n\\[\n\\cos(x)\\cos(2x) = \\frac{\\cos(3x) + \\cos(x)}{2}\n\\]\nEach term integrates to zero over \\([-\\pi, \\pi]\\):\n\\[\n\\int_{-\\pi}^{\\pi} \\cos(kx)\\,dx = 0 \\quad (k \\neq 0)\n\\]\nTherefore:\n\\[\n\\int_{-\\pi}^{\\pi} \\cos(x)\\cos(2x)\\,dx = 0\n\\]\nSo \\(\\cos x \\perp \\cos 2x\\).\n\n\n\nOrthogonality of cos x and cos 2x\n\n\n\n\nGeneral Orthogonality Rules\nFor the Fourier basis on \\([0, 2\\pi]\\) (or \\([-\\pi, \\pi]\\)):\n\n\\(\\cos mx \\perp \\cos nx\\) for \\(m \\neq n\\)\n\\(\\sin mx \\perp \\sin nx\\) for \\(m \\neq n\\)\n\\(\\cos mx \\perp \\sin nx\\) for all \\(m, n\\)\n\nThese orthogonality properties are what make Fourier series work."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture24-markov-fourier.html#deriving-fourier-coefficients",
    "href": "Math/MIT18.06/mit1806-lecture24-markov-fourier.html#deriving-fourier-coefficients",
    "title": "MIT 18.06 Lecture 24: Markov Matrices and Fourier Series",
    "section": "Deriving Fourier Coefficients",
    "text": "Deriving Fourier Coefficients\n\nFinding \\(a_0\\) (The DC Component)\nIntegrate both sides of the Fourier series over \\([0, 2\\pi]\\):\n\\[\n\\int_0^{2\\pi} f(x)\\,dx = \\int_0^{2\\pi} a_0\\,dx + \\sum_{n=1}^{\\infty} a_n \\int_0^{2\\pi} \\cos(nx)\\,dx + \\sum_{n=1}^{\\infty} b_n \\int_0^{2\\pi} \\sin(nx)\\,dx\n\\]\nUsing orthogonality, all the cosine and sine integrals vanish:\n\\[\n\\int_0^{2\\pi} f(x)\\,dx = a_0 \\cdot 2\\pi\n\\]\nTherefore:\n\\[\n\\boxed{a_0 = \\frac{1}{2\\pi} \\int_0^{2\\pi} f(x)\\,dx}\n\\]\nInterpretation: For a \\(2\\pi\\)-periodic function, the DC term \\(a_0\\) equals the average value of \\(f\\) over one period.\n\n\nFinding \\(a_1\\) (The First Cosine Coefficient)\nMultiply both sides by \\(\\cos x\\) and integrate:\n\\[\nf(x)\\cos x = a_0 \\cos x + a_1 \\cos^2 x + b_1 \\sin x \\cos x + a_2 \\cos 2x \\cos x + b_2 \\sin 2x \\cos x + \\cdots\n\\]\nIntegrate over \\([0, 2\\pi]\\):\n\\[\n\\int_0^{2\\pi} f(x)\\cos x\\,dx = a_0 \\int_0^{2\\pi} \\cos x\\,dx + a_1 \\int_0^{2\\pi} \\cos^2 x\\,dx + b_1 \\int_0^{2\\pi} \\sin x \\cos x\\,dx + \\cdots\n\\]\nUsing orthogonality: - \\(\\int_0^{2\\pi} \\cos x\\,dx = 0\\) - \\(\\int_0^{2\\pi} \\sin x \\cos x\\,dx = 0\\) - \\(\\int_0^{2\\pi} \\cos x \\cos(2x)\\,dx = 0\\) - And so on‚Ä¶\nOnly one term survives:\n\\[\n\\int_0^{2\\pi} f(x)\\cos x\\,dx = a_1 \\int_0^{2\\pi} \\cos^2 x\\,dx\n\\]\nTo evaluate \\(\\int_0^{2\\pi} \\cos^2 x\\,dx\\), use the identity \\(\\cos^2 x = \\frac{1 + \\cos 2x}{2}\\):\n\\[\n\\int_0^{2\\pi} \\cos^2 x\\,dx = \\frac{1}{2} \\int_0^{2\\pi} 1\\,dx + \\frac{1}{2} \\int_0^{2\\pi} \\cos 2x\\,dx = \\frac{1}{2}(2\\pi) + 0 = \\pi\n\\]\nTherefore:\n\\[\n\\int_0^{2\\pi} f(x)\\cos x\\,dx = a_1 \\cdot \\pi\n\\]\n\\[\n\\boxed{a_1 = \\frac{1}{\\pi} \\int_0^{2\\pi} f(x)\\cos x\\,dx}\n\\]\n\n\nFinding \\(b_1\\) (The First Sine Coefficient)\nThe same process applies: multiply by \\(\\sin x\\) and integrate. By orthogonality, only the \\(b_1 \\sin^2 x\\) term survives, giving:\n\\[\n\\boxed{b_1 = \\frac{1}{\\pi} \\int_0^{2\\pi} f(x)\\sin x\\,dx}\n\\]\n\n\nGeneral Pattern\nFor any \\(n \\geq 1\\):\n\\[\na_n = \\frac{1}{\\pi} \\int_0^{2\\pi} f(x)\\cos(nx)\\,dx\n\\]\n\\[\nb_n = \\frac{1}{\\pi} \\int_0^{2\\pi} f(x)\\sin(nx)\\,dx\n\\]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture24-markov-fourier.html#the-connection-orthogonality-everywhere",
    "href": "Math/MIT18.06/mit1806-lecture24-markov-fourier.html#the-connection-orthogonality-everywhere",
    "title": "MIT 18.06 Lecture 24: Markov Matrices and Fourier Series",
    "section": "The Connection: Orthogonality Everywhere",
    "text": "The Connection: Orthogonality Everywhere\nThis lecture reveals a beautiful parallel:\n\nMarkov matrices: Eigenvalues and eigenvectors provide an orthogonal decomposition in state space\nFourier series: Sines and cosines provide an orthogonal decomposition in function space\n\nIn both cases, orthogonality makes the decomposition clean and the coefficients easy to compute. This is why eigenvalues and Fourier analysis are such powerful tools in mathematics and engineering."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html",
    "href": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html",
    "title": "MIT 18.06SC Lecture 9: Independence, Basis, and Dimension",
    "section": "",
    "text": "My lecture notes\nThis lecture develops three fundamental concepts: linear independence (no redundancy), basis (minimal spanning set), and dimension (fundamental measure of vector space size). These concepts unify our understanding of vector spaces through the rank-nullity theorem."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#context",
    "href": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#context",
    "title": "MIT 18.06SC Lecture 9: Independence, Basis, and Dimension",
    "section": "",
    "text": "My lecture notes\nThis lecture develops three fundamental concepts: linear independence (no redundancy), basis (minimal spanning set), and dimension (fundamental measure of vector space size). These concepts unify our understanding of vector spaces through the rank-nullity theorem."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#linear-independence",
    "href": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#linear-independence",
    "title": "MIT 18.06SC Lecture 9: Independence, Basis, and Dimension",
    "section": "Linear Independence",
    "text": "Linear Independence\n\nDefinition\nVectors \\(x_1, x_2, \\ldots, x_n\\) are linearly independent if the only linear combination that produces the zero vector is the trivial combination (all coefficients zero):\n\\[\nc_1x_1 + c_2x_2 + \\cdots + c_nx_n = 0 \\quad \\Rightarrow \\quad c_1 = c_2 = \\cdots = c_n = 0\n\\]\nEquivalently, vectors are dependent if there exists a non-trivial combination (some \\(c_i \\neq 0\\)) that gives zero:\n\\[\nc_1x_1 + c_2x_2 + \\cdots + c_nx_n = 0 \\quad \\text{with some } c_i \\neq 0\n\\]\n\n\nWhen Are Vectors Dependent?\nCase 1: Zero vector present\nIf one of the vectors is \\(\\vec{0}\\), the vectors are automatically dependent: \\[\nr \\cdot \\vec{0} + 0 \\cdot x_1 + \\cdots + 0 \\cdot x_n = 0 \\quad \\text{(non-trivial combination)}\n\\]\nCase 2: Vectors in the same direction (collinear)\nIf any two vectors are scalar multiples of each other, the set is dependent.\n\n\n\nCollinear vectors\n\n\nExample: Vectors \\(v_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) and \\(v_2 = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\\) are dependent because: \\[\n2v_1 - v_2 = 2\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n\\]\nGeneral case: If \\(v_2 = cv_1\\) for some scalar \\(c\\), then: \\[\ncv_1 - v_2 = 0 \\quad \\text{(non-trivial combination)}\n\\]\nCase 3: More vectors than dimensions (\\(n &gt; m\\))\nIf we have \\(n\\) vectors in \\(\\mathbb{R}^m\\) with \\(n &gt; m\\), and the first \\(m\\) vectors are linearly independent (not collinear/coplanar), then all \\(n\\) vectors must be dependent.\nExample in \\(\\mathbb{R}^2\\): Consider three vectors \\(x_1, x_2, x_3\\) in the plane where \\(x_1\\) and \\(x_2\\) are not collinear.\n\n\n\nThree vectors in plane\n\n\n\nSince \\(x_1\\) and \\(x_2\\) are linearly independent, their combinations \\(c_1x_1 + c_2x_2\\) span the entire plane\nTherefore, \\(x_3\\) can be expressed as some combination of \\(x_1\\) and \\(x_2\\)\nThis means we can find \\(c_1, c_2\\) such that \\(c_1x_1 + c_2x_2 = x_3\\)\nRearranging: \\(c_1x_1 + c_2x_2 - x_3 = 0\\) (a non-trivial combination equals zero)\nThus \\(x_1, x_2, x_3\\) are dependent\n\n\n\nNull Space Interpretation\nTo test independence of vectors \\(v_1, v_2, \\ldots, v_n\\), form the matrix: \\[\nA = [v_1 \\mid v_2 \\mid \\cdots \\mid v_n]\n\\]\nThe equation \\(c_1v_1 + c_2v_2 + \\cdots + c_nv_n = 0\\) becomes \\(Ax = 0\\) where \\(x = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}\\).\nVectors are independent if and only if: - \\(N(A) = \\{\\vec{0}\\}\\) (null space contains only zero vector) - \\(\\text{rank}(A) = n\\) (full column rank) - No free variables\nVectors are dependent if and only if: - \\(N(A)\\) contains non-zero vectors - \\(\\text{rank}(A) &lt; n\\) (rank deficient) - Has free variables (\\(n - r &gt; 0\\))\nExample: For 3 vectors in \\(\\mathbb{R}^2\\) (underdetermined system with \\(m = 2 &lt; n = 3\\)): - We have \\(r = 2 = m &lt; n = 3\\) - Number of free variables: \\(n - r = 3 - 2 = 1\\) - \\(N(A)\\) has infinitely many solutions - Therefore, the vectors are dependent"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#spanning-sets",
    "href": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#spanning-sets",
    "title": "MIT 18.06SC Lecture 9: Independence, Basis, and Dimension",
    "section": "Spanning Sets",
    "text": "Spanning Sets\n\nDefinition\nVectors \\(v_1, v_2, \\ldots, v_l\\) span a space \\(S\\) if every vector in \\(S\\) can be written as a linear combination of \\(v_1, \\ldots, v_l\\):\n\\[\nS = \\text{span}(v_1, \\ldots, v_l) = \\{c_1v_1 + c_2v_2 + \\cdots + c_lv_l \\mid c_i \\in \\mathbb{R}\\}\n\\]\nInterpretation: The span is the set of all possible linear combinations of the vectors.\n\n\n\nSpan illustration"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#basis",
    "href": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#basis",
    "title": "MIT 18.06SC Lecture 9: Independence, Basis, and Dimension",
    "section": "Basis",
    "text": "Basis\n\nDefinition\nA basis for a vector space \\(S\\) is a sequence of vectors \\(v_1, v_2, \\ldots, v_n\\) that satisfies two properties:\n\nIndependence: The vectors are linearly independent\n\nEnsures no redundancy (can‚Äôt remove any vector)\nRank = number of vectors\n\nSpanning: The vectors span the space \\(S\\)\n\nEvery vector in \\(S\\) can be expressed as a combination\nRank = dimension of space\n\n\nKey insight: A basis is a minimal spanning set (independent) and a maximal independent set (spanning).\n\n\nStandard Basis for \\(\\mathbb{R}^n\\)\nFor the space \\(\\mathbb{R}^n\\), the standard basis consists of \\(n\\) vectors.\nExample in \\(\\mathbb{R}^3\\): \\[\n\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad\n\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\n\\]\nThese are the columns of the identity matrix \\(I_3\\).\n\n\nProperties of Bases\n\nEvery vector has a unique representation: If \\(\\{v_1, \\ldots, v_n\\}\\) is a basis for \\(S\\), then every \\(x \\in S\\) can be written uniquely as: \\[\nx = c_1v_1 + c_2v_2 + \\cdots + c_nv_n\n\\]\nAll bases have the same size: Any two bases for the same space have the same number of vectors (this number is the dimension)\nBasis matrix is invertible: If we form a matrix \\(B = [v_1 \\mid \\cdots \\mid v_n]\\) where \\(\\{v_1, \\ldots, v_n\\}\\) is a basis for \\(\\mathbb{R}^n\\), then \\(B\\) is square (\\(n \\times n\\)) and invertible"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#dimension",
    "href": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#dimension",
    "title": "MIT 18.06SC Lecture 9: Independence, Basis, and Dimension",
    "section": "Dimension",
    "text": "Dimension\n\nDefinition\nThe dimension of a vector space \\(S\\) is the number of vectors in any basis for \\(S\\).\nNotation: \\(\\dim(S)\\)\n\n\nDimension Formulas\nFor an \\(m \\times n\\) matrix \\(A\\) with rank \\(r\\):\n\nDimension of column space: \\[\n\\dim(C(A)) = r\n\\] The pivot columns form a basis for \\(C(A)\\).\nDimension of null space (also called nullity): \\[\n\\dim(N(A)) = n - r\n\\] The number of special solutions equals the number of free variables.\nRank-Nullity Theorem: \\[\n\\dim(C(A)) + \\dim(N(A)) = n\n\\] or equivalently: \\[\nr + (n - r) = n\n\\]\n\n\n\nExamples\nExample 1: Matrix with full column rank \\[\nA = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\\\ 3 & 2 \\end{bmatrix}, \\quad r = 2, \\, m = 3, \\, n = 2\n\\]\n\n\\(\\dim(C(A)) = 2\\) (columns are independent, they form a basis)\n\\(\\dim(N(A)) = n - r = 2 - 2 = 0\\) (only zero vector in null space)\n\nExample 2: Rank-deficient matrix \\[\nA = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{bmatrix}, \\quad r = 1, \\, m = 2, \\, n = 3\n\\]\n\n\\(\\dim(C(A)) = 1\\) (only one independent column)\n\\(\\dim(N(A)) = n - r = 3 - 1 = 2\\) (2-dimensional null space)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html#summary",
    "title": "MIT 18.06SC Lecture 9: Independence, Basis, and Dimension",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\nConcept\nDefinition\nTest\n\n\n\n\nIndependence\nNo non-trivial combination gives zero\n\\(N(A) = \\{\\vec{0}\\}\\) or \\(r = n\\)\n\n\nSpanning\nAll vectors in space are combinations\n\\(C(A) = S\\) or \\(r = \\dim(S)\\)\n\n\nBasis\nIndependent + Spanning\n\\(r = n = \\dim(S)\\)\n\n\nDimension\nNumber of vectors in basis\n\\(\\dim(C(A)) = r\\), \\(\\dim(N(A)) = n - r\\)\n\n\n\nKey relationships: - Independence prevents redundancy (no vector is a combination of others) - Spanning ensures completeness (every vector in the space is reachable) - Basis achieves both: minimal spanning set = maximal independent set - Dimension is the fundamental measure of ‚Äúsize‚Äù of a vector space\n\nSource: MIT 18.06SC Linear Algebra, Lecture 9"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html",
    "href": "Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html",
    "title": "MIT 18.06 Lecture 25: Symmetric Matrices and Positive Definiteness",
    "section": "",
    "text": "This lecture explores symmetric matrices and introduces the important concept of positive definiteness. We‚Äôll see that symmetric matrices have special properties that make them especially important in applications: real eigenvalues, orthogonal eigenvectors, and a beautiful spectral decomposition."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html#symmetric-matrices",
    "href": "Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html#symmetric-matrices",
    "title": "MIT 18.06 Lecture 25: Symmetric Matrices and Positive Definiteness",
    "section": "Symmetric Matrices",
    "text": "Symmetric Matrices\nA matrix \\(A\\) is symmetric if it equals its transpose:\n\\[\nA = A^{\\top}\n\\]\nExample:\n\\[\nA = \\begin{bmatrix}2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2\\end{bmatrix}\n\\]\n\nMain Properties\nSymmetric matrices have two fundamental properties:\n\nAll eigenvalues are real (when matrix entries are real)\nEigenvectors corresponding to different eigenvalues are orthogonal\n\nThese properties make symmetric matrices particularly well-behaved and useful in applications."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html#spectral-theorem-diagonalization-of-symmetric-matrices",
    "href": "Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html#spectral-theorem-diagonalization-of-symmetric-matrices",
    "title": "MIT 18.06 Lecture 25: Symmetric Matrices and Positive Definiteness",
    "section": "Spectral Theorem: Diagonalization of Symmetric Matrices",
    "text": "Spectral Theorem: Diagonalization of Symmetric Matrices\n\nGeneral vs Symmetric Diagonalization\n\nGeneral case: \\(A = S\\Lambda S^{-1}\\), where \\(S\\) contains eigenvectors\nSymmetric case: \\(A = Q\\Lambda Q^{\\top}\\), where \\(Q\\) is an orthogonal matrix\n\nThe key difference is that for symmetric matrices, we can choose the eigenvector matrix \\(Q\\) to be orthogonal (meaning \\(Q^{\\top}Q = I\\) and \\(Q^{-1} = Q^{\\top}\\)).\nThis decomposition is called the spectral theorem or spectral decomposition.\n\n\nWhy This Matters\nSince \\(Q^{-1} = Q^{\\top}\\), we have:\n\\[\nA = Q\\Lambda Q^{\\top}\n\\]\nThis is computationally simpler than the general case because: - Computing \\(Q^{-1}\\) is trivial (just transpose) - Orthogonal matrices preserve lengths and angles - The decomposition is numerically stable"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html#proof-eigenvalues-of-symmetric-matrices-are-real",
    "href": "Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html#proof-eigenvalues-of-symmetric-matrices-are-real",
    "title": "MIT 18.06 Lecture 25: Symmetric Matrices and Positive Definiteness",
    "section": "Proof: Eigenvalues of Symmetric Matrices Are Real",
    "text": "Proof: Eigenvalues of Symmetric Matrices Are Real\nWe‚Äôll prove that if \\(A = A^{\\top}\\) (with real entries), then all eigenvalues \\(\\lambda\\) must be real.\n\nComplex Conjugate Review\nFor a complex number \\(z = a + bi\\): - The complex conjugate is \\(\\bar{z} = a - bi\\) - If \\(z = \\bar{z}\\), then \\(z\\) is real (no imaginary part)\n\n\nProof Strategy\nStart with the eigenvalue equation:\n\\[\nAx = \\lambda x\n\\]\nwhere \\(x\\) may be complex.\nStep 1: Take the complex conjugate of the eigenvalue equation:\n\\[\n\\overline{Ax} = \\overline{\\lambda x}\n\\]\nSince \\(A\\) has real entries, \\(\\overline{A} = A\\):\n\\[\nA\\bar{x} = \\bar{\\lambda}\\bar{x}\n\\]\nSo \\(\\bar{x}\\) is an eigenvector with eigenvalue \\(\\bar{\\lambda}\\).\nStep 2: Multiply the original equation on the left by \\(\\bar{x}^{\\top}\\):\n\\[\n\\bar{x}^{\\top}Ax = \\bar{x}^{\\top}\\lambda x = \\lambda(\\bar{x}^{\\top}x)\n\\]\nStep 3: Take the transpose of \\(A\\bar{x} = \\bar{\\lambda}\\bar{x}\\):\n\\[\n\\bar{x}^{\\top}A^{\\top} = \\bar{x}^{\\top}\\bar{\\lambda}\n\\]\nMultiply on the right by \\(x\\):\n\\[\n\\bar{x}^{\\top}A^{\\top}x = \\bar{\\lambda}(\\bar{x}^{\\top}x)\n\\]\nStep 4: Since \\(A = A^{\\top}\\), we have \\(A^{\\top}x = Ax\\), so:\n\\[\n\\bar{x}^{\\top}Ax = \\bar{\\lambda}(\\bar{x}^{\\top}x)\n\\]\nStep 5: Compare the two expressions for \\(\\bar{x}^{\\top}Ax\\):\n\\[\n\\lambda(\\bar{x}^{\\top}x) = \\bar{\\lambda}(\\bar{x}^{\\top}x)\n\\]\nSince \\(\\bar{x}^{\\top}x = |x_1|^2 + |x_2|^2 + \\cdots + |x_n|^2 &gt; 0\\) (eigenvectors are non-zero), we can divide both sides:\n\\[\n\\lambda = \\bar{\\lambda}\n\\]\nConclusion: \\(\\lambda\\) equals its complex conjugate, so \\(\\lambda\\) must be real."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html#spectral-decomposition-sum-of-projections",
    "href": "Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html#spectral-decomposition-sum-of-projections",
    "title": "MIT 18.06 Lecture 25: Symmetric Matrices and Positive Definiteness",
    "section": "Spectral Decomposition: Sum of Projections",
    "text": "Spectral Decomposition: Sum of Projections\nThe spectral theorem can be rewritten in an illuminating form:\n\\[\nA = Q\\Lambda Q^{\\top} = \\begin{bmatrix}| & | & \\cdots & | \\\\ q_1 & q_2 & \\cdots & q_n \\\\ | & | & \\cdots & |\\end{bmatrix}\\begin{bmatrix}\\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n\\end{bmatrix}\\begin{bmatrix}- & q_1^{\\top} & - \\\\ - & q_2^{\\top} & - \\\\ & \\vdots & \\\\ - & q_n^{\\top} & -\\end{bmatrix}\n\\]\nExpanding the matrix multiplication:\n\\[\nA = \\lambda_1 q_1q_1^{\\top} + \\lambda_2 q_2q_2^{\\top} + \\cdots + \\lambda_n q_nq_n^{\\top}\n\\]\nInterpretation: Every symmetric matrix is a weighted sum of mutually orthogonal projection matrices \\(q_iq_i^{\\top}\\), where the weights are the eigenvalues.\nEach term \\(q_iq_i^{\\top}\\) projects vectors onto the one-dimensional subspace spanned by \\(q_i\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html#positive-definite-matrices",
    "href": "Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html#positive-definite-matrices",
    "title": "MIT 18.06 Lecture 25: Symmetric Matrices and Positive Definiteness",
    "section": "Positive Definite Matrices",
    "text": "Positive Definite Matrices\nPositive definite matrices form an important subset of symmetric matrices.\n\nDefinition\nA symmetric matrix \\(A\\) is positive definite if for all non-zero vectors \\(x\\):\n\\[\nx^{\\top}Ax &gt; 0\n\\]\nGeometric interpretation: The quadratic form \\(x^{\\top}Ax\\) is always positive, like the square of a distance.\n\n\nEquivalent Conditions\nFor a symmetric matrix \\(A\\), the following are equivalent:\n\nAll eigenvalues are positive: \\(\\lambda_i &gt; 0\\) for all \\(i\\)\nAll pivots are positive: \\(d_i &gt; 0\\) for all \\(i\\) (from elimination)\nAll upper-left subdeterminants are positive:\n\n\\(\\det(A_1) &gt; 0\\), \\(\\det(A_2) &gt; 0\\), ‚Ä¶, \\(\\det(A_n) &gt; 0\\)\nwhere \\(A_k\\) is the \\(k \\times k\\) upper-left submatrix\n\nEnergy test: \\(x^{\\top}Ax &gt; 0\\) for all \\(x \\neq 0\\)\n\nKey insight: The signs of pivots are the same as the signs of eigenvalues for symmetric matrices.\n\n\nWhy This Connection?\nFrom the spectral decomposition:\n\\[\nx^{\\top}Ax = x^{\\top}(Q\\Lambda Q^{\\top})x = (Q^{\\top}x)^{\\top}\\Lambda(Q^{\\top}x) = y^{\\top}\\Lambda y\n\\]\nwhere \\(y = Q^{\\top}x\\).\nSince \\(Q\\) is orthogonal, \\(y \\neq 0\\) when \\(x \\neq 0\\), so:\n\\[\nx^{\\top}Ax = \\lambda_1 y_1^2 + \\lambda_2 y_2^2 + \\cdots + \\lambda_n y_n^2\n\\]\nThis is positive for all \\(x \\neq 0\\) if and only if all \\(\\lambda_i &gt; 0\\).\n\n\nExamples\nPositive definite:\n\\[\nA = \\begin{bmatrix}2 & -1 \\\\ -1 & 2\\end{bmatrix}\n\\]\n\nEigenvalues: \\(\\lambda_1 = 3\\), \\(\\lambda_2 = 1\\) (both positive)\nPivots: \\(2\\), \\(\\frac{3}{2}\\) (both positive)\nDeterminants: \\(\\det(A_1) = 2 &gt; 0\\), \\(\\det(A_2) = 3 &gt; 0\\)\n\nNot positive definite:\n\\[\nB = \\begin{bmatrix}2 & 6 \\\\ 6 & 7\\end{bmatrix}\n\\]\n\nEigenvalues: one negative\nThe upper-left \\(1 \\times 1\\) determinant is \\(2 &gt; 0\\), but \\(\\det(B) = 14 - 36 = -22 &lt; 0\\)\nTherefore \\(B\\) is not positive definite\n\n\n\nOther Definiteness Categories\n\nPositive semidefinite: \\(x^{\\top}Ax \\geq 0\\) for all \\(x\\) (allows zero), \\(\\lambda_i \\geq 0\\)\nNegative definite: \\(x^{\\top}Ax &lt; 0\\) for all \\(x \\neq 0\\), \\(\\lambda_i &lt; 0\\)\nIndefinite: \\(x^{\\top}Ax\\) can be positive or negative depending on \\(x\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html#summary",
    "href": "Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html#summary",
    "title": "MIT 18.06 Lecture 25: Symmetric Matrices and Positive Definiteness",
    "section": "Summary",
    "text": "Summary\nThis lecture reveals the beautiful structure of symmetric matrices:\n\nSpectral theorem: Symmetric matrices can be diagonalized by orthogonal matrices: \\(A = Q\\Lambda Q^{\\top}\\)\nReal eigenvalues: All eigenvalues of real symmetric matrices are real\nOrthogonal eigenvectors: Eigenvectors for different eigenvalues are perpendicular\nSpectral decomposition: \\(A = \\sum_i \\lambda_i q_iq_i^{\\top}\\) expresses \\(A\\) as a weighted sum of projections\nPositive definiteness: A symmetric matrix is positive definite if and only if all eigenvalues (or equivalently, all pivots) are positive\n\nThese properties make symmetric matrices the most important and well-understood class of matrices in linear algebra, with applications throughout mathematics, physics, statistics, and engineering."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture19-determinant-formulas.html#by-2-matrix",
    "href": "Math/MIT18.06/mit1806-lecture19-determinant-formulas.html#by-2-matrix",
    "title": "MIT 18.06 Lecture 19: Determinant Formulas and Cofactors",
    "section": "2 by 2 Matrix",
    "text": "2 by 2 Matrix\n\\[\n\\begin{vmatrix}a&b\\\\c&d\\end{vmatrix}=\\begin{vmatrix}a&0\\\\c&d\\end{vmatrix}+\\begin{vmatrix}0&b\\\\c&d\\end{vmatrix}=\\\\\n\\begin{vmatrix}a&0\\\\0&d\\end{vmatrix}+\\begin{vmatrix}a&0\\\\c&0\\end{vmatrix}+\\begin{vmatrix}0&b\\\\c&0\\end{vmatrix}+\\begin{vmatrix}0&b\\\\0&d\\end{vmatrix}=\\\\\n\\begin{vmatrix}a&0\\\\0&d\\end{vmatrix}+0+\\begin{vmatrix}0&b\\\\c&0\\end{vmatrix}+0\n\\]\n\n\\(\\begin{vmatrix}a&0\\\\0&d\\end{vmatrix}=ad\\) is the product of the diagonal\n\\(\\begin{vmatrix}0&b\\\\c&0\\end{vmatrix}=-bc\\) requires exchanging rows, then taking the product of the diagonal\n\n\nThe Method\n\nKeep rows 2, 3, ‚Ä¶, n unchanged\nSplit the first row into n pieces, one nonzero element per part\n\n\\[\n|A|=\\begin{bmatrix}a&0&0\\\\.&\\text{row}_2&.\\\\.&\\text{row}_n&.\\end{bmatrix}+\n\\begin{bmatrix}0&b&0\\\\.&\\text{row}_2&.\\\\.&\\text{row}_n&.\\end{bmatrix}+\n\\begin{bmatrix}0&0&c\\\\.&\\text{row}_2&.\\\\.&\\text{row}_n&.\\end{bmatrix}+\n\\ldots\n\\]\n\nNow we have n smaller matrices, each matrix keeps all lower rows the same and has only 1 active element entry in the first row\nRepeat this step recursively\nWe can apply the same splitting process to the second row, then the third, and so on ‚Äî each time expanding the matrix into smaller pieces\nEventually, we will obtain many (\\(n^n\\)) matrices, each with only 1 active (non-zero) entry per row"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture19-determinant-formulas.html#by-3-matrix",
    "href": "Math/MIT18.06/mit1806-lecture19-determinant-formulas.html#by-3-matrix",
    "title": "MIT 18.06 Lecture 19: Determinant Formulas and Cofactors",
    "section": "3 by 3 Matrix",
    "text": "3 by 3 Matrix\n\\[\n\\begin{vmatrix}a_{11}&a_{12}&a_{13}\\\\a_{21}&a_{22}&a_{23}\\\\a_{31}&a_{32}&a_{33}\\end{vmatrix}\n\\]\n\nThe Survivors\nThe survivors have one entry in each row and each column. Because if one column is all zeros, the matrix is singular and the determinant is 0.\nProof: - For an \\(n \\times n\\) matrix, after the processing, there should be \\(n^n\\) matrices - For each matrix, each row has only 1 non-zero entry, so in total there are n non-zero entries - If one column has &gt;1 non-zero entry, there must be at least one other column that is all zero - Reason: we have n columns and n non-zero entries - If one column has &gt;1 non-zero entries, the other columns (n-1 in total) have &lt;n-1 non-zeros to share\nSurvivors of 3√ó3:\n\\[\n\\begin{vmatrix}a_{11}&0&0\\\\0&a_{22}&0\\\\0&0&a_{33}\\end{vmatrix}+\\begin{vmatrix}a_{11}&0&0\\\\0&0&a_{23}\\\\0&a_{32}&0\\end{vmatrix}+\\begin{vmatrix}0&a_{12}&0\\\\a_{21}&0&0\\\\0&0&a_{33}\\end{vmatrix}+\\\\\n\\begin{vmatrix}0&a_{12}&0\\\\0&0&a_{23}\\\\a_{31}&0&0\\end{vmatrix}+\\begin{vmatrix}0&0&a_{13}\\\\a_{21}&0&0\\\\0&a_{32}&0 \\end{vmatrix}+\\begin{vmatrix}0&0&a_{13}\\\\0&a_{22}&0\\\\a_{31}&0&0 \\end{vmatrix}\n\\]\nThe determinants:\n\\[\na_{11}a_{22}a_{33}-a_{11}a_{23}a_{32}-a_{12}a_{21}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{13}a_{22}a_{31}\n\\]\n\nEach part is the product of the diagonal\nThe sign is determined by how many row exchanges are needed to make it diagonal"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture19-determinant-formulas.html#the-big-formula",
    "href": "Math/MIT18.06/mit1806-lecture19-determinant-formulas.html#the-big-formula",
    "title": "MIT 18.06 Lecture 19: Determinant Formulas and Cofactors",
    "section": "The Big Formula",
    "text": "The Big Formula\nFor an n by n matrix, we have \\(n!\\) (n factorial) terms of survivors and the determinant is:\n\\[\n\\det A=\\sum_{n!} \\pm a_{1\\alpha}a_{2\\beta}a_{3\\gamma}\\ldots a_{n\\omega}\n\\]\nwhere \\((\\alpha, \\beta, \\gamma,\\ldots,\\omega)\\) is a permutation of \\((1,\\ldots,n)\\).\n\nWhy n! Terms?\nIt is because of permutations: - Row 1 can be chosen in n ways - Row 2 can be chosen in n-1 ways - ‚Ä¶ - Row n can be chosen in 1 way\nTherefore: \\(n \\times (n-1) \\times \\ldots \\times 1 = n!\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture19-determinant-formulas.html#det-a0-and-singularity",
    "href": "Math/MIT18.06/mit1806-lecture19-determinant-formulas.html#det-a0-and-singularity",
    "title": "MIT 18.06 Lecture 19: Determinant Formulas and Cofactors",
    "section": "\\(\\det A=0\\) and Singularity",
    "text": "\\(\\det A=0\\) and Singularity\nConsider this 4√ó4 matrix:\n\\[\n\\begin{vmatrix}0&0&1&1\\\\0&1&1&0\\\\1&1&0&0\\\\1&0&0&1\\end{vmatrix}\n\\]\nTry the big formula, we can find 2 permutations that each make every row and column have a 1:\n\n\\(D_1=\\begin{vmatrix}0&0&1&\\underline{1}\\\\0&1&\\underline{1}&0\\\\1&\\underline{1}&0&0\\\\\\underline{1}&0&0&1\\end{vmatrix}\\)\n\\(D_2=\\begin{vmatrix}0&0&\\underline{1}&1\\\\0&\\underline{1}&1&0\\\\\\underline{1}&1&0&0\\\\1&0&0&\\underline{1}\\end{vmatrix}\\)\n\n\\[\n\\det D_1+\\det D_2=1-1=0\n\\]\nThe determinant of the 4√ó4 matrix is 0, and there are dependent rows: row‚ÇÅ + row‚ÇÉ = row‚ÇÇ + row‚ÇÑ, which proves the finding."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture19-determinant-formulas.html#cofactor",
    "href": "Math/MIT18.06/mit1806-lecture19-determinant-formulas.html#cofactor",
    "title": "MIT 18.06 Lecture 19: Determinant Formulas and Cofactors",
    "section": "Cofactor",
    "text": "Cofactor\n\n3√ó3 Example\n\\[\n\\det A=a_{11}(a_{22}a_{33}-a_{23}a_{32})+\\\\\na_{12}(\\ldots)+\\\\\na_{13}(\\ldots)\n\\]\nThe picture for \\(a_{11}(a_{22}a_{33}-a_{23}a_{32})\\):\n\\[\n\\begin{vmatrix}a_{11}&0&0\\\\0&a_{22}&a_{23}\\\\0&a_{32}&a_{33}\\end{vmatrix}\n\\]\nThis is just \\(a_{11} \\det \\begin{vmatrix}a_{22}&a_{23}\\\\a_{32}&a_{33}\\end{vmatrix}\\).\n\n\nCofactor Definition\nThe cofactor of \\(a_{ij}=C_{ij}\\) is \\(\\pm \\det\\) of the (n-1) √ó (n-1) matrix with \\(\\text{row}_i\\) and \\(\\text{col}_j\\) erased.\n\\[\nC_{ij} = \\begin{cases}+ \\det M_{ij} & \\text{ if } i+j \\text{ is even}\\\\ - \\det M_{ij} & \\text{ if } i+j \\text{ is odd} \\end{cases}\n\\]\nwhere \\(M_{ij}\\) is the matrix with row i and column j removed.\n\n\nCofactor Formula\n\\[\n\\det A=a_{11}C_{11}+a_{12}C_{12}+\\ldots+a_{1n}C_{1n}\n\\]\nThis formula allows us to expand along any row (or column) to compute the determinant."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture19-determinant-formulas.html#three-formulas-for-computing-determinants",
    "href": "Math/MIT18.06/mit1806-lecture19-determinant-formulas.html#three-formulas-for-computing-determinants",
    "title": "MIT 18.06 Lecture 19: Determinant Formulas and Cofactors",
    "section": "Three Formulas for Computing Determinants",
    "text": "Three Formulas for Computing Determinants\n\nProduct of pivots (from elimination)\nBig formula (sum over all n! permutations)\nCofactor formula (recursive expansion along a row or column)\n\n\nSource: MIT 18.06SC Linear Algebra - Lecture 19"
  },
  {
    "objectID": "Math/MIT18.065/lectures.html",
    "href": "Math/MIT18.065/lectures.html",
    "title": "MIT 18.065: Linear Algebra Applications",
    "section": "",
    "text": "My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. This course explores how linear algebra powers modern applications in data analysis, signal processing, and machine learning.\n\n\n\nLecture 8: Norms of Vectors and Matrices Understanding vector p-norms (\\(\\|v\\|_p\\)) and when they satisfy the triangle inequality (only for \\(p \\geq 1\\)). The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù creates non-convex unit balls for strong sparsity. S-norms \\(\\sqrt{v^T S v}\\) generalize to ellipses. Matrix norms: spectral \\(\\|A\\|_2 = \\sigma_1\\), Frobenius \\(\\|A\\|_F = \\sqrt{\\sum \\sigma_i^2}\\), nuclear \\(\\|A\\|_N = \\sum \\sigma_i\\)‚Äîall expressed via singular values.\n\n\nLecture 9: Four Ways to Solve Least Squares Problems Four equivalent methods for solving \\(Ax = b\\) when \\(A\\) has no inverse: pseudo-inverse \\(\\hat{x} = A^+ b\\) using SVD, normal equations \\(A^T A \\hat{x} = A^T b\\), algebraic minimization of \\(\\|Ax - b\\|_2^2\\), and geometric projection of \\(b\\) onto \\(C(A)\\). All methods converge to the same solution when \\(A^T A\\) is invertible: \\(\\hat{x} = (A^T A)^{-1}A^T b\\)."
  },
  {
    "objectID": "Math/MIT18.065/lectures.html#all-lectures",
    "href": "Math/MIT18.065/lectures.html#all-lectures",
    "title": "MIT 18.065: Linear Algebra Applications",
    "section": "",
    "text": "My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. This course explores how linear algebra powers modern applications in data analysis, signal processing, and machine learning.\n\n\n\nLecture 8: Norms of Vectors and Matrices Understanding vector p-norms (\\(\\|v\\|_p\\)) and when they satisfy the triangle inequality (only for \\(p \\geq 1\\)). The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù creates non-convex unit balls for strong sparsity. S-norms \\(\\sqrt{v^T S v}\\) generalize to ellipses. Matrix norms: spectral \\(\\|A\\|_2 = \\sigma_1\\), Frobenius \\(\\|A\\|_F = \\sqrt{\\sum \\sigma_i^2}\\), nuclear \\(\\|A\\|_N = \\sum \\sigma_i\\)‚Äîall expressed via singular values.\n\n\nLecture 9: Four Ways to Solve Least Squares Problems Four equivalent methods for solving \\(Ax = b\\) when \\(A\\) has no inverse: pseudo-inverse \\(\\hat{x} = A^+ b\\) using SVD, normal equations \\(A^T A \\hat{x} = A^T b\\), algebraic minimization of \\(\\|Ax - b\\|_2^2\\), and geometric projection of \\(b\\) onto \\(C(A)\\). All methods converge to the same solution when \\(A^T A\\) is invertible: \\(\\hat{x} = (A^T A)^{-1}A^T b\\)."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "",
    "text": "This lecture covers vector and matrix norms with applications to regularization and sparsity:\n\nReview vector p-norms and the geometry of unit balls\nExplain when norms are valid (triangle inequality) and the S-norm defined by \\(v^T S v\\)\nCompare \\(\\ell_1\\), \\(\\ell_2\\), and the ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù as regularizers when solving \\(Ax = b\\)\nIntroduce matrix norms (spectral, Frobenius, nuclear) and relate them to singular values"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#overview",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#overview",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "",
    "text": "This lecture covers vector and matrix norms with applications to regularization and sparsity:\n\nReview vector p-norms and the geometry of unit balls\nExplain when norms are valid (triangle inequality) and the S-norm defined by \\(v^T S v\\)\nCompare \\(\\ell_1\\), \\(\\ell_2\\), and the ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù as regularizers when solving \\(Ax = b\\)\nIntroduce matrix norms (spectral, Frobenius, nuclear) and relate them to singular values"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#vector-norms",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#vector-norms",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Vector Norms",
    "text": "Vector Norms\n\nDefinition\nThe p-norm of a vector \\(v \\in \\mathbb{R}^n\\) is defined as:\n\\[\n\\|v\\|_p = \\sqrt[p]{|v_1|^p + |v_2|^p + \\cdots + |v_n|^p}\n\\]\nCommon values: \\(p = 0, 1, 2, \\infty\\)\n\n\\(\\|v\\|_0\\): Number of nonzero components (not a true norm)\n\\(\\|v\\|_1\\): Sum of absolute values (Manhattan norm)\n\\(\\|v\\|_2\\): Euclidean length\n\\(\\|v\\|_\\infty\\): Maximum absolute value"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#geometry-of-unit-balls",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#geometry-of-unit-balls",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Geometry of Unit Balls",
    "text": "Geometry of Unit Balls\nThe unit ball \\(\\|v\\|_p = 1\\) in \\(\\mathbb{R}^2\\) has different shapes for different \\(p\\):\n Figure: Unit balls \\(\\|v\\|_p = 1\\) in \\(\\mathbb{R}^2\\) for different values of \\(p\\). As \\(p\\) increases, the unit ball transitions from diamond (\\(p=1\\)) to circle (\\(p=2\\)) to square (\\(p=\\infty\\))."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#when-is-it-a-true-norm",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#when-is-it-a-true-norm",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "When Is It a True Norm?",
    "text": "When Is It a True Norm?\nTriangle inequality requirement: \\(\\|x + y\\| \\leq \\|x\\| + \\|y\\|\\)\n\nWhen \\(p \\geq 1\\): \\(\\|\\cdot\\|_p\\) is a valid norm\nWhen \\(p &lt; 1\\): Not a true norm (triangle inequality fails)\n\n\nThe ‚Äú\\(\\frac{1}{2}\\)-Norm‚Äù\nFor \\(\\|x\\|_{1/2}\\), this is not a true norm since \\(p &lt; 1\\), but it provides a very strong sparsity penalty that pushes many components of \\(x\\) to zero when minimized.\nIntuition: A norm must satisfy the triangle inequality (going straight is never longer than going in two steps). For \\(p &lt; 1\\), the unit ball is not convex, so the triangle inequality fails.\n Figure: The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù unit ball is non-convex. The triangle inequality fails because the straight path between two points can be longer than the sum of their norms."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#s-norm",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#s-norm",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "S-Norm",
    "text": "S-Norm\nLet \\(S\\) be a symmetric positive definite matrix. The S-norm is defined as:\n\\[\n\\|v\\|_S = \\sqrt{v^T S v}\n\\]\nSpecial case: When \\(S = I\\) (identity matrix), we get \\(\\|v\\|_2\\) (Euclidean norm).\n\nExample\n\\[\nS = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3\n\\end{bmatrix}\n\\]\nThe unit ball \\(v^T S v = 1\\) forms an ellipse whose axes are determined by the eigenvectors of \\(S\\).\n Figure: The unit ball for the S-norm \\(\\sqrt{v^T S v} = 1\\) forms an ellipse. The shape and orientation depend on the eigenvalues and eigenvectors of the symmetric positive definite matrix \\(S\\)."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#minimizing-norms-regularization",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#minimizing-norms-regularization",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Minimizing Norms: Regularization",
    "text": "Minimizing Norms: Regularization\nWhen solving \\(Ax = b\\), we often want to minimize \\(\\|x\\|_p\\) to prefer certain solutions:\n\n\\(\\ell_1\\) Regularization\n\nProperty: Sparse solutions\nWinner: Solution has many zero components (e.g., \\([0, b]\\) or \\([a, 0]\\))\nUse case: Feature selection, compressed sensing\n\n\n\n\\(\\ell_2\\) Regularization\n\nProperty: Smooth, distributed solutions\nGeometric interpretation: Find the point on the constraint line \\(c_1 x_1 + c_2 x_2 = 0\\) that intersects the smallest \\(\\|v\\|_2 = c\\) level set (circle)\nUse case: Ridge regression, preventing overfitting\n\n Figure: Comparison of minimizing \\(\\ell_1\\) norm (diamond-shaped level sets, leading to sparse solutions at axes) versus \\(\\ell_2\\) norm (circular level sets, leading to distributed solutions). The constraint line intersects different norms at different points, illustrating why \\(\\ell_1\\) regularization produces sparsity."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#matrix-norms",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#matrix-norms",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Matrix Norms",
    "text": "Matrix Norms\n\nSpectral Norm: \\(\\|A\\|_2 = \\sigma_1\\)\nThe spectral norm measures the maximum ‚Äúblow-up‚Äù of a vector:\n\\[\n\\|A\\|_2 = \\max_x \\frac{\\|Ax\\|_2}{\\|x\\|_2} = \\sigma_1\n\\]\nwhere \\(\\sigma_1\\) is the largest singular value of \\(A\\).\nWinner: \\(x = v_1\\) (first right singular vector)\n\n\n\nFrobenius Norm: \\(\\|A\\|_F\\)\nThe Frobenius norm is the square root of the sum of all squared entries:\n\\[\n\\|A\\|_F = \\sqrt{a_{11}^2 + a_{12}^2 + \\cdots + a_{mn}^2}\n\\]\nConnection to SVD:\n\\[\n\\|A\\|_F = \\sqrt{\\sigma_1^2 + \\sigma_2^2 + \\cdots + \\sigma_r^2}\n\\]\nWhy? Because \\(A = U \\Sigma V^T\\), and both \\(U\\) and \\(V\\) are orthonormal matrices (they preserve \\(\\ell_2\\) norm).\n\n\n\nNuclear Norm: \\(\\|A\\|_N\\)\nThe nuclear norm (also called trace norm) is the sum of singular values:\n\\[\n\\|A\\|_N = \\sigma_1 + \\sigma_2 + \\cdots + \\sigma_r\n\\]\nUse case: Low-rank matrix completion (convex relaxation of rank minimization)"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#summary-of-matrix-norms",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#summary-of-matrix-norms",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Summary of Matrix Norms",
    "text": "Summary of Matrix Norms\n Figure: Comparison of matrix norms - spectral norm \\(\\|A\\|_2\\) (largest singular value), Frobenius norm \\(\\|A\\|_F\\) (root sum of squared singular values), and nuclear norm \\(\\|A\\|_N\\) (sum of singular values).\n\n\n\n\n\n\n\n\nNorm\nFormula\nGeometric Meaning\n\n\n\n\nSpectral\n\\(\\|A\\|_2 = \\sigma_1\\)\nMaximum amplification\n\n\nFrobenius\n\\(\\|A\\|_F = \\sqrt{\\sigma_1^2 + \\cdots + \\sigma_r^2}\\)\nRoot mean square of entries\n\n\nNuclear\n\\(\\|A\\|_N = \\sigma_1 + \\cdots + \\sigma_r\\)\nConvex surrogate for rank\n\n\n\nKey insight: All three matrix norms can be expressed in terms of singular values, connecting them to the fundamental SVD decomposition."
  },
  {
    "objectID": "ML/cnn-efficient-convolution.html",
    "href": "ML/cnn-efficient-convolution.html",
    "title": "Chapter 9.8: Efficient Convolution Algorithms",
    "section": "",
    "text": "A convolution kernel is separable when its multi-dimensional weights can be written as the outer product of several one-dimensional filters. In this case, a 2D convolution with a \\(k \\times k\\) kernel can be executed as two sequential 1D convolutions: first applying a vertical filter, then a horizontal one. This decomposition does not change the output‚Äîeach pixel still aggregates information from the full \\(k \\times k\\) neighborhood‚Äîbut it significantly reduces computational cost.\nStandard 2D convolution requires \\(O(HWk^2)\\) operations, while separable convolution reduces this to \\(O(HWk)\\). Parameter storage also shrinks from \\(k^2\\) to \\(2k\\). When such a factorization exists, it enables faster and more memory-efficient models without sacrificing accuracy, and is therefore an important strategy for designing efficient convolutional networks.\n\n Figure: Separable convolution decomposes a 2D kernel into two 1D filters (vertical and horizontal), reducing computational complexity from O(HWk¬≤) to O(HWk) while maintaining the same receptive field."
  },
  {
    "objectID": "ML/cnn-efficient-convolution.html#separable-convolution",
    "href": "ML/cnn-efficient-convolution.html#separable-convolution",
    "title": "Chapter 9.8: Efficient Convolution Algorithms",
    "section": "",
    "text": "A convolution kernel is separable when its multi-dimensional weights can be written as the outer product of several one-dimensional filters. In this case, a 2D convolution with a \\(k \\times k\\) kernel can be executed as two sequential 1D convolutions: first applying a vertical filter, then a horizontal one. This decomposition does not change the output‚Äîeach pixel still aggregates information from the full \\(k \\times k\\) neighborhood‚Äîbut it significantly reduces computational cost.\nStandard 2D convolution requires \\(O(HWk^2)\\) operations, while separable convolution reduces this to \\(O(HWk)\\). Parameter storage also shrinks from \\(k^2\\) to \\(2k\\). When such a factorization exists, it enables faster and more memory-efficient models without sacrificing accuracy, and is therefore an important strategy for designing efficient convolutional networks.\n\n Figure: Separable convolution decomposes a 2D kernel into two 1D filters (vertical and horizontal), reducing computational complexity from O(HWk¬≤) to O(HWk) while maintaining the same receptive field."
  },
  {
    "objectID": "ML/cnn-efficient-convolution.html#key-insight",
    "href": "ML/cnn-efficient-convolution.html#key-insight",
    "title": "Chapter 9.8: Efficient Convolution Algorithms",
    "section": "Key Insight",
    "text": "Key Insight\nSeparable convolution achieves the same output as standard 2D convolution but with dramatically reduced computational cost. By factorizing a \\(k \\times k\\) kernel into two \\(k \\times 1\\) filters, we transform an \\(O(k^2)\\) operation into \\(O(k)\\)‚Äîa massive speedup for large kernels. This decomposition is particularly valuable in mobile and edge deployments where computational resources are limited, and forms the foundation of efficient architectures like MobileNet."
  },
  {
    "objectID": "ML/activation-functions.html",
    "href": "ML/activation-functions.html",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "",
    "text": "This exploration of Deep Learning Chapter 6.3 reveals how activation functions shape the behavior of hidden units in neural networks - and why choosing the right one matters.\nüìì For the complete implementation with additional exercises, see the notebook on GitHub.\nüìö For theoretical background and summary, see the chapter summary."
  },
  {
    "objectID": "ML/activation-functions.html#why-activation-functions-matter",
    "href": "ML/activation-functions.html#why-activation-functions-matter",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "Why Activation Functions Matter",
    "text": "Why Activation Functions Matter\nLinear transformations alone can only represent linear relationships. No matter how many layers you stack, \\(W_3(W_2(W_1x))\\) is still just a linear function. Activation functions introduce the non-linearity that makes deep learning powerful.\nBut which activation function should you use? The answer depends on understanding their mathematical properties and how they affect gradient flow during training.\n\n\n\n\n\n\n\n\n\nActivation\nOutput Range\nKey Property\nBest For\n\n\n\n\nReLU\n\\([0, \\infty)\\)\nZero for negatives\nHidden layers (default choice)\n\n\nSigmoid\n\\((0, 1)\\)\nSquashing, smooth\nBinary classification output\n\n\nTanh\n\\((-1, 1)\\)\nZero-centered\nHidden layers (when centering helps)"
  },
  {
    "objectID": "ML/activation-functions.html#exploring-activation-functions-shape-and-derivatives",
    "href": "ML/activation-functions.html#exploring-activation-functions-shape-and-derivatives",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "üéØ Exploring Activation Functions: Shape and Derivatives",
    "text": "üéØ Exploring Activation Functions: Shape and Derivatives\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Configure plotting\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['axes.grid'] = True\nplt.rcParams['grid.alpha'] = 0.3\n\n\nThe behavior of an activation function is determined by two things: 1. Its shape - how it transforms inputs 2. Its derivative - how gradients flow backward during training\n\nDefine Activation Functions\n\n\nShow code\ndef relu(x):\n    return np.clip(x, 0, np.inf)\n\ndef relu_derivative(x):\n    return np.where(x &gt; 0, 1, 0)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    return 1 - np.tanh(x)**2\n\n\n\n\nPlot Functions and Derivatives\n\n\nShow code\nx = np.linspace(-5, 5, 1000)\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 8))\nfig.suptitle('Common Activation Functions and Their Derivatives', fontsize=16)\n\n# ReLU\naxes[0, 0].plot(x, relu(x), linewidth=2, color='blue')\naxes[0, 0].set_title('ReLU', fontsize=12)\naxes[0, 0].set_ylabel('f(x)', fontsize=11)\naxes[1, 0].plot(x, relu_derivative(x), linewidth=2, color='blue')\naxes[1, 0].set_title('ReLU Derivative', fontsize=12)\naxes[1, 0].set_ylabel(\"f'(x)\", fontsize=11)\naxes[1, 0].set_xlabel('x', fontsize=11)\n\n# Sigmoid\naxes[0, 1].plot(x, sigmoid(x), linewidth=2, color='red')\naxes[0, 1].set_title('Sigmoid', fontsize=12)\naxes[1, 1].plot(x, sigmoid_derivative(x), linewidth=2, color='red')\naxes[1, 1].set_title('Sigmoid Derivative', fontsize=12)\naxes[1, 1].set_xlabel('x', fontsize=11)\n\n# Tanh\naxes[0, 2].plot(x, tanh(x), linewidth=2, color='green')\naxes[0, 2].set_title('Tanh', fontsize=12)\naxes[1, 2].plot(x, tanh_derivative(x), linewidth=2, color='green')\naxes[1, 2].set_title('Tanh Derivative', fontsize=12)\naxes[1, 2].set_xlabel('x', fontsize=11)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nKey observations:\n\nReLU: \\(f(x) = \\max(0, x)\\) - Zero for negative inputs, identity for positive. Derivative is 0 or 1 (simple!).\nSigmoid: \\(f(x) = \\frac{1}{1+e^{-x}}\\) - Squashes inputs to \\((0, 1)\\). Derivative peaks at 0, vanishes at extremes (gradient vanishing problem).\nTanh: \\(f(x) = \\tanh(x)\\) - Similar to sigmoid but outputs in \\((-1, 1)\\). Zero-centered with stronger gradients than sigmoid."
  },
  {
    "objectID": "ML/activation-functions.html#the-dead-relu-problem-when-neurons-stop-learning",
    "href": "ML/activation-functions.html#the-dead-relu-problem-when-neurons-stop-learning",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "The Dead ReLU Problem: When Neurons Stop Learning",
    "text": "The Dead ReLU Problem: When Neurons Stop Learning\nReLU‚Äôs simplicity is its strength, but also its weakness. A ReLU neuron can ‚Äúdie‚Äù - permanently outputting zero and never learning again.\nWhy does this happen?\nWhen a neuron‚Äôs pre-activation values are consistently negative (due to poor initialization, high learning rate, or bad gradients), ReLU outputs zero. Since the derivative is also zero for negative inputs, no gradient flows backward. The neuron is stuck forever.\n\n\nShow code\n# Generate input data\nx = torch.randn(1000, 10)  # 1000 samples, 10 features\nlinear = nn.Linear(10, 5)   # 5 hidden units\n\n# Set bias to large negative values to \"kill\" neurons\nwith torch.no_grad():\n    linear.bias.fill_(-10.0)\n\n# Forward pass\npre_activation = linear(x)\npost_activation = torch.relu(pre_activation)\n\n# Calculate statistics\ndead_percentage = (post_activation == 0).float().mean() * 100\nprint(f\"Percentage of dead neurons: {dead_percentage:.2f}%\\n\")\n\n# Display table showing ReLU input vs output\nprint(\"ReLU Input vs Output (first 10 samples, neuron 0):\")\nprint(\"-\" * 50)\nprint(f\"{'Sample':&lt;10} {'Pre-Activation':&lt;20} {'Post-Activation':&lt;20}\")\nprint(\"-\" * 50)\n\nfor i in range(10):\n    pre_val = pre_activation[i, 0].item()\n    post_val = post_activation[i, 0].item()\n    print(f\"{i:&lt;10} {pre_val:&lt;20.4f} {post_val:&lt;20.4f}\")\n\nprint(\"\\nObservation: All negative inputs become 0 after ReLU ‚Üí Dead neuron!\")\n\n\nPercentage of dead neurons: 100.00%\n\nReLU Input vs Output (first 10 samples, neuron 0):\n--------------------------------------------------\nSample     Pre-Activation       Post-Activation     \n--------------------------------------------------\n0          -9.7837              0.0000              \n1          -10.0322             0.0000              \n2          -10.4466             0.0000              \n3          -10.3243             0.0000              \n4          -10.5448             0.0000              \n5          -9.7712              0.0000              \n6          -10.8104             0.0000              \n7          -11.3418             0.0000              \n8          -9.8559              0.0000              \n9          -8.6873              0.0000              \n\nObservation: All negative inputs become 0 after ReLU ‚Üí Dead neuron!\n\n\nWith a large negative bias, every input becomes negative after the linear transformation. ReLU zeros them all out. The gradient is zero everywhere. The neuron never updates. It‚Äôs dead."
  },
  {
    "objectID": "ML/activation-functions.html#experiment-do-different-activations-make-a-difference",
    "href": "ML/activation-functions.html#experiment-do-different-activations-make-a-difference",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "Experiment: Do Different Activations Make a Difference?",
    "text": "Experiment: Do Different Activations Make a Difference?\nTheory is nice, but let‚Äôs see activation functions in action. We‚Äôll train three identical networks with different activations on a simple regression task: \\(y = \\sin(x) + x^2 + 1\\).\n\nGenerate Data\n\n\nShow code\n# Training data\nx_train = np.random.rand(200, 1)\ny_train = np.sin(x_train) + np.power(x_train, 2) + 1\n\n# Test data\nx_test = np.random.rand(50, 1)\ny_test = np.sin(x_test) + np.power(x_test, 2) + 1\n\n# Convert to PyTorch tensors\nx_train_tensor = torch.FloatTensor(x_train)\ny_train_tensor = torch.FloatTensor(y_train)\nx_test_tensor = torch.FloatTensor(x_test)\ny_test_tensor = torch.FloatTensor(y_test)\n\n\n\n\nCreate and Train Models\n\n\nShow code\ndef create_regression_model(activation_fn):\n    \"\"\"Create a 2-layer network with specified activation\"\"\"\n    return nn.Sequential(\n        nn.Linear(1, 20),\n        activation_fn,\n        nn.Linear(20, 1)\n    )\n\n# Create 3 models with different activations\nmodels = {\n    'ReLU': create_regression_model(nn.ReLU()),\n    'Sigmoid': create_regression_model(nn.Sigmoid()),\n    'Tanh': create_regression_model(nn.Tanh())\n}\n\n# Training configuration\nn_epochs = 100\nlearning_rate = 0.01\nloss_fn = nn.MSELoss()\n\n# Track metrics\nloss_history = {name: [] for name in models.keys()}\ntest_mse_history = {name: [] for name in models.keys()}\n\n# Train each model\nfor name, model in models.items():\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    for epoch in range(n_epochs):\n        # Training\n        model.train()\n        y_pred = model(x_train_tensor)\n        loss = loss_fn(y_pred, y_train_tensor)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loss_history[name].append(loss.item())\n\n        # Evaluation on test set\n        model.eval()\n        with torch.no_grad():\n            y_test_pred = model(x_test_tensor)\n            test_mse = loss_fn(y_test_pred, y_test_tensor).item()\n            test_mse_history[name].append(test_mse)\n\n\n\n\nCompare Learning Curves\n\n\nShow code\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\ncolors = {'ReLU': 'blue', 'Sigmoid': 'red', 'Tanh': 'green'}\n\n# Plot training loss\nfor name, losses in loss_history.items():\n    axes[0].plot(losses, label=name, linewidth=2, color=colors[name])\n\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Training Loss (MSE)', fontsize=12)\naxes[0].set_title('Training Loss Over Time', fontsize=14)\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\naxes[0].set_yscale('log')\n\n# Plot test MSE\nfor name, test_mse in test_mse_history.items():\n    axes[1].plot(test_mse, label=name, linewidth=2, color=colors[name])\n\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Test Loss (MSE)', fontsize=12)\naxes[1].set_title('Test Loss Over Time', fontsize=14)\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\naxes[1].set_yscale('log')\n\nplt.tight_layout()\nplt.show()\n\n# Print final metrics\nprint(\"\\nFinal Metrics after {} epochs:\".format(n_epochs))\nprint(\"-\" * 60)\nprint(f\"{'Activation':&lt;15} {'Train Loss':&lt;15} {'Test Loss':&lt;15}\")\nprint(\"-\" * 60)\nfor name in models.keys():\n    train_loss = loss_history[name][-1]\n    test_loss = test_mse_history[name][-1]\n    print(f\"{name:&lt;15} {train_loss:&lt;15.6f} {test_loss:&lt;15.6f}\")\n\n\n\n\n\n\n\n\n\n\nFinal Metrics after 100 epochs:\n------------------------------------------------------------\nActivation      Train Loss      Test Loss      \n------------------------------------------------------------\nReLU            0.007420        0.008211       \nSigmoid         0.227441        0.247947       \nTanh            0.035384        0.038743"
  },
  {
    "objectID": "ML/bagging-ensemble.html",
    "href": "ML/bagging-ensemble.html",
    "title": "Chapter 7.11: Bagging and Other Ensemble Methods",
    "section": "",
    "text": "Definition: Also known as model averaging, bagging trains several different models and combines their outputs through averaging or voting.\nKey idea: Train multiple models on slightly different datasets, then aggregate their predictions to reduce variance."
  },
  {
    "objectID": "ML/bagging-ensemble.html#bagging-bootstrap-aggregating",
    "href": "ML/bagging-ensemble.html#bagging-bootstrap-aggregating",
    "title": "Chapter 7.11: Bagging and Other Ensemble Methods",
    "section": "",
    "text": "Definition: Also known as model averaging, bagging trains several different models and combines their outputs through averaging or voting.\nKey idea: Train multiple models on slightly different datasets, then aggregate their predictions to reduce variance."
  },
  {
    "objectID": "ML/bagging-ensemble.html#mathematical-analysis-of-ensemble-error",
    "href": "ML/bagging-ensemble.html#mathematical-analysis-of-ensemble-error",
    "title": "Chapter 7.11: Bagging and Other Ensemble Methods",
    "section": "2. Mathematical Analysis of Ensemble Error",
    "text": "2. Mathematical Analysis of Ensemble Error\n\nSetup\nConsider an ensemble of \\(k\\) models with prediction errors:\n\nError of model \\(i\\): \\(\\epsilon_i\\)\nVariance: \\(\\mathbb{E}[\\epsilon_i^2] = v\\)\nCovariance: \\(\\mathbb{E}[\\epsilon_i \\epsilon_j] = c\\) (for \\(i \\neq j\\))\nNumber of models: \\(k\\)\n\n\n\nExpected Squared Error of Ensemble\nEquation 7.50 - Ensemble error analysis:\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\left(\\frac{1}{k} \\sum_i \\epsilon_i\\right)^2\\right] &= \\frac{1}{k^2}\\mathbb{E}\\left[\\sum_i \\left(\\epsilon_i^2 + \\sum_{i \\neq j} \\epsilon_i \\epsilon_j \\right) \\right] \\\\\n&= \\frac{1}{k}v + \\frac{k-1}{k}c\n\\end{aligned}\n\\]\nDerivation: - Expand the squared sum: \\((\\sum_i \\epsilon_i)^2 = \\sum_i \\epsilon_i^2 + \\sum_{i \\neq j} \\epsilon_i \\epsilon_j\\) - Take expectation: \\(\\mathbb{E}[\\epsilon_i^2] = v\\) and \\(\\mathbb{E}[\\epsilon_i \\epsilon_j] = c\\) - There are \\(k\\) terms with \\(\\epsilon_i^2\\) and \\(k(k-1)\\) terms with \\(\\epsilon_i \\epsilon_j\\) - Divide by \\(k^2\\) from the average\n\n\n\nAnalysis of Different Cases\nEquation 7.51 - Case 1: Perfectly correlated errors (\\(c = v\\))\n\\[\n\\frac{1}{k}v + \\frac{k-1}{k}v = \\frac{1}{k}v + v - \\frac{1}{k}v = v\n\\]\nInterpretation: The expected error equals \\(v\\), meaning bagging provides no benefit. Models make the same mistakes.\n\nCase 2: Uncorrelated errors (\\(c = 0\\))\n\\[\n\\frac{1}{k}v + \\frac{k-1}{k} \\cdot 0 = \\frac{1}{k}v\n\\]\nInterpretation: Expected error is \\(\\frac{1}{k}v\\). As \\(k\\) increases, error approaches zero. This is the ideal case for bagging.\n\nCase 3: Partially correlated errors (\\(0 &lt; c &lt; v\\))\n\\[\n\\frac{1}{k}v &lt; \\text{Expected Error} &lt; v\n\\]\nInterpretation: Error is between \\(\\frac{1}{k}v\\) and \\(v\\). Bagging provides some benefit, but not as much as the uncorrelated case.\n\nCase 4: Perfectly anti-correlated errors (\\(c &gt; v\\))\n\\[\n\\text{Not possible in practice}\n\\]\nReason: Covariance cannot exceed variance for prediction errors."
  },
  {
    "objectID": "ML/bagging-ensemble.html#training-approach",
    "href": "ML/bagging-ensemble.html#training-approach",
    "title": "Chapter 7.11: Bagging and Other Ensemble Methods",
    "section": "3. Training Approach",
    "text": "3. Training Approach\n\nStep 1: Bootstrap Sampling\nProcess: - From the original dataset, draw multiple new datasets with replacement - Each bootstrap dataset has the same size as the original - Contains some duplicated and some omitted examples - On average, each bootstrap dataset includes about two-thirds of the unique samples\n\n\n\nStep 2: Train Multiple Models\nProcess: - Train one model (ensemble member) on each bootstrap dataset - Because training data differ slightly, each model learns slightly different patterns - Each model makes different errors on different regions of input space\nEffect: Creates diversity among models, reducing correlation between errors.\n\n\n\nStep 3: Average Predictions\nInference: - All models predict on the same input - Final output is the average (regression) or majority vote (classification) of all predictions\nAggregation formulas:\nRegression:\n\\[\n\\hat{y} = \\frac{1}{k} \\sum_{i=1}^k f_i(x)\n\\]\nClassification:\n\\[\n\\hat{y} = \\operatorname{mode}\\{f_1(x), f_2(x), \\ldots, f_k(x)\\}\n\\]"
  },
  {
    "objectID": "ML/bagging-ensemble.html#example-digit-recognition",
    "href": "ML/bagging-ensemble.html#example-digit-recognition",
    "title": "Chapter 7.11: Bagging and Other Ensemble Methods",
    "section": "4. Example: Digit Recognition",
    "text": "4. Example: Digit Recognition\nOriginal dataset: Contains digits \\(\\{8, 6, 9\\}\\)\nBootstrap samples: - Bootstrap Sample 1 ‚Üí \\(\\{9, 6, 8\\}\\) - Bootstrap Sample 2 ‚Üí \\(\\{9, 9, 8\\}\\)\nTraining: - Each dataset trains a model to recognize the digit ‚Äú8‚Äù - Samples differ (one has more 9‚Äôs, another lacks 6‚Äôs) - Models learn different decision boundaries\nKey insight: - Individually, each model may be unreliable - Their average output is much more stable - Errors tend to cancel out through averaging\n\nSource: Deep Learning Book, Chapter 7.11"
  },
  {
    "objectID": "ML/rnn-bidirectional.html",
    "href": "ML/rnn-bidirectional.html",
    "title": "Chapter 10.3: Bidirectional RNN",
    "section": "",
    "text": "In some applications, the prediction of \\(y^t\\) may rely on the entire input sequence, not only \\(y_1,...y^{t-1}\\). For example, in speech recognition, correctly interpreting the current sound often depends on acoustic cues that appear later in the sequence.\nBidirectional RNNs were designed specifically to address such cases.\nBy incorporating information from both past and future inputs, they have achieved great success in inherently bidirectional tasks such as handwritten text recognition, speech recognition, and biological sequence analysis."
  },
  {
    "objectID": "ML/rnn-bidirectional.html#architecture",
    "href": "ML/rnn-bidirectional.html#architecture",
    "title": "Chapter 10.3: Bidirectional RNN",
    "section": "Architecture",
    "text": "Architecture\nObviously, a bidirectional RNN combines two RNNs: one that processes the sequence from the beginning to the end, and another that runs in the reverse direction.\nThis allows the output unit \\(o^{(t)}\\) to receive information from both the past and the future.\nBidirectional RNNs can also be extended to operate on 2-D structured inputs, such as images, where four directional RNNs (left, right, up, and down) may be used to capture context in all spatial directions.\n\n\n\n\nBidirectional RNN architecture"
  },
  {
    "objectID": "ML/adaptive-learning-rates.html",
    "href": "ML/adaptive-learning-rates.html",
    "title": "Chapter 8.5: Algorithms with Adaptive Learning Rates",
    "section": "",
    "text": "A fundamental challenge in optimization is choosing the right learning rate. Too large, and training diverges; too small, and progress is painfully slow. Moreover, different parameters may benefit from different learning rates‚Äîsome require large steps while others need fine-tuning.\nAdaptive learning rate algorithms address this challenge by automatically adjusting the learning rate for each parameter based on the history of gradients. This section covers three major algorithms: AdaGrad, RMSProp, and Adam."
  },
  {
    "objectID": "ML/adaptive-learning-rates.html#adagrad",
    "href": "ML/adaptive-learning-rates.html#adagrad",
    "title": "Chapter 8.5: Algorithms with Adaptive Learning Rates",
    "section": "AdaGrad",
    "text": "AdaGrad\nCore idea: AdaGrad scales the learning rate for each parameter inversely proportional to the square root of the cumulative sum of its past squared gradients.\nIntuition: Parameters with large gradients have received large updates in the past and should now take smaller steps. Parameters with small gradients have moved little and can afford larger steps.\n\nAlgorithm\nHyperparameters:\n\nLearning rate \\(\\epsilon\\)\nSmall constant \\(\\delta\\) (typically \\(10^{-7}\\) for numerical stability)\nInitial gradient accumulator \\(r = 0\\)\n\nTraining procedure:\nWhile stopping criterion not met:\n\nSample \\(m\\) examples from the training set\nCompute the gradient:\n\n\\[\ng \\leftarrow \\frac{1}{m}\\nabla_{\\theta}\\sum_{i=1}^m L(f(x^{(i)};\\theta), y^{(i)})\n\\]\n\nAccumulate squared gradients:\n\n\\[\nr \\leftarrow r + g \\odot g\n\\]\nwhere \\(\\odot\\) denotes element-wise multiplication.\n\nCompute the parameter update:\n\n\\[\n\\Delta\\theta \\leftarrow -\\frac{\\epsilon}{\\delta + \\sqrt{r}} \\odot g\n\\]\n\nUpdate parameters:\n\n\\[\n\\theta \\leftarrow \\theta + \\Delta\\theta\n\\]\n\n\nKey Properties\nAdvantages:\n\nAutomatically adapts learning rates for each parameter\nNo manual learning rate tuning required for each parameter\nWorks well for sparse features (e.g., in NLP tasks)\n\nDisadvantages:\n\nThe accumulator \\(r\\) grows monotonically, causing learning rates to shrink continuously\nEventually, learning rates become infinitesimally small, and learning stops\nThis makes AdaGrad unsuitable for training deep neural networks"
  },
  {
    "objectID": "ML/adaptive-learning-rates.html#rmsprop",
    "href": "ML/adaptive-learning-rates.html#rmsprop",
    "title": "Chapter 8.5: Algorithms with Adaptive Learning Rates",
    "section": "RMSProp",
    "text": "RMSProp\nCore idea: RMSProp (Root Mean Square Propagation) uses an exponential decay to discount very old gradients, allowing the algorithm to forget distant history and achieve faster convergence once it reaches a convex bowl.\nIntuition: Unlike AdaGrad, which accumulates all past gradients forever, RMSProp uses an exponentially weighted moving average. This allows the learning rate to increase again if recent gradients are small, even if very old gradients were large.\n\nAlgorithm\nHyperparameters:\n\nLearning rate \\(\\epsilon\\) (typically 0.001)\nDecay rate \\(\\rho\\) (typically 0.9)\nSmall constant \\(\\delta\\) (typically \\(10^{-6}\\))\nInitial gradient accumulator \\(r = 0\\)\n\nTraining procedure:\nWhile stopping criterion not met:\n\nSample \\(m\\) examples from the training set\nCompute the gradient:\n\n\\[\ng \\leftarrow \\frac{1}{m}\\nabla_{\\theta}\\sum_{i=1}^m L(f(x^{(i)};\\theta), y^{(i)})\n\\]\n\nAccumulate squared gradients with exponential decay:\n\n\\[\nr \\leftarrow \\rho r + (1 - \\rho) g \\odot g\n\\]\nThis is the key difference from AdaGrad: instead of \\(r \\leftarrow r + g \\odot g\\), we use a weighted average.\n\nCompute the parameter update:\n\n\\[\n\\Delta\\theta \\leftarrow -\\frac{\\epsilon}{\\delta + \\sqrt{r}} \\odot g\n\\]\n\nUpdate parameters:\n\n\\[\n\\theta \\leftarrow \\theta + \\Delta\\theta\n\\]\n\n\nKey Properties\nAdvantages:\n\nOvercomes AdaGrad‚Äôs aggressive learning rate decay\nLearning rates can increase when recent gradients are smaller than historical averages\nGenerally more robust than AdaGrad for non-convex optimization\n\nComparison to AdaGrad:\n\nAdaGrad: \\(r_t = r_{t-1} + g_t^2\\) (monotonically increasing)\nRMSProp: \\(r_t = \\rho r_{t-1} + (1-\\rho)g_t^2\\) (can increase or decrease)"
  },
  {
    "objectID": "ML/adaptive-learning-rates.html#rmsprop-with-nesterov-momentum",
    "href": "ML/adaptive-learning-rates.html#rmsprop-with-nesterov-momentum",
    "title": "Chapter 8.5: Algorithms with Adaptive Learning Rates",
    "section": "RMSProp with Nesterov Momentum",
    "text": "RMSProp with Nesterov Momentum\nCombining RMSProp‚Äôs adaptive learning rates with Nesterov momentum‚Äôs lookahead gradient computation often yields better performance.\n\nAlgorithm\nHyperparameters:\n\nLearning rate \\(\\epsilon\\)\nDecay rate \\(\\rho\\)\nSmall constant \\(\\delta\\)\nMomentum coefficient \\(\\alpha\\) (typically 0.9)\nInitial gradient accumulator \\(r = 0\\)\nInitial velocity \\(v = 0\\)\n\nTraining procedure:\nWhile stopping criterion not met:\n\nSample \\(m\\) examples from the training set\nCompute the lookahead parameters:\n\n\\[\n\\tilde{\\theta} \\leftarrow \\theta + \\alpha v\n\\]\n\nCompute the gradient at the lookahead position:\n\n\\[\ng \\leftarrow \\frac{1}{m}\\nabla_{\\tilde{\\theta}}\\sum_{i=1}^m L(f(x^{(i)};\\tilde{\\theta}), y^{(i)})\n\\]\n\nAccumulate squared gradients with exponential decay:\n\n\\[\nr \\leftarrow \\rho r + (1 - \\rho) g \\odot g\n\\]\n\nUpdate velocity:\n\n\\[\nv \\leftarrow \\alpha v - \\frac{\\epsilon}{\\delta + \\sqrt{r}} \\odot g\n\\]\n\nUpdate parameters:\n\n\\[\n\\theta \\leftarrow \\theta + v\n\\]\nThis combines the best of both worlds: Nesterov‚Äôs anticipatory gradient and RMSProp‚Äôs adaptive learning rates."
  },
  {
    "objectID": "ML/adaptive-learning-rates.html#adam",
    "href": "ML/adaptive-learning-rates.html#adam",
    "title": "Chapter 8.5: Algorithms with Adaptive Learning Rates",
    "section": "Adam",
    "text": "Adam\nCore idea: Adam (Adaptive Moment Estimation) combines the benefits of RMSProp and momentum by keeping track of both first-order moments (mean) and second-order moments (variance) of the gradients, along with bias correction for initialization.\nIntuition: Adam maintains two moving averages:\n\n\\(s\\): The first moment (mean) of gradients, providing momentum-like behavior\n\\(r\\): The second moment (uncentered variance) of gradients, providing adaptive learning rates like RMSProp\n\n\nAlgorithm\nHyperparameters:\n\nLearning rate \\(\\epsilon\\) (default: 0.001)\nDecay rates:\n\n\\(\\rho_1\\) for first moment (default: 0.9)\n\\(\\rho_2\\) for second moment (default: 0.999)\n\nSmall constant \\(\\delta\\) (typically \\(10^{-8}\\))\nInitial first moment \\(s = 0\\)\nInitial second moment \\(r = 0\\)\nTime step \\(t = 0\\)\n\nTraining procedure:\nWhile stopping criterion not met:\n\nIncrement time step: \\(t \\leftarrow t + 1\\)\nSample \\(m\\) examples from the training set\nCompute the gradient:\n\n\\[\ng \\leftarrow \\frac{1}{m}\\nabla_{\\theta}\\sum_{i=1}^m L(f(x^{(i)};\\theta), y^{(i)})\n\\]\n\nUpdate first moment estimate (momentum-like term):\n\n\\[\ns \\leftarrow \\rho_1 s + (1 - \\rho_1) g\n\\]\n\nApply bias correction to first moment:\n\n\\[\n\\hat{s} \\leftarrow \\frac{s}{1 - \\rho_1^t}\n\\]\n\nUpdate second moment estimate (adaptive learning rate term):\n\n\\[\nr \\leftarrow \\rho_2 r + (1 - \\rho_2) g \\odot g\n\\]\n\nApply bias correction to second moment:\n\n\\[\n\\hat{r} \\leftarrow \\frac{r}{1 - \\rho_2^t}\n\\]\n\nCompute the parameter update:\n\n\\[\n\\Delta\\theta \\leftarrow -\\epsilon \\frac{\\hat{s}}{\\sqrt{\\hat{r}} + \\delta}\n\\]\n\nUpdate parameters:\n\n\\[\n\\theta \\leftarrow \\theta + \\Delta\\theta\n\\]\n\n\nUnderstanding Bias Correction\nAt the beginning of training, Adam initializes its moving averages to zero:\n\\[\ns_0 = 0, \\quad r_0 = 0\n\\]\nThis causes the first few estimates of the mean (\\(s_t\\)) and variance (\\(r_t\\)) of the gradients to be biased toward zero, simply because there is not enough historical data yet.\nMathematical analysis:\nThe expected value of the uncorrected first moment is:\n\\[\n\\mathbb{E}[s_t] = (1 - \\rho_1^t)\\mathbb{E}[g_t]\n\\]\nSo \\(s_t\\) underestimates the true mean by a factor of \\(1 - \\rho_1^t\\).\nBias correction: To correct this ‚Äúcold start‚Äù bias, Adam divides each estimate by that same factor:\n\\[\n\\hat{s}_t = \\frac{s_t}{1 - \\rho_1^t}, \\quad \\hat{r}_t = \\frac{r_t}{1 - \\rho_2^t}\n\\]\nAfter correction, the expected values become unbiased:\n\\[\n\\mathbb{E}[\\hat{s}_t] = \\mathbb{E}[g_t]\n\\]\nNote: As \\(t\\) increases, \\(\\rho_1^t \\to 0\\) and \\(\\rho_2^t \\to 0\\), so the bias correction becomes negligible after the initial training phase. The correction is most important in the first few iterations.\n\n\nInterpreting the Update Rule\nThink of \\(\\hat{s}_t\\) as the average direction we want to go, and \\(\\sqrt{\\hat{r}_t}\\) as the estimated magnitude or volatility of that direction.\nThe update rule \\(\\Delta\\theta = -\\epsilon \\frac{\\hat{s}}{\\sqrt{\\hat{r}} + \\delta}\\) means:\nParameters with large, noisy gradients ‚Üí smaller updates\n\nHigh gradient variance (\\(\\hat{r}\\) is large) ‚Üí larger denominator ‚Üí smaller step size\nThis prevents overshooting in directions with high uncertainty\n\nParameters with small, stable gradients ‚Üí larger updates\n\nLow gradient variance (\\(\\hat{r}\\) is small) ‚Üí smaller denominator ‚Üí larger step size\nThis accelerates progress in directions with consistent signal\n\nThis adaptive behavior is why Adam works well across a wide range of problems without extensive hyperparameter tuning.\n\n\nKey Properties\nAdvantages:\n\nCombines benefits of momentum and adaptive learning rates\nBias correction prevents underestimation in early training\nGenerally robust default hyperparameters (often works with \\(\\epsilon = 0.001\\), \\(\\rho_1 = 0.9\\), \\(\\rho_2 = 0.999\\))\nWidely used in practice for training deep neural networks\n\nWhen to use each algorithm:\n\nAdaGrad: Sparse data, convex problems (but not deep learning)\nRMSProp: Good general-purpose optimizer, especially for RNNs\nAdam: Most popular default choice for deep learning"
  },
  {
    "objectID": "ML/adaptive-learning-rates.html#summary-comparison-of-adaptive-methods",
    "href": "ML/adaptive-learning-rates.html#summary-comparison-of-adaptive-methods",
    "title": "Chapter 8.5: Algorithms with Adaptive Learning Rates",
    "section": "Summary: Comparison of Adaptive Methods",
    "text": "Summary: Comparison of Adaptive Methods\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nFirst Moment\nSecond Moment\nBias Correction\nBest Use Case\n\n\n\n\nSGD + Momentum\n‚úì (momentum)\n‚úó\n‚úó\nSimple, well-understood problems\n\n\nAdaGrad\n‚úó\n‚úì (cumulative)\n‚úó\nSparse features, convex optimization\n\n\nRMSProp\n‚úó\n‚úì (exponential avg)\n‚úó\nRNNs, non-convex problems\n\n\nAdam\n‚úì (exponential avg)\n‚úì (exponential avg)\n‚úì\nGeneral deep learning (most popular)\n\n\n\nThe evolution from SGD to Adam represents a progression toward algorithms that require less manual tuning while adapting automatically to the optimization landscape."
  },
  {
    "objectID": "ML/cnn-infinitely-strong-prior.html",
    "href": "ML/cnn-infinitely-strong-prior.html",
    "title": "Chapter 9.4: Convolution and Pooling as an Infinitely Strong Prior",
    "section": "",
    "text": "This section examines how convolutional architectures encode strong assumptions about the data:\n\nStructural priors: Local connectivity and weight sharing as architectural constraints\nInfinitely strong prior: CNNs as restricted fully connected networks\nTranslation invariance: How pooling enforces spatial insensitivity\nTask-dependent effectiveness: When these priors help or hurt performance\nDomain specificity: Why convolution is not universally appropriate\n\nUnderstanding CNNs as embodying strong priors helps explain both their effectiveness on images and their limitations on other data types."
  },
  {
    "objectID": "ML/cnn-infinitely-strong-prior.html#overview",
    "href": "ML/cnn-infinitely-strong-prior.html#overview",
    "title": "Chapter 9.4: Convolution and Pooling as an Infinitely Strong Prior",
    "section": "",
    "text": "This section examines how convolutional architectures encode strong assumptions about the data:\n\nStructural priors: Local connectivity and weight sharing as architectural constraints\nInfinitely strong prior: CNNs as restricted fully connected networks\nTranslation invariance: How pooling enforces spatial insensitivity\nTask-dependent effectiveness: When these priors help or hurt performance\nDomain specificity: Why convolution is not universally appropriate\n\nUnderstanding CNNs as embodying strong priors helps explain both their effectiveness on images and their limitations on other data types."
  },
  {
    "objectID": "ML/cnn-infinitely-strong-prior.html#convolution-and-pooling-introduce-strong-structural-priors",
    "href": "ML/cnn-infinitely-strong-prior.html#convolution-and-pooling-introduce-strong-structural-priors",
    "title": "Chapter 9.4: Convolution and Pooling as an Infinitely Strong Prior",
    "section": "1. Convolution and Pooling Introduce Strong Structural Priors",
    "text": "1. Convolution and Pooling Introduce Strong Structural Priors\nFully connected networks have an enormous amount of freedom and require vast amounts of data.\nConvolution imposes two major constraints:\n\nLocal connectivity: Each hidden unit only interacts with a small spatial neighborhood\nWeight sharing: The same filter is applied everywhere\n\nThese constraints act as strong priors that greatly reduce model complexity, improve statistical efficiency, and guide learning toward functions that are meaningful for images.\nWhy this matters: Without these priors, a network would need to learn separately that edge detection at position (10, 10) should use the same weights as edge detection at position (50, 50). The convolutional prior encodes this knowledge directly into the architecture."
  },
  {
    "objectID": "ML/cnn-infinitely-strong-prior.html#a-convolutional-network-as-a-fully-connected-network-with-infinitely-strong-restrictions",
    "href": "ML/cnn-infinitely-strong-prior.html#a-convolutional-network-as-a-fully-connected-network-with-infinitely-strong-restrictions",
    "title": "Chapter 9.4: Convolution and Pooling as an Infinitely Strong Prior",
    "section": "2. A Convolutional Network as a Fully Connected Network with Infinitely Strong Restrictions",
    "text": "2. A Convolutional Network as a Fully Connected Network with Infinitely Strong Restrictions\nIf we start with a fully connected model and then force: - All weights outside a local window to be exactly zero - All local windows to reuse the same set of weights\nWe obtain a convolutional network.\nThis can be interpreted as introducing an ‚Äúinfinitely strong prior‚Äù ‚Äî only a very specific family of functions (local, translation-equivariant ones) is allowed, and all others are ruled out by architectural design.\nComparison:\n\n\n\n\n\n\n\n\nAspect\nFully Connected\nConvolutional\n\n\n\n\nWeight constraints\nNone (all weights independent)\nLocal + shared weights\n\n\nPrior strength\nWeak (relies on data)\nInfinitely strong (architectural)\n\n\nParameter efficiency\nLow (millions of parameters)\nHigh (thousands of parameters)\n\n\nInductive bias\nMinimal\nStrong (locality + stationarity)\n\n\n\nInterpretation: A Bayesian prior assigns probability to different functions. An ‚Äúinfinitely strong prior‚Äù assigns probability 1 to functions satisfying the architectural constraints and probability 0 to all others."
  },
  {
    "objectID": "ML/cnn-infinitely-strong-prior.html#pooling-adds-another-infinitely-strong-prior-translation-invariance",
    "href": "ML/cnn-infinitely-strong-prior.html#pooling-adds-another-infinitely-strong-prior-translation-invariance",
    "title": "Chapter 9.4: Convolution and Pooling as an Infinitely Strong Prior",
    "section": "3. Pooling Adds Another Infinitely Strong Prior: Translation Invariance",
    "text": "3. Pooling Adds Another Infinitely Strong Prior: Translation Invariance\nPooling enforces the assumption that the exact spatial location within a small region does not matter.\nOnly the presence of a feature in the region matters.\nFor each pooling window: - The output is determined solely by the largest (or average) activation - Not by where inside the region the activation occurs\nThis encodes a strong prior that the model should be insensitive to small translations, reducing variance and improving robustness.\nExample: Consider detecting the edge of an object. With pooling: - Edge at pixel (15, 20): max pool value ‚âà 0.9 - Edge at pixel (16, 21): max pool value ‚âà 0.9 (nearly unchanged)\nThe classifier sees nearly identical features regardless of the 1-pixel shift."
  },
  {
    "objectID": "ML/cnn-infinitely-strong-prior.html#these-priors-can-help-or-hurt-depending-on-the-task",
    "href": "ML/cnn-infinitely-strong-prior.html#these-priors-can-help-or-hurt-depending-on-the-task",
    "title": "Chapter 9.4: Convolution and Pooling as an Infinitely Strong Prior",
    "section": "4. These Priors Can Help or Hurt, Depending on the Task",
    "text": "4. These Priors Can Help or Hurt, Depending on the Task\n\nWhen the Priors Help\nFor image classification, these priors are extremely useful because natural images: - Contain localized structures (edges, textures, objects) - Exhibit stationarity across spatial positions (statistics don‚Äôt change with location) - Benefit from translation invariance (a cat is a cat regardless of position)\nThe strong priors dramatically reduce the amount of data needed to train effective models.\n\n\nWhen the Priors Hurt\nFor tasks requiring precise spatial relationships ‚Äî e.g., localization, medical imaging, fine-grained keypoint prediction ‚Äî pooling may discard important information and increase bias.\nExample problems: - Object localization: Need exact coordinates, but pooling discards spatial precision - Medical imaging: Exact tumor position matters, not just presence - Pose estimation: Keypoint coordinates must be precise\nSome CNN variants (e.g., Inception, Szegedy et al.) redesign pooling strategy to avoid this issue.\nTrade-off: Strong priors reduce variance (need less data) but increase bias (restrict function class). The optimal choice depends on whether the task aligns with the prior assumptions."
  },
  {
    "objectID": "ML/cnn-infinitely-strong-prior.html#different-domains-require-different-priors-convolution-is-not-universally-appropriate",
    "href": "ML/cnn-infinitely-strong-prior.html#different-domains-require-different-priors-convolution-is-not-universally-appropriate",
    "title": "Chapter 9.4: Convolution and Pooling as an Infinitely Strong Prior",
    "section": "5. Different Domains Require Different Priors; Convolution Is Not Universally Appropriate",
    "text": "5. Different Domains Require Different Priors; Convolution Is Not Universally Appropriate\nThe section emphasizes that convolution + pooling is very effective for images, but the same assumptions may not hold for:\n\nSets (requiring permutation invariance)\nGraphs (requiring relational/structural priors)\nPoint clouds or 3D geometric data\nTasks where invariance must be learned, not assumed\n\nKey insight: The success of CNNs on images comes from the alignment between: 1. The architectural prior (locality + translation equivariance) 2. The statistical structure of natural images\nFor other data types, different architectures with different priors may be more appropriate: - Graph neural networks: For relational data with graph structure - Transformers: For sequence data with long-range dependencies - Set networks: For permutation-invariant tasks - Equivariant networks: For data with specific symmetries (rotation, scale, etc.)\n Figure: Different domains require different architectural priors - convolution excels for images due to locality and translation equivariance, while other data types benefit from architectures with priors matching their structure (graphs, sequences, sets, etc.)."
  },
  {
    "objectID": "ML/cnn-infinitely-strong-prior.html#summary",
    "href": "ML/cnn-infinitely-strong-prior.html#summary",
    "title": "Chapter 9.4: Convolution and Pooling as an Infinitely Strong Prior",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\nConcept\nKey Idea\n\n\n\n\nArchitectural Prior\nConvolution and pooling encode assumptions directly into network structure\n\n\nInfinitely Strong Prior\nCNN = fully connected network with zero probability on non-local, non-shared weight configurations\n\n\nStructural Constraints\nLocal connectivity + weight sharing reduce parameters and improve sample efficiency\n\n\nTranslation Invariance\nPooling enforces insensitivity to exact spatial locations within neighborhoods\n\n\nBias-Variance Trade-off\nStrong priors reduce variance but increase bias if assumptions don‚Äôt match task\n\n\nDomain Specificity\nConvolution is optimized for images; other domains need different architectural priors\n\n\n\nFundamental principle: The effectiveness of an architecture depends on how well its built-in priors match the statistical structure of the data and task requirements. CNNs succeed on images because their priors (locality, stationarity, translation invariance) align perfectly with the properties of natural images."
  },
  {
    "objectID": "ML/rnn-encoder-decoder.html",
    "href": "ML/rnn-encoder-decoder.html",
    "title": "Chapter 10.4: Encoder-Decoder Sequence-to-Sequence Architecture",
    "section": "",
    "text": "The encoder‚Äìdecoder (seq2seq) architecture was introduced to model conditional sequence distributions where the input and output sequences may have different lengths and different semantic roles. This generalizes earlier RNN architectures, which assumed a one-to-one correspondence between input and output over time \\((i.e., n_x = n_y = \\tau).\\)"
  },
  {
    "objectID": "ML/rnn-encoder-decoder.html#encoder",
    "href": "ML/rnn-encoder-decoder.html#encoder",
    "title": "Chapter 10.4: Encoder-Decoder Sequence-to-Sequence Architecture",
    "section": "1. Encoder",
    "text": "1. Encoder\nThe encoder RNN reads the entire input sequence \\[x^{(1)}, x^{(2)}, \\dots, x^{(n_x)},\\] and gradually compresses all information into a fixed-dimensional context vector: \\[C = \\text{Encoder}(x^{(1:n_x)}).\\] This context vector represents the input sequence as a single summary embedding."
  },
  {
    "objectID": "ML/rnn-encoder-decoder.html#decoder",
    "href": "ML/rnn-encoder-decoder.html#decoder",
    "title": "Chapter 10.4: Encoder-Decoder Sequence-to-Sequence Architecture",
    "section": "2. Decoder",
    "text": "2. Decoder\nThe decoder RNN takes the context vector C and generates the output sequence \\[y^{(1)}, y^{(2)}, \\dots, y^{(n_y)},\\] one step at a time.\nAt each time step, the decoder conditions on: - the context vector C - the previous outputs \\(y^{(1:t-1)}\\) (teacher forcing during training)\nThus the conditional distribution over the output is: \\[P(Y \\mid X)\n= \\prod_{t=1}^{n_y} P\\big(y^{(t)} \\mid y^{(1:t-1)},\\, C\\big).\\]"
  },
  {
    "objectID": "ML/rnn-encoder-decoder.html#training-objective",
    "href": "ML/rnn-encoder-decoder.html#training-objective",
    "title": "Chapter 10.4: Encoder-Decoder Sequence-to-Sequence Architecture",
    "section": "3. Training Objective",
    "text": "3. Training Objective\nThe encoder and decoder are trained jointly to maximize the conditional log-likelihood: \\[\\log P\\big(y^{(1)}, \\dots, y^{(n_y)}\n\\mid\nx^{(1)}, \\dots, x^{(n_x)}\\big).\\] This formulation places no restriction on the relationship between \\(n_x\\) and \\(n_y\\), enabling flexible sequence transformation.\n\nThe encoder‚Äìdecoder framework enables RNNs to address problems where: - input length ‚â† output length - temporal alignment is unknown - outputs depend on both the entire input and previously generated outputs\nThis architecture is the foundation for many key applications, such as: - machine translation - summarization - dialogue generation - image captioning (when combined with CNN encoders)\nIt also forms the conceptual basis for the later development of attention mechanisms.\n\n\n\n\nEncoder-Decoder Sequence-to-Sequence Architecture"
  },
  {
    "objectID": "ML/second-order-methods.html",
    "href": "ML/second-order-methods.html",
    "title": "Chapter 8.6: Second-Order Optimization Methods",
    "section": "",
    "text": "While first-order methods like gradient descent rely only on the gradient, second-order methods incorporate curvature information through the Hessian matrix. This section explores classical second-order optimization techniques: Newton‚Äôs method, Conjugate Gradient, and BFGS.\nImportant context: Although these methods are theoretically elegant and powerful, they are rarely used in modern deep learning due to computational cost and scalability issues. Practical deep learning almost exclusively relies on first-order methods (SGD, Momentum, RMSProp, Adam)."
  },
  {
    "objectID": "ML/second-order-methods.html#newtons-method",
    "href": "ML/second-order-methods.html#newtons-method",
    "title": "Chapter 8.6: Second-Order Optimization Methods",
    "section": "Newton‚Äôs Method",
    "text": "Newton‚Äôs Method\nNewton‚Äôs method uses a second-order Taylor approximation to find the optimal step direction.\n\nSecond-order Taylor Approximation\nAround a point \\(\\theta_0\\), we approximate the objective function using both first and second derivatives:\n\\[\nJ(\\theta) \\approx J(\\theta_0) + (\\theta - \\theta_0)^{\\top}\\nabla_{\\theta}J(\\theta_0) + \\frac{1}{2}(\\theta - \\theta_0)^{\\top}H(\\theta_0)(\\theta - \\theta_0) \\tag{8.26}\n\\]\nwhere \\(H(\\theta_0)\\) is the Hessian matrix at \\(\\theta_0\\):\n\\[\nH_{ij} = \\frac{\\partial^2 J}{\\partial \\theta_i \\partial \\theta_j}\n\\]\nThis approximation is exact for quadratic functions and captures local curvature for general functions.\n\n\nNewton‚Äôs Update Rule\nTo minimize the quadratic approximation, we find where its gradient equals zero:\n\\[\n\\theta^* = \\theta_0 - H^{-1}\\nabla_{\\theta_0}J(\\theta_0) \\tag{8.27}\n\\]\nThis is the Newton step: instead of moving in the direction of the gradient, we move in the direction \\(H^{-1}g\\), which accounts for the curvature of the loss surface.\n\n\nDeriving the Newton Step\nLet‚Äôs derive equation 8.27 from equation 8.26.\nStep 1: Define simplified notation:\n\n\\(\\Delta = \\theta - \\theta_0\\) (the step we want to find)\n\\(g = \\nabla_{\\theta}J(\\theta_0)\\) (gradient at current point)\n\\(H = H(\\theta_0)\\) (Hessian at current point)\n\nStep 2: Rewrite the approximation:\n\\[\n\\tilde{J}(\\theta) \\approx J(\\theta_0) + \\Delta^{\\top}g + \\frac{1}{2}\\Delta^{\\top}H\\Delta\n\\]\nStep 3: Find the minimum by taking the derivative with respect to \\(\\Delta\\):\n\\[\n\\frac{\\partial \\tilde{J}}{\\partial \\Delta} = g + H\\Delta\n\\]\nExplanation of the derivative:\n\nDerivative of \\(\\Delta^{\\top}g\\) is \\(g\\) (linear term)\nDerivative of \\(\\frac{1}{2}\\Delta^{\\top}H\\Delta\\) is \\(H\\Delta\\) (quadratic term, using symmetry of \\(H\\))\n\nStep 4: Set the derivative to zero:\n\\[\ng + H\\Delta = 0\n\\]\n\\[\nH\\Delta = -g\n\\]\n\\[\n\\Delta = -H^{-1}g\n\\]\nStep 5: Substitute back \\(\\theta^* = \\theta_0 + \\Delta\\):\n\\[\n\\theta^* = \\theta_0 - H^{-1}\\nabla_{\\theta_0}J(\\theta_0)\n\\]\n\n\nKey Property: One-step Convergence for Quadratics\nIf the Hessian is positive definite and the objective is quadratic, Newton‚Äôs method jumps directly to the minimum in one step.\nFor non-quadratic objectives or non-positive-definite Hessians, we must iterate.\n\n\nNewton‚Äôs Method Algorithm\nHyperparameters:\n\nInitial parameters \\(\\theta_0\\)\nConvergence threshold\n\nTraining procedure:\nWhile stopping criterion not met:\n\nCompute gradient:\n\n\\[\ng = \\frac{1}{m}\\sum_{i=1}^m \\nabla_\\theta L(f(x^{(i)};\\theta), y^{(i)})\n\\]\n\nCompute Hessian:\n\n\\[\nH = \\frac{1}{m}\\sum_{i=1}^m \\nabla_\\theta^2 L(f(x^{(i)};\\theta), y^{(i)})\n\\]\n\nCompute Hessian inverse: \\(H^{-1}\\)\nCompute update step:\n\n\\[\n\\Delta\\theta = -H^{-1}g\n\\]\n\nUpdate parameters:\n\n\\[\n\\theta \\leftarrow \\theta + \\Delta\\theta\n\\]\n\n\nRegularization: Handling Non-positive-definite Hessians\nWhen the Hessian is not positive definite (which happens at saddle points or near maxima), the Newton update may move in an ascent direction.\nTo stabilize the update, we use a damped or regularized Newton step:\n\\[\n\\theta^* = \\theta_0 - [H(f(\\theta_0)) + \\alpha I]^{-1}\\nabla_{\\theta_0}f(\\theta_0) \\tag{8.28}\n\\]\nHow regularization works:\n\nThe term \\(\\alpha I\\) shifts all eigenvalues of \\(H\\) by \\(+\\alpha\\)\nNegative eigenvalues (indicating concave directions) become closer to zero or positive\nThis ensures the regularized matrix is positive definite for sufficiently large \\(\\alpha\\)\n\nBehavior as \\(\\alpha\\) varies:\n\nSmall \\(\\alpha\\): Nearly pure Newton step, fast convergence near minima\nLarge \\(\\alpha\\): Update approaches \\(-\\frac{1}{\\alpha}\\nabla_\\theta f(\\theta_0)\\), which is essentially a scaled gradient descent step\nThis creates a smooth interpolation between Newton‚Äôs method and gradient descent\n\n\n\nComputational Challenge\nNewton‚Äôs method faces severe scalability issues:\nFor a model with \\(k\\) parameters:\n\nHessian is a \\(k \\times k\\) matrix with \\(k^2\\) elements\nComputing the Hessian requires \\(O(k^2)\\) operations\nInverting the Hessian requires \\(O(k^3)\\) operations\n\nIn modern deep learning:\n\nNetworks easily have millions of parameters (\\(k \\approx 10^6\\) or more)\nStoring the Hessian would require terabytes of memory\nComputing \\(H^{-1}\\) would be prohibitively expensive\n\nConclusion: Newton‚Äôs method is practical only for very small models with at most a few thousand parameters."
  },
  {
    "objectID": "ML/second-order-methods.html#conjugate-gradient",
    "href": "ML/second-order-methods.html#conjugate-gradient",
    "title": "Chapter 8.6: Second-Order Optimization Methods",
    "section": "Conjugate Gradient",
    "text": "Conjugate Gradient\nConjugate Gradient (CG) is a sophisticated algorithm that achieves near-Newton performance without computing or storing the full Hessian.\n\nThe Zig-zag Problem in Steepest Descent\nStandard gradient descent on ill-conditioned problems exhibits zig-zag behavior: each step is orthogonal to the previous step, causing the algorithm to repeatedly undo its own progress.\nWhy this happens:\n\nAt each step, gradient descent moves along the steepest direction\nAfter one step, the new gradient is orthogonal to the previous direction (for quadratic functions with exact line search)\nThis orthogonality causes oscillation in narrow valleys\n\nReference: Visualization of ill-conditioned optimization and zig-zag\n\n\nA-orthogonality: The Key Concept\nConjugate Gradient eliminates zig-zag by using conjugate directions instead of orthogonal directions.\nDefinition: Two directions \\(d_i\\) and \\(d_j\\) are conjugate with respect to matrix \\(A\\) if:\n\\[\nd_i^{\\top}Ad_j = 0\n\\]\nThis is also called A-orthogonality because the directions are orthogonal under the metric defined by \\(A\\) (the Hessian).\nWhy this matters: A-orthogonal directions have a crucial property: a line search along a new direction does not undo progress made along any previous direction.\nIn the context of minimizing a quadratic function \\(J(\\theta) = \\frac{1}{2}\\theta^{\\top}A\\theta + b^{\\top}\\theta + c\\):\n\nOrthogonal directions: \\(d_i^{\\top}d_j = 0\\) (geometric orthogonality)\nConjugate directions: \\(d_i^{\\top}Ad_j = 0\\) (orthogonality in curvature space)\n\nConjugate directions preserve all previous progress, eliminating zig-zag.\n\n\nConjugate Gradient Update Rule\nAt each iteration, we construct a search direction that is conjugate to all previous directions:\n\\[\nd_t = -\\nabla_{\\theta}J(\\theta) + \\beta_t d_{t-1} \\tag{8.29}\n\\]\nwhere:\n\n\\(-\\nabla_{\\theta}J(\\theta)\\) is the current gradient\n\\(\\beta_t\\) controls how much we add from the previous direction\nThe combination ensures \\(d_t\\) is conjugate to \\(d_{t-1}\\)\n\n\n\nComputing \\(\\beta_t\\): Fletcher-Reeves\nThe Fletcher-Reeves formula computes \\(\\beta_t\\) based on gradient magnitudes:\n\\[\n\\beta_t = \\frac{\\nabla_{\\theta}J(\\theta_t)^{\\top}\\nabla_{\\theta}J(\\theta_t)}{\\nabla_{\\theta}J(\\theta_{t-1})^{\\top}\\nabla_{\\theta}J(\\theta_{t-1})} \\tag{8.30}\n\\]\nDerivation (for quadratic objectives with exact line search):\nStep 1: Impose the conjugacy condition:\n\\[\nd_t^{\\top}Ad_{t-1} = 0 \\tag{1}\n\\]\nStep 2: Substitute the update rule \\(d_t = -g_t + \\beta_t d_{t-1}\\):\n\\[\n(-g_t + \\beta_t d_{t-1})^{\\top}Ad_{t-1} = 0\n\\]\n\\[\n-g_t^{\\top}Ad_{t-1} + \\beta_t d_{t-1}^{\\top}Ad_{t-1} = 0\n\\]\nStep 3: Solve for \\(\\beta_t\\):\n\\[\n\\beta_t = \\frac{g_t^{\\top}\\cancel{Ad_{t-1}}}{d_{t-1}^{\\top}\\cancel{Ad_{t-1}}} \\tag{3}\n\\]\nStep 4: Replace \\(Ad_{t-1}\\) using gradient update properties:\nUnder a quadratic objective with exact line search, the gradient evolves as:\n\\[\ng_t = g_{t-1} + \\alpha_{t-1}Ad_{t-1}\n\\]\nwhere \\(\\alpha_{t-1}\\) is the step size from the previous iteration.\nThis gives us:\n\\[\nAd_{t-1} = \\frac{1}{\\alpha_{t-1}}(g_t - g_{t-1})\n\\]\nAdditionally, exact line search ensures:\n\\[\ng_t^{\\top}d_{t-1} = 0\n\\]\n(the new gradient is orthogonal to the previous search direction).\nStep 5: Substitute \\(Ad_{t-1}\\) in equation (3):\n\\[\n\\beta_t = \\frac{g_t^{\\top}\\left(\\frac{1}{\\alpha_{t-1}}(g_t - g_{t-1})\\right)}{d_{t-1}^{\\top}\\left(\\frac{1}{\\alpha_{t-1}}(g_t - g_{t-1})\\right)}\n\\]\nThe factor \\(\\frac{1}{\\alpha_{t-1}}\\) cancels:\n\\[\n\\beta_t = \\frac{g_t^{\\top}(g_t - g_{t-1})}{d_{t-1}^{\\top}(g_t - g_{t-1})}\n\\]\nStep 6: Apply the orthogonality condition \\(g_t^{\\top}d_{t-1} = 0\\):\nIn the numerator:\n\\[\ng_t^{\\top}(g_t - g_{t-1}) = g_t^{\\top}g_t - g_t^{\\top}g_{t-1}\n\\]\nIn the denominator, using \\(g_t^{\\top}d_{t-1} = 0\\):\n\\[\nd_{t-1}^{\\top}(g_t - g_{t-1}) = d_{t-1}^{\\top}g_t - d_{t-1}^{\\top}g_{t-1} = 0 - d_{t-1}^{\\top}g_{t-1} = -d_{t-1}^{\\top}g_{t-1}\n\\]\nSince \\(d_{t-1} = -g_{t-1} + \\beta_{t-1}d_{t-2}\\) and by repeated application of orthogonality, we have \\(d_{t-1}^{\\top}g_{t-1} \\propto -g_{t-1}^{\\top}g_{t-1}\\).\nThis yields:\n\\[\n\\beta_t = \\frac{g_t^{\\top}g_t}{g_{t-1}^{\\top}g_{t-1}}\n\\]\nThis is the Fletcher-Reeves formula. The derivation shows how the curvature term \\(Ad_{t-1}\\) is replaced by the change in gradients \\((g_t - g_{t-1})\\), which then simplifies using the orthogonality property \\(g_t^{\\top}d_{t-1} = 0\\) from exact line search.\n\n\nComputing \\(\\beta_t\\): Polak-Ribi√®re\nThe Polak-Ribi√®re formula uses the change in gradients to incorporate curvature information:\n\\[\n\\beta_t = \\frac{(\\nabla J(\\theta_t) - \\nabla J(\\theta_{t-1}))^{\\top}\\nabla J(\\theta_t)}{\\nabla J(\\theta_{t-1})^{\\top}\\nabla J(\\theta_{t-1})} \\tag{8.31}\n\\]\nKey difference from Fletcher-Reeves:\n\nFletcher-Reeves: Depends only on gradient norms\nPolak-Ribi√®re: Uses \\((g_t - g_{t-1})\\), which approximates \\(Ad_{t-1}\\) and captures curvature\n\nAdvantage: Polak-Ribi√®re is more adaptive and often more effective on nonlinear problems, as it adjusts based on how gradients change rather than just their magnitude.\n\n\nLine Search: Choosing the Step Size \\(\\epsilon^*\\)\nUnlike gradient descent with a fixed learning rate, Conjugate Gradient uses line search to find the optimal step size at each iteration.\nGiven a search direction \\(\\rho_t\\), we solve:\n\\[\n\\rho_t = -g_t + \\beta_t\\rho_{t-1}\n\\]\n\\[\n\\epsilon^* = \\arg\\min_{\\epsilon} \\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta_t + \\epsilon\\rho_t), y^{(i)})\n\\]\nHow line search works:\n\nStart with an initial bracket or step size\nIteratively shrink or adjust the interval\nStop when sufficient decrease conditions are met (e.g., Armijo or Wolfe conditions)\n\nThis dynamic search ensures each step is optimal along \\(\\rho_t\\) and preserves the conjugacy properties needed to avoid zig-zag behavior.\n\n\nConjugate Gradient in Practice\nIdeal case: For quadratic objectives with exact line search, CG converges in at most \\(n\\) iterations (where \\(n\\) is the number of parameters).\nPractical case: For general nonlinear functions:\n\nConjugacy degrades over iterations\nGradient orthogonality \\(g_t^{\\top}Ad_{t-1} = 0\\) no longer holds exactly\nLine search is approximate (using mini-batches introduces noise)\n\nResult: CG is effective for medium-sized problems but struggles with stochastic gradients and high dimensionality in deep learning."
  },
  {
    "objectID": "ML/second-order-methods.html#bfgs",
    "href": "ML/second-order-methods.html#bfgs",
    "title": "Chapter 8.6: Second-Order Optimization Methods",
    "section": "BFGS",
    "text": "BFGS\nBFGS (Broyden-Fletcher-Goldfarb-Shanno) is a quasi-Newton method that approximates the inverse Hessian without computing it directly.\n\nCore Idea\nInstead of computing \\(H^{-1}\\) explicitly, BFGS maintains an approximation \\(M_t \\approx H^{-1}\\) and updates it iteratively using gradient information.\nUpdate rule:\n\\[\n\\theta_{t+1} = \\theta_t + \\epsilon^*\\rho_t \\tag{8.33}\n\\]\nwhere:\n\n\\(\\rho_t = -M_tg_t\\) is the search direction (quasi-Newton step)\n\\(\\epsilon^*\\) is found via line search\n\\(M_t\\) is updated using curvature information from successive gradients\n\n\n\nHow \\(M_t\\) is Updated\nBFGS uses the secant condition to update \\(M_t\\):\n\\[\nM_{t+1}(g_{t+1} - g_t) = \\theta_{t+1} - \\theta_t\n\\]\nThis ensures \\(M_{t+1}\\) approximates \\(H^{-1}\\) by matching the observed change in gradients to the change in parameters.\nThe update formula is:\n\\[\nM_{t+1} = M_t + \\text{correction terms based on } (g_{t+1} - g_t) \\text{ and } (\\theta_{t+1} - \\theta_t)\n\\]\nAdvantages:\n\nNo need to compute or store the full Hessian\n\\(M_t\\) is always symmetric and positive definite (with proper initialization)\nConverges superlinearly near the optimum\n\nStorage cost: Still requires \\(O(k^2)\\) memory to store \\(M_t\\), making it impractical for networks with millions of parameters.\n\n\nLimited-Memory BFGS (L-BFGS)\nTo reduce memory requirements, L-BFGS stores only the last \\(m\\) updates (typically \\(m = 10\\)) instead of the full matrix \\(M_t\\).\nMemory cost: Reduced from \\(O(k^2)\\) to \\(O(mk)\\), making it feasible for moderate-sized problems.\nTrade-off: L-BFGS converges slower than full BFGS but is much more memory-efficient."
  },
  {
    "objectID": "ML/second-order-methods.html#why-second-order-methods-are-rarely-used-in-deep-learning",
    "href": "ML/second-order-methods.html#why-second-order-methods-are-rarely-used-in-deep-learning",
    "title": "Chapter 8.6: Second-Order Optimization Methods",
    "section": "Why Second-order Methods Are Rarely Used in Deep Learning",
    "text": "Why Second-order Methods Are Rarely Used in Deep Learning\nDespite their theoretical elegance, second-order methods face fundamental challenges in modern deep learning:\n\n1. Computational Cost\n\nHessian computation: \\(O(k^2)\\) for storage, \\(O(k^3)\\) for inversion\nModern networks have \\(k \\approx 10^6\\) to \\(10^9\\) parameters\nEven approximate methods like BFGS require \\(O(k^2)\\) memory\n\n\n\n2. Stochastic Gradients\n\nSecond-order methods assume accurate gradients and curvature\nMini-batch training introduces noise, breaking line search and conjugacy\nHessian estimates from mini-batches are unreliable\n\n\n\n3. Non-convexity\n\nNeural networks are highly non-convex with many saddle points\nHessian may have negative eigenvalues, requiring expensive regularization\nConjugacy assumptions (quadratic approximation) break down far from optima\n\n\n\n4. Scalability\n\nFirst-order methods scale to billions of parameters and massive datasets\nSecond-order methods require full-batch or large-batch training\nDistributed training is much simpler with gradient-based methods\n\n\n\nPractical Consequence\nModern deep learning almost exclusively uses first-order methods: SGD, Momentum, RMSProp, and Adam. These methods:\n\nRequire only gradient computation (\\(O(k)\\) per iteration)\nWork well with mini-batch stochastic gradients\nScale to massive models and datasets\nAre robust to non-convexity and noise\n\nSecond-order methods remain important in small-scale optimization, classical machine learning (e.g., logistic regression), and as theoretical tools for understanding optimization landscapes."
  },
  {
    "objectID": "ML/second-order-methods.html#summary",
    "href": "ML/second-order-methods.html#summary",
    "title": "Chapter 8.6: Second-Order Optimization Methods",
    "section": "Summary",
    "text": "Summary\nThis section covered three classical second-order optimization methods:\n\nNewton‚Äôs Method: Uses \\(H^{-1}g\\) for optimal quadratic convergence, but requires \\(O(k^3)\\) computation\nConjugate Gradient: Achieves near-Newton performance through conjugate directions and line search, avoiding Hessian computation\nBFGS: Approximates \\(H^{-1}\\) iteratively using gradient information, with L-BFGS reducing memory requirements\n\nWhile theoretically powerful, these methods are rarely used in deep learning due to computational cost, incompatibility with stochastic gradients, and scalability challenges. First-order adaptive methods (Chapter 8.5) dominate in practice."
  },
  {
    "objectID": "ML/parameter-initialization.html",
    "href": "ML/parameter-initialization.html",
    "title": "Chapter 8.4: Parameter Initialization Strategies",
    "section": "",
    "text": "Deep learning relies on iterative optimization, so the initialization of parameters must be specified carefully. Because deep networks are highly nonlinear and complex, the initial values can strongly influence whether and how fast the model converges.\nImportant consideration: Some initialization points are beneficial to optimization but harmful to generalization. Finding the right balance is part of the art of deep learning."
  },
  {
    "objectID": "ML/parameter-initialization.html#what-we-know-and-dont-know",
    "href": "ML/parameter-initialization.html#what-we-know-and-dont-know",
    "title": "Chapter 8.4: Parameter Initialization Strategies",
    "section": "What We Know and Don‚Äôt Know",
    "text": "What We Know and Don‚Äôt Know\nDespite decades of research, parameter initialization remains partly an art and partly a science. The only thing we can say with certainty is that deep learning relies on symmetry breaking ‚Äî if two neurons receive the same inputs, they must start with different initial parameters.\nWithout symmetry breaking, all neurons in a layer would compute identical functions and receive identical gradient updates, making it impossible for the network to learn diverse features."
  },
  {
    "objectID": "ML/parameter-initialization.html#the-danger-of-uniform-initialization-null-space-and-symmetry",
    "href": "ML/parameter-initialization.html#the-danger-of-uniform-initialization-null-space-and-symmetry",
    "title": "Chapter 8.4: Parameter Initialization Strategies",
    "section": "The Danger of Uniform Initialization: Null Space and Symmetry",
    "text": "The Danger of Uniform Initialization: Null Space and Symmetry\nIf we initialize all parameters with the same values, the resulting weight matrices will have identical rows (and possibly columns). Such matrices are singular and thus have a large null space.\nConsequence: When inputs pass through these matrices, any component that lies in the null space is lost, meaning part of the input information vanishes during forward propagation. This creates a permanent information bottleneck that cannot be recovered in later layers.\nFor a deeper understanding of null spaces, see: Column Space and Null Space"
  },
  {
    "objectID": "ML/parameter-initialization.html#orthogonal-matrix-initialization",
    "href": "ML/parameter-initialization.html#orthogonal-matrix-initialization",
    "title": "Chapter 8.4: Parameter Initialization Strategies",
    "section": "Orthogonal Matrix Initialization",
    "text": "Orthogonal Matrix Initialization\nWe can initialize the parameters as an orthogonal matrix, which helps prevent redundancy among weight vectors and allows for more efficient and stable learning.\nSaxe et al.¬†(2013) suggests random orthogonal initialization with carefully selected gain:\n\\[\nW = Q \\cdot g\n\\]\nwhere \\(Q\\) is a random orthogonal matrix and \\(g\\) is a gain factor.\nBenefits:\n\nOrthogonal matrices preserve the norm of vectors, preventing signal explosion or vanishing\nThey maximize diversity among weight vectors\nThey provide stable gradient flow during backpropagation\n\nFor more on orthogonal matrices: Orthogonal Matrices and Gram-Schmidt"
  },
  {
    "objectID": "ML/parameter-initialization.html#the-dilemma-of-large-weights",
    "href": "ML/parameter-initialization.html#the-dilemma-of-large-weights",
    "title": "Chapter 8.4: Parameter Initialization Strategies",
    "section": "The Dilemma of Large Weights",
    "text": "The Dilemma of Large Weights\nWe usually initialize the weights by sampling from a Gaussian (normal) or uniform distribution. The scale of this distribution creates a fundamental trade-off:\nLarger initial weights:\n\n‚úì Provide stronger symmetry breaking effects\n‚úó Can cause gradient explosion\n‚úó Conflict with regularization, which favors smaller weights for better generalization\n\nThe challenge is finding the ‚ÄúGoldilocks zone‚Äù ‚Äî weights that are neither too large nor too small."
  },
  {
    "objectID": "ML/parameter-initialization.html#inspired-initialization-methods",
    "href": "ML/parameter-initialization.html#inspired-initialization-methods",
    "title": "Chapter 8.4: Parameter Initialization Strategies",
    "section": "Inspired Initialization Methods",
    "text": "Inspired Initialization Methods\n\nXavier/Glorot Initialization\nOne inspiring method is to sample from distribution \\(U(-\\frac{1}{\\sqrt{m}}, \\frac{1}{\\sqrt{m}})\\), where \\(m\\) is the number of inputs.\nGlorot and Bengio (2010) suggested an improved normalized initialization that considers both input and output dimensions:\n\\[\nW_{i,k} \\sim U\\left(-\\sqrt{\\frac{6}{m+n}}, \\sqrt{\\frac{6}{m+n}}\\right) \\tag{8.23}\n\\]\nwhere:\n\n\\(m\\) is the number of input units\n\\(n\\) is the number of output units\n\nIntuition: This initialization keeps the variance of activations roughly constant across layers, preventing signals from growing or shrinking exponentially as they propagate through the network.\n\n\nRisks and Limitations\nThe best practices of parameter initialization may not lead to the best result. Several reasons explain this:\n\nIncorrect criteria: We may have used an incorrect criterion to define what ‚Äúgood initialization‚Äù means\nProperties don‚Äôt persist: The properties enforced at initialization may not remain valid throughout learning\nConflicts with regularization: These properties may conflict with regularization methods\n\n\n\nThe Vanishing Output Problem\nAnother potential risk arises when all parameters are initialized with the same standard deviation. For example, if we initialize parameters with standard deviation \\(\\frac{1}{\\sqrt{m}}\\):\n\nA large \\(m\\) results in small standard deviation\nSmall standard deviation makes the expected output magnitude very small\nThis can cause the network to learn very slowly or get stuck in poor local minima\n\nExample: In a deep network with 1000-dimensional input (\\(m = 1000\\)), the variance would be \\(\\frac{1}{1000} = 0.001\\), which can lead to extremely small activations in early training."
  },
  {
    "objectID": "ML/parameter-initialization.html#sparse-initialization",
    "href": "ML/parameter-initialization.html#sparse-initialization",
    "title": "Chapter 8.4: Parameter Initialization Strategies",
    "section": "Sparse Initialization",
    "text": "Sparse Initialization\nMartens (2010) proposed sparse initialization as an alternative approach. Instead of drawing all weights from a distribution, this method:\n\nSets most weights to zero\nInitializes only a fixed number of connections per neuron with non-zero values\n\nAdvantage: This keeps the total input variance independent of \\(m\\), preventing the output magnitude from shrinking as \\(m\\) increases.\nTrade-off: It implicitly increases the prior on the network‚Äôs parameters, effectively imposing a strong sparsity assumption that may not be appropriate for all problems."
  },
  {
    "objectID": "ML/parameter-initialization.html#bias-initialization",
    "href": "ML/parameter-initialization.html#bias-initialization",
    "title": "Chapter 8.4: Parameter Initialization Strategies",
    "section": "Bias Initialization",
    "text": "Bias Initialization\n\nThe Default: Zero Initialization\nIn most cases, initializing the biases to zero works well. Since weights are initialized randomly, zero biases still allow for symmetry breaking through the weights.\n\n\nWhen Non-Zero Biases Are Beneficial\nThere are important cases where we don‚Äôt want to initialize biases to zero:\n1. Output units: If the inputs to the output layer are small or the target distribution is imbalanced, it can be beneficial to initialize the biases to non-zero values that reflect the prior distribution of outputs.\nExample: For binary classification with 90% positive examples, initializing the output bias to \\(\\log(9) \\approx 2.2\\) gives the network a head start.\n2. Avoiding saturation: Sometimes we set non-zero biases to prevent early saturation. For example:\n\nThe sigmoid function \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) saturates near 0 or 1\nInitializing biases to small positive values (e.g., 0.1) keeps neurons in the linear region initially\nThis ensures strong gradients in early training\n\n3. Gating units: When a unit serves as a gate for other units (e.g., in LSTMs), we often initialize its bias to a positive value so that the gate is open at the beginning of training.\nExample: In LSTMs, the forget gate bias is typically initialized to 1 or 2, allowing the network to remember information by default until it learns when to forget."
  },
  {
    "objectID": "ML/parameter-initialization.html#beyond-weights-and-biases-other-parameters",
    "href": "ML/parameter-initialization.html#beyond-weights-and-biases-other-parameters",
    "title": "Chapter 8.4: Parameter Initialization Strategies",
    "section": "Beyond Weights and Biases: Other Parameters",
    "text": "Beyond Weights and Biases: Other Parameters\nParameter initialization goes beyond just weights and biases. Some models also include variance or precision parameters that need to be initialized properly.\n\nExample: Gaussian Conditional Models\nConsider a Gaussian conditional model:\n\\[\np(y \\mid x) = \\mathcal{N}(y \\mid w^\\top x + b, 1/\\beta) \\tag{8.24}\n\\]\nwhere:\n\n\\(w^\\top x + b\\) represents the mean prediction\n\\(1/\\beta\\) denotes the variance (with \\(\\beta\\) as the precision parameter)\n\nStandard practice: Initialize variance to 1 (or equivalently, precision \\(\\beta = 1\\)). This represents a neutral prior belief about the spread of predictions.\nAs training progresses, the model learns to adjust this variance based on the data, potentially increasing it for uncertain predictions or decreasing it for confident ones."
  },
  {
    "objectID": "ML/parameter-initialization.html#summary-guiding-principles",
    "href": "ML/parameter-initialization.html#summary-guiding-principles",
    "title": "Chapter 8.4: Parameter Initialization Strategies",
    "section": "Summary: Guiding Principles",
    "text": "Summary: Guiding Principles\nWhile there is no universal initialization strategy, several principles guide good practice:\n\nBreak symmetry: Never initialize all parameters to the same value\nScale appropriately: Choose variance based on layer dimensions to maintain signal magnitude\nConsider the architecture: Different activation functions and architectures may benefit from different initialization schemes\nBalance competing goals: Trade off between symmetry breaking, gradient stability, and regularization\nExperiment: The best initialization often depends on the specific problem and architecture\n\nRemember that initialization is just the starting point ‚Äî a good optimizer can often overcome poor initialization, though it may take longer to converge."
  },
  {
    "objectID": "ML/xor-deep-learning.html",
    "href": "ML/xor-deep-learning.html",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.1 shows how ReLU activations let neural networks solve the XOR problem that defeats any linear model.\nüìì For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "ML/xor-deep-learning.html#the-xor-problem-a-challenge-for-linear-models",
    "href": "ML/xor-deep-learning.html#the-xor-problem-a-challenge-for-linear-models",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "The XOR Problem: A Challenge for Linear Models",
    "text": "The XOR Problem: A Challenge for Linear Models\nXOR (Exclusive OR) returns 1 precisely when the two binary inputs differ:\n\\[\\text{XOR}(x_1, x_2) = \\begin{pmatrix}0 & 1\\\\1 & 0\\end{pmatrix}\\]\nThe XOR truth table shows why this is challenging for linear models - the positive class (1) appears at diagonally opposite corners, making it impossible to separate with any single straight line.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Define XOR dataset\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([0, 1, 1, 0])\n\nprint(\"XOR Truth Table:\")\nprint(\"================\")\nprint()\nprint(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\nprint(\"‚îÇ Input   ‚îÇ Output ‚îÇ\")\nprint(\"‚îÇ (x‚ÇÅ,x‚ÇÇ) ‚îÇ  XOR   ‚îÇ\")\nprint(\"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\nfor i in range(4):\n    input_str = f\"({X[i,0]}, {X[i,1]})\"\n    output_str = f\"{y[i]}\"\n    print(f\"‚îÇ {input_str:7} ‚îÇ   {output_str:2}   ‚îÇ\")\nprint(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\nprint()\nprint(\"Notice: XOR = 1 when inputs differ, XOR = 0 when inputs match\")\n\n\nXOR Truth Table:\n================\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Input   ‚îÇ Output ‚îÇ\n‚îÇ (x‚ÇÅ,x‚ÇÇ) ‚îÇ  XOR   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ (0, 0)  ‚îÇ   0    ‚îÇ\n‚îÇ (0, 1)  ‚îÇ   1    ‚îÇ\n‚îÇ (1, 0)  ‚îÇ   1    ‚îÇ\n‚îÇ (1, 1)  ‚îÇ   0    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nNotice: XOR = 1 when inputs differ, XOR = 0 when inputs match"
  },
  {
    "objectID": "ML/xor-deep-learning.html#limitation-1-single-layer-linear-model",
    "href": "ML/xor-deep-learning.html#limitation-1-single-layer-linear-model",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Limitation 1: Single Layer Linear Model",
    "text": "Limitation 1: Single Layer Linear Model\nA single layer perceptron can only create linear decision boundaries. Let‚Äôs see what happens when we try to solve XOR with logistic regression:\n\n\nShow code\n# Demonstrate single layer linear model failure\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\ncolors = ['red', 'blue']\n\n# Plot XOR data\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'Class {i}', edgecolors='black', linewidth=2)\n\n# Overlay representative linear separators to illustrate the impossibility\nx_line = np.linspace(-0.2, 1.2, 100)\nax.plot(x_line, 0.5 * np.ones_like(x_line), '--', color='gray', alpha=0.7, label='candidate lines')\nax.plot(0.5 * np.ones_like(x_line), x_line, '--', color='orange', alpha=0.7)\nax.plot(x_line, x_line, '--', color='green', alpha=0.7)\nax.plot(x_line, 1 - x_line, '--', color='purple', alpha=0.7)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x‚ÇÅ', fontsize=12)\nax.set_ylabel('x‚ÇÇ', fontsize=12)\nax.set_title('XOR Problem: No Linear Solution', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Fit logistic regression just to report its performance\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\naccuracy = log_reg.score(X, y)\nprint(f'Single layer model accuracy: {accuracy:.1%} - still misclassifies XOR.')\n\n\n\n\n\n\n\n\n\nSingle layer model accuracy: 50.0% - still misclassifies XOR."
  },
  {
    "objectID": "ML/xor-deep-learning.html#limitation-2-multiple-layer-linear-model-without-activation",
    "href": "ML/xor-deep-learning.html#limitation-2-multiple-layer-linear-model-without-activation",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Limitation 2: Multiple Layer Linear Model (Without Activation)",
    "text": "Limitation 2: Multiple Layer Linear Model (Without Activation)\nEven stacking multiple linear layers doesn‚Äôt help! Multiple linear transformations are mathematically equivalent to a single linear transformation.\nMathematical proof:\n\\[\\text{Layer 1: } h_1 = W_1 x + b_1\\] \\[\\text{Layer 2: } h_2 = W_2 h_1 + b_2 = W_2(W_1 x + b_1) + b_2 = (W_2W_1)x + (W_2b_1 + b_2)\\]\nResult: Still just \\(Wx + b\\) (a single linear transformation)\nConclusion: Stacking linear layers without activation functions doesn‚Äôt increase the model‚Äôs expressive power!"
  },
  {
    "objectID": "ML/xor-deep-learning.html#the-solution-relu-activation-function",
    "href": "ML/xor-deep-learning.html#the-solution-relu-activation-function",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "The Solution: ReLU Activation Function",
    "text": "The Solution: ReLU Activation Function\nReLU (Rectified Linear Unit) provides the nonlinearity needed to solve XOR: - ReLU(z) = max(0, z) - Clips negative values to zero, keeping positive values unchanged\nUsing the hand-crafted network from the next code cell, the forward pass can be written compactly in matrix form:\n\\[\nX = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix},\n\\quad\nW_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix},\n\\quad\nb_1 = \\begin{bmatrix} 0 & 0 \\end{bmatrix}\n\\]\n\\[\nZ = X W_1^{\\top} + b_1 = \\begin{bmatrix} 0 & 0 \\\\ -1 & 1 \\\\ 1 & -1 \\\\ 0 & 0 \\end{bmatrix},\n\\qquad\nH = \\text{ReLU}(Z) = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \\end{bmatrix}\n\\]\nWith output parameters \\[\nw_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix},\n\\quad\nb_2 = -0.5\n\\] the final linear scores are \\[\na = H w_2^{\\top} + b_2 = \\begin{bmatrix} -0.5 \\\\ 0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}\n\\Rightarrow\n\\text{sign}_+(a) = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\n\\]\nHere \\(\\text{sign}_+(a)\\) maps non-negative entries to 1 and negative entries to 0. Let‚Äôs see how ReLU transforms the XOR problem to make it solvable.\n\n\nShow code\n# Hand-crafted network weights and biases that solve XOR\nfrom IPython.display import display, Math\n\ndef relu(z):\n    return np.maximum(0, z)\n\nW1 = np.array([[1, -1],\n               [-1, 1]])\nb1 = np.array([0, 0])\nw2 = np.array([1, 1])\nb2 = -0.5\n\ndisplay(Math(r\"\\text{Hidden layer: } W_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}\"))\ndisplay(Math(r\"b_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\"))\ndisplay(Math(r\"\\text{Output layer: } w_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix}\"))\ndisplay(Math(r\"b_2 = -0.5\"))\n\ndef forward_pass(X, W1, b1, w2, b2):\n    z1 = X @ W1.T + b1\n    h1 = relu(z1)\n    logits = h1 @ w2 + b2\n    return logits, h1, z1\n\nlogits, hidden_activations, pre_activations = forward_pass(X, W1, b1, w2, b2)\npredictions = (logits &gt;= 0).astype(int)\n\nprint(\"Step-by-step Forward Pass Results:\")\nprint(\"=\" * 80)\nprint()\nprint(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\nprint(\"‚îÇ Input   ‚îÇ  Before ReLU     ‚îÇ  After ReLU      ‚îÇ  Logit  ‚îÇ   Pred   ‚îÇ\")\nprint(\"‚îÇ (x‚ÇÅ,x‚ÇÇ) ‚îÇ    (z‚ÇÅ, z‚ÇÇ)      ‚îÇ    (h‚ÇÅ, h‚ÇÇ)      ‚îÇ  score  ‚îÇ  class   ‚îÇ\")\nprint(\"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\nfor i in range(len(X)):\n    x1, x2 = X[i]\n    z1_vals = pre_activations[i]\n    h1_vals = hidden_activations[i]\n    logit = logits[i]\n    pred = predictions[i]\n    \n    input_str = f\"({x1:.0f}, {x2:.0f})\"\n    pre_relu_str = f\"({z1_vals[0]:4.1f}, {z1_vals[1]:4.1f})\"\n    post_relu_str = f\"({h1_vals[0]:4.1f}, {h1_vals[1]:4.1f})\"\n    logit_str = f\"{logit:6.2f}\"\n    pred_str = f\"{pred:4d}\"\n    \n    print(f\"‚îÇ {input_str:7} ‚îÇ {pre_relu_str:16} ‚îÇ {post_relu_str:16} ‚îÇ {logit_str:7} ‚îÇ {pred_str:8} ‚îÇ\")\nprint(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n\naccuracy = (predictions == y).mean()\nprint(f\"\\nNetwork Accuracy: {accuracy:.0%} ‚úÖ\")\nprint(\"\\nKey transformations:\")\nprint(\"‚Ä¢ (-1, 1) ‚Üí (0, 1) makes XOR(0,1) = 1 separable\")\nprint(\"‚Ä¢ ( 1,-1) ‚Üí (1, 0) makes XOR(1,0) = 1 separable\")\n\n\n\\(\\displaystyle \\text{Hidden layer: } W_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle b_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle \\text{Output layer: } w_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle b_2 = -0.5\\)\n\n\nStep-by-step Forward Pass Results:\n================================================================================\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Input   ‚îÇ  Before ReLU     ‚îÇ  After ReLU      ‚îÇ  Logit  ‚îÇ   Pred   ‚îÇ\n‚îÇ (x‚ÇÅ,x‚ÇÇ) ‚îÇ    (z‚ÇÅ, z‚ÇÇ)      ‚îÇ    (h‚ÇÅ, h‚ÇÇ)      ‚îÇ  score  ‚îÇ  class   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ (0, 0)  ‚îÇ ( 0.0,  0.0)     ‚îÇ ( 0.0,  0.0)     ‚îÇ  -0.50  ‚îÇ    0     ‚îÇ\n‚îÇ (0, 1)  ‚îÇ (-1.0,  1.0)     ‚îÇ ( 0.0,  1.0)     ‚îÇ   0.50  ‚îÇ    1     ‚îÇ\n‚îÇ (1, 0)  ‚îÇ ( 1.0, -1.0)     ‚îÇ ( 1.0,  0.0)     ‚îÇ   0.50  ‚îÇ    1     ‚îÇ\n‚îÇ (1, 1)  ‚îÇ ( 0.0,  0.0)     ‚îÇ ( 0.0,  0.0)     ‚îÇ  -0.50  ‚îÇ    0     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nNetwork Accuracy: 100% ‚úÖ\n\nKey transformations:\n‚Ä¢ (-1, 1) ‚Üí (0, 1) makes XOR(0,1) = 1 separable\n‚Ä¢ ( 1,-1) ‚Üí (1, 0) makes XOR(1,0) = 1 separable\n\n\n\nTransformation Table: How ReLU Solves XOR\nLet‚Äôs trace through exactly what happens to each input:\n\n\nShow code\n\n# Create detailed transformation table\nprint(\"Complete Transformation Table:\")\nprint(\"=============================\")\nprint()\nprint(\"Input   | Pre-ReLU  | Post-ReLU | Logit | Prediction | Target | Correct?\")\nprint(\"(x‚ÇÅ,x‚ÇÇ) | (z‚ÇÅ, z‚ÇÇ)  | (h‚ÇÅ, h‚ÇÇ)  | score | class      | y      |\")\nprint(\"--------|-----------|-----------|-------|------------|--------|----------\")\n\nfor i in range(4):\n    input_str = f\"({X[i,0]},{X[i,1]})\"\n    pre_relu_str = f\"({pre_activations[i,0]:2.0f},{pre_activations[i,1]:2.0f})\"\n    post_relu_str = f\"({hidden_activations[i,0]:.0f},{hidden_activations[i,1]:.0f})\"\n    logit_str = f\"{logits[i]:.2f}\"\n    pred_str = f\"{predictions[i]}\"\n    target_str = f\"{y[i]}\"\n    correct_str = \"‚úì\" if predictions[i] == y[i] else \"‚úó\"\n\n    print(f\"{input_str:7} | {pre_relu_str:9} | {post_relu_str:9} | {logit_str:5} | {pred_str:10} | {target_str:6} | {correct_str}\")\n\nprint()\nprint(\"Key Insight: ReLU transforms (-1,1) ‚Üí (0,1) and (1,-1) ‚Üí (1,0)\")\nprint(\"This makes the XOR classes linearly separable in the hidden space!\")\n\n\nComplete Transformation Table:\n=============================\n\nInput   | Pre-ReLU  | Post-ReLU | Logit | Prediction | Target | Correct?\n(x‚ÇÅ,x‚ÇÇ) | (z‚ÇÅ, z‚ÇÇ)  | (h‚ÇÅ, h‚ÇÇ)  | score | class      | y      |\n--------|-----------|-----------|-------|------------|--------|----------\n(0,0)   | ( 0, 0)   | (0,0)     | -0.50 | 0          | 0      | ‚úì\n(0,1)   | (-1, 1)   | (0,1)     | 0.50  | 1          | 1      | ‚úì\n(1,0)   | ( 1,-1)   | (1,0)     | 0.50  | 1          | 1      | ‚úì\n(1,1)   | ( 0, 0)   | (0,0)     | -0.50 | 0          | 0      | ‚úì\n\nKey Insight: ReLU transforms (-1,1) ‚Üí (0,1) and (1,-1) ‚Üí (1,0)\nThis makes the XOR classes linearly separable in the hidden space!\n\n\n\n\nStep 1: Original Input Space\nThe XOR problem in its raw form - notice how no single line can separate the classes:\n\n\nShow code\n# Step 1 visualization: Original Input Space\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'XOR = {i}', edgecolors='black', linewidth=2)\n\n# Annotate each point\nfor i in range(4):\n    ax.annotate(f'({X[i,0]},{X[i,1]})', X[i], xytext=(10, 10), \n                textcoords='offset points', fontsize=10)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x‚ÇÅ', fontsize=12)\nax.set_ylabel('x‚ÇÇ', fontsize=12)\nax.set_title('Step 1: Original Input Space', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Linear Transformation (Before ReLU)\nThe network applies weights W‚ÇÅ and biases b‚ÇÅ to transform the input space:\n\n\nShow code\n# Step 2 visualization: Pre-activation space\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\nfor i in range(4):\n    ax.scatter(pre_activations[i, 0], pre_activations[i, 1], \n               c=colors[y[i]], s=200, edgecolors='black', linewidth=2)\n\n# Draw ReLU boundaries\nax.axhline(0, color='gray', linestyle='-', alpha=0.8, linewidth=2)\nax.axvline(0, color='gray', linestyle='-', alpha=0.8, linewidth=2)\n\n# Shade all regions where coordinates turn negative (and thus get clipped by ReLU)\nax.axvspan(-1.2, 0, alpha=0.15, color='red')\nax.axhspan(-1.2, 0, alpha=0.15, color='red')\nax.text(-0.75, 0.85, 'Negative z‚ÇÅ ‚Üí ReLU sets to 0', ha='left', fontsize=10,\n        bbox=dict(boxstyle='round', facecolor='red', alpha=0.25))\nax.text(0.95, -0.75, 'Negative z‚ÇÇ ‚Üí ReLU sets to 0', ha='right', fontsize=10,\n        bbox=dict(boxstyle='round', facecolor='red', alpha=0.25))\n\n# Annotate points with input labels\nlabels = ['(0,0)', '(0,1)', '(1,0)', '(1,1)']\nfor i, label in enumerate(labels):\n    pre_coord = f'({pre_activations[i,0]:.0f},{pre_activations[i,1]:.0f})'\n    ax.annotate(f'{label}‚Üí{pre_coord}', pre_activations[i], xytext=(10, 10), \n                textcoords='offset points', fontsize=9)\n\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1.2, 1.2)\nax.set_xlabel('z‚ÇÅ (Pre-activation)', fontsize=12)\nax.set_ylabel('z‚ÇÇ (Pre-activation)', fontsize=12)\nax.set_title('Step 2: Before ReLU (Linear Transform)', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 3: ReLU Transformation\nReLU clips negative values to zero, transforming the space to make it linearly separable:\n\n\nShow code\n# Step 3 visualization: ReLU transformation with arrows\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\n\nfor i in range(4):\n    # Pre-ReLU positions (X marks)\n    ax.scatter(pre_activations[i, 0], pre_activations[i, 1], \n               marker='x', s=150, c=colors[y[i]], alpha=0.5, linewidth=3)\n    # Post-ReLU positions (circles) \n    ax.scatter(hidden_activations[i, 0], hidden_activations[i, 1], \n               marker='o', s=200, c=colors[y[i]], edgecolors='black', linewidth=2)\n    \n    # Draw transformation arrows\n    start = pre_activations[i]\n    end = hidden_activations[i]\n    if not np.array_equal(start, end):\n        ax.annotate('', xy=end, xytext=start,\n                    arrowprops=dict(arrowstyle='-&gt;', lw=2, color=colors[y[i]], alpha=0.8))\n\n\n# Add text box explaining the key transformation\nax.text(0.5, 0.8, 'ReLU clips negative coordinates to zero\\n(-1,1) ‚Üí (0,1) and (1,-1) ‚Üí (1,0)', \n        ha='center', va='center', fontsize=11, \n        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n\nax.set_xlim(-1.2, 1.1)\nax.set_ylim(-1.2, 1.1)\nax.set_xlabel('Hidden dimension 1', fontsize=12)\nax.set_ylabel('Hidden dimension 2', fontsize=12)\nax.set_title('Step 3: ReLU Mapping (Before ‚Üí After)', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Final Classification\nWith the transformed hidden representation, the network can now perfectly classify XOR:\n\n\nShow code\n\n\n# Step 4 visualization: Final classification results\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\n# Create decision boundary\nxx, yy = np.meshgrid(np.linspace(-0.2, 1.2, 100), np.linspace(-0.2, 1.2, 100))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\ngrid_logits, _, _ = forward_pass(grid_points, W1, b1, w2, b2)\ngrid_preds = (grid_logits &gt;= 0).astype(int).reshape(xx.shape)\n\nax.contourf(xx, yy, grid_preds, levels=[-0.5, 0.5, 1.5], \n            colors=['#ffcccc', '#ccccff'], alpha=0.6)\nax.contour(xx, yy, grid_logits.reshape(xx.shape), levels=[0], \n           colors='black', linewidths=2, linestyles='--')\n\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'XOR = {i}', edgecolors='black', linewidth=2)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x‚ÇÅ', fontsize=12)\nax.set_ylabel('x‚ÇÇ', fontsize=12)\nax.set_title('Step 4: Final Classification (100% Accuracy)', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nsample_logits, _, _ = forward_pass(X, W1, b1, w2, b2)\nsample_preds = (sample_logits &gt;= 0).astype(int)\nfor i in range(4):\n    pred_text = f'Pred: {sample_preds[i]}'\n    ax.annotate(pred_text, X[i], xytext=(10, -15), \n                textcoords='offset points', fontsize=9,\n                bbox=dict(boxstyle='round,pad=0.2', facecolor='lightgreen', alpha=0.7))\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ML/xor-deep-learning.html#conclusion",
    "href": "ML/xor-deep-learning.html#conclusion",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Conclusion",
    "text": "Conclusion\nThe XOR problem demonstrates several fundamental principles in deep learning:\n\nNecessity of Nonlinearity: Linear models cannot solve XOR, establishing the critical role of nonlinear activation functions.\nUniversal Approximation: Even simple architectures with sufficient nonlinearity can solve complex classification problems."
  },
  {
    "objectID": "ML/l2-regularization.html",
    "href": "ML/l2-regularization.html",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "",
    "text": "My lecture notes\nL2 regularization (ridge regression) adds a penalty term to the loss function to prevent overfitting. This post walks through the math behind how L2 regularization affects the optimal weights, using eigenvalue decomposition to show that it shrinks weights differently in different directions based on the Hessian‚Äôs curvature."
  },
  {
    "objectID": "ML/l2-regularization.html#context",
    "href": "ML/l2-regularization.html#context",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "",
    "text": "My lecture notes\nL2 regularization (ridge regression) adds a penalty term to the loss function to prevent overfitting. This post walks through the math behind how L2 regularization affects the optimal weights, using eigenvalue decomposition to show that it shrinks weights differently in different directions based on the Hessian‚Äôs curvature."
  },
  {
    "objectID": "ML/l2-regularization.html#three-unproven-theorems",
    "href": "ML/l2-regularization.html#three-unproven-theorems",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Three Unproven Theorems",
    "text": "Three Unproven Theorems\nThese theorems are used directly here without proof. Note: Proofs will be provided in an additional blog post; here we just use them.\n\n\\(H = Q\\Lambda Q^T\\)\n\\([QAQ^T]^{-1} = (Q^T)^{-1}A^{-1}Q^{-1} = QA^{-1}Q^T\\)\n\nWhen Q is an orthogonal matrix, \\(Q^T = Q^{-1}\\)\n\nThe loss from w to w* is \\(\\frac{1}{2}(w-w^*)^TH(w-w^*)\\)\n\nThis is similar to the kinematic formula \\(s = \\frac{1}{2}at^2\\), with two key differences:\n\nHere the dimension is not time, but the displacement from w to w*\nHere the dimension is not the 0-dimensional t, but w which is a vector with dimensions, so we use H"
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.1",
    "href": "ML/l2-regularization.html#formula-7.1",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.1",
    "text": "Formula 7.1\n\\[\n\\tilde{J}(\\theta; x, y) = J(\\theta; x, y) + \\alpha \\Omega(\\theta)\n\\]\nTotal objective including regularization."
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.2",
    "href": "ML/l2-regularization.html#formula-7.2",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.2",
    "text": "Formula 7.2\nHere \\(w^Tw\\) represents the L2 norm of parameters, \\(\\alpha\\) represents the penalty coefficient, between 0-1, and relatively close to 0.\n\\[\n\\tilde{J}(w; \\theta; y) = \\frac{\\alpha}{2}w^Tw + J(w; X; y)\n\\]"
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.3",
    "href": "ML/l2-regularization.html#formula-7.3",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.3",
    "text": "Formula 7.3\n\\[\n\\nabla \\tilde{J}(w; X; y) = \\alpha w + \\nabla_w J(w; X; y)\n\\]\nTo understand this formula, the key is understanding the gradient of \\(w^Tw\\).\nIn the one-dimensional world, if we have \\(f(x) = x^2\\), which is equivalent to L2, then the gradient is \\(f'(x) = 2x\\).\nDerivation: \\[\n\\begin{align}\n(x + \\Delta x)^2 &= x^2 + 2x\\Delta x + \\Delta x^2 \\\\\n(x + \\Delta x)^2 - x^2 &= 2x\\Delta x + \\Delta x^2 \\\\\n\\frac{f(x) - f(\\Delta x)}{\\Delta x} &= 2x + \\Delta x\n\\end{align}\n\\]\nExtending to higher dimensions, \\(\\nabla(w^Tw) = 2w\\), so the gradient of \\(\\frac{\\alpha}{2}w^Tw\\) is \\(\\alpha w\\)."
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.4-7.5",
    "href": "ML/l2-regularization.html#formula-7.4-7.5",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.4 & 7.5",
    "text": "Formula 7.4 & 7.5\nThis is straightforward to understand: \\(\\epsilon\\) is the learning rate, or step size.\n\\[\nw \\leftarrow w - \\epsilon (\\alpha w + \\nabla J(w; X; y))\n\\]"
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.6",
    "href": "ML/l2-regularization.html#formula-7.6",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.6",
    "text": "Formula 7.6\nThis formula makes a quadratic approximation in the neighborhood of w* (the optimal solution) without introducing L2, yielding:\n\\[\n\\hat{J}(\\theta) = J(w^*) + \\frac{1}{2}(w - w^*)^T H(w - w^*)\n\\]\nThis can actually be seen as L2 weighted by the Hessian matrix. Because the curvature differs in each direction, we multiply the distance in each direction by the curvature in that direction to get a weighted squared distance.\nTo understand this, we need the Taylor expansion. For one dimension: \\[\nf(x) \\approx f(x^*) + f'(x)(x - x^*) + \\frac{1}{2}f''(x)(x - x^*)^2\n\\]\nFor second order: \\[\nJ(w) \\approx J(w^*) + (w - w^*)^\\top \\nabla J(w^*) + \\frac{1}{2}(w - w^*)^\\top H(w - w^*)\n\\]\nSince w* is the optimal solution, the gradient is 0, so we can directly remove \\((w - w^*)^\\top \\nabla J(w^*)\\).\nThis directly gives us formula 7.6."
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.7",
    "href": "ML/l2-regularization.html#formula-7.7",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.7",
    "text": "Formula 7.7\n\\[\n\\nabla \\hat{J}(w) = H(w - w^*)\n\\]\nFormula 7.7 is the derivative of formula 7.6. Since \\(J(w^*)\\) is a constant with gradient 0, we only need to find the gradient of \\(\\frac{1}{2}(w - w^*)^T H(w - w^*)\\), which is \\((w - w^*)\\) times H (can be derived from one dimension)."
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.87.97.10",
    "href": "ML/l2-regularization.html#formula-7.87.97.10",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.8/7.9/7.10",
    "text": "Formula 7.8/7.9/7.10\nFormula 7.8 combines 7.3 and 7.7: \\[\n\\alpha \\tilde{w} + H(\\tilde{w} - w^*) = 0\n\\]\nFrom 7.3, we already have: \\[\n\\nabla J(w; X; y) = \\alpha w + \\nabla_w J(w; X; y)\n\\]\nSubstituting into 7.7 gives us 7.8.\nFormula 7.9 is a transformation of 7.8: \\[\n\\begin{align}\n\\alpha \\tilde{w} + H\\tilde{w} &= Hw^* \\\\\n(H + \\alpha I)\\tilde{w} &= Hw^*\n\\end{align}\n\\]\nMultiplying both sides by the inverse of \\((H + \\alpha I)\\), the left side becomes just \\(\\tilde{w}\\): \\[\n\\tilde{w} = (H + \\alpha I)^{-1} Hw^*\n\\]\nWhen \\(\\alpha \\approx 0\\), \\(\\alpha I \\approx \\mathbf{0}\\), so \\(\\tilde{w} \\approx HH^{-1}w^* \\approx w^*\\)."
  },
  {
    "objectID": "ML/l2-regularization.html#formula-7.11-7.12-7.13",
    "href": "ML/l2-regularization.html#formula-7.11-7.12-7.13",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Formula 7.11 & 7.12 & 7.13",
    "text": "Formula 7.11 & 7.12 & 7.13\nWe transform H to \\(H = Q\\Lambda Q^T\\), since it‚Äôs real symmetric, we can do this transformation.\nWe get formula 7.11: \\[\n\\tilde{w} = (Q\\Lambda Q^T + \\alpha I)^{-1} Q\\Lambda Q^T w^*\n\\]\nReplace I with \\(QQ^T\\): \\[\nQ\\Lambda Q^T + \\alpha QQ^T\n\\]\nThe common factors are Q and \\(Q^T\\), \\(\\Lambda\\) can be added with \\(\\alpha\\) because \\(\\alpha\\) is a constant multiplied by I.\nThis gives us formula 7.12: \\[\n\\tilde{w} = [Q(\\Lambda + \\alpha I)Q^T]^{-1} Q\\Lambda Q^T w^*\n\\]\nSubstituting \\([QAQ^T]^{-1} = QA^{-1}Q^T\\): \\[\n\\tilde{w} = Q(\\Lambda + \\alpha I)^{-1} \\Lambda Q^T w^*\n\\]\nTransform this formula to \\(\\frac{\\lambda_i}{\\lambda_i + \\alpha}\\):\n\n\\(\\Lambda = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_n), \\quad \\Lambda + \\alpha I = \\operatorname{diag}(\\lambda_1 + \\alpha, \\dots, \\lambda_n + \\alpha)\\)\n\\((\\Lambda + \\alpha I)^{-1} = \\operatorname{diag}\\!\\Big(\\tfrac{1}{\\lambda_1 + \\alpha}, \\dots, \\tfrac{1}{\\lambda_n + \\alpha}\\Big)\\)\n\\((\\Lambda + \\alpha I)^{-1}\\Lambda = \\operatorname{diag}\\!\\Big(\\tfrac{1}{\\lambda_i + \\alpha}\\Big) \\, \\operatorname{diag}(\\lambda_i) = \\operatorname{diag}\\!\\Big(\\tfrac{\\lambda_i}{\\lambda_i + \\alpha}\\Big)\\)\nSo: \\[\n\\tilde{w} = Q \\operatorname{diag}\\Big(\\frac{\\lambda_i}{\\lambda_i + \\alpha}\\Big) Q^T w^*\n\\]\n\nThis transformation tells us: in directions with large eigenvalues, w is preserved more; in directions with small eigenvalues, w is preserved less."
  },
  {
    "objectID": "ML/l2-regularization.html#contour-lines",
    "href": "ML/l2-regularization.html#contour-lines",
    "title": "Deep Learning Book Chapter 7.1.1: L2 Regularization",
    "section": "Contour Lines",
    "text": "Contour Lines\n\n\n\nL2 Regularization Contour Lines\n\n\nUnderstanding this figure:\n\nSolid lines are the true contour lines of the loss function. These ellipses represent contour curves under different loss scenarios. On each contour curve, each point means we need to move in the direction perpendicular to the tangent, otherwise there‚Äôs no change (gradient).\nDashed lines are the regularized loss. Because it‚Äôs a scalar, it affects all directions equally, so the contour lines are circles.\nWhen the first ellipse of the solid line intersects with a circle layer of the dashed line, that point achieves balance for both (gradients are opposite in direction, equal in magnitude).\nOur ellipse shows that change along the x-axis is larger, y-axis is smaller, proving that for equal loss, we need to move more along x, less along y.\nAt the intersection point, our regularization curve pulls up more along the x-axis, less along the y-axis, proving that the y-axis is preserved better, while x has relatively large adjustments.\n\n\nThe ellipse shows the sensitivity of the loss function in different directions. The circle shows uniform penalty of regularization in all directions. The point where they are tangent has balanced gradients and minimum loss, which is the optimal solution for ridge regression‚Äî shrinking more in unstable directions, less in stable directions.\n\n\nSource: Deep Learning Book, Chapter 7.1.1"
  },
  {
    "objectID": "ML/convolution-computation.html",
    "href": "ML/convolution-computation.html",
    "title": "Chapter 9.1: Convolution Computation",
    "section": "",
    "text": "This section introduces the mathematical foundation of convolutional operations:\n\nContinuous convolution: Weighted averaging with integral formula\nDiscrete convolution: Summation formula for sampled signals\n2D convolution: Extension to images and spatial data\nCross-correlation: The operation actually used in most deep learning frameworks\nConcrete example: Step-by-step convolution computation\n\nConvolution is the fundamental operation in convolutional neural networks (CNNs), enabling parameter sharing and translation equivariance."
  },
  {
    "objectID": "ML/convolution-computation.html#overview",
    "href": "ML/convolution-computation.html#overview",
    "title": "Chapter 9.1: Convolution Computation",
    "section": "",
    "text": "This section introduces the mathematical foundation of convolutional operations:\n\nContinuous convolution: Weighted averaging with integral formula\nDiscrete convolution: Summation formula for sampled signals\n2D convolution: Extension to images and spatial data\nCross-correlation: The operation actually used in most deep learning frameworks\nConcrete example: Step-by-step convolution computation\n\nConvolution is the fundamental operation in convolutional neural networks (CNNs), enabling parameter sharing and translation equivariance."
  },
  {
    "objectID": "ML/convolution-computation.html#motivating-example-noisy-sensor-readings",
    "href": "ML/convolution-computation.html#motivating-example-noisy-sensor-readings",
    "title": "Chapter 9.1: Convolution Computation",
    "section": "1. Motivating Example: Noisy Sensor Readings",
    "text": "1. Motivating Example: Noisy Sensor Readings\n\nProblem Setup\nImagine a spaceship whose exact position at time \\(t\\) is uncertain because sensor readings include noise.\n\n\\(x(t)\\): Raw sensor measurement at time \\(t\\) (noisy)\n\\(s(t)\\): Our estimate of the true position at time \\(t\\)\n\nInstead of trusting a single noisy reading \\(x(t)\\), we estimate position by taking a weighted average of nearby measurements.\n\n\nWeighting Function\nDefine \\(w(a)\\) as a weighting function where:\n\n\\(w(t - a)\\) tells us how much the measurement at time \\(a\\) should contribute to our estimate at time \\(t\\)\nNearby measurements get higher weight\nDistant measurements get lower weight\n\nTypical choice: A Gaussian centered at \\(t\\):\n\\[\nw(t - a) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(t-a)^2}{2\\sigma^2}\\right)\n\\]\nThis gives high weight to measurements close to time \\(t\\) and low weight to measurements far from \\(t\\)."
  },
  {
    "objectID": "ML/convolution-computation.html#continuous-convolution",
    "href": "ML/convolution-computation.html#continuous-convolution",
    "title": "Chapter 9.1: Convolution Computation",
    "section": "2. Continuous Convolution",
    "text": "2. Continuous Convolution\n\nDefinition\nThe estimated position \\(s(t)\\) is computed as:\n\\[\ns(t) = \\int x(a) w(t - a) \\, da \\tag{9.1}\n\\]\nInterpretation:\n\nFor each time \\(a\\), take the measurement \\(x(a)\\)\nWeight it by \\(w(t - a)\\) (how relevant time \\(a\\) is to time \\(t\\))\nSum (integrate) over all times \\(a\\)\n\nThis operation is called convolution.\n\n\nNotation\n\\[\ns(t) = (x * w)(t) \\tag{9.2}\n\\]\nThe symbol \\(*\\) denotes convolution.\n Figure: Physical interpretation of convolution - the output at time \\(t\\) is computed by sliding the flipped kernel \\(w(t-a)\\) over the input signal \\(x(a)\\) and computing the weighted sum.\n\n\nProperties\n\nCommutativity: \\(x * w = w * x\\)\nAssociativity: \\((x * w) * z = x * (w * z)\\)\nDistributivity: \\(x * (w + z) = x * w + x * z\\)\nSmoothing: Convolution with a smooth kernel (like Gaussian) smooths the input signal\nTranslation equivariance: If input shifts by \\(\\Delta t\\), output shifts by same amount"
  },
  {
    "objectID": "ML/convolution-computation.html#discrete-convolution",
    "href": "ML/convolution-computation.html#discrete-convolution",
    "title": "Chapter 9.1: Convolution Computation",
    "section": "3. Discrete Convolution",
    "text": "3. Discrete Convolution\nIn practice, we work with sampled signals (discrete time points), not continuous functions.\n\nDefinition\nFor discrete signals \\(x\\) and \\(w\\), the convolution is:\n\\[\ns(t) = (x * w)(t) = \\sum_{a=-\\infty}^{\\infty} x(a) w(t - a) \\tag{9.3}\n\\]\nAlgorithm:\n\nFor each output position \\(t\\)\nFor each input position \\(a\\):\n\nTake input value \\(x(a)\\)\nMultiply by weight \\(w(t - a)\\)\n\nSum all products\n\n\n\nExample\nLet \\(x = [1, 2, 3, 4]\\) and \\(w = [0.5, 1, 0.5]\\) (centered at index 1).\nFor \\(t = 2\\):\n\\[\ns(2) = x(0)w(2) + x(1)w(1) + x(2)w(0) + x(3)w(-1) + \\cdots\n\\]\nAssuming zero-padding outside the input range:\n\\[\ns(2) = 1 \\cdot 0 + 2 \\cdot 0.5 + 3 \\cdot 1 + 4 \\cdot 0.5 = 0 + 1 + 3 + 2 = 6\n\\]"
  },
  {
    "objectID": "ML/convolution-computation.html#two-dimensional-convolution",
    "href": "ML/convolution-computation.html#two-dimensional-convolution",
    "title": "Chapter 9.1: Convolution Computation",
    "section": "4. Two-Dimensional Convolution",
    "text": "4. Two-Dimensional Convolution\nFor images and spatial data, we extend convolution to 2D.\n\nMathematical Definition (Form 1)\n\\[\nS(i, j) = (I * K)(i, j) = \\sum_m \\sum_n I(m, n) K(i - m, j - n) \\tag{9.4}\n\\]\n\n\\(I(m, n)\\): Input image at position \\((m, n)\\)\n\\(K(i - m, j - n)\\): Kernel weight for offset \\((i - m, j - n)\\)\n\\(S(i, j)\\): Output feature map at position \\((i, j)\\)\n\nInterpretation: For each output position \\((i, j)\\), slide the flipped kernel over the input and compute dot product.\n\n\nAlternative Form (Form 2)\n\\[\nS(i, j) = (I * K)(i, j) = \\sum_m \\sum_n I(i - m, j - n) K(m, n) \\tag{9.5}\n\\]\nInterpretation: Equivalently, we can flip the input coordinates instead of the kernel coordinates.\n\n\nRelationship\nEquations (9.4) and (9.5) are equivalent due to commutativity of convolution:\n\\[\nI * K = K * I\n\\]\nIn practice, we use whichever form is more convenient for implementation."
  },
  {
    "objectID": "ML/convolution-computation.html#cross-correlation",
    "href": "ML/convolution-computation.html#cross-correlation",
    "title": "Chapter 9.1: Convolution Computation",
    "section": "5. Cross-Correlation",
    "text": "5. Cross-Correlation\n\nDefinition\nMost deep learning libraries actually implement cross-correlation, not true convolution:\n\\[\nS(i, j) = (I \\star K)(i, j) = \\sum_m \\sum_n I(i + m, j + n) K(m, n) \\tag{9.6}\n\\]\nKey difference: The kernel is not flipped (no negative indices \\(i - m, j - n\\)).\n\n\nWhy the Confusion?\n\nLearned kernels: In CNNs, kernel weights are learned from data, so flipping doesn‚Äôt matter‚Äîthe network will learn the flipped version if needed\nTerminology: The operation is still called ‚Äúconvolution‚Äù in deep learning, even though it‚Äôs technically cross-correlation\nTranslation equivariance: Both operations preserve translation equivariance, which is the key property for CNNs\n\n\n\nMathematical Relationship\nIf \\(K_{\\text{flipped}}(m, n) = K(-m, -n)\\), then:\n\\[\n(I * K)(i, j) = (I \\star K_{\\text{flipped}})(i, j)\n\\]\nTrue convolution with kernel \\(K\\) equals cross-correlation with flipped kernel \\(K_{\\text{flipped}}\\)."
  },
  {
    "objectID": "ML/convolution-computation.html#concrete-cnn-example",
    "href": "ML/convolution-computation.html#concrete-cnn-example",
    "title": "Chapter 9.1: Convolution Computation",
    "section": "6. Concrete CNN Example",
    "text": "6. Concrete CNN Example\n\nInput Image\n\\[\nI = \\begin{bmatrix}\na & b & c & d \\\\\ne & f & g & h \\\\\ni & j & k & l\n\\end{bmatrix}\n\\]\nShape: \\(3 \\times 4\\) (3 rows, 4 columns)\n\n\nKernel (Filter)\n\\[\nK = \\begin{bmatrix}\nw & x \\\\\ny & z\n\\end{bmatrix}\n\\]\nShape: \\(2 \\times 2\\)\n\n\nComputing the Output (Cross-Correlation)\nUsing equation (9.6), we compute \\(S(i, j) = \\sum_m \\sum_n I(i + m, j + n) K(m, n)\\).\nPosition \\((0, 0)\\) (top-left):\n\\[\nS(0, 0) = I(0, 0)K(0, 0) + I(0, 1)K(0, 1) + I(1, 0)K(1, 0) + I(1, 1)K(1, 1)\n\\]\n\\[\n= aw + bx + ey + fz\n\\]\nPosition \\((0, 1)\\):\n\\[\nS(0, 1) = I(0, 1)K(0, 0) + I(0, 2)K(0, 1) + I(1, 1)K(1, 0) + I(1, 2)K(1, 1)\n\\]\n\\[\n= bw + cx + fy + gz\n\\]\nPosition \\((0, 2)\\):\n\\[\nS(0, 2) = I(0, 2)K(0, 0) + I(0, 3)K(0, 1) + I(1, 2)K(1, 0) + I(1, 3)K(1, 1)\n\\]\n\\[\n= cw + dx + gy + hz\n\\]\nPosition \\((1, 0)\\):\n\\[\nS(1, 0) = I(1, 0)K(0, 0) + I(1, 1)K(0, 1) + I(2, 0)K(1, 0) + I(2, 1)K(1, 1)\n\\]\n\\[\n= ew + fx + iy + jz\n\\]\nPosition \\((1, 1)\\):\n\\[\nS(1, 1) = I(1, 1)K(0, 0) + I(1, 2)K(0, 1) + I(2, 1)K(1, 0) + I(2, 2)K(1, 1)\n\\]\n\\[\n= fw + gx + jy + kz\n\\]\nPosition \\((1, 2)\\):\n\\[\nS(1, 2) = I(1, 2)K(0, 0) + I(1, 3)K(0, 1) + I(2, 2)K(1, 0) + I(2, 3)K(1, 1)\n\\]\n\\[\n= gw + hx + ky + lz\n\\]\n\n\nOutput Feature Map\n\\[\nS = \\begin{bmatrix}\naw + bx + ey + fz & bw + cx + fy + gz & cw + dx + gy + hz \\\\\new + fx + iy + jz & fw + gx + jy + kz & gw + hx + ky + lz\n\\end{bmatrix}\n\\]\nShape: \\(2 \\times 3\\)\n\n\nOutput Size Formula\nFor input size \\((H, W)\\) and kernel size \\((K_H, K_W)\\), the output size (with no padding, stride 1) is:\n\\[\n\\text{Output height} = H - K_H + 1 = 3 - 2 + 1 = 2\n\\]\n\\[\n\\text{Output width} = W - K_W + 1 = 4 - 2 + 1 = 3\n\\]\n Figure: Visualization of 2D convolution operation - the kernel slides over the input image, computing dot products at each position to produce the output feature map."
  },
  {
    "objectID": "ML/convolution-computation.html#key-insights",
    "href": "ML/convolution-computation.html#key-insights",
    "title": "Chapter 9.1: Convolution Computation",
    "section": "Key Insights",
    "text": "Key Insights\n\n1. Parameter Sharing\nThe same kernel \\(K\\) is applied at every spatial location. This dramatically reduces the number of parameters compared to fully connected layers.\nExample: For a \\(32 \\times 32\\) image with 64 filters of size \\(5 \\times 5\\):\n\nConvolutional layer: \\(5 \\times 5 \\times 64 = 1{,}600\\) parameters\nFully connected layer: \\(32 \\times 32 \\times 32 \\times 32 \\times 64 = 67{,}108{,}864\\) parameters\n\n\n\n2. Translation Equivariance\nIf the input shifts, the output shifts by the same amount:\n\\[\nI'(i, j) = I(i - \\Delta i, j - \\Delta j) \\implies S'(i, j) = S(i - \\Delta i, j - \\Delta j)\n\\]\nThis makes CNNs naturally suited for tasks where features can appear anywhere in the image (e.g., object detection).\n\n\n3. Local Connectivity\nEach output unit depends only on a local patch of the input (the receptive field), not the entire input. This:\n\nReduces computational cost\nCaptures local spatial structure (edges, corners, textures)\nEnables hierarchical feature learning (low-level ‚Üí mid-level ‚Üí high-level features)"
  },
  {
    "objectID": "ML/convolution-computation.html#summary",
    "href": "ML/convolution-computation.html#summary",
    "title": "Chapter 9.1: Convolution Computation",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\nOperation\nFormula\nUsed In\n\n\n\n\nContinuous Convolution\n\\(s(t) = \\int x(a) w(t - a) \\, da\\)\nSignal processing, theory\n\n\nDiscrete Convolution\n\\(s(t) = \\sum_a x(a) w(t - a)\\)\nDigital signal processing\n\n\n2D Convolution\n\\(S(i,j) = \\sum_m \\sum_n I(m,n) K(i-m, j-n)\\)\nImage processing (traditional)\n\n\n2D Cross-Correlation\n\\(S(i,j) = \\sum_m \\sum_n I(i+m, j+n) K(m,n)\\)\nDeep learning (CNNs)\n\n\n\nKey takeaway: Modern deep learning frameworks use cross-correlation (no kernel flipping) but call it ‚Äúconvolution.‚Äù Since kernels are learned, the distinction doesn‚Äôt affect performance‚Äîthe network will learn the appropriate kernel regardless of whether it‚Äôs flipped or not."
  },
  {
    "objectID": "ML/optimization-challenges.html",
    "href": "ML/optimization-challenges.html",
    "title": "Chapter 8.2: Challenges in Deep Learning Optimization",
    "section": "",
    "text": "Deep learning optimization faces unique challenges that distinguish it from classical convex optimization. Understanding these challenges is crucial for designing effective training algorithms."
  },
  {
    "objectID": "ML/optimization-challenges.html#convex-function",
    "href": "ML/optimization-challenges.html#convex-function",
    "title": "Chapter 8.2: Challenges in Deep Learning Optimization",
    "section": "Convex Function",
    "text": "Convex Function\nA function is convex if for all \\(x, y \\in \\mathbb{R}^n\\) and for all \\(\\alpha \\in (0,1)\\):\n\\[\nf(\\alpha x+(1-\\alpha)y) \\leq \\alpha f(x)+(1-\\alpha)f(y)\n\\]\n\nExample: \\(f(x) = x^2\\)\nConsider two points: - A = (-2, 4), where \\(x_1 = -2\\), \\(f(x_1) = 4\\) - B = (2, 4), where \\(x_2 = 2\\), \\(f(x_2) = 4\\)\nFor any \\(\\alpha \\in (0,1)\\): - \\(\\alpha f(x_1) + (1-\\alpha)f(x_2) = 4\\) - \\(f(\\alpha x_1 + (1-\\alpha)x_2) \\leq 4\\)\nThis shows that any point on the line segment AB lies below or on the function curve, confirming convexity.\n\n\n\nConvex Function\n\n\nYou can explore this interactively with my GeoGebra link to slide the alpha value and adjust \\(x_1\\) and \\(x_2\\)."
  },
  {
    "objectID": "ML/optimization-challenges.html#ill-conditioned-functions",
    "href": "ML/optimization-challenges.html#ill-conditioned-functions",
    "title": "Chapter 8.2: Challenges in Deep Learning Optimization",
    "section": "Ill-Conditioned Functions",
    "text": "Ill-Conditioned Functions\n\n\n\nIll-Conditioned\n\n\nConsider the ill-conditioned function:\n\\[\nf = \\frac{1}{2}(100x^2 + y^2)\n\\]\nThe Hessian matrix is:\n\\[\nH = \\begin{bmatrix}100 & 0\\\\0 & 1\\end{bmatrix}\n\\]\nExample: Starting from point A(0.5, 1) with loss = 13, near the error contour line 10.\nThe gradient at A is \\(\\nabla f = [50, 1]\\). Using learning rate \\(\\eta = 0.05\\):\n\\[\nx_{new} = 0.5 - 0.05 \\times 50 = -2\\\\\ny_{new} = 1 - 0.05 \\times 1 = 0.95\n\\]\nThis gives point B(-2, 0.95), but the new loss is:\n\\[\nf(B) = 0.5(100 \\times 2^2 + 0.95^2) \\approx 201\n\\]\nThe loss increased dramatically! This demonstrates how ill-conditioning causes gradient descent to overshoot.\n\n\nSecond-Order Taylor Expansion of Loss Function\n\\[\nJ(\\theta') - J(\\theta) \\approx \\frac{1}{2}\\epsilon^2 g^\\top H g - \\epsilon g^\\top g\n\\]\nIf \\(\\frac{1}{2}\\epsilon^2 g^\\top H g &gt; \\epsilon g^\\top g\\), the loss will increase instead of decrease."
  },
  {
    "objectID": "ML/optimization-challenges.html#local-minima",
    "href": "ML/optimization-challenges.html#local-minima",
    "title": "Chapter 8.2: Challenges in Deep Learning Optimization",
    "section": "Local Minima",
    "text": "Local Minima\nFor convex functions, any local minimum is a global minimum, as there is only one local minimum.\nHowever, for non-convex functions, there might be multiple local minima.\n\nBenign Local Minima\nNeural networks may have infinitely many equivalent local minima due to parameter symmetries (model non-identifiability), but this phenomenon is not caused by the non-convexity of the loss function.\n\n\n\nBenign Local Minima\n\n\n\n\nSuboptimal Local Minimum\nThis example shows a suboptimal local minimum‚Äîits loss is higher than the global minimum. In non-convex problems, a local minimum need not coincide with the global minimum.\nBy contrast, many local minima in neural networks are benign: they are equivalent or have nearly the same loss due to parameter symmetries (non-identifiability).\n\\[\nf(x,y) = (x^2-1)^2 + y^2 + 0.6x\n\\]\n\n\n\nSuboptimal Local Minimum\n\n\nRecent research suggests that, in large neural networks, most local minima have sufficiently low loss values, so finding the exact global minimum is often unnecessary. Moreover, one should not attribute all optimization challenges to local minima. Instead, it is important to examine the gradient norm and determine whether the optimization has converged‚Äîif it has, the issue is likely not caused by local minima."
  },
  {
    "objectID": "ML/optimization-challenges.html#plateaus-saddles-and-other-flat-regions",
    "href": "ML/optimization-challenges.html#plateaus-saddles-and-other-flat-regions",
    "title": "Chapter 8.2: Challenges in Deep Learning Optimization",
    "section": "Plateaus, Saddles and Other Flat Regions",
    "text": "Plateaus, Saddles and Other Flat Regions\n\nSaddle Points\nConsider the function:\n\\[\nf = x^2 - y^2\\\\\n\\frac{\\partial f}{\\partial x} = 2x\\\\\n\\frac{\\partial f}{\\partial y} = -2y\n\\]\nWhen \\((x,y) = (0,0)\\), the gradient \\(\\nabla f = \\begin{bmatrix}0\\\\0\\end{bmatrix}\\), and training will get stuck at this point.\nThe Hessian \\(H = \\begin{bmatrix}2 & 0\\\\0 & -2\\end{bmatrix}\\) indicates positive curvature along the x-axis and negative curvature along the y-axis. Therefore, moving along the y direction decreases the function value, revealing the presence of a saddle point.\n\n\n\nSaddle Point"
  },
  {
    "objectID": "ML/optimization-challenges.html#cliffs-and-exploding-gradients",
    "href": "ML/optimization-challenges.html#cliffs-and-exploding-gradients",
    "title": "Chapter 8.2: Challenges in Deep Learning Optimization",
    "section": "Cliffs and Exploding Gradients",
    "text": "Cliffs and Exploding Gradients\n\n\n\nCliffs\n\n\nDeep networks often contain regions in parameter space where the loss surface changes extremely rapidly with respect to the parameters. These steep regions are called cliffs.\nWhen the optimization step crosses a cliff, the gradient magnitude becomes extremely large‚Äîleading to exploding gradients and unstable parameter updates.\n\nSolution: Gradient Clipping\nRescale gradients when their norm exceeds a threshold \\(\\tau\\):\n\\[\n\\nabla_\\theta J \\leftarrow \\frac{\\nabla_\\theta J}{\\max(1, \\|\\nabla_\\theta J\\|/\\tau)}\n\\]\nThis keeps updates within a safe range."
  },
  {
    "objectID": "ML/optimization-challenges.html#long-term-dependencies",
    "href": "ML/optimization-challenges.html#long-term-dependencies",
    "title": "Chapter 8.2: Challenges in Deep Learning Optimization",
    "section": "Long-Term Dependencies",
    "text": "Long-Term Dependencies\nIn networks where a weight matrix \\(W\\) needs to be multiplied many times (e.g., in RNNs), we can analyze stability using eigenvalue decomposition:\n\\[\nW^t = (V \\operatorname{diag}(\\lambda) V^{-1})^t = V \\operatorname{diag}(\\lambda^t) V^{-1}\n\\]\n\nIf \\(|\\lambda| &lt; 1\\), then \\(\\lambda^t \\to 0\\) as t increases, causing vanishing gradients\nIf \\(|\\lambda| &gt; 1\\), then \\(\\lambda^t \\to \\infty\\), leading to exploding gradients\nOnly when \\(|\\lambda| = 1\\) can the signal propagate stably over time‚Äîthough this case is extremely rare in practice"
  },
  {
    "objectID": "ML/optimization-challenges.html#imperfect-gradient-information",
    "href": "ML/optimization-challenges.html#imperfect-gradient-information",
    "title": "Chapter 8.2: Challenges in Deep Learning Optimization",
    "section": "Imperfect Gradient Information",
    "text": "Imperfect Gradient Information\nIn practice, optimization rarely has access to the exact gradient or Hessian.\nGradients are estimated from noisy minibatches or numerical approximations.\nThus, most learning algorithms operate under stochastic and imperfect gradient information.\nThe objective function is effectively a noisy landscape, and optimization seeks robustness to this uncertainty."
  },
  {
    "objectID": "ML/optimization-challenges.html#weak-correspondence-between-local-and-global-structure",
    "href": "ML/optimization-challenges.html#weak-correspondence-between-local-and-global-structure",
    "title": "Chapter 8.2: Challenges in Deep Learning Optimization",
    "section": "Weak Correspondence Between Local and Global Structure",
    "text": "Weak Correspondence Between Local and Global Structure\nThe local geometry of the loss function (such as saddle points or flat regions) does not necessarily reflect the global optimum.\nDeep networks often contain large flat areas or cliffs where gradients vanish or oscillate.\nOptimization may stagnate even without true local minima.\nHence, success depends more on finding good initialization and stable descent paths than on reaching a perfect local minimum."
  },
  {
    "objectID": "ML/optimization-challenges.html#theoretical-limits-of-optimization",
    "href": "ML/optimization-challenges.html#theoretical-limits-of-optimization",
    "title": "Chapter 8.2: Challenges in Deep Learning Optimization",
    "section": "Theoretical Limits of Optimization",
    "text": "Theoretical Limits of Optimization\nTheoretical analyses (Blum & Rivest, 1992; Judd, 1989; Wolpert & MacReady, 1997) show that:\n\nTraining neural networks is NP-hard\nThere is no universally optimal optimization algorithm (No Free Lunch Theorem)\nEvery algorithm performs well only for a subset of problems\n\nTherefore, the practical goal is to design approximate optimization methods that generalize well and work reliably in large-scale neural networks.\n\nSource: Deep Learning (Ian Goodfellow, Yoshua Bengio, and Aaron Courville) - Chapter 8.2"
  },
  {
    "objectID": "ML/learning-vs-optimization.html",
    "href": "ML/learning-vs-optimization.html",
    "title": "Deep Learning Chapter 8.1: How Learning Differs from Pure Optimization",
    "section": "",
    "text": "The objective of pure optimization:\n\\[\n\\min_{\\theta}J(\\theta)\n\\]\nIn pure optimization, the function is determined and usually has a solution. For example, \\(\\min f(x)=(x-3)^2\\) has solution \\(x=3\\).\n\n\n\nIn machine learning, we optimize the empirical risk:\n\\[\nJ(\\theta)=\\mathbb{E}_{(x,y)\\sim \\hat{p}_{data}}L(f(x;\\theta),y) \\tag{8.1}\n\\]\nwhere: - \\(L\\) is the loss function - \\(f(x;\\theta)\\) is the model output - \\(\\hat{p}_{data}\\) is the empirical distribution on training data\n\nIn machine learning, our true goal is to minimize the expected loss on the true distribution, but we can only minimize the empirical loss on the training set. Thus, the optimization problem is only an approximation ‚Äî and it often has no exact solution.\n\n\\(\\hat{p}_{data}\\) approximates the true data distribution \\(p_{data}\\) by assigning equal probability to each training sample.\n\n\n\nGenerally, our ultimate goal is to minimize the expected loss under the true data distribution \\(p_{data}\\), rather than the empirical distribution \\(\\hat{p}_{data}\\) estimated from the training data.\n\\[\nJ^*(\\theta)=\\mathbb{E}_{(x,y)\\sim {p}_{data}}L(f(x;\\theta),y) \\tag{8.2}\n\\]"
  },
  {
    "objectID": "ML/learning-vs-optimization.html#pure-optimization-vs.-machine-learning",
    "href": "ML/learning-vs-optimization.html#pure-optimization-vs.-machine-learning",
    "title": "Deep Learning Chapter 8.1: How Learning Differs from Pure Optimization",
    "section": "",
    "text": "The objective of pure optimization:\n\\[\n\\min_{\\theta}J(\\theta)\n\\]\nIn pure optimization, the function is determined and usually has a solution. For example, \\(\\min f(x)=(x-3)^2\\) has solution \\(x=3\\).\n\n\n\nIn machine learning, we optimize the empirical risk:\n\\[\nJ(\\theta)=\\mathbb{E}_{(x,y)\\sim \\hat{p}_{data}}L(f(x;\\theta),y) \\tag{8.1}\n\\]\nwhere: - \\(L\\) is the loss function - \\(f(x;\\theta)\\) is the model output - \\(\\hat{p}_{data}\\) is the empirical distribution on training data\n\nIn machine learning, our true goal is to minimize the expected loss on the true distribution, but we can only minimize the empirical loss on the training set. Thus, the optimization problem is only an approximation ‚Äî and it often has no exact solution.\n\n\\(\\hat{p}_{data}\\) approximates the true data distribution \\(p_{data}\\) by assigning equal probability to each training sample.\n\n\n\nGenerally, our ultimate goal is to minimize the expected loss under the true data distribution \\(p_{data}\\), rather than the empirical distribution \\(\\hat{p}_{data}\\) estimated from the training data.\n\\[\nJ^*(\\theta)=\\mathbb{E}_{(x,y)\\sim {p}_{data}}L(f(x;\\theta),y) \\tag{8.2}\n\\]"
  },
  {
    "objectID": "ML/learning-vs-optimization.html#empirical-risk-minimization",
    "href": "ML/learning-vs-optimization.html#empirical-risk-minimization",
    "title": "Deep Learning Chapter 8.1: How Learning Differs from Pure Optimization",
    "section": "Empirical Risk Minimization",
    "text": "Empirical Risk Minimization\nThe challenge in machine learning is that we only have access to the empirical distribution \\(\\hat{p}_{data}(x, y)\\) derived from the training set, but the true data distribution \\(p_{data}(x, y)\\) remains unknown.\nThe easiest way is to turn it to empirical risk minimization:\n\\[\n\\mathbb{E}_{(x,y)\\sim \\hat{p}_{data}}[L(f(x;\\theta),y)]=\\frac{1}{m}\\sum_{i=1}^m L(f(x^i;\\theta),y^i) \\tag{8.3}\n\\]\nHowever, some useful loss functions cannot be directly used in deep learning. For example, the 0‚Äì1 loss is discrete and non-differentiable, which makes it impossible to optimize using gradient-based methods. In such cases, we replace it with a proxy (or surrogate) loss function."
  },
  {
    "objectID": "ML/learning-vs-optimization.html#proxy-loss-function-and-early-stop",
    "href": "ML/learning-vs-optimization.html#proxy-loss-function-and-early-stop",
    "title": "Deep Learning Chapter 8.1: How Learning Differs from Pure Optimization",
    "section": "Proxy Loss Function and Early Stop",
    "text": "Proxy Loss Function and Early Stop\n\nMotivation\nSome loss functions that truly reflect the model‚Äôs performance (such as the 0‚Äì1 loss for classification accuracy) cannot be optimized efficiently ‚Äî they are discrete, non-differentiable, and sometimes even NP-hard to minimize.\nTherefore, in practice, we replace them with surrogate (proxy) loss functions that are smooth and differentiable, making them compatible with gradient-based optimization.\n\n\nSurrogate Loss Functions\nA surrogate loss serves as a stand-in objective for the true but intractable loss. It not only makes optimization feasible but often brings useful properties such as smoother gradients or better generalization.\nGenerated by ChatGPT:\n\n\n\n\n\n\n\n\nTrue Objective\nSurrogate Loss Used\nNotes\n\n\n\n\n0‚Äì1 loss (classification error)\nLogistic loss / Cross-Entropy\ndifferentiable, smooth, convex\n\n\nMargin-based classification\nHinge loss (SVM)\nencourages large margins\n\n\nLikelihood maximization\nNegative log-likelihood\nprobabilistic interpretation"
  },
  {
    "objectID": "ML/learning-vs-optimization.html#batch-and-mini-batch-algorithm",
    "href": "ML/learning-vs-optimization.html#batch-and-mini-batch-algorithm",
    "title": "Deep Learning Chapter 8.1: How Learning Differs from Pure Optimization",
    "section": "Batch and Mini-Batch Algorithm",
    "text": "Batch and Mini-Batch Algorithm\nIn machine learning, the optimization algorithm updates the parameters using an estimate of the expectation over a batch of training samples.\nFor example, the maximum likelihood estimation:\n\\[\n\\theta_{ML}=\\arg\\max_{\\theta}\\sum_{i=1}^m \\log p_{model}(x^i,y^i;\\theta) \\tag{8.4}\n\\]\nMaximizing the true expectation is approximated by maximizing the expectation over the empirical training data:\n\\[\nJ(\\theta)=\\mathbb{E}_{x,y \\sim \\hat{p}_{data}}\\log p_{model}(x,y;\\theta) \\tag{8.5}\n\\]\nThe gradient is:\n\\[\n\\nabla_{\\theta} J(\\theta)=\\mathbb{E}_{x,y \\sim \\hat{p}_{data}}\\nabla_{\\theta} \\log p_{model}(x,y;\\theta) \\tag{8.6}\n\\]\nIt is hard to calculate the expectation over the entire training data. In practice, we can use stochastic methods to sample a mini-batch.\n\nTerms\n\nBatch/deterministic: use the entire training data\nStochastic/online: use 1 single sample\nMini-batch/minibatch stochastic: use &gt;1 but not entire training data\n\n\n\nWhy Mini-Batch Works\n\n1. Trade-off Between Computation and Variance\n\nLarger batches provide more accurate gradient estimates but require more computation.\nThe improvement is not linear ‚Äî the standard deviation of the mean decreases as:\n\n\\[\n\\text{Std}(\\bar{Z}) = \\frac{\\sigma}{\\sqrt{n}}\n\\]\n\nFor example, if one approach uses 10,000 samples and another uses 100 samples, the first requires 100√ó more computation but only reduces the standard deviation by 10√ó.\n\n\n\n2. Hardware Efficiency\n\nMini-batch processing naturally fits parallel hardware such as GPUs, TPUs, or multi-core CPUs.\nUsing batch sizes that are powers of two (e.g., \\(2^n\\)) often improves memory alignment and throughput.\n\n\n\n3. Regularization Effect\n\nMini-batch stochasticity introduces noise into gradient estimates.\nThis noise can serve as a form of implicit regularization, helping the model avoid overfitting and generalize better.\n\n\nSummary: Mini-batches strike a balance between computational efficiency and statistical efficiency ‚Äî they make optimization faster while preserving enough stochasticity to improve generalization."
  },
  {
    "objectID": "ML/learning-vs-optimization.html#problems-of-hessian-matrix",
    "href": "ML/learning-vs-optimization.html#problems-of-hessian-matrix",
    "title": "Deep Learning Chapter 8.1: How Learning Differs from Pure Optimization",
    "section": "Problems of Hessian Matrix",
    "text": "Problems of Hessian Matrix\nSecond-order methods that use the Hessian matrix \\(H\\) (such as Newton‚Äôs method) require computing and inverting \\(H^{-1}g\\), which demands very large batches to obtain accurate curvature estimates.\nHowever, even small estimation errors in \\(H\\) can be amplified when multiplied by its inverse, making parameter updates highly unstable‚Äîespecially under noisy minibatch conditions.\nBecause of this high computational cost and sensitivity to noise, deep learning rarely uses second-order methods, relying instead on first-order algorithms like SGD or Adam for more stable and efficient optimization."
  },
  {
    "objectID": "ML/learning-vs-optimization.html#stochastic-gradient-descent-as-an-unbiased-estimator",
    "href": "ML/learning-vs-optimization.html#stochastic-gradient-descent-as-an-unbiased-estimator",
    "title": "Deep Learning Chapter 8.1: How Learning Differs from Pure Optimization",
    "section": "Stochastic Gradient Descent as an Unbiased Estimator",
    "text": "Stochastic Gradient Descent as an Unbiased Estimator\nWhen both x and y are discrete variables, the expected loss over the data distribution can be conveniently expressed as a double summation over all possible pairs (x, y).\nIn this case, the generalization objective \\(J^*(\\theta)\\) becomes the weighted average of the loss \\(L(f(x; \\theta), y)\\), where the weights are given by the data distribution \\(p_{\\text{data}}(x, y)\\).\nThis formulation directly connects the theoretical expectation with a computable form ‚Äî leading to equations (8.7) and (8.8), which explicitly write out the expected objective and its gradient under discrete data assumptions.\n\\[\nJ^*(\\theta)=\\sum_x\\sum_yp_{data}(x,y)L(f(x;\\theta),y) \\tag{8.7}\n\\]\n\\[\ng=\\nabla_{\\theta}J^*(\\theta)=\\sum_x\\sum_yp_{data}(x,y)\\nabla_{\\theta} L(f(x;\\theta),y) \\tag{8.8}\n\\]\n\\[\n\\hat{g}=\\frac{1}{m}\\nabla_{\\theta}\\sum_iL(f(x^i;\\theta),y^i) \\tag{8.9}\n\\]\nFor continuous x and y, similar results can be obtained by replacing the sums with integrals.\n\nSource: Deep Learning (Ian Goodfellow, Yoshua Bengio, Aaron Courville) - Chapter 8.1"
  },
  {
    "objectID": "ML/rnn-recurrent-neural-networks.html",
    "href": "ML/rnn-recurrent-neural-networks.html",
    "title": "Chapter 10.2: Recurrent Neural Networks",
    "section": "",
    "text": "RNN architecture with hidden-to-hidden connections\n\n\nAssume the output is discrete (e.g., predicting the next token), and the activation function is tanh.\n\\[\na^{(t)} = Wh^{(t-1)} + Ux^{(t)} + b \\tag{10.8}\n\\]\n\\[\nh^{(t)} = \\tanh(a^{(t)}) \\tag{10.9}\n\\]\n\\[\no^{(t)} = Vh^{(t)} + c \\tag{10.10}\n\\]\n\\[\n\\hat{y}^{(t)} = \\text{softmax}(o^{(t)}) \\tag{10.11}\n\\]\nParameters:\n\n\\(W\\): weight matrix connecting hidden state \\(h^{(t-1)}\\) to next hidden state\n\\(U\\): weight matrix connecting input \\(x\\) to hidden state\n\\(b\\): bias for hidden state\n\\(\\tanh\\): activation function\n\\(V\\): weight matrix connecting hidden layer \\(h^{(t)}\\) to output\n\\(c\\): output bias\n\nThe loss of a given input sequence x is the sum of the prediction losses at all time steps:\n\\[\nL(\\{x^{(1)},...,x^{(\\tau)}\\}, \\{y^{(1)},...,y^{(\\tau)}\\}) = \\sum_t L^{(t)} = -\\sum_t \\log p_{\\text{model}}(y^{(t)} | \\{x^{(1)},...,x^{(\\tau)}\\}) \\tag{10.12-10.14}\n\\]\nComplexity:\n\nTime complexity is \\(O(\\tau)\\): Both forward propagation and back-propagation through time (BPTT) must proceed sequentially across all time steps and cannot be parallelized.\nSpace complexity is \\(O(\\tau)\\): BPTT requires storing all intermediate hidden states and activations for all time steps."
  },
  {
    "objectID": "ML/rnn-recurrent-neural-networks.html#rnn-architecture",
    "href": "ML/rnn-recurrent-neural-networks.html#rnn-architecture",
    "title": "Chapter 10.2: Recurrent Neural Networks",
    "section": "",
    "text": "RNN architecture with hidden-to-hidden connections\n\n\nAssume the output is discrete (e.g., predicting the next token), and the activation function is tanh.\n\\[\na^{(t)} = Wh^{(t-1)} + Ux^{(t)} + b \\tag{10.8}\n\\]\n\\[\nh^{(t)} = \\tanh(a^{(t)}) \\tag{10.9}\n\\]\n\\[\no^{(t)} = Vh^{(t)} + c \\tag{10.10}\n\\]\n\\[\n\\hat{y}^{(t)} = \\text{softmax}(o^{(t)}) \\tag{10.11}\n\\]\nParameters:\n\n\\(W\\): weight matrix connecting hidden state \\(h^{(t-1)}\\) to next hidden state\n\\(U\\): weight matrix connecting input \\(x\\) to hidden state\n\\(b\\): bias for hidden state\n\\(\\tanh\\): activation function\n\\(V\\): weight matrix connecting hidden layer \\(h^{(t)}\\) to output\n\\(c\\): output bias\n\nThe loss of a given input sequence x is the sum of the prediction losses at all time steps:\n\\[\nL(\\{x^{(1)},...,x^{(\\tau)}\\}, \\{y^{(1)},...,y^{(\\tau)}\\}) = \\sum_t L^{(t)} = -\\sum_t \\log p_{\\text{model}}(y^{(t)} | \\{x^{(1)},...,x^{(\\tau)}\\}) \\tag{10.12-10.14}\n\\]\nComplexity:\n\nTime complexity is \\(O(\\tau)\\): Both forward propagation and back-propagation through time (BPTT) must proceed sequentially across all time steps and cannot be parallelized.\nSpace complexity is \\(O(\\tau)\\): BPTT requires storing all intermediate hidden states and activations for all time steps."
  },
  {
    "objectID": "ML/rnn-recurrent-neural-networks.html#teacher-forcing-and-networks-with-output-recurrence",
    "href": "ML/rnn-recurrent-neural-networks.html#teacher-forcing-and-networks-with-output-recurrence",
    "title": "Chapter 10.2: Recurrent Neural Networks",
    "section": "Teacher Forcing and Networks with Output Recurrence",
    "text": "Teacher Forcing and Networks with Output Recurrence\nIn an output-recurrent network, there is no hidden-to-hidden connection. The hidden state at time \\(t\\) depends only on the current input and the output from the previous time step:\n\\[\nh^{(t)} = f(Ux^{(t)} + Wy^{(t-1)})\n\\]\n\n\n\nOutput recurrence architecture\n\n\nTeacher Forcing:\nDuring training, the model does not use its own approximate predictions as inputs. Instead, it receives the ground truth from the previous time step, a technique known as teacher forcing.\nThis allows training to be parallelized because each time step no longer needs to wait for the output of the previous step.\nThe conditional log-likelihood is:\n\\[\n\\log p(y^{(1)}, y^{(2)} | x^{(1)}, x^{(2)}) = \\log p(y^{(2)} | y^{(1)}, x^{(1)}, x^{(2)}) + \\log p(y^{(1)} | x^{(1)}, x^{(2)}) \\tag{10.16}\n\\]\nTraining vs.¬†Inference:\n\nDuring training: the model uses the ground-truth output \\(y^{(t)}\\) as the input for the next time step\nAt test time: the ground truth is not available, so the model feeds its own prediction \\(\\hat{y}^{(t)}\\) into the next step instead"
  },
  {
    "objectID": "ML/rnn-recurrent-neural-networks.html#gradient-of-rnn-back-propagation-through-time",
    "href": "ML/rnn-recurrent-neural-networks.html#gradient-of-rnn-back-propagation-through-time",
    "title": "Chapter 10.2: Recurrent Neural Networks",
    "section": "Gradient of RNN (Back-Propagation Through Time)",
    "text": "Gradient of RNN (Back-Propagation Through Time)\nThe partial derivative of \\(L^{(t)}\\) with respect to \\(L\\) is 1:\n\\[\nL = L^{(1)} + ... + L^{(\\tau)}, \\quad \\frac{\\partial L}{\\partial L^{(t)}} = 1 \\tag{10.17}\n\\]\nSuppose the output \\(o^{(t)}\\) is the parameter of softmax. For each \\(i, t\\) (where \\(i\\) is the index of y labels and \\(t\\) is the time step):\n\\[\n(\\nabla_{o^{(t)}} L)_i = \\frac{\\partial L}{\\partial o_i^{(t)}} = \\frac{\\partial L}{\\partial L^{(t)}} \\frac{\\partial L^{(t)}}{\\partial o_i^{(t)}} = \\hat{y}_i^{(t)} - \\mathbf{1}_{i=y^{(t)}} \\tag{10.18}\n\\]\nThe gradient of softmax is: \\(\\boxed{\\frac{\\partial L}{\\partial o_i} = \\hat{y}_i - \\mathbf{1}_{i=y}}\\)\n\nGradient at Last Time Step\nAt the last step \\(\\tau\\), the hidden unit \\(h\\) has no future step, so the gradient is:\n\\[\n\\nabla_{h^{(\\tau)}} L = V^\\top \\nabla_{o^{(\\tau)}} L \\tag{10.19}\n\\]\nsince \\(o^{(\\tau)} = Vh^{(\\tau)} + c\\).\n\n\nBack-Propagation from \\(\\tau-1\\) to 1\nFor all steps from \\(\\tau-1 \\to 1\\), \\(h^{(t)}\\) has 2 downstream nodes: \\(o^{(t)}\\) and \\(h^{(t+1)}\\).\nThe gradient is:\n\\[\n\\nabla_{h^{(t)}} L = \\left(\\frac{\\partial h^{(t+1)}}{\\partial h^{(t)}}\\right)^\\top (\\nabla_{h^{(t+1)}} L) + \\left(\\frac{\\partial o^{(t)}}{\\partial h^{(t)}}\\right)^\\top \\nabla_{o^{(t)}} L \\tag{10.20}\n\\]\nWhere:\n\n\\(\\frac{\\partial h^{(t+1)}}{\\partial h^{(t)}}\\): Jacobian mapping changes in \\(h^{(t)}\\) to changes in \\(h^{(t+1)}\\)\n\nFrom: \\(h^{(t+1)} = \\tanh(Wh^{(t)} + Ux^{(t+1)} + b)\\)\n\n\\(\\frac{\\partial o^{(t)}}{\\partial h^{(t)}}\\): Jacobian mapping changes in \\(h^{(t)}\\) to changes in \\(o^{(t)}\\)\n\nFrom: \\(o^{(t)} = Vh^{(t)} + c\\)\n\n\n\\[\n\\nabla_{h^{(t)}} L = W^\\top \\operatorname{diag}(1-(h^{(t+1)})^2) \\nabla_{h^{(t+1)}} L + V^\\top \\nabla_{o^{(t)}} L \\tag{10.21}\n\\]\n\n\nGradients of Parameters\nBias \\(c\\) (forward gradient is 1):\n\\[\n\\nabla_c L = \\sum_t \\left(\\frac{\\partial o^{(t)}}{\\partial c}\\right)^\\top \\nabla_{o^{(t)}} L = \\sum_t \\nabla_{o^{(t)}} L \\tag{10.22}\n\\]\nBias \\(b\\) (forward gradient is \\(\\text{diag}(1 - (h^{(t)})^2)\\)):\n\\[\n\\nabla_b L = \\sum_t \\left(\\frac{\\partial h^{(t)}}{\\partial b}\\right)^\\top \\nabla_{h^{(t)}} L = \\sum_t \\text{diag}(1 - (h^{(t)})^2) \\nabla_{h^{(t)}} L \\tag{10.23}\n\\]\nWeight \\(V\\) (forward gradient is \\((h^{(t)})^\\top\\)):\n\\[\n\\nabla_V L = \\sum_t \\nabla_{o^{(t)}} L (h^{(t)})^\\top \\tag{10.24}\n\\]\nWeight \\(W\\) (forward gradient is \\(\\text{diag}(1 - (h^{(t)})^2)(h^{(t-1)})^\\top\\)):\n\\[\n\\nabla_W L = \\sum_t \\text{diag}(1 - (h^{(t)})^2) \\nabla_{h^{(t)}} L (h^{(t-1)})^\\top \\tag{10.25}\n\\]\nWeight \\(U\\) (forward gradient is \\(\\text{diag}(1 - (h^{(t)})^2)(x^{(t)})^\\top\\)):\n\\[\n\\nabla_U L = \\sum_t \\text{diag}(1 - (h^{(t)})^2) \\nabla_{h^{(t)}} L (x^{(t)})^\\top \\tag{10.26}\n\\]\n\n\n\nBPTT gradient flow"
  },
  {
    "objectID": "ML/rnn-recurrent-neural-networks.html#rnn-as-directed-graph",
    "href": "ML/rnn-recurrent-neural-networks.html#rnn-as-directed-graph",
    "title": "Chapter 10.2: Recurrent Neural Networks",
    "section": "RNN as Directed Graph",
    "text": "RNN as Directed Graph\nThe objective of RNN is to maximize the log-likelihood:\n\\[\n\\log p(y^{(t)} | x^{(1)}, ..., x^{(t)}) \\tag{10.29}\n\\]\nOr the input can include the connection between the current step and the output from the previous step:\n\\[\n\\log p(y^{(t)} | x^{(1)}, ..., x^{(t)}, y^{(1)}, ..., y^{(t-1)}) \\tag{10.30}\n\\]\nBy only looking at sequence \\(\\mathbf{Y}\\), the probability can be represented using chain rule:\n\\[\nP(\\mathbf{Y}) = \\prod_{t=1}^{\\tau} P(y^{(t)} | y^{(t-1)}, ..., y^{(1)}) \\tag{10.31}\n\\]\nThe negative log-likelihood is:\n\\[\nL = \\sum_t L^{(t)} \\tag{10.32}\n\\]\n\\[\nL^{(t)} = -\\log P(y^{(t)} | y^{(t-1)}, ..., y^{(1)}) \\tag{10.33}\n\\]\nParameter Complexity:\nA naive fully-connected model would need \\(O(k^\\tau)\\) parameters. An RNN uses a fixed-size hidden state, so the number of parameters is independent of \\(\\tau\\).\n\n\n\nFull graph vs RNN: parameter efficiency"
  },
  {
    "objectID": "ML/rnn-recurrent-neural-networks.html#context-based-rnn-model",
    "href": "ML/rnn-recurrent-neural-networks.html#context-based-rnn-model",
    "title": "Chapter 10.2: Recurrent Neural Networks",
    "section": "Context-Based RNN Model",
    "text": "Context-Based RNN Model\nSometimes we need a fixed context \\(x\\) as the input (e.g., an image embedding), and we want the RNN to generate a sequence that describes the image.\nMore generally, the context itself can be an input sequence \\(\\mathbf{X} = (x^{(1)}, ..., x^{(\\tau_x)})\\), and the RNN models a conditional distribution over an output sequence \\(\\mathbf{Y} = (y^{(1)}, ..., y^{(\\tau_y)})\\) of possibly different length.\nExample: In machine translation, the source sentence serves as the context for generating the target sentence.\n\n\n\nThree types of context-based RNN models"
  },
  {
    "objectID": "ML/rnn-recurrent-neural-networks.html#key-insights",
    "href": "ML/rnn-recurrent-neural-networks.html#key-insights",
    "title": "Chapter 10.2: Recurrent Neural Networks",
    "section": "Key Insights",
    "text": "Key Insights\nRNNs enable sequence modeling through parameter sharing. The same weights \\((W, U, V)\\) are reused across all time steps, making RNNs efficient for variable-length sequences. However, this comes at the cost of sequential computation‚Äîforward and backward passes cannot be parallelized across time.\nTeacher forcing accelerates training by using ground-truth outputs instead of model predictions, enabling parallel computation during training. The tradeoff is exposure bias: during inference, the model must use its own predictions, which may differ from the training distribution.\nBPTT computes gradients by unfolding the network through time. The gradient at each hidden state combines contributions from both the immediate output and all future time steps, enabling the network to learn long-range dependencies (though vanishing/exploding gradients remain a challenge)."
  },
  {
    "objectID": "ML/parameter-tying-sharing.html",
    "href": "ML/parameter-tying-sharing.html",
    "title": "Chapter 7.9: Parameter Tying and Parameter Sharing",
    "section": "",
    "text": "Neural networks can benefit from constraints on parameters in two distinct ways:\n\nParameter Tying: Encourages parameters to be similar through regularization\nParameter Sharing: Forces parameters to be identical by design\n\nBoth approaches reduce effective model capacity and improve generalization, but they differ fundamentally in how strictly they enforce parameter relationships."
  },
  {
    "objectID": "ML/parameter-tying-sharing.html#overview",
    "href": "ML/parameter-tying-sharing.html#overview",
    "title": "Chapter 7.9: Parameter Tying and Parameter Sharing",
    "section": "",
    "text": "Neural networks can benefit from constraints on parameters in two distinct ways:\n\nParameter Tying: Encourages parameters to be similar through regularization\nParameter Sharing: Forces parameters to be identical by design\n\nBoth approaches reduce effective model capacity and improve generalization, but they differ fundamentally in how strictly they enforce parameter relationships."
  },
  {
    "objectID": "ML/parameter-tying-sharing.html#parameter-tying",
    "href": "ML/parameter-tying-sharing.html#parameter-tying",
    "title": "Chapter 7.9: Parameter Tying and Parameter Sharing",
    "section": "Parameter Tying",
    "text": "Parameter Tying\n\nDefinition\nParameter tying constrains parameters of different models or layers to be similar by adding a penalty term to the loss function.\n\n\nMathematical Formulation\n\\[\n\\Omega(w^{(A)}, w^{(B)}) = \\|w^{(A)} - w^{(B)}\\|_2^2\n\\]\nHow it works:\n\nAdd this penalty term to the loss function\nForces two models (or layers) to learn similar parameters\nThe parameters are still independent, but regularization encourages similarity\n\n\n\n\n\n\n\nNoteNature of Constraint\n\n\n\nThis is a soft constraint that allows some deviation while encouraging parameter alignment.\n\n\n\n\nReal-World Applications\nNote: The following table is generated by ChatGPT.\n\n\n\n\n\n\n\n\n\nApplication\nMechanism\nPurpose\nReference\n\n\n\n\nWord Embedding Tying\nInput embedding matrix and output softmax matrix tied: \\(W_{\\text{out}} = E^T\\)\nReduce parameters; consistent embedding space\nPress & Wolf, 2017\n\n\nAutoencoder\nDecoder weights tied to encoder transpose: \\(W_{\\text{dec}} = W_{\\text{enc}}^T\\)\nRegularize; stabilize training; mimic PCA\nHinton & Salakhutdinov, 2006\n\n\nMulti-task Learning\nDifferent tasks‚Äô parameters constrained to be similar: \\(\\Omega = \\|w^{(A)} - w^{(B)}\\|^2\\)\nEncourage knowledge sharing between tasks\nCaruana, 1997\n\n\nKnowledge Distillation\nStudent layers tied to teacher via loss constraint: \\(\\|h_s^{(l)} - h_t^{(l)}\\|^2\\)\nTransfer intermediate representations\nSanh et al., 2019 ‚Äî DistilBERT"
  },
  {
    "objectID": "ML/parameter-tying-sharing.html#parameter-sharing",
    "href": "ML/parameter-tying-sharing.html#parameter-sharing",
    "title": "Chapter 7.9: Parameter Tying and Parameter Sharing",
    "section": "Parameter Sharing",
    "text": "Parameter Sharing\n\nDefinition\nParameter sharing uses the exact same set of parameters across multiple locations or time steps.\n\n\nCNN Example\nThe same kernel (set of weights) is applied across all spatial locations of the input:\n\\[\ny_{i,j} = \\sum_{u,v} w_{u,v} x_{i+u, j+v}\n\\]\nBenefits:\n\nPattern detection: Detects the same pattern (e.g., edge or texture) anywhere in the image\nParameter reduction: Dramatically reduces the number of parameters\nTranslation equivariance: Output shifts when input shifts\n\n\n\n\n\n\n\nImportantNature of Constraint\n\n\n\nThis is a hard constraint where parameters are identical by design, not just similar.\n\n\n\n\nReal-World Applications\nNote: The following table is generated by ChatGPT.\n\n\n\n\n\n\n\n\n\nApplication\nMechanism\nPurpose\nReference\n\n\n\n\nCNN\nSame kernel slides across all spatial positions: \\(y_{i,j} = \\sum_{u,v} w_{u,v} x_{i+u, j+v}\\)\nDetect same pattern anywhere; reduce parameters; translation equivariance\nLeCun et al., 1998 ‚Äî LeNet\n\n\nRNN / LSTM / GRU\nSame weights used at each time step: \\(h_t = f(W_h h_{t-1} + W_x x_t)\\)\nTemporal consistency; handle variable-length sequences\nHochreiter & Schmidhuber, 1997\n\n\nTransformer (ALBERT)\nAll encoder layers share parameters: \\(\\theta_1 = \\theta_2 = \\dots = \\theta_L\\)\nReduce memory; efficient deep sharing\nLan et al., 2020 ‚Äî ALBERT\n\n\nSiamese / Twin Networks\nTwo (or more) branches share all parameters: \\(f_\\theta(x_1), f_\\theta(x_2)\\)\nCompare similarity; representation consistency\nBromley et al., 1993 ‚Äî Siamese Nets\n\n\n\n\nSource: Deep Learning Book, Chapter 7.9"
  },
  {
    "objectID": "ML/cnn-unsupervised-learning.html",
    "href": "ML/cnn-unsupervised-learning.html",
    "title": "Chapter 9.9: Unsupervised or Semi-Supervised Feature Learning",
    "section": "",
    "text": "Before modern deep learning, visual features were extracted using manually designed filters such as Sobel, Laplacian, Gaussian blur, sharpening, and emboss kernels. These filters capture simple patterns‚Äîedges, gradients, corners, and blobs‚Äîbased on human intuition about what matters in images. They work reasonably well for low-level vision tasks but cannot automatically adapt to new data distributions or complex visual concepts."
  },
  {
    "objectID": "ML/cnn-unsupervised-learning.html#hand-crafted-convolution-kernels",
    "href": "ML/cnn-unsupervised-learning.html#hand-crafted-convolution-kernels",
    "title": "Chapter 9.9: Unsupervised or Semi-Supervised Feature Learning",
    "section": "",
    "text": "Before modern deep learning, visual features were extracted using manually designed filters such as Sobel, Laplacian, Gaussian blur, sharpening, and emboss kernels. These filters capture simple patterns‚Äîedges, gradients, corners, and blobs‚Äîbased on human intuition about what matters in images. They work reasonably well for low-level vision tasks but cannot automatically adapt to new data distributions or complex visual concepts."
  },
  {
    "objectID": "ML/cnn-unsupervised-learning.html#unsupervised-feature-learning",
    "href": "ML/cnn-unsupervised-learning.html#unsupervised-feature-learning",
    "title": "Chapter 9.9: Unsupervised or Semi-Supervised Feature Learning",
    "section": "2) Unsupervised Feature Learning",
    "text": "2) Unsupervised Feature Learning\nTo avoid manually designing all filters, researchers explored unsupervised or semi-supervised methods that automatically learn useful features from unlabeled data. Approaches such as sparse coding, autoencoders, k-means feature extraction, and clustering-based templates attempted to replace hand-crafted kernels by discovering edge-like or texture-like patterns directly from raw images without labels. These methods were effective for simple patterns and could reduce the need for labeled data, but they were limited in scale, depth, and representational power."
  },
  {
    "objectID": "ML/cnn-unsupervised-learning.html#why-modern-systems-still-rely-on-cnns-to-learn-features",
    "href": "ML/cnn-unsupervised-learning.html#why-modern-systems-still-rely-on-cnns-to-learn-features",
    "title": "Chapter 9.9: Unsupervised or Semi-Supervised Feature Learning",
    "section": "3) Why Modern Systems Still Rely on CNNs to Learn Features",
    "text": "3) Why Modern Systems Still Rely on CNNs to Learn Features\nAlthough hand-crafted kernels and unsupervised feature learning provided meaningful progress, they cannot match the expressiveness, depth, and adaptability of convolutional neural networks. CNNs learn hierarchical features end-to-end‚Äîfrom simple edges to complex textures, shapes, and object parts‚Äîoptimized directly for the final task. CNNs scale to large datasets, capture richer invariances, and consistently outperform manually designed or shallow unsupervised methods. As a result, feature extraction today is dominated by CNNs (or even more powerful architectures like Vision Transformers), making handcrafted filters and early unsupervised methods largely obsolete except for instructional or specialized uses.\n\n Figure: Classic hand-crafted convolution kernels including Sobel (edge detection), Laplacian (corners and edges), Gaussian blur (smoothing), sharpening, and emboss filters. These filters were the foundation of computer vision before deep learning, capturing simple patterns based on human intuition about visual features."
  },
  {
    "objectID": "ML/cnn-unsupervised-learning.html#key-insight",
    "href": "ML/cnn-unsupervised-learning.html#key-insight",
    "title": "Chapter 9.9: Unsupervised or Semi-Supervised Feature Learning",
    "section": "Key Insight",
    "text": "Key Insight\nThe evolution from hand-crafted to learned features represents a fundamental shift in computer vision. Hand-crafted kernels like Sobel and Laplacian were limited to simple, predefined patterns. Unsupervised learning methods (sparse coding, autoencoders, k-means) attempted to discover features automatically but lacked depth and scalability. Modern CNNs surpass both by learning hierarchical features end-to-end‚Äîfrom low-level edges to high-level semantic concepts‚Äîoptimized directly for the task at hand. This adaptability and representational power explain why CNNs became the dominant paradigm in visual feature extraction."
  },
  {
    "objectID": "ML/cnn-neuroscience.html",
    "href": "ML/cnn-neuroscience.html",
    "title": "Chapter 9.10: Neuroscientific Basis for Convolutional Networks",
    "section": "",
    "text": "V1 is a part of the brain, also named as the primary visual cortex. It exhibits three key properties that inspired convolutional neural networks:\n\nSpatial mapping: Neurons in V1 form a topographic map of the visual field\nSimple cells: Detect features within local receptive fields\nComplex cells: Exhibit translation invariance\n\n Figure: The primary visual cortex (V1) contains simple cells with local receptive fields that detect oriented edges, and complex cells that pool over simple cells to achieve translation invariance‚Äîinspiring the convolution and pooling operations in CNNs."
  },
  {
    "objectID": "ML/cnn-neuroscience.html#v1-primary-visual-cortex",
    "href": "ML/cnn-neuroscience.html#v1-primary-visual-cortex",
    "title": "Chapter 9.10: Neuroscientific Basis for Convolutional Networks",
    "section": "",
    "text": "V1 is a part of the brain, also named as the primary visual cortex. It exhibits three key properties that inspired convolutional neural networks:\n\nSpatial mapping: Neurons in V1 form a topographic map of the visual field\nSimple cells: Detect features within local receptive fields\nComplex cells: Exhibit translation invariance\n\n Figure: The primary visual cortex (V1) contains simple cells with local receptive fields that detect oriented edges, and complex cells that pool over simple cells to achieve translation invariance‚Äîinspiring the convolution and pooling operations in CNNs."
  },
  {
    "objectID": "ML/cnn-neuroscience.html#grandmother-cell",
    "href": "ML/cnn-neuroscience.html#grandmother-cell",
    "title": "Chapter 9.10: Neuroscientific Basis for Convolutional Networks",
    "section": "Grandmother Cell",
    "text": "Grandmother Cell\nThe grandmother cell is a classic (and partly humorous) idea in neuroscience suggesting that a single neuron might respond exclusively to a very specific concept‚Äîsuch as your grandmother‚Äôs face and nothing else.\nAlthough this extreme form of representation is unlikely to exist exactly as described, experiments have revealed highly selective neurons in the human medial temporal lobe, such as the famous ‚ÄúJennifer Aniston neuron‚Äù, which respond strongly to one particular person or concept across different images and modalities.\n Figure: The ‚Äúgrandmother cell‚Äù hypothesis: highly selective neurons that respond to specific concepts or people. While extreme single-neuron selectivity is rare, the Jennifer Aniston neuron demonstrates that some neurons can be remarkably selective for particular individuals."
  },
  {
    "objectID": "ML/cnn-neuroscience.html#differences-between-cnns-and-animal-vision-systems",
    "href": "ML/cnn-neuroscience.html#differences-between-cnns-and-animal-vision-systems",
    "title": "Chapter 9.10: Neuroscientific Basis for Convolutional Networks",
    "section": "Differences Between CNNs and Animal Vision Systems",
    "text": "Differences Between CNNs and Animal Vision Systems\n\n1. Retinal Resolution and Saccades\nThe human eye has very low resolution over most of the visual field, except for a small high-resolution region called the fovea. In contrast, most CNNs receive uniformly high-resolution images as input.\nTo compensate for the retina‚Äôs limited resolution, the brain performs rapid eye movements called saccades, selectively directing the fovea toward the most informative or task-relevant parts of the scene. The brain then integrates these glimpses into a coherent perception.\nThis biological mechanism closely relates to modern attention mechanisms in deep learning, which dynamically focus computation on informative regions rather than processing every pixel uniformly.\n\n\n2. Global Scene Understanding\nAnimal vision does not merely recognize isolated objects‚Äîit constructs an understanding of the entire scene, including relationships between objects, their spatial layout, depth cues, and how everything interacts with the body‚Äôs position in 3-D space.\nCNNs, however, operate primarily as object recognizers trained on static images. They lack the integrated multi-modal processing found in biological systems, such as combining vision with touch, motion, memory, and expectation. As a result, CNNs often fail on tasks that require holistic scene reasoning, contextual interpretation, or understanding how objects interact in physical environments.\n\n\n3. Multisensory Integration (Vision + Auditory System)\nThe human visual system operates in tight coordination with other sensory systems, especially hearing. Visual perception is continuously influenced by auditory cues‚Äîsuch as detecting motion from sound, inferring object properties, or resolving ambiguity when visual information is incomplete.\n\n\n4. Strong Top-Down Feedback in Biological Vision\nEven early visual areas like V1 receive significant top-down feedback from higher cortical regions (V2, V4, IT, PFC). This feedback modulates attention, context interpretation, and perceptual expectations. Most CNNs rely on purely feed-forward processing with limited or no feedback mechanisms.\n\n\n5. V1 Receptive Fields Are Gabor-like but More Complex\nWhile many V1 neurons resemble Gabor filters, their actual responses are more dynamic and context-dependent. CNN filters are fixed, static linear kernels, whereas biological receptive fields adapt to stimuli, feedback, and behavioral state. Thus CNN filter banks are only an approximation of biological V1."
  },
  {
    "objectID": "ML/cnn-neuroscience.html#gabor-function-and-simple-cells",
    "href": "ML/cnn-neuroscience.html#gabor-function-and-simple-cells",
    "title": "Chapter 9.10: Neuroscientific Basis for Convolutional Networks",
    "section": "Gabor Function and Simple Cells",
    "text": "Gabor Function and Simple Cells\nSimple cells in V1 can be modeled by a response function that computes a weighted sum over the image:\n\\[\nS(I) = \\sum_{x \\in X} \\sum_{y \\in Y} w(x,y) I(x,y) \\tag{9.15}\n\\]\nwhere \\(w(x,y)\\) takes the form of a Gabor function:\n\\[\nw(x,y;\\alpha,\\beta_x,\\beta_y,f,\\phi,x_0,y_0,\\tau) = \\alpha \\exp(-\\beta_x x'^2 - \\beta_y y'^2) \\cos(fx' + \\phi) \\tag{9.16}\n\\]\nwith rotated coordinates:\n\\[\nx' = (x - x_0) \\cos(\\tau) + (y - y_0) \\sin(\\tau) \\tag{9.17}\n\\]\n\\[\ny' = -(x - x_0) \\sin(\\tau) + (y - y_0) \\cos(\\tau) \\tag{9.18}\n\\]\n\nGabor Function Parameters\n\n\\(x_0, y_0\\) ‚Äî Receptive field center: Specifies where the filter is located on the image plane\n\\(\\tau\\) ‚Äî Orientation angle: Defines the preferred edge orientation of the simple cell\n\\(\\beta_x, \\beta_y\\) ‚Äî Gaussian width parameters: Control the shape and spatial extent of the receptive field\n\\(f\\) ‚Äî Spatial frequency: Determines how many cycles of the cosine grating appear within the receptive field (higher \\(f\\) detects finer edges)\n\\(\\phi\\) ‚Äî Phase offset: Controls whether the cell responds to dark-to-light edges, light-to-dark edges, or symmetric patterns\n\\(\\alpha\\) ‚Äî Amplitude scaling: Controls the overall response magnitude\n\nTogether, these parameters determine a simple cell‚Äôs preferred position, orientation, spatial frequency, and phase, allowing the Gabor function to closely approximate the tuning properties observed in biological V1 neurons.\n Figure: Gabor filters model V1 simple cells. The parameters (position, orientation, frequency, phase) define a simple cell‚Äôs selectivity for specific edge patterns at particular locations and orientations in the visual field."
  },
  {
    "objectID": "ML/cnn-neuroscience.html#complex-cells-and-pooling",
    "href": "ML/cnn-neuroscience.html#complex-cells-and-pooling",
    "title": "Chapter 9.10: Neuroscientific Basis for Convolutional Networks",
    "section": "Complex Cells and Pooling",
    "text": "Complex Cells and Pooling\nComplex cells combine the outputs of several simple cells that share the same orientation and spatial frequency but differ in phase. By pooling this quadrature pair of simple cells, complex cells become phase-invariant‚Äîthey respond to an oriented edge regardless of small shifts, contrast polarity, or phase changes.\nThis gives them local translation invariance, and similar behavior emerges in machine learning through pooling operations over Gabor-like filters."
  },
  {
    "objectID": "ML/cnn-neuroscience.html#key-insight",
    "href": "ML/cnn-neuroscience.html#key-insight",
    "title": "Chapter 9.10: Neuroscientific Basis for Convolutional Networks",
    "section": "Key Insight",
    "text": "Key Insight\nCNNs draw deep inspiration from V1, but biological vision is far richer and more complex. Simple cells act like Gabor filters, detecting oriented edges at specific locations. Complex cells pool over simple cells to achieve translation invariance, mirroring CNN pooling layers. However, CNNs lack key features of biological vision: saccadic attention, multisensory integration, top-down feedback, and dynamic receptive fields. While CNNs excel at feed-forward object recognition, they remain far from the holistic, context-aware, adaptive processing that characterizes animal vision systems. Understanding these differences guides future architectures toward more biologically plausible and robust visual intelligence."
  },
  {
    "objectID": "ML/likelihood-loss-functions.html",
    "href": "ML/likelihood-loss-functions.html",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.2 reveals the fundamental connection between probabilistic assumptions and the loss functions we use to train neural networks.\nüìì For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#the-hidden-connection-why-these-loss-functions",
    "href": "ML/likelihood-loss-functions.html#the-hidden-connection-why-these-loss-functions",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "The Hidden Connection: Why These Loss Functions?",
    "text": "The Hidden Connection: Why These Loss Functions?\nEver wondered why we use mean squared error for regression, cross-entropy for classification, and other specific loss functions? The answer lies in maximum likelihood estimation - each common loss function corresponds to the negative log-likelihood of a specific probabilistic model.\n\n\n\n\n\n\n\n\nProbabilistic Model\nLoss Function\nUse Case\n\n\n\n\nGaussian likelihood\nMean Squared Error\nRegression\n\n\nBernoulli likelihood\nBinary Cross-Entropy\nBinary Classification\n\n\nCategorical likelihood\nSoftmax Cross-Entropy\nMulticlass Classification"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#exploring-the-connection-probabilistic-models-loss-functions",
    "href": "ML/likelihood-loss-functions.html#exploring-the-connection-probabilistic-models-loss-functions",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "üéØ Exploring the Connection: Probabilistic Models ‚Üí Loss Functions",
    "text": "üéØ Exploring the Connection: Probabilistic Models ‚Üí Loss Functions\n\n\nShow code\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-1-gaussian-likelihood-mean-squared-error",
    "href": "ML/likelihood-loss-functions.html#connection-1-gaussian-likelihood-mean-squared-error",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 1: Gaussian Likelihood ‚Üí Mean Squared Error",
    "text": "Connection 1: Gaussian Likelihood ‚Üí Mean Squared Error\nThe Setup: When we assume our targets have Gaussian noise around our predictions:\n\\[p(y|x) = \\mathcal{N}(y; \\hat{y}, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\hat{y})^2}{2\\sigma^2}\\right)\\]\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = \\frac{(y-\\hat{y})^2}{2\\sigma^2} + \\frac{1}{2}\\log(2\\pi\\sigma^2)\\]\nThe Result: Minimizing this is equivalent to minimizing MSE (the constant term doesn‚Äôt affect optimization)!\n\n\nShow code\n# Demonstrate Gaussian likelihood = MSE connection\nnp.random.seed(0)\nx = np.linspace(-1, 1, 20)\ny_true = 2 * x + 1\ny = y_true + np.random.normal(0, 0.1, size=x.shape)  # Gaussian noise\n\n# Simple linear model predictions\nw, b = 1.0, 0.0\ny_pred = w * x + b\n\n# Compute MSE\nmse = np.mean((y - y_pred)**2)\n\n# Compute Gaussian negative log-likelihood\nsigma_squared = 0.1**2\nquadratic_term = 0.5 * np.mean((y - y_pred)**2) / sigma_squared\nconst_term = 0.5 * np.log(2 * np.pi * sigma_squared)\nnll_gaussian = quadratic_term + const_term\n\nprint(\"üìä Gaussian Likelihood ‚Üî MSE Connection\")\nprint(\"=\" * 45)\nprint(f\"üìà Mean Squared Error:     {mse:.6f}\")\nprint(f\"üìä Gaussian NLL:           {nll_gaussian:.6f}\")\nprint(f\"   ‚îú‚îÄ Quadratic term:      {quadratic_term:.6f}\")\nprint(f\"   ‚îî‚îÄ Constant term:       {const_term:.6f}\")\n\nscaling_factor = 1 / (2 * sigma_squared)\nprint(f\"\\nüîó Mathematical Connection:\")\nprint(f\"   Quadratic term = {scaling_factor:.1f} √ó MSE\")\nprint(f\"   {quadratic_term:.6f} = {scaling_factor:.1f} √ó {mse:.6f}\")\nprint(f\"\\n‚úÖ Minimizing MSE ‚â° Maximizing Gaussian likelihood\")\n\n\nüìä Gaussian Likelihood ‚Üî MSE Connection\n=============================================\nüìà Mean Squared Error:     1.450860\nüìä Gaussian NLL:           71.159339\n   ‚îú‚îÄ Quadratic term:      72.542985\n   ‚îî‚îÄ Constant term:       -1.383647\n\nüîó Mathematical Connection:\n   Quadratic term = 50.0 √ó MSE\n   72.542985 = 50.0 √ó 1.450860\n\n‚úÖ Minimizing MSE ‚â° Maximizing Gaussian likelihood"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-2-bernoulli-likelihood-binary-cross-entropy",
    "href": "ML/likelihood-loss-functions.html#connection-2-bernoulli-likelihood-binary-cross-entropy",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 2: Bernoulli Likelihood ‚Üí Binary Cross-Entropy",
    "text": "Connection 2: Bernoulli Likelihood ‚Üí Binary Cross-Entropy\nThe Setup: For binary classification, we assume Bernoulli-distributed targets:\n\\[p(y|x) = \\sigma(z)^y (1-\\sigma(z))^{1-y}\\]\nwhere \\(\\sigma(z) = \\frac{1}{1+e^{-z}}\\) is the sigmoid function.\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = -y\\log\\sigma(z) - (1-y)\\log(1-\\sigma(z))\\]\nThe Result: This is exactly binary cross-entropy loss!\n\n\nShow code\n# Demonstrate Bernoulli likelihood = Binary cross-entropy connection\nz = torch.tensor([-0.5, -0.8, 0.0, 0.8, 0.5])  # Model logits\ny = torch.tensor([0.0, 0.0, 1.0, 1.0, 1.0])     # Binary labels\np = torch.sigmoid(z)  # Convert to probabilities\n\nprint(\"üé≤ Bernoulli Likelihood ‚Üî Binary Cross-Entropy\")\nprint(\"=\" * 50)\nprint(\"Input Data:\")\nprint(f\"   Logits:        {z.numpy()}\")\nprint(f\"   Labels:        {y.numpy()}\")\nprint(f\"   Probabilities: {p.numpy()}\")\n\n# Manual Bernoulli NLL computation\nbernoulli_nll = torch.mean(-(y * torch.log(p) + (1 - y) * torch.log(1 - p)))\n\n# PyTorch binary cross-entropy\nbce_loss = F.binary_cross_entropy(p, y)\n\nprint(f\"\\nüìä Loss Function Comparison:\")\nprint(f\"   Manual Bernoulli NLL:  {bernoulli_nll:.6f}\")\nprint(f\"   PyTorch BCE Loss:      {bce_loss:.6f}\")\n\n# Verify they're identical\ndifference = torch.abs(bernoulli_nll - bce_loss)\nprint(f\"\\nüîó Verification:\")\nprint(f\"   Absolute difference:   {difference:.10f}\")\nprint(f\"\\n‚úÖ Binary cross-entropy IS Bernoulli negative log-likelihood!\")\n\n\nüé≤ Bernoulli Likelihood ‚Üî Binary Cross-Entropy\n==================================================\nInput Data:\n   Logits:        [-0.5 -0.8  0.   0.8  0.5]\n   Labels:        [0. 0. 1. 1. 1.]\n   Probabilities: [0.37754068 0.3100255  0.5        0.6899745  0.62245935]\n\nüìä Loss Function Comparison:\n   Manual Bernoulli NLL:  0.476700\n   PyTorch BCE Loss:      0.476700\n\nüîó Verification:\n   Absolute difference:   0.0000000000\n\n‚úÖ Binary cross-entropy IS Bernoulli negative log-likelihood!"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-3-categorical-likelihood-softmax-cross-entropy",
    "href": "ML/likelihood-loss-functions.html#connection-3-categorical-likelihood-softmax-cross-entropy",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 3: Categorical Likelihood ‚Üí Softmax Cross-Entropy",
    "text": "Connection 3: Categorical Likelihood ‚Üí Softmax Cross-Entropy\nThe Setup: For multiclass classification, we use the categorical distribution:\n\\[p(y=i|x) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} = \\text{softmax}(z)_i\\]\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = -\\log \\frac{e^{z_y}}{\\sum_j e^{z_j}} = -z_y + \\log\\sum_j e^{z_j}\\]\nThe Result: This is exactly softmax cross-entropy loss!\n\n\nShow code\n# Demonstrate Categorical likelihood = Softmax cross-entropy connection\nz = torch.tensor([[0.1, 0.2, 0.7],    # Sample 1: class 2 highest\n                  [0.1, 0.7, 0.2],    # Sample 2: class 1 highest  \n                  [0.7, 0.1, 0.2]])   # Sample 3: class 0 highest\n\ny = torch.tensor([2, 1, 0])           # True class indices\n\nprint(\"üéØ Categorical Likelihood ‚Üî Softmax Cross-Entropy\")\nprint(\"=\" * 55)\nprint(\"Input Data:\")\nprint(f\"   Logits shape:    {z.shape}\")\nprint(f\"   True classes:    {y.numpy()}\")\n\n# Convert to probabilities\nsoftmax_probs = F.softmax(z, dim=1)\nprint(f\"\\nSoftmax Probabilities:\")\nfor i, (logit_row, prob_row, true_class) in enumerate(zip(z, softmax_probs, y)):\n    print(f\"   Sample {i+1}: {prob_row.numpy()} ‚Üí Class {true_class}\")\n\n# Manual categorical NLL (using log-softmax for numerical stability)\nlog_softmax = F.log_softmax(z, dim=1)\ncategorical_nll = -torch.mean(log_softmax[range(len(y)), y])\n\n# PyTorch cross-entropy\nce_loss = F.cross_entropy(z, y)\n\nprint(f\"\\nüìä Loss Function Comparison:\")\nprint(f\"   Manual Categorical NLL: {categorical_nll:.6f}\")\nprint(f\"   PyTorch Cross-Entropy:  {ce_loss:.6f}\")\n\n# Verify they're identical\ndifference = torch.abs(categorical_nll - ce_loss)\nprint(f\"\\nüîó Verification:\")\nprint(f\"   Absolute difference:    {difference:.10f}\")\nprint(f\"\\n‚úÖ Cross-entropy IS categorical negative log-likelihood!\")\n\n\nüéØ Categorical Likelihood ‚Üî Softmax Cross-Entropy\n=======================================================\nInput Data:\n   Logits shape:    torch.Size([3, 3])\n   True classes:    [2 1 0]\n\nSoftmax Probabilities:\n   Sample 1: [0.25462854 0.28140804 0.46396342] ‚Üí Class 2\n   Sample 2: [0.25462854 0.46396342 0.28140804] ‚Üí Class 1\n   Sample 3: [0.46396342 0.25462854 0.28140804] ‚Üí Class 0\n\nüìä Loss Function Comparison:\n   Manual Categorical NLL: 0.767950\n   PyTorch Cross-Entropy:  0.767950\n\nüîó Verification:\n   Absolute difference:    0.0000000000\n\n‚úÖ Cross-entropy IS categorical negative log-likelihood!"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#why-this-matters-bce-vs-mse-for-classification",
    "href": "ML/likelihood-loss-functions.html#why-this-matters-bce-vs-mse-for-classification",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Why This Matters: BCE vs MSE for Classification",
    "text": "Why This Matters: BCE vs MSE for Classification\nUnderstanding the probabilistic foundation explains why binary cross-entropy works better than MSE for classification, even though both can theoretically solve binary problems.\nKey Differences: - BCE gradient: \\(\\sigma(z) - y\\) (simple, well-behaved) - MSE gradient: \\(2(\\sigma(z) - y) \\times \\sigma(z) \\times (1 - \\sigma(z))\\) (can vanish!)\nLet‚Äôs see this in practice:"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#key-takeaways",
    "href": "ML/likelihood-loss-functions.html#key-takeaways",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nUnderstanding the probabilistic foundation of loss functions reveals:\n\nMSE = Gaussian NLL: Mean squared error emerges from assuming Gaussian noise\nBCE = Bernoulli NLL: Binary cross-entropy is exactly Bernoulli negative log-likelihood\n\nCross-entropy = Categorical NLL: Softmax cross-entropy corresponds to categorical distributions\nBetter gradients: Probabilistically-motivated loss functions provide better optimization dynamics\n\nThis connection between probability theory and optimization is fundamental to understanding why certain loss functions work well for specific tasks.\n\nThis mathematical foundation helps explain not just which loss function to use, but why it works so effectively for the given problem type."
  },
  {
    "objectID": "ML/k_means_clustering.html",
    "href": "ML/k_means_clustering.html",
    "title": "K-means Clustering",
    "section": "",
    "text": "Setup points and K\nwe will implement a KNN algorithm to cluster the points\n\n\nX=[[1,1],[2,2.1],[3,2.5],[6,7],[7,7.1],[9,7.5]]\nk=2\n\nmax_iter=3\n\n\n# Visualize the data\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter([x[0] for x in X],[x[1] for x in X])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pure python implementation of K-means clustering\ndef knn_iter(X,centroids):\n    # set up new clusters\n    new_clusters=[[] for _ in range(len(centroids))]\n    # k=len(centroids)\n    # assign each point to the nearest centroid\n    for x in X:\n        k,distance=0,(x[0]-centroids[0][0])**2+(x[1]-centroids[0][1])**2\n        for i,c in enumerate(centroids[1:],1):\n            if (x[0]-c[0])**2+(x[1]-c[1])**2&lt;distance:\n                k=i\n                distance=(x[0]-c[0])**2+(x[1]-c[1])**2\n        new_clusters[k].append(x)\n    \n    # calculate new centroids\n    new_centroids=[[\n        sum([x[0] for x in cluster])/len(cluster),\n        sum([x[1] for x in cluster])/len(cluster)\n    ] if cluster else centroids[i] for i,cluster in enumerate(new_clusters)]\n    return new_centroids\n\n\n\n\n\n\n\n\ndef iter_and_draw(X,k,max_iter):\n    centroids=X[:k]  # Randomly select 2 centroids\n    fig, axes = plt.subplots(max_iter//3+(1 if max_iter%3!=0 else 0),\n        3, figsize=(15, 10))\n    axes=axes.flatten()\n    for i in range(max_iter):\n        \n        # Plot points and centroids\n\n\n        # Assign each point to nearest centroid and plot with corresponding color\n        colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n        for j, x in enumerate(X):\n            # Find nearest centroid\n            min_dist = float('inf')\n            nearest_centroid = 0\n            for k, c in enumerate(centroids):\n                dist = (x[0]-c[0])**2 + (x[1]-c[1])**2\n                if dist &lt; min_dist:\n                    min_dist = dist\n                    nearest_centroid = k\n            # Plot point with color corresponding to its cluster\n            axes[i].scatter(x[0], x[1], c=colors[nearest_centroid % len(colors)], label=f'Cluster {nearest_centroid+1}' if j==0 else \"\")\n        axes[i].scatter([c[0] for c in centroids], [c[1] for c in centroids], c='red', marker='*', s=200, label='Centroids')\n        axes[i].set_title(f'Iteration {i}')\n        centroids = knn_iter(X, centroids)\n\n    plt.tight_layout()\n    plt.show()\n\niter_and_draw(X,k,max_iter)\n# print(centroids)\n\n\n\n\n\n\n\n\n\n# A 3 clusters example\n\nimport numpy as np\n\nX1=np.random.rand(20,2)+5 # Some points in the upper right corner\nX2=np.random.rand(20,2)+3 # Some points in the middle\nX3=np.random.rand(20,2) # Some points in the lower left corner\n\niter_and_draw(np.concatenate((X1,X2,X3)),3,5)\n\n\n\n\n\n\n\n\n\n\nA question?\n\nWhat to do if one cluster has no assigned points during iteration?\n\n\n\nFormula Derivation\nThe goal is to minimize the loss of inertia which is sum of the points to cluster centroids.\n\\[\nLoss= \\sum_{i=1}^n \\sum_{x \\in C_i} ||x-\\mu_i||^2\n\\]\nTo iter \\(\\mu\\) for each cluster, let us find the derivative of the following function. \\[\nf(\\mu)=\\sum_{i=1}^n ||x_i-\\mu||^2 =\n\\sum_{i=1}^n {x_i}^2+\\mu^2-2x_i\\mu\n\\]\nGiven a \\(\\nabla \\mu\\), \\[\nf(\\mu + \\nabla \\mu)=\\sum_{i=1}^n ||x_i+\\nabla \\mu -\\mu||^2 =\n\\sum_{i=1}^n  {x_i}^2+\\mu^2+{\\nabla \\mu}^2-2{x_i \\mu}-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\nf(\\mu + \\nabla \\mu)-f(\\mu)=\n\\sum_{i=1}^n {\\nabla \\mu}^2-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\n\\frac {f(\\mu + \\nabla \\mu)-f(\\mu)}{\\nabla \\mu}=\\sum_{i=1}^n {\\nabla \\mu} -2 \\mu +2{x_i} = 2\\sum_{i=1}^n x_i - 2n\\mu\n\\]\nNow we can see if \\(n\\mu = \\sum_{i=1}^n x_i\\), then the derivative is 0, this is why in each iteration, we need to set the center of the cluster as centroid."
  },
  {
    "objectID": "ML/noise-robustness.html",
    "href": "ML/noise-robustness.html",
    "title": "Noise Robustness: How Weight Perturbation Leads to Regularization",
    "section": "",
    "text": "Noise injection can be used as a regularization technique to improve model robustness and generalization. This section explores how adding random perturbations to weights leads to an effective regularization term."
  },
  {
    "objectID": "ML/noise-robustness.html#overview",
    "href": "ML/noise-robustness.html#overview",
    "title": "Noise Robustness: How Weight Perturbation Leads to Regularization",
    "section": "",
    "text": "Noise injection can be used as a regularization technique to improve model robustness and generalization. This section explores how adding random perturbations to weights leads to an effective regularization term."
  },
  {
    "objectID": "ML/noise-robustness.html#random-perturbation-on-weights",
    "href": "ML/noise-robustness.html#random-perturbation-on-weights",
    "title": "Noise Robustness: How Weight Perturbation Leads to Regularization",
    "section": "Random Perturbation on Weights",
    "text": "Random Perturbation on Weights\n\nOriginal Error Function\nThe standard mean squared error:\n\\[\nJ = \\mathbb{E}_{p(x,y)} \\left[ (\\hat{y}(x) - y)^2 \\right]\n\\]\n\n\nWeight Noise Model\nAdd Gaussian noise to the weights:\n\\[\n\\epsilon_W \\sim \\mathcal{N}(0, \\eta I)\n\\]\nThis is a normal distribution with:\n\nMean: \\(0\\)\nCovariance: \\(\\eta I\\) (where \\(\\eta\\) controls the noise magnitude)\n\n\n\nObjective Function with Noisy Weights\nLet \\(\\hat{y}_{\\epsilon_W}(x) = \\hat{y}_{W + \\epsilon_W}(x)\\) denote the model output with perturbed weights.\nThe new objective becomes:\n\\[\n\\tilde{J}_W = \\mathbb{E}_{p(x, y, \\epsilon_W)} \\left[ (\\hat{y}_{\\epsilon_W}(x) - y)^2 \\right]\n\\]\nFormula 7.31: This expectation is over the data distribution and the weight noise."
  },
  {
    "objectID": "ML/noise-robustness.html#deriving-the-regularization-term",
    "href": "ML/noise-robustness.html#deriving-the-regularization-term",
    "title": "Noise Robustness: How Weight Perturbation Leads to Regularization",
    "section": "Deriving the Regularization Term",
    "text": "Deriving the Regularization Term\n\nExpanding the Squared Error\n\\[\n\\tilde{J} = \\mathbb{E}_{p(x, y, \\epsilon_W)} \\left[ \\hat{y}_{\\epsilon_W}^2(x) - 2y \\hat{y}_{\\epsilon_W}(x) + y^2 \\right]\n\\]\nFormula 7.32\n\n\nTaylor Approximation\nWhen \\(\\eta\\) is small, we can approximate:\n\\[\n\\hat{y}_{W + \\epsilon_W}(x) \\approx \\hat{y}_W(x) + \\epsilon_W^T \\nabla_W \\hat{y}_W(x)\n\\]\n\n\n\n\n\n\nNoteInterpretation\n\n\n\nThe change in output is approximately the inner product of the weight noise \\(\\epsilon_W\\) and the gradient \\(\\nabla_W \\hat{y}_W(x)\\) ‚Äî i.e., the noise projected onto the gradient direction.\n\n\n\n\nSimplification\nLet:\n\n\\(a = \\hat{y}_W(x) - y\\) (prediction error)\n\\(b = \\epsilon_W^T \\nabla_W \\hat{y}_W(x)\\) (noise-induced perturbation)\n\nThen:\n\\[\n\\tilde{J} = \\mathbb{E}[a^2] + \\mathbb{E}[2ab] + \\mathbb{E}[b^2]\n\\]\nKey observations:\n\nCross-term vanishes: \\[\n\\mathbb{E}[ab] = 0\n\\] Because \\(\\epsilon_W\\) has zero mean and is independent of \\(a\\).\nNoise variance contributes regularization: \\[\n\\mathbb{E}[b^2] = \\mathbb{E}\\left[(\\epsilon_W^T \\nabla_W \\hat{y}_W(x))^2\\right]\n\\]\n\nSince \\(\\epsilon_W \\sim \\mathcal{N}(0, \\eta I)\\):\n\\[\n\\mathbb{E}[b^2] = \\eta ||\\nabla_W \\hat{y}_W(x)||^2\n\\]\n\n\n\n\n\n\nTipDerivation Detail\n\n\n\nFor a Gaussian random vector \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\\) and any vector \\(v\\):\n\\[\n\\mathbb{E}[(\\epsilon^T v)^2] = \\sigma^2 ||v||^2\n\\]"
  },
  {
    "objectID": "ML/noise-robustness.html#final-regularized-objective",
    "href": "ML/noise-robustness.html#final-regularized-objective",
    "title": "Noise Robustness: How Weight Perturbation Leads to Regularization",
    "section": "Final Regularized Objective",
    "text": "Final Regularized Objective\nCombining the terms:\n\\[\n\\tilde{J}(W; x, y) = J(W) + \\eta \\mathbb{E}_{p(x,y)} \\left[ ||\\nabla_W \\hat{y}_W(x)||^2 \\right]\n\\]\nInterpretation:\n\nFirst term: Original loss function\nSecond term: Regularization penalty proportional to the squared gradient norm\n\n\n\n\n\n\n\nImportantKey Insight\n\n\n\nAdding Gaussian noise to weights is equivalent to penalizing large gradients of the output with respect to the weights."
  },
  {
    "objectID": "ML/noise-robustness.html#geometric-interpretation",
    "href": "ML/noise-robustness.html#geometric-interpretation",
    "title": "Noise Robustness: How Weight Perturbation Leads to Regularization",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\nThe regularization term \\(||\\nabla_W \\hat{y}_W(x)||^2\\) measures how sensitive the output is to weight perturbations.\nWhat this encourages:\n\nFlat minima: Solutions where small weight changes don‚Äôt dramatically affect predictions\nRobust features: The model relies on stable patterns rather than fine-grained weight configurations\nGeneralization: Prevents overfitting to exact weight values\n\n\n\n\nRandom Perturbation Visualization"
  },
  {
    "objectID": "ML/noise-robustness.html#injecting-noise-at-the-output-targets",
    "href": "ML/noise-robustness.html#injecting-noise-at-the-output-targets",
    "title": "Noise Robustness: How Weight Perturbation Leads to Regularization",
    "section": "Injecting Noise at the Output Targets",
    "text": "Injecting Noise at the Output Targets\n\nLabel Smoothing\nInstead of using hard 0/1 targets, label smoothing softens the target distribution:\n\\[\ny'_k =\n\\begin{cases}\n1 - \\varepsilon, & \\text{if } k \\text{ is the correct class} \\\\\n\\varepsilon / (K - 1), & \\text{otherwise}\n\\end{cases}\n\\]\nwhere:\n\n\\(K\\) is the number of classes\n\\(\\varepsilon\\) is the smoothing parameter (typically 0.1)\n\nExample: For 3-class classification with correct class = 1 and \\(\\varepsilon = 0.1\\):\n\nOriginal: \\([0, 1, 0]\\)\nSmoothed: \\([0.05, 0.9, 0.05]\\)\n\n\n\nBenefits\n\nPrevents overconfidence: The model doesn‚Äôt push probabilities to exact 0 or 1\nImproves calibration: Predicted probabilities better reflect true uncertainty\nRegularization effect: Acts as implicit regularization on the output layer\n\n\n\n\n\n\n\nTipInterpretation\n\n\n\nLabel smoothing can be viewed as injecting small noise into the target distribution, making the model less overconfident and more robust.\n\n\n\nSource: Deep Learning Book, Chapter 7.5"
  },
  {
    "objectID": "ML/representation-sparsity.html",
    "href": "ML/representation-sparsity.html",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "",
    "text": "Previous chapters focused on parameter regularization ‚Äî constraining the weights \\(W\\) of a model (see Chapter 7.1.2: L1 Regularization and Chapter 7.1.1: L2 Regularization).\nThis chapter introduces representation regularization ‚Äî constraining the activations \\(h\\) (the learned representations).\nKey distinction:\n\nParameter sparsity: Makes the weight matrix sparse (few connections)\nRepresentation sparsity: Makes the activation vector sparse (few active neurons)"
  },
  {
    "objectID": "ML/representation-sparsity.html#overview",
    "href": "ML/representation-sparsity.html#overview",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "",
    "text": "Previous chapters focused on parameter regularization ‚Äî constraining the weights \\(W\\) of a model (see Chapter 7.1.2: L1 Regularization and Chapter 7.1.1: L2 Regularization).\nThis chapter introduces representation regularization ‚Äî constraining the activations \\(h\\) (the learned representations).\nKey distinction:\n\nParameter sparsity: Makes the weight matrix sparse (few connections)\nRepresentation sparsity: Makes the activation vector sparse (few active neurons)"
  },
  {
    "objectID": "ML/representation-sparsity.html#parameter-regularization",
    "href": "ML/representation-sparsity.html#parameter-regularization",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "Parameter Regularization",
    "text": "Parameter Regularization\nEquation 7.46 - Linear system with parameter matrix:\n\\[\n\\begin{array}{c}\n\\begin{bmatrix}\n18\\\\\n5\\\\\n15\\\\\n-9\\\\\n-3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n4 & 0 & 0 & -2 & 0\\\\\n0 & -1 & 0 & 3 & 0\\\\\n1 & 0 & 0 & 0 & 3\\\\\n0 & 5 & 0 & -1 & 0\\\\\n1 & 0 & 0 & -5 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n2\\\\\n3\\\\\n-2\\\\\n-5\\\\\n1\n\\end{bmatrix}\n\\\\[6pt]\ny \\in \\mathbb{R}^m,\\quad\nA \\in \\mathbb{R}^{m\\times n},\\quad\nx \\in \\mathbb{R}^n\n\\end{array}\n\\]\nL1 regularization on parameters:\n\\[\n\\Omega(A) = \\|A\\|_1 = \\sum_{i,j} |A_{ij}|\n\\]\nEffect: Encourages sparsity in the parameter matrix \\(A\\) itself, making many weights zero."
  },
  {
    "objectID": "ML/representation-sparsity.html#representation-sparsity",
    "href": "ML/representation-sparsity.html#representation-sparsity",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "Representation Sparsity",
    "text": "Representation Sparsity\nEquation 7.47 - Linear system with sparse representation:\n\\[\n\\begin{array}{c}\n\\begin{bmatrix}\n-14\\\\\n1\\\\\n3\\\\\n2\\\\\n23\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n3 & -1 & 2 & -5 & 4 & 1\\\\\n4 & -2 & -3 & -1 & 3 & 0\\\\\n1 & 5 & 2 & -4 & 0 & 0\\\\\n3 & 4 & -3 & 0 & 2 & 0\\\\\n-5 & -4 & 2 & 5 & -1 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\n0\\\\\n0\\\\\n1\\\\\n0\\\\\n0\\\\\n0\n\\end{bmatrix}\n\\\\[6pt]\ny \\in \\mathbb{R}^m, \\quad\nB \\in \\mathbb{R}^{m\\times n}, \\quad\nh \\in \\mathbb{R}^n\n\\end{array}\n\\]\nL1 regularization on representation:\n\\[\n\\Omega(h) = \\|h\\|_1 = \\sum_{i} |h_i|\n\\]\nEquation 7.48 - Loss function with representation regularization:\n\\[\n\\tilde{J}(\\theta; x, y) = J(\\theta; x, y) + \\alpha \\Omega(h)\n\\]\nwhere \\(\\alpha \\in [0, \\infty)\\) controls the contribution of the norm penalty to the total loss.\n\nKey Difference: Parameter vs Representation Regularization\n\n\n\n\n\n\n\n\n\nType\nWhat is Regularized\nEffect\nExample\n\n\n\n\nParameter regularization\nWeight matrix \\(A\\) or \\(W\\)\nMakes weights sparse\nL1/L2 weight decay\n\n\nRepresentation regularization\nActivation vector \\(h\\)\nMakes activations sparse\nSparse autoencoders\n\n\n\nInterpretation:\n\nParameter sparsity: Few connections between layers (network pruning)\nRepresentation sparsity: Few neurons active at once (sparse coding)"
  },
  {
    "objectID": "ML/representation-sparsity.html#orthogonal-matching-pursuit",
    "href": "ML/representation-sparsity.html#orthogonal-matching-pursuit",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "Orthogonal Matching Pursuit",
    "text": "Orthogonal Matching Pursuit\nEquation 7.49 - Sparse approximation problem:\n\\[\n\\arg\\min_h \\|x - Wh\\|^2 \\quad \\text{subject to} \\quad \\|h\\|_0 &lt; k\n\\]\nGoal: Find both \\(W\\) and \\(h\\) such that:\n\n\\(Wh\\) closely approximates \\(x\\) (reconstruction accuracy)\n\\(h\\) is sparse ‚Äî the number of nonzero elements in \\(h\\) (denoted \\(\\|h\\|_0\\)) is less than \\(k\\)\n\nInterpretation: Use \\(Wh\\) to represent \\(x\\) with only a few active components.\n\n\n\n\n\n\nNoteOrthogonal Matching Pursuit\n\n\n\nWhen the columns of \\(W\\) are orthogonal, this problem can be efficiently solved using orthogonal matching pursuit (OMP) algorithm.\nKey insight: Orthogonality of basis vectors (columns of \\(W\\)) enables efficient sparse decomposition."
  },
  {
    "objectID": "ML/representation-sparsity.html#real-world-applications",
    "href": "ML/representation-sparsity.html#real-world-applications",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nNote: The following table is generated by ChatGPT.\n\n\n\n\n\n\n\n\n\n\nCategory\nTechnique / Model\nRepresentation Type\nHow it Uses Sparsity / Orthogonality\nReal-World Example / Effect\n\n\n\n\nüß† Neuroscience & Vision\nSparse coding (Olshausen & Field, 1996)\nSparse\nThe brain represents visual inputs by activating only a few neurons for each stimulus.\nExplains receptive fields in V1 visual cortex ‚Äî neurons respond only to specific edges or orientations.\n\n\nüßÆ Machine Learning\nLASSO Regression\nSparse\nAdds \\(\\lambda \\|w\\|_1\\) penalty to make weights sparse, selecting only key features.\nFeature selection in predictive models (finance, genomics, etc.).\n\n\nüß© Autoencoders\nSparse Autoencoder\nSparse\nAdds regularizer \\(\\Omega(h) = \\|h\\|_1\\) or KL divergence to force sparse activations.\nUsed in image compression and pretraining deep networks (unsupervised learning).\n\n\nüñºÔ∏è Image Processing / Compression\nDictionary Learning / K-SVD\nSparse\nRepresent an image as a combination of few learned basis patches (atoms).\nJPEG-like compression, denoising, super-resolution.\n\n\nüîä Speech Processing\nNon-negative Matrix Factorization (NMF)\nSparse + Nonnegative\nDecomposes sound spectrograms into few additive components.\nSource separation (e.g., separating voice from music).\n\n\nüß† Transform-based Compression\nDCT (Discrete Cosine Transform)\nOrthogonal\nProjects signals onto orthogonal cosine basis vectors.\nJPEG compression ‚Äî energy concentrated in few coefficients.\n\n\nüì∂ Signal Processing\nFourier Transform\nOrthogonal\nRepresents time-domain signals in orthogonal sinusoidal basis.\nAudio, RF, vibration analysis.\n\n\nüñ•Ô∏è Dimensionality Reduction\nPrincipal Component Analysis (PCA)\nOrthogonal\nFinds orthogonal axes (principal components) that maximize variance.\nDimensionality reduction, data visualization.\n\n\nüéôÔ∏è Source Separation\nIndependent Component Analysis (ICA)\nSparse-like / Orthogonal\nSeeks statistically independent (often sparse) components.\nBlind source separation (‚Äúcocktail party problem‚Äù).\n\n\nü§ñ CNN Filters\nOrthogonal Regularization\nOrthogonal\nForces convolution filters to be orthogonal to prevent redundancy.\nImproves training stability and generalization.\n\n\nüíæ Transformers\nEmbedding Orthogonalization\nOrthogonal or Near-orthogonal\nOrthogonalizes embedding vectors for better separation in latent space.\nImproves language model expressiveness and prevents collapse.\n\n\n\n\nSource: Deep Learning Book, Chapter 7.10"
  },
  {
    "objectID": "ML/tangent-prop-manifold.html",
    "href": "ML/tangent-prop-manifold.html",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "",
    "text": "The assumption is that samples lie on a low-dimensional manifold embedded in a high-dimensional space.\nIf we measure their distance using the Euclidean metric, the points might appear far apart, even though they actually reside on the same manifold."
  },
  {
    "objectID": "ML/tangent-prop-manifold.html#the-manifold-assumption",
    "href": "ML/tangent-prop-manifold.html#the-manifold-assumption",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "",
    "text": "The assumption is that samples lie on a low-dimensional manifold embedded in a high-dimensional space.\nIf we measure their distance using the Euclidean metric, the points might appear far apart, even though they actually reside on the same manifold."
  },
  {
    "objectID": "ML/tangent-prop-manifold.html#tangent-distance-difficulties-and-alternative",
    "href": "ML/tangent-prop-manifold.html#tangent-distance-difficulties-and-alternative",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "Tangent Distance: Difficulties and Alternative",
    "text": "Tangent Distance: Difficulties and Alternative\n\nThe Challenge\nComputing tangent distances directly is computationally expensive. For every pair of samples, it requires solving an optimization problem to find the minimal distance between two tangent planes that approximate their local manifolds. This becomes infeasible with large datasets or high-dimensional input spaces.\n\n\nThe Alternative\nAs an alternative, the method approximates the manifold locally using the tangent plane at a single point. Instead of explicitly finding the closest points between two manifolds, we measure distances using these local linear approximations. This greatly reduces computational cost while still capturing local invariance properties."
  },
  {
    "objectID": "ML/tangent-prop-manifold.html#tangent-prop",
    "href": "ML/tangent-prop-manifold.html#tangent-prop",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "Tangent Prop",
    "text": "Tangent Prop\nThe figure illustrates how Tangent Propagation enforces local invariance of the model output along the tangent directions of the data manifold.\nEach data point lies on a smooth low-dimensional surface embedded in high-dimensional space. The tangent plane represents all directions along which the data can move without changing its semantic meaning (e.g., small translation or rotation).\nThe normal vector points in the direction orthogonal to the manifold ‚Äî changes along this direction correspond to changes in class or semantic meaning.\n\n\n\nTangent Vector and Normal Vector\n\n\nThis approach achieves partial consistency by forcing \\(\\nabla_xf(x) \\perp v^{(i)}\\).\n\nRegularization Term\n\\[\n\\Omega(f)=\\sum_i\\left((\\nabla_xf(x)^\\top v^{(i)})^2\\right)\n\\]\nThis regularization term penalizes the sensitivity of the network‚Äôs output \\(f(x)\\) to small movements along each tangent direction \\(v^{(i)}\\).\nMinimizing this term encourages the model‚Äôs gradient \\(\\nabla_x f(x)\\) to be orthogonal to all tangent vectors, ensuring that \\(f(x)\\) remains approximately constant when the input slides along the manifold.\nIn short: Tangent Propagation achieves local smoothness along the manifold (invariance to small deformations), while still allowing sharp variation in directions orthogonal to the manifold, which separate different classes.\n\n\nTangent Direction Vectors\nIn Tangent Propagation, each \\(v^{(i)}\\) represents a tangent direction of the data manifold at the point \\(x\\).\nIt describes a small, meaningful variation of the input that should not change the output of the network‚Äîfor example, a slight translation, rotation, or scaling of an image.\nMathematically, \\(v^{(i)}\\) can be obtained as the derivative of a transformation \\(T(x, \\alpha_i)\\) with respect to its parameter:\n\\[\nv^{(i)} = \\left.\\frac{\\partial T(x, \\alpha_i)}{\\partial \\alpha_i}\\right|_{\\alpha_i=0}\n\\]\nDuring training, the model is penalized if its output changes along these directions, which enforces invariance and smoothness of \\(f(x)\\) along the manifold."
  },
  {
    "objectID": "ML/tangent-prop-manifold.html#from-tangent-propagation-to-manifold-learning",
    "href": "ML/tangent-prop-manifold.html#from-tangent-propagation-to-manifold-learning",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "From Tangent Propagation to Manifold Learning",
    "text": "From Tangent Propagation to Manifold Learning\nTangent Propagation regularizes the model so that its output remains invariant along directions of known transformations, such as translations or rotations.\nIt can be seen as an analytical version of data augmentation‚Äîrather than generating new samples, it directly penalizes the model‚Äôs sensitivity to those transformations.\n\nConnections to Other Methods\nThis idea connects to bidirectional propagation and adversarial training, both of which encourage the model to stay locally smooth in input space.\nAdversarial training extends Tangent Propagation by finding, for each input, the direction that most changes the model‚Äôs prediction and then enforcing robustness along that direction.\nThe Manifold Tangent Classifier (Rifai et al., 2011d) further removes the need to explicitly specify tangent directions.\nIt uses an autoencoder to learn the manifold structure and derive tangent vectors automatically, allowing the network to regularize itself along the data manifold without handcrafted transformations.\n\n\nManual Specification of Tangent Directions\nA key limitation of Tangent Propagation is that these tangent directions \\(v^{(i)}\\) must be manually specified based on prior knowledge about the task.\nFor example, in handwritten digit recognition, we explicitly define directions corresponding to translation or rotation.\nWhile this makes the method interpretable, it also limits its applicability‚Äîmanual definitions are impractical for complex, high-dimensional data.\n\nSource: Deep Learning (Ian Goodfellow, Yoshua Bengio, Aaron Courville) - Chapter 7.14"
  },
  {
    "objectID": "ML/rnn-recursive.html",
    "href": "ML/rnn-recursive.html",
    "title": "Chapter 10.6: Recursive Neural Network",
    "section": "",
    "text": "Recursive neural networks (RvNNs) are an extension of recurrent neural networks in which the computation is carried out over a tree structure rather than a linear chain.\nInstead of applying the same transition function along time, RvNNs apply a shared composition function at every internal node of a tree, combining the representations of its children into a representation of the parent."
  },
  {
    "objectID": "ML/rnn-recursive.html#basic-idea-as-described-in-the-book",
    "href": "ML/rnn-recursive.html#basic-idea-as-described-in-the-book",
    "title": "Chapter 10.6: Recursive Neural Network",
    "section": "1. Basic Idea (as described in the book)",
    "text": "1. Basic Idea (as described in the book)\nAn RvNN performs bottom-up computation on a fixed tree:\n\\[h_{\\text{parent}} = f_{\\theta}(h_{c_1}, h_{c_2}, \\ldots)\\]\nLeaf nodes obtain their representation directly from the input, and internal nodes recursively build higher-level representations. Figure 10.14 in the book illustrates this process with shared parameters (U, W, V) applied at different nodes of the tree."
  },
  {
    "objectID": "ML/rnn-recursive.html#origins-and-applications-based-on-book-citations",
    "href": "ML/rnn-recursive.html#origins-and-applications-based-on-book-citations",
    "title": "Chapter 10.6: Recursive Neural Network",
    "section": "2. Origins and Applications (based on book citations)",
    "text": "2. Origins and Applications (based on book citations)\nThe book notes that:\n\nRecursive networks were introduced by Pollack (1990).\nBottou (2011) discussed their potential for general learning and inference tasks.\nFrasconi et al.¬†(1997, 1998) applied recursive networks to data given as trees.\nSocher et al.¬†(2011a, 2011c) successfully used RvNNs to process syntactic trees in natural language.\nRecursive models have also been applied to computational vision (Socher et al., 2011b).\n\nThese examples highlight that RvNNs are suitable for tasks where the input naturally exhibits a hierarchical tree structure."
  },
  {
    "objectID": "ML/rnn-recursive.html#key-advantage-explicitly-stated-in-the-book",
    "href": "ML/rnn-recursive.html#key-advantage-explicitly-stated-in-the-book",
    "title": "Chapter 10.6: Recursive Neural Network",
    "section": "3. Key Advantage (explicitly stated in the book)",
    "text": "3. Key Advantage (explicitly stated in the book)\nFor sequences of length œÑ, the depth of computation in an RvNN can be drastically reduced:\n\nRNN chain ‚Üí depth = œÑ\nRecursive tree (e.g., balanced binary tree) ‚Üí depth = O(log œÑ)\n\nThus, when a hierarchical structure is available, recursive networks can provide shorter effective paths between distant elements, potentially helping with long-term dependencies."
  },
  {
    "objectID": "ML/rnn-recursive.html#a-major-challenge-as-the-book-highlights",
    "href": "ML/rnn-recursive.html#a-major-challenge-as-the-book-highlights",
    "title": "Chapter 10.6: Recursive Neural Network",
    "section": "4. A Major Challenge (as the book highlights)",
    "text": "4. A Major Challenge (as the book highlights)\nA critical difficulty remains:\n\nIn many applications, the tree structure is not given\n\nThe model must rely on an external source (e.g., a syntactic parser) to provide the structure, or one must design a suitable hierarchy manually.\nThe book notes that an ideal learning system would automatically discover the appropriate tree structure, but this remains an unsolved problem."
  },
  {
    "objectID": "ML/early-stopping.html",
    "href": "ML/early-stopping.html",
    "title": "Chapter 7.8: Early Stopping",
    "section": "",
    "text": "Early stopping is a simple yet effective regularization technique that stops training when validation performance begins to degrade."
  },
  {
    "objectID": "ML/early-stopping.html#overview",
    "href": "ML/early-stopping.html#overview",
    "title": "Chapter 7.8: Early Stopping",
    "section": "",
    "text": "Early stopping is a simple yet effective regularization technique that stops training when validation performance begins to degrade."
  },
  {
    "objectID": "ML/early-stopping.html#the-overfitting-problem",
    "href": "ML/early-stopping.html#the-overfitting-problem",
    "title": "Chapter 7.8: Early Stopping",
    "section": "1. The Overfitting Problem",
    "text": "1. The Overfitting Problem\n\n\n\nOverfitting Visualization\n\n\nObservation: Overfitting almost always occurs during training.\nWhat happens:\n\nTraining error continues to decrease\nValidation error initially decreases, then starts increasing\nThe gap between training and validation error grows\nModel memorizes training data instead of learning generalizable patterns\n\nSolution: Stop training when validation error reaches its minimum."
  },
  {
    "objectID": "ML/early-stopping.html#two-approaches-to-early-stopping",
    "href": "ML/early-stopping.html#two-approaches-to-early-stopping",
    "title": "Chapter 7.8: Early Stopping",
    "section": "2. Two Approaches to Early Stopping",
    "text": "2. Two Approaches to Early Stopping\nSplit data into training \\((x, y)\\) and validation \\((x_{\\text{valid}}, y_{\\text{valid}})\\):\n\nApproach 1: Find Optimal Steps, Then Retrain\n\nTrain on \\((x, y)\\) while monitoring \\(y_{\\text{valid}}\\)\nIdentify the optimal number of steps \\(\\tau^*\\) where validation error is minimized\nRetrain from scratch on full dataset for exactly \\(\\tau^*\\) steps\n\nAdvantage: Uses all data for final training\nDisadvantage: Requires two full training runs\n\n\nApproach 2: Keep Best Model\n\nTrain on \\((x, y)\\) while monitoring \\(y_{\\text{valid}}\\)\nKeep a copy of the model whenever validation performance improves\nStop when validation error stops improving\nUse the saved best model\n\nAdvantage: Only requires one training run\nDisadvantage: Validation data is not used for training\n\n\n\nEarly Stopping Approaches"
  },
  {
    "objectID": "ML/early-stopping.html#costs-of-early-stopping",
    "href": "ML/early-stopping.html#costs-of-early-stopping",
    "title": "Chapter 7.8: Early Stopping",
    "section": "3. Costs of Early Stopping",
    "text": "3. Costs of Early Stopping\nData requirements:\n\nNeed to hold out a portion of data as validation set\nReduces effective training data size\nValidation set typically 10-20% of total data\n\nComputational requirements:\n\nNeed to keep a copy of the best-performing model\nRequires periodic evaluation on validation set\nMay need multiple checkpoints if using approach 1"
  },
  {
    "objectID": "ML/early-stopping.html#benefits-of-early-stopping",
    "href": "ML/early-stopping.html#benefits-of-early-stopping",
    "title": "Chapter 7.8: Early Stopping",
    "section": "4. Benefits of Early Stopping",
    "text": "4. Benefits of Early Stopping\nRegularization:\n\nPrevents overfitting without modifying the loss function\nActs as implicit L2 regularization (proven mathematically below)\nNo hyperparameter tuning needed for regularization strength\n\nEfficiency:\n\nSaves computation by stopping early\nAutomatic hyperparameter selection (number of iterations)\nOften faster than training with explicit regularization to convergence\n\nSimplicity:\n\nEasy to implement\nWidely applicable across different model types\nWorks well in practice without fine-tuning"
  },
  {
    "objectID": "ML/early-stopping.html#mathematical-connection-to-l2-regularization",
    "href": "ML/early-stopping.html#mathematical-connection-to-l2-regularization",
    "title": "Chapter 7.8: Early Stopping",
    "section": "5. Mathematical Connection to L2 Regularization",
    "text": "5. Mathematical Connection to L2 Regularization\n\nGradient Descent Update Rule\nStarting from a quadratic approximation around the optimal weights \\(w^*\\):\nEquation 7.33 - Quadratic approximation of loss:\n\\[\n\\hat{J}(\\theta) = J(w^*) + \\frac{1}{2}(w - w^*)^T H(w - w^*)\n\\]\nwhere \\(H\\) is the Hessian matrix at \\(w^*\\).\nEquation 7.34 - Gradient:\n\\[\n\\nabla_w \\hat{J}(w) = H(w - w^*)\n\\]\nEquation 7.35 - Gradient descent update:\n\\[\nw^{(\\tau)} = w^{(\\tau-1)} - \\epsilon \\nabla_w \\hat{J}(w^{(\\tau-1)})\n\\]\nEquation 7.36 - Substituting the gradient:\n\\[\nw^{(\\tau)} = w^{(\\tau-1)} - \\epsilon H(w^{(\\tau-1)} - w^*)\n\\]\nEquation 7.37 - Rearranging:\n\\[\nw^{(\\tau)} - w^* = (I - \\epsilon H)(w^{(\\tau-1)} - w^*)\n\\]\n\n\nEigendecomposition of Hessian\nEquation 7.38 - Decompose \\(H = Q\\Lambda Q^T\\):\n\\[\nw^{(\\tau)} - w^* = (I - \\epsilon Q\\Lambda Q^T)(w^{(\\tau-1)} - w^*)\n\\]\nEquation 7.39 - Multiply both sides by \\(Q^T\\):\n\\[\nQ^T(w^{(\\tau)} - w^*) = (I - \\epsilon \\Lambda)Q^T(w^{(\\tau-1)} - w^*)\n\\]\n\n\nDeriving the Recursive Formula\nSince \\((I - \\epsilon\\Lambda)\\) is a constant diagonal matrix, we can apply this recursion:\nStep 1 - After 1 iteration:\n\\[\nQ^T(w^{(1)} - w^*) = (I - \\epsilon \\Lambda)Q^T(w^{(0)} - w^*)\n\\]\nStep 2 - After 2 iterations:\n\\[\n\\begin{aligned}\nQ^T(w^{(2)} - w^*) &= (I - \\epsilon \\Lambda)Q^T(w^{(1)} - w^*) \\\\\n&= (I - \\epsilon \\Lambda)Q^T(I - \\epsilon \\Lambda)(w^{(0)} - w^*) \\\\\n&= (I - \\epsilon \\Lambda)^2 Q^T(w^{(0)} - w^*)\n\\end{aligned}\n\\]\nStep 3 - After 3 iterations:\n\\[\n\\begin{aligned}\nQ^T(w^{(3)} - w^*) &= (I - \\epsilon \\Lambda)Q^T(w^{(2)} - w^*) \\\\\n&= (I - \\epsilon \\Lambda)Q^T(I - \\epsilon \\Lambda)^2(w^{(0)} - w^*) \\\\\n&= (I - \\epsilon \\Lambda)^3 Q^T(w^{(0)} - w^*)\n\\end{aligned}\n\\]\nGeneral pattern - After \\(\\tau\\) iterations:\n\\[\nQ^T(w^{(\\tau)} - w^*) = (I - \\epsilon \\Lambda)^\\tau Q^T(w^{(0)} - w^*)\n\\]\n\n\nAssuming Zero Initialization\nEquation 7.40 - If \\(w^{(0)} = 0\\):\n\\[\n\\begin{aligned}\nQ^T(w^{(\\tau)} - w^*) &= (I - \\epsilon \\Lambda)^\\tau Q^T(0 - w^*) \\\\\n&= -(I - \\epsilon \\Lambda)^\\tau Q^T w^*\n\\end{aligned}\n\\]\nTherefore:\n\\[\nQ^T w^{(\\tau)} = Q^T w^* - (I - \\epsilon \\Lambda)^\\tau Q^T w^*\n\\]\n\\[\nQ^T w^{(\\tau)} = [I - (I - \\epsilon \\Lambda)^\\tau] Q^T w^*\n\\]\n\n\nL2 Regularization Solution\nEquation 7.41 - L2 regularized solution:\n\\[\nQ^T \\tilde{w} = (\\Lambda + \\alpha I)^{-1} \\Lambda Q^T w^*\n\\]\nEquation 7.42 - Rewriting:\n\\[\nQ^T \\tilde{w} = [I - (\\Lambda + \\alpha I)^{-1} \\alpha] Q^T w^*\n\\]\n\n\nEquivalence Condition\nEquation 7.43 - Comparing equations 7.40 and 7.42:\nIf we can set:\n\\[\n(I - \\epsilon \\Lambda)^\\tau = (\\Lambda + \\alpha I)^{-1} \\alpha\n\\]\nthen early stopping is equivalent to L2 regularization."
  },
  {
    "objectID": "ML/early-stopping.html#relationship-between-training-steps-and-regularization-strength",
    "href": "ML/early-stopping.html#relationship-between-training-steps-and-regularization-strength",
    "title": "Chapter 7.8: Early Stopping",
    "section": "6. Relationship Between Training Steps and Regularization Strength",
    "text": "6. Relationship Between Training Steps and Regularization Strength\n\nDeriving \\(\\tau\\) from \\(\\alpha\\)\nMathematical tools:\n\nLogarithm properties: \\(\\log(ab) = \\log(a) + \\log(b)\\) and \\(\\log(a^b) = b\\log(a)\\)\nTaylor series approximation: \\(\\log(1 + x) \\approx x\\) and \\(\\log(1 - x) \\approx -x\\)\n\nStarting from equation 7.43:\n\\[\n\\tau \\log(I - \\epsilon \\Lambda) = \\log((\\Lambda + \\alpha I)^{-1} \\alpha)\n\\]\nRight side:\n\\[\n\\log((\\Lambda + \\alpha I)^{-1} \\alpha) = -\\log(\\Lambda + \\alpha I) + \\log(\\alpha)\n\\]\nFactor out \\(\\alpha\\):\n\\[\n\\Lambda + \\alpha I = \\alpha\\left(\\frac{\\Lambda}{\\alpha} + I\\right)\n\\]\nTherefore:\n\\[\n-\\log(\\Lambda + \\alpha I) + \\log(\\alpha) = -\\log(\\alpha) - \\log\\left(\\frac{\\Lambda}{\\alpha} + I\\right) + \\log(\\alpha) = -\\log\\left(\\frac{\\Lambda}{\\alpha} + I\\right)\n\\]\nLeft side using Taylor approximation:\n\\[\n\\log(I - \\epsilon \\Lambda) \\approx -\\epsilon \\Lambda\n\\]\nRight side using Taylor approximation:\n\\[\n\\log\\left(I + \\frac{\\Lambda}{\\alpha}\\right) \\approx \\frac{\\Lambda}{\\alpha}\n\\]\nCombining:\n\\[\n\\tau(-\\epsilon \\Lambda) \\approx -\\frac{\\Lambda}{\\alpha}\n\\]\nEquation 7.44 - Solving for \\(\\tau\\):\n\\[\n\\tau \\approx \\frac{1}{\\epsilon \\alpha}\n\\]\nEquation 7.45 - Solving for \\(\\alpha\\):\n\\[\n\\alpha \\approx \\frac{1}{\\epsilon \\tau}\n\\]\n\n\nKey Insight\nThe number of training steps \\(\\tau\\) is inversely proportional to the L2 regularization strength \\(\\alpha\\).\nInterpretation:\n\nMore training steps (\\(\\tau\\) large) ‚ÜîÔ∏é Weaker regularization (\\(\\alpha\\) small)\nFewer training steps (\\(\\tau\\) small) ‚ÜîÔ∏é Stronger regularization (\\(\\alpha\\) large)\nEarly stopping implicitly applies L2 regularization with strength \\(\\alpha \\approx \\frac{1}{\\epsilon \\tau}\\)\n\nPractical implication: Choosing when to stop training is equivalent to choosing the regularization strength.\n\nKey concepts:\n\nEarly stopping: Stop training when validation error stops improving\nTwo approaches: Find optimal steps and retrain, or keep best model\nCosts: Requires validation data and model checkpointing\nBenefits: Simple, effective, computationally efficient regularization\nMathematical equivalence: Early stopping ‚âà implicit L2 regularization\nInverse relationship: \\(\\tau \\approx \\frac{1}{\\epsilon \\alpha}\\) (more steps = less regularization)\n\n\nSource: Deep Learning Book, Chapter 7.8"
  },
  {
    "objectID": "ML/rnn-unfold-computation-graph.html",
    "href": "ML/rnn-unfold-computation-graph.html",
    "title": "Chapter 10.1: Unfold Computation Graph",
    "section": "",
    "text": "Recurrent Model (RNN) is specified for sequence data processing. While CNN is specified for grid data (images), RNN is designed to process sequences \\((x^{(1)},x^{(2)},...,x^{(t)})\\)."
  },
  {
    "objectID": "ML/rnn-unfold-computation-graph.html#parameter-sharing-through-unfolding",
    "href": "ML/rnn-unfold-computation-graph.html#parameter-sharing-through-unfolding",
    "title": "Chapter 10.1: Unfold Computation Graph",
    "section": "Parameter Sharing Through Unfolding",
    "text": "Parameter Sharing Through Unfolding\nUnfolding the computation graph results in the parameters to be shared.\n\\[\ns^{(t)}=f(s^{(t-1)};\\theta) \\tag{10.1}\n\\]\nwhere \\(s^{(t)}\\) is the system state.\nThe definition of \\(s\\) at time \\(t\\) depends on the moment of \\(t-1\\), so Equation 10.1 is recurrent.\n\\[\ns^{(3)}=f(s^{(2)};\\theta) \\tag{10.2}\n\\]\n\\[\ns^{(3)}=f(f(s^{(1)};\\theta);\\theta) \\tag{10.3}\n\\]\n\n\n\nRNN with output at each timestep"
  },
  {
    "objectID": "ML/rnn-unfold-computation-graph.html#dynamic-systems-with-external-input",
    "href": "ML/rnn-unfold-computation-graph.html#dynamic-systems-with-external-input",
    "title": "Chapter 10.1: Unfold Computation Graph",
    "section": "Dynamic Systems with External Input",
    "text": "Dynamic Systems with External Input\nAnother example is a dynamic system driven by external signal \\(x^{(t)}\\):\n\\[\ns^{(t)}=f(s^{(t-1)}, x^{(t)};\\theta) \\tag{10.4}\n\\]\nA real case: Consider the sentence ‚ÄúI love deep learning‚Äù. When \\(t=3\\):\n\n\\(s^{(t-1)}\\) is the memory of ‚ÄúI love‚Äù\n\\(x^{(t)}\\) is the embedding of ‚Äúdeep‚Äù"
  },
  {
    "objectID": "ML/rnn-unfold-computation-graph.html#hidden-states",
    "href": "ML/rnn-unfold-computation-graph.html#hidden-states",
    "title": "Chapter 10.1: Unfold Computation Graph",
    "section": "Hidden States",
    "text": "Hidden States\nIn deep learning, we usually name \\(s^{(t)}, s^{(t-1)}\\) as hidden units:\n\\[\nh^{(t)}=f(h^{(t-1)}, x^{(t)};\\theta) \\tag{10.5}\n\\]\n\n\n\nRNN unfolded computation graph\n\n\nKey insight: When training an RNN to use past information to predict the future, the network must learn a compressed, lossy summary of the history. It is unnecessary‚Äîand usually impossible‚Äîto store the entire past sequence. Instead, the hidden state learns to retain only the task-relevant information needed for future predictions."
  },
  {
    "objectID": "ML/rnn-unfold-computation-graph.html#the-function-gt",
    "href": "ML/rnn-unfold-computation-graph.html#the-function-gt",
    "title": "Chapter 10.1: Unfold Computation Graph",
    "section": "The Function \\(g^{(t)}\\)",
    "text": "The Function \\(g^{(t)}\\)\n\\(g^{(t)}\\) is the composed transformation of the past \\(t\\) steps, not the loop itself:\n\\[\nh^{(t)}=g^{(t)}(x^{(t)},x^{(t-1)},x^{(t-2)},...,x^{(2)},x^{(1)}) \\tag{10.6}\n\\]\nThis is equivalent to Equation 10.5.\nThe function \\(g^{(t)}\\) takes the entire history of inputs \\((x^{(t)}, x^{(t-1)}, \\ldots, x^{(1)})\\) as its argument.\nThe unrolled recurrent architecture allows us to express \\(g^{(t)}\\) as a repeated composition of the same transition function \\(f\\).\n\nTwo Key Advantages\nThis formulation offers two key advantages:\n\nArbitrary input length: It allows inputs of arbitrary length to be mapped to a fixed-size hidden state\nParameter sharing: It enables parameter sharing, since the same transition function \\(f\\) with the same parameters is reused at every time step\n\nRNN models can also be generalized to unseen input lengths.\n\n\n\nDifferent RNN architectures"
  },
  {
    "objectID": "ML/rnn-unfold-computation-graph.html#key-insight",
    "href": "ML/rnn-unfold-computation-graph.html#key-insight",
    "title": "Chapter 10.1: Unfold Computation Graph",
    "section": "Key Insight",
    "text": "Key Insight\nUnfolding computation graphs in RNNs enables parameter sharing across time steps. The same function \\(f\\) with parameters \\(\\theta\\) is applied repeatedly, allowing the model to process sequences of any length while maintaining a fixed number of parameters. The hidden state \\(h^{(t)}\\) compresses the entire input history into a fixed-size representation, learning to retain only task-relevant information. This architecture generalizes naturally to unseen sequence lengths and enables three fundamental patterns: sequence-to-sequence (many-to-many), sequence-to-vector (many-to-one), and vector-to-sequence (one-to-many) mappings."
  }
]