[
  {
    "objectID": "ML/hessian-prerequisites.html",
    "href": "ML/hessian-prerequisites.html",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "",
    "text": "My notebook\nBefore diving into optimization algorithms for deep learning (Chapter 7), we need to understand second-order derivatives in multiple dimensions. The Hessian matrix is the key tool that generalizes the concept of curvature to high-dimensional spaces."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#context",
    "href": "ML/hessian-prerequisites.html#context",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "",
    "text": "My notebook\nBefore diving into optimization algorithms for deep learning (Chapter 7), we need to understand second-order derivatives in multiple dimensions. The Hessian matrix is the key tool that generalizes the concept of curvature to high-dimensional spaces."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#why-second-derivatives-matter",
    "href": "ML/hessian-prerequisites.html#why-second-derivatives-matter",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Why Second Derivatives Matter",
    "text": "Why Second Derivatives Matter\nIn one dimension, optimizing \\(f(x)\\) involves:\n\nFirst derivative \\(f'(x) = 0\\) → Find critical points\n\nWhy set \\(f'(x) = 0\\)? At a minimum or maximum, the slope is flat (zero)\nThink of a hill: at the very top, you stop going up → slope = 0\nAt the bottom of a valley, you stop going down → slope = 0\nExample: For \\(f(x) = x^2\\), we have \\(f'(x) = 2x\\). Setting \\(f'(x) = 0\\) gives \\(x = 0\\) (the minimum)\n\nSecond derivative \\(f''(x)\\) → Classify the critical point:\n\n\\(f''(x) &gt; 0\\) → Local minimum (curves upward like a bowl)\n\\(f''(x) &lt; 0\\) → Local maximum (curves downward like a dome)\n\\(f''(x) = 0\\) → Inconclusive (could be an inflection point)\nWhy needed? Not all points where \\(f'(x) = 0\\) are minima! For \\(f(x) = x^3\\), we have \\(f'(0) = 0\\) but it’s neither a min nor max.\n\n\nThe challenge: How do we extend this to functions of many variables \\(f(x_1, x_2, \\ldots, x_n)\\)?\nThe answer: The Hessian matrix captures all second-order information.\n\nVisualizing Second Derivatives\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = np.linspace(-3, 3, 200)\n\n# Three functions with different second derivatives\nf1 = x**2           # f''(x) = 2 (positive, curves up)\nf2 = -x**2          # f''(x) = -2 (negative, curves down)\nf3 = x**3           # f''(x) = 6x (changes sign at x=0)\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\n\n# Function 1: f(x) = x²\naxes[0, 0].plot(x, f1, 'b-', linewidth=2)\naxes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 0].set_title(\"f(x) = x²\\nf''(x) = 2 &gt; 0\\n(Curves UP)\", fontsize=10)\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel('f(x)')\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].annotate('Minimum', xy=(0, 0), xytext=(0.5, 2),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Function 2: f(x) = -x²\naxes[0, 1].plot(x, f2, 'r-', linewidth=2)\naxes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 1].set_title(\"f(x) = -x²\\nf''(x) = -2 &lt; 0\\n(Curves DOWN)\", fontsize=10)\naxes[0, 1].set_xlabel('x')\naxes[0, 1].set_ylabel('f(x)')\naxes[0, 1].grid(True, alpha=0.3)\naxes[0, 1].annotate('Maximum', xy=(0, 0), xytext=(0.5, -2),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Function 3: f(x) = x³\naxes[1, 0].plot(x, f3, 'g-', linewidth=2)\naxes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[1, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[1, 0].set_title(\"f(x) = x³\\nf''(x) = 6x\\n(Changes sign)\", fontsize=10)\naxes[1, 0].set_xlabel('x')\naxes[1, 0].set_ylabel('f(x)')\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].annotate('Inflection point', xy=(0, 0), xytext=(1, -10),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Hide the unused subplot\naxes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nKey observations:\n\n\n\n\n\n\n\n\n\nSecond Derivative\nCurvature\nShape\nPoint Type\n\n\n\n\n\\(f''(x) &gt; 0\\)\nCurves upward\nBowl shape\nPotential minimum\n\n\n\\(f''(x) &lt; 0\\)\nCurves downward\nDome shape\nPotential maximum\n\n\n\\(f''(x) = 0\\) (at critical point)\nChanges sign\nFlat at that point\nInflection point\n\n\n\nNote on the third example: For \\(f(x) = x^3\\), we have \\(f''(x) = 6x\\). At the critical point \\(x = 0\\), \\(f''(0) = 0\\), which is inconclusive. The curvature changes sign: negative for \\(x &lt; 0\\) and positive for \\(x &gt; 0\\)."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#the-hessian-matrix",
    "href": "ML/hessian-prerequisites.html#the-hessian-matrix",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "The Hessian Matrix",
    "text": "The Hessian Matrix\n\nDefinition\nFor a scalar function \\(f(\\mathbf{x}) = f(x_1, x_2, \\ldots, x_n)\\), the Hessian matrix is the square matrix of all second-order partial derivatives:\n\\[\nH(f) =\n\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n\\]\n\n\nKey Properties\n\nSymmetric: If mixed partial derivatives are continuous, then \\(\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}\\), so \\(H = H^T\\).\nShape: Always \\(n \\times n\\) (determined by number of variables, not terms in the function)\nDescribes curvature in all directions simultaneously\nEigenvalue decomposition: Since the Hessian is symmetric, it can be decomposed as \\(H = Q\\Lambda Q^T\\) where \\(Q\\) contains orthonormal eigenvectors and \\(\\Lambda\\) is a diagonal matrix of eigenvalues\n\n\n\nSimple Example\nFor \\(f(x, y) = x^2 + 3y^2\\):\nStep 1: Compute first derivatives \\[\n\\frac{\\partial f}{\\partial x} = 2x, \\quad \\frac{\\partial f}{\\partial y} = 6y\n\\]\nStep 2: Compute second derivatives \\[\n\\frac{\\partial^2 f}{\\partial x^2} = 2, \\quad \\frac{\\partial^2 f}{\\partial y^2} = 6, \\quad \\frac{\\partial^2 f}{\\partial x \\partial y} = 0\n\\]\nStep 3: Build Hessian \\[\nH = \\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}\n\\]\nInterpretation: - Curvature along \\(x\\)-axis: 2 - Curvature along \\(y\\)-axis: 6 - No cross-dependency (off-diagonal = 0)\n\n\nExample with Cross Terms\nFor \\(f(x, y) = x^2 + xy + y^2\\):\n\\[\nH = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\n\\]\nThe off-diagonal term (1) indicates that \\(x\\) and \\(y\\) are coupled—changing one affects the rate of change of the other."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#matrix-definiteness",
    "href": "ML/hessian-prerequisites.html#matrix-definiteness",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Matrix Definiteness",
    "text": "Matrix Definiteness\nFor a symmetric matrix \\(A\\), its definiteness is determined by the signs of its eigenvalues.\n\n\n\n\n\n\n\n\n\nType\nEigenvalues\nQuadratic Form \\(x^T A x\\)\nGeometric Shape\n\n\n\n\nPositive definite (PD)\nall \\(&gt; 0\\)\n\\(&gt; 0\\) for all \\(x \\neq 0\\)\nBowl (curves upward)\n\n\nNegative definite (ND)\nall \\(&lt; 0\\)\n\\(&lt; 0\\) for all \\(x \\neq 0\\)\nDome (curves downward)\n\n\nIndefinite\nsome \\(+\\), some \\(-\\)\ndepends on direction\nSaddle\n\n\nPositive semi-definite (PSD)\nall \\(\\geq 0\\)\n\\(\\geq 0\\) for all \\(x\\)\nFlat-bottom bowl\n\n\nNegative semi-definite (NSD)\nall \\(\\leq 0\\)\n\\(\\leq 0\\) for all \\(x\\)\nFlat-top dome\n\n\n\n\nQuick Test (2×2 case)\nFor \\(A = \\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}\\):\n\nPositive definite if \\(a &gt; 0\\) and \\(ac - b^2 &gt; 0\\)\nNegative definite if \\(a &lt; 0\\) and \\(ac - b^2 &gt; 0\\)\nIndefinite if \\(ac - b^2 &lt; 0\\)\n\n\n\nExamples\n\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}\\): eigenvalues = [2, 6] → Positive definite\n\\(\\begin{bmatrix} -2 & 0 \\\\ 0 & -3 \\end{bmatrix}\\): eigenvalues = [-2, -3] → Negative definite\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\\): eigenvalues = [2, -2] → Indefinite\n\\(\\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix}\\): eigenvalues = [4, 0] → Positive semi-definite"
  },
  {
    "objectID": "ML/hessian-prerequisites.html#interpreting-hessian-at-critical-points",
    "href": "ML/hessian-prerequisites.html#interpreting-hessian-at-critical-points",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Interpreting Hessian at Critical Points",
    "text": "Interpreting Hessian at Critical Points\nAt a critical point where \\(\\nabla f = 0\\), the Hessian determines the nature of the point:\n\n\n\n\n\n\n\n\n\nHessian Type\nEigenvalues\nSurface Shape\nPoint Type\n\n\n\n\nPositive definite\nall positive\nBowl (convex)\nLocal minimum\n\n\nNegative definite\nall negative\nDome (concave)\nLocal maximum\n\n\nIndefinite\nmixed signs\nSaddle\nNeither min nor max\n\n\nSemi-definite\nsome zero\nFlat in some directions\nInconclusive\n\n\n\n\nVisualization: Different Surface Types\n\n\nShow code\n# Create grid for plotting\nx_grid = np.linspace(-2, 2, 100)\ny_grid = np.linspace(-2, 2, 100)\nX, Y = np.meshgrid(x_grid, y_grid)\n\n# Define different functions with different Hessian types\ndef positive_definite(x, y):\n    \"\"\"Minimum: f = x² + y²\"\"\"\n    return x**2 + y**2\n\ndef negative_definite(x, y):\n    \"\"\"Maximum: f = -x² - y²\"\"\"\n    return -x**2 - y**2\n\ndef indefinite(x, y):\n    \"\"\"Saddle: f = x² - y²\"\"\"\n    return x**2 - y**2\n\ndef semi_definite(x, y):\n    \"\"\"Flat direction: f = x²\"\"\"\n    return x**2\n\n# Create 3D surface plots in 2x2 grid\nfig = plt.figure(figsize=(12, 10))\n\nfunctions = [\n    (positive_definite, \"Positive Definite\\n(Bowl - Minimum)\", \"Greens\"),\n    (negative_definite, \"Negative Definite\\n(Dome - Maximum)\", \"Reds\"),\n    (indefinite, \"Indefinite\\n(Saddle Point)\", \"RdBu\"),\n    (semi_definite, \"Semi-Definite\\n(Flat Direction)\", \"YlOrRd\")\n]\n\nfor idx, (func, title, cmap) in enumerate(functions, 1):\n    ax = fig.add_subplot(2, 2, idx, projection='3d')\n    Z = func(X, Y)\n\n    surf = ax.plot_surface(X, Y, Z, cmap=cmap, alpha=0.8,\n                           linewidth=0, antialiased=True)\n\n    ax.set_xlabel('x', fontsize=9)\n    ax.set_ylabel('y', fontsize=9)\n    ax.set_zlabel('f(x,y)', fontsize=9)\n    ax.set_title(title, fontsize=10)\n    ax.view_init(elev=25, azim=45)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nContour plots for better understanding:\n\n\nShow code\n# Contour plots in 2x2 grid\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\naxes = axes.flatten()\n\nfor idx, (func, title, cmap) in enumerate(functions):\n    ax = axes[idx]\n    Z = func(X, Y)\n\n    contour = ax.contour(X, Y, Z, levels=15, cmap=cmap)\n    ax.clabel(contour, inline=True, fontsize=7)\n\n    # Mark the critical point at origin\n    ax.plot(0, 0, 'r*', markersize=12, label='Critical point')\n\n    ax.set_xlabel('x', fontsize=9)\n    ax.set_ylabel('y', fontsize=9)\n    ax.set_title(title, fontsize=10)\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=8)\n    ax.set_aspect('equal')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ML/kmeans.html",
    "href": "ML/kmeans.html",
    "title": "K-means Clustering",
    "section": "",
    "text": "Setup points and K\nwe will implement a KNN algorithm to cluster the points\n\n\nX=[[1,1],[2,2.1],[3,2.5],[6,7],[7,7.1],[9,7.5]]\nk=2\n\nmax_iter=3\n\n\n# Visualize the data\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter([x[0] for x in X],[x[1] for x in X])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pure python implementation of K-means clustering\ndef knn_iter(X,centroids):\n    # set up new clusters\n    new_clusters=[[] for _ in range(len(centroids))]\n    # k=len(centroids)\n    # assign each point to the nearest centroid\n    for x in X:\n        k,distance=0,(x[0]-centroids[0][0])**2+(x[1]-centroids[0][1])**2\n        for i,c in enumerate(centroids[1:],1):\n            if (x[0]-c[0])**2+(x[1]-c[1])**2&lt;distance:\n                k=i\n                distance=(x[0]-c[0])**2+(x[1]-c[1])**2\n        new_clusters[k].append(x)\n    \n    # calculate new centroids\n    new_centroids=[[\n        sum([x[0] for x in cluster])/len(cluster),\n        sum([x[1] for x in cluster])/len(cluster)\n    ] if cluster else centroids[i] for i,cluster in enumerate(new_clusters)]\n    return new_centroids\n\n\n\n\n\n\n\n\ndef iter_and_draw(X,k,max_iter):\n    centroids=X[:k]  # Randomly select 2 centroids\n    fig, axes = plt.subplots(max_iter//3+(1 if max_iter%3!=0 else 0),\n        3, figsize=(15, 10))\n    axes=axes.flatten()\n    for i in range(max_iter):\n        \n        # Plot points and centroids\n\n\n        # Assign each point to nearest centroid and plot with corresponding color\n        colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n        for j, x in enumerate(X):\n            # Find nearest centroid\n            min_dist = float('inf')\n            nearest_centroid = 0\n            for k, c in enumerate(centroids):\n                dist = (x[0]-c[0])**2 + (x[1]-c[1])**2\n                if dist &lt; min_dist:\n                    min_dist = dist\n                    nearest_centroid = k\n            # Plot point with color corresponding to its cluster\n            axes[i].scatter(x[0], x[1], c=colors[nearest_centroid % len(colors)], label=f'Cluster {nearest_centroid+1}' if j==0 else \"\")\n        axes[i].scatter([c[0] for c in centroids], [c[1] for c in centroids], c='red', marker='*', s=200, label='Centroids')\n        axes[i].set_title(f'Iteration {i}')\n        centroids = knn_iter(X, centroids)\n\n    plt.tight_layout()\n    plt.show()\n\niter_and_draw(X,k,max_iter)\n# print(centroids)\n\n\n\n\n\n\n\n\n\n# A 3 clusters example\n\nimport numpy as np\n\nX1=np.random.rand(20,2)+5 # Some points in the upper right corner\nX2=np.random.rand(20,2)+3 # Some points in the middle\nX3=np.random.rand(20,2) # Some points in the lower left corner\n\niter_and_draw(np.concatenate((X1,X2,X3)),3,5)\n\n\n\n\n\n\n\n\n\n\nA question?\n\nWhat to do if one cluster has no assigned points during iteration?\n\n\n\nFormula Derivation\nThe goal is to minimize the loss of inertia which is sum of the points to cluster centroids.\n\\[\nLoss= \\sum_{i=1}^n \\sum_{x \\in C_i} ||x-\\mu_i||^2\n\\]\nTo iter \\(\\mu\\) for each cluster, let us find the derivative of the following function. \\[\nf(\\mu)=\\sum_{i=1}^n ||x_i-\\mu||^2 =\n\\sum_{i=1}^n {x_i}^2+\\mu^2-2x_i\\mu\n\\]\nGiven a \\(\\nabla \\mu\\), \\[\nf(\\mu + \\nabla \\mu)=\\sum_{i=1}^n ||x_i+\\nabla \\mu -\\mu||^2 =\n\\sum_{i=1}^n  {x_i}^2+\\mu^2+{\\nabla \\mu}^2-2{x_i \\mu}-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\nf(\\mu + \\nabla \\mu)-f(\\mu)=\n\\sum_{i=1}^n {\\nabla \\mu}^2-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\n\\frac {f(\\mu + \\nabla \\mu)-f(\\mu)}{\\nabla \\mu}=\\sum_{i=1}^n {\\nabla \\mu} -2 \\mu +2{x_i} = 2\\sum_{i=1}^n x_i - 2n\\mu\n\\]\nNow we can see if \\(n\\mu = \\sum_{i=1}^n x_i\\), then the derivative is 0, this is why in each iteration, we need to set the center of the cluster as centroid."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#projection-of-a-vector-onto-a-line",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#projection-of-a-vector-onto-a-line",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "1. Projection of a Vector onto a Line",
    "text": "1. Projection of a Vector onto a Line\n\n\n\nProjection onto a Line\n\n\nSetup: Project vector \\(b\\) onto vector \\(a\\).\nKey equations:\n\\[\n\\begin{aligned}\np &= xa \\\\\na^T(b - xa) &= 0 \\\\\na^T b &= xa^T a \\\\\nx &= \\frac{a^T b}{a^T a}\n\\end{aligned}\n\\]\nComponents: - \\(p\\): Projection of \\(b\\) onto \\(a\\) - \\(e\\): Error vector (\\(e = b - p\\)) - \\(e \\perp a\\) (error is perpendicular to \\(a\\)) - \\(p + e = b\\) - \\(p = b - e\\)\nProjection formula:\n\\[\np = a\\frac{a^T b}{a^T a}\n\\]\nProperties: - If \\(b\\) is doubled, then \\(p\\) is doubled - If \\(a\\) is doubled, then \\(p\\) does not change at all"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#projection-matrix",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#projection-matrix",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "2. Projection Matrix",
    "text": "2. Projection Matrix\nMatrix form:\n\\[\np = Pb\n\\]\nwhere\n\\[\nP = \\frac{aa^T}{a^T a}\n\\]\nAnalysis: - \\(aa^T\\) is \\((n \\times 1)(1 \\times n) = n \\times n\\) - \\(a^T a\\) is a scalar - \\(P\\) is \\(n \\times n\\) - Column space of \\(P\\) is the line through \\(a\\) - Rank is 1 - \\(P\\) is symmetric: \\(P^T = P\\) - \\(P^2 = P\\) (if I project twice, the result is the same)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#why-projection",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#why-projection",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "3. Why Projection?",
    "text": "3. Why Projection?\nProblem: The equation \\(Ax = b\\) may have 0 solutions when \\(m &gt; n\\).\nSolution: Solve \\(A\\hat{x} = p\\) instead, where \\(p\\) is the projection of \\(b\\) onto the column space.\n\n\n\nProjection in Higher Dimensions\n\n\nSetup: - \\(e = b - p\\) is perpendicular to the plane (spanned by \\(a_1\\) and \\(a_2\\)) - \\(A = \\begin{bmatrix}| & | \\\\ a_1 & a_2 \\\\ | & |\\end{bmatrix}\\) - \\(p = \\hat{x}_1 a_1 + \\hat{x}_2 a_2 = A\\hat{x}\\)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#finding-hatx",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#finding-hatx",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "4. Finding \\(\\hat{x}\\)",
    "text": "4. Finding \\(\\hat{x}\\)\nKey idea: \\(b - A\\hat{x} \\perp C(A)\\) (error is perpendicular to column space)\nDerivation: Error \\(e\\) is perpendicular to \\(a_1\\) and \\(a_2\\):\n\\[\n\\begin{aligned}\na_1^T(b - A\\hat{x}) &= 0 \\text{ and } a_2^T(b - A\\hat{x}) = 0 \\\\\n\\begin{bmatrix}a_1^T \\\\ a_2^T\\end{bmatrix}(b - A\\hat{x}) &= \\begin{bmatrix}0 \\\\ 0\\end{bmatrix} \\\\\nA^T(b - A\\hat{x}) &= \\mathbf{0}\n\\end{aligned}\n\\]\nKey relationships: - \\(e = b - A\\hat{x}\\) is in the left null space - \\(e \\in N(A^T)\\) - \\(e \\perp C(A)\\) (error is perpendicular to column space of \\(A\\))\nNormal equations:\n\\[\nA^T b = A^T A\\hat{x}\n\\]\nSolution:\n\\[\n\\hat{x} = (A^T A)^{-1} A^T b\n\\]\nProjection:\n\\[\np = A\\hat{x} = A(A^T A)^{-1} A^T b\n\\]\nProjection matrix:\n\\[\nP = A(A^T A)^{-1} A^T\n\\]\nSpecial case: If \\(A\\) is invertible, then \\(P = I\\) (identity matrix).\nProperties in high dimensions: - \\(P^T = P\\) (symmetric) - \\(P^2 = P\\) (idempotent)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#least-squares-application",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#least-squares-application",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "5. Least Squares Application",
    "text": "5. Least Squares Application\n\n\n\nLeast Squares\n\n\nProblem: Fit a line \\(y = c + dt\\) to data points \\((1,1)\\), \\((2,2)\\), and \\((3,3)\\).\nSetup:\n\\[\nA = \\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3\n\\end{bmatrix}, \\quad\nb = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}, \\quad\nx = \\begin{bmatrix}\nc \\\\\nd\n\\end{bmatrix}\n\\]\nEquation system: \\(Ax = b\\) - Row 1: \\(c + 1 \\cdot d = 1\\) - Row 2: \\(c + 2 \\cdot d = 2\\) - Row 3: \\(c + 3 \\cdot d = 3\\)\nIssue: Cannot find exact solution for \\(x\\) because: - \\(A\\) is not invertible (not square: \\(3 \\times 2\\)) - System is overdetermined (3 equations, 2 unknowns)\nSolution: Use projection to find \\(\\hat{x}\\) that minimizes \\(\\|Ax - b\\|^2\\)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture15-projections.html#geometric-interpretation",
    "href": "Math/MIT18.06/mit1806-lecture15-projections.html#geometric-interpretation",
    "title": "MIT 18.06 Lecture 15: Projection onto Subspaces",
    "section": "6. Geometric Interpretation",
    "text": "6. Geometric Interpretation\n3 Lines in 3D: If we have 3 lines in 3-dimensional space, then \\(A\\) will be square.\nIf \\(A\\) is invertible (full rank): - \\(Ax = b\\) has an exact solution - The solution geometrically means: find the plane spanned by column 1 and column 2, then find where column 3 crosses it\n\nSource: MIT 18.06SC Linear Algebra, Lecture 15"
  }
]