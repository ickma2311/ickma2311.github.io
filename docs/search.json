[
  {
    "objectID": "Math/mit1806-lecture1-geometry.html",
    "href": "Math/mit1806-lecture1-geometry.html",
    "title": "MIT 18.06 Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "This is for MIT 18.06 Lecture 1, covering how to understand linear systems from two perspectives: geometry (row picture) and algebra (column picture)."
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#the-central-question",
    "href": "Math/mit1806-lecture1-geometry.html#the-central-question",
    "title": "MIT 18.06 Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "Let’s start with a concrete example that will guide our entire exploration:\n\\[\\begin{align}\nx + 2y &= 5 \\\\\n3x + 4y &= 6\n\\end{align}\\]\nThis simple system holds the key to understanding linear algebra’s geometric foundation. But here’s the twist: we can interpret this system in two completely different ways, each revealing unique insights about the nature of linear equations."
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#perspective-1-the-row-picture-lines-and-intersections",
    "href": "Math/mit1806-lecture1-geometry.html#perspective-1-the-row-picture-lines-and-intersections",
    "title": "MIT 18.06 Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "In the row picture, each equation represents a geometric object: - In 2D: each equation is a line - In 3D: each equation is a plane\n- In higher dimensions: each equation is a hyperplane\nThe solution is simply where all these objects intersect!\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the equations in the form y = mx + c\n# Line 1: x + 2y = 5  =&gt;  y = -1/2*x + 5/2\n# Line 2: 3x + 4y = 6  =&gt;  y = -3/4*x + 3/2\nx = np.linspace(-10, 10, 100)\ny1 = -1/2 * x + 5/2\ny2 = -3/4 * x + 3/2\n\n# Solve for intersection point\nA = np.array([[1, 2], [3, 4]])\nb = np.array([5, 6])\nsolution = np.linalg.solve(A, b)\n\n# Plot both lines and intersection\nplt.figure(figsize=(12, 8))\nplt.plot(x, y1, 'b-', label='Line 1: x + 2y = 5', linewidth=3)\nplt.plot(x, y2, 'r-', label='Line 2: 3x + 4y = 6', linewidth=3)\nplt.scatter(solution[0], solution[1], color='green', s=200, zorder=5, \n           label=f'Solution: ({solution[0]:.1f}, {solution[1]:.1f})', edgecolor='white', linewidth=3)\n\nplt.xlim(-8, 8)\nplt.ylim(-1, 8)\nplt.xlabel('x', fontsize=14, fontweight='bold')\nplt.ylabel('y', fontsize=14, fontweight='bold')\nplt.title('Row Picture: Where Lines Meet\\nEach equation = one line, solution = intersection', \n          fontsize=16, fontweight='bold')\nplt.legend(fontsize=12, framealpha=0.9)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"🎯 Solution found: x = {solution[0]:.3f}, y = {solution[1]:.3f}\")\nprint(f\"✅ Verification: {A @ solution} equals {b} ✓\")\nprint(f\"\\n💡 Row Picture Insight: The solution is the unique point where both lines intersect!\")\n\n\n\n\n\n\n\n\n\n🎯 Solution found: x = -4.000, y = 4.500\n✅ Verification: [5. 6.] equals [5 6] ✓\n\n💡 Row Picture Insight: The solution is the unique point where both lines intersect!"
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#perspective-2-the-column-picture-vector-combinations",
    "href": "Math/mit1806-lecture1-geometry.html#perspective-2-the-column-picture-vector-combinations",
    "title": "MIT 18.06 Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "Now comes the beautiful twist! The column picture reframes the exact same system as a question about vector combinations:\n\\[x \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} + y \\begin{bmatrix}2 \\\\ 4\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nInstead of asking “where do lines intersect?”, we’re now asking: “Can we combine these two vectors to reach our target?”\n\n\nCode\n# Define column vectors and target vector\na1 = np.array([1, 3])\na2 = np.array([2, 4])\nb = np.array([5, 6])\n\n# Solve for coefficients\nA = np.column_stack([a1, a2])\nsolution = np.linalg.solve(A, b)\nx, y = solution[0], solution[1]\n\nprint(f\"🎯 The Question: Can we write b as a linear combination of a₁ and a₂?\")\nprint(f\"📐 Answer: {x:.3f} × a₁ + {y:.3f} × a₂ = b\")\nprint(f\"\\nStep-by-step construction:\")\nprint(f\"  🔵 x × a₁ = {x:.3f} × {a1} = {x*a1}\")\nprint(f\"  🟢 y × a₂ = {y:.3f} × {a2} = {y*a2}\")\nprint(f\"  🔴 Final result: {x*a1} + {y*a2} = {x*a1 + y*a2}\")\n\n# Visualize the step-by-step construction\nplt.figure(figsize=(14, 10))\n\n# Step 1: Draw x*a1 (scaled version)\nplt.arrow(0, 0, x*a1[0], x*a1[1], head_width=0.25, head_length=0.25, \n         fc='darkblue', ec='darkblue', linewidth=5,\n         label=f'Step 1: {x:.2f} × a₁ = [{x*a1[0]:.1f}, {x*a1[1]:.1f}]')\n\n# Mark the endpoint of x*a1 clearly\nplt.scatter(x*a1[0], x*a1[1], color='darkblue', s=200, zorder=5, \n           edgecolor='white', linewidth=3)\n\n# Step 2: Draw y*a2 starting from the tip of x*a1\nplt.arrow(x*a1[0], x*a1[1], y*a2[0], y*a2[1], head_width=0.25, head_length=0.25, \n         fc='darkgreen', ec='darkgreen', linewidth=5,\n         label=f'Step 2: + {y:.2f} × a₂ = + [{y*a2[0]:.1f}, {y*a2[1]:.1f}]')\n\n# Show final result vector b\nplt.arrow(0, 0, b[0], b[1], head_width=0.3, head_length=0.3, \n         fc='red', ec='red', linewidth=6, alpha=0.8,\n         label=f'Target: b = [{b[0]}, {b[1]}]')\n\n# Add clear annotations\nplt.text(x*a1[0]/2 - 0.8, x*a1[1]/2 - 0.5, f'{x:.1f}×a₁', fontsize=16, fontweight='bold', \n         color='darkblue', bbox=dict(boxstyle=\"round,pad=0.4\", facecolor='lightblue', alpha=0.9))\n\nplt.text(x*a1[0] + y*a2[0]/2 + 0.3, x*a1[1] + y*a2[1]/2 + 0.3, f'{y:.1f}×a₂', \n         fontsize=16, fontweight='bold', color='darkgreen', \n         bbox=dict(boxstyle=\"round,pad=0.4\", facecolor='lightgreen', alpha=0.9))\n\nplt.text(b[0]/2 + 0.5, b[1]/2 + 0.5, 'b', fontsize=18, fontweight='bold', \n         color='red', bbox=dict(boxstyle=\"round,pad=0.4\", facecolor='yellow', alpha=0.9))\n\n# Add construction lines\nplt.plot([0, x*a1[0]], [0, x*a1[1]], 'b--', alpha=0.6, linewidth=2)\nplt.plot([x*a1[0], b[0]], [x*a1[1], b[1]], 'g--', alpha=0.6, linewidth=2)\n\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\n\n# Dynamic limits\nall_x = [0, x*a1[0], x*a1[0] + y*a2[0], b[0]]\nall_y = [0, x*a1[1], x*a1[1] + y*a2[1], b[1]]\nmargin = 1.5\nplt.xlim(min(all_x) - margin, max(all_x) + margin)\nplt.ylim(min(all_y) - margin, max(all_y) + margin)\n\nplt.xlabel('x-component', fontsize=14, fontweight='bold')\nplt.ylabel('y-component', fontsize=14, fontweight='bold')\nplt.title('Column Picture: Building Vectors Step by Step\\nCombining a₁ and a₂ to reach target b', \n          fontsize=16, fontweight='bold')\nplt.legend(fontsize=12, framealpha=0.9, loc='upper left')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n💡 Column Picture Insight: Every solution is a recipe for combining column vectors!\")\nprint(f\"🔍 This perspective reveals the fundamental concept of 'linear combinations'\")\n\n\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n🎯 The Question: Can we write b as a linear combination of a₁ and a₂?\n📐 Answer: -4.000 × a₁ + 4.500 × a₂ = b\n\nStep-by-step construction:\n  🔵 x × a₁ = -4.000 × [1 3] = [ -4. -12.]\n  🟢 y × a₂ = 4.500 × [2 4] = [ 9. 18.]\n  🔴 Final result: [ -4. -12.] + [ 9. 18.] = [5. 6.]\n\n\n\n\n\n\n\n\n\n\n💡 Column Picture Insight: Every solution is a recipe for combining column vectors!\n🔍 This perspective reveals the fundamental concept of 'linear combinations'"
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#the-three-faces-of-linear-systems",
    "href": "Math/mit1806-lecture1-geometry.html#the-three-faces-of-linear-systems",
    "title": "MIT 18.06 Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "Not all linear systems behave the same way. Understanding the three possible outcomes is crucial for both theoretical insight and practical applications:\n\nUnique solution - The “goldilocks” case\nNo solution - When our target is unreachable\n\nInfinitely many solutions - When we have too much freedom\n\nLet’s explore each case through the lens of both pictures!\n\n\nCode\n# Case (a): Unique solution - non-parallel vectors\nprint(\"🎯 Case (a) - Unique Solution:\")\nA_a = np.array([[1, 2], [3, 4]])\nb_a = np.array([5, 6])\nsolution_a = np.linalg.solve(A_a, b_a)\ndet_a = np.linalg.det(A_a)\nprint(f\"   Solution: {solution_a}\")\nprint(f\"   Matrix determinant: {det_a:.3f} ≠ 0 → linearly independent columns\")\nprint(f\"   Column space: ENTIRE 2D plane (any point reachable)\")\n\n# Case (b): No solution - parallel vectors, b not in span\nprint(f\"\\n❌ Case (b) - No Solution:\")\nA_b = np.array([[1, 2], [2, 4]])  # Columns are parallel\nb_b = np.array([5, 6])            # b not in span\ndet_b = np.linalg.det(A_b)\nprint(f\"   Matrix determinant: {det_b:.3f} = 0 → linearly dependent columns\")\nprint(f\"   Column space: 1D line only (most points unreachable)\")\nprint(f\"   Target b = {b_b} is NOT on the line → No solution exists\")\n\n# Case (c): Infinitely many solutions - parallel vectors, b in span\nprint(f\"\\n♾️  Case (c) - Infinitely Many Solutions:\")\nA_c = np.array([[1, 2], [2, 4]])  # Same parallel columns\nb_c = np.array([3, 6])            # b = 3 * [1, 2], so b is in span\ndet_c = np.linalg.det(A_c)\nprint(f\"   Matrix determinant: {det_c:.3f} = 0 → linearly dependent columns\")\nprint(f\"   Column space: 1D line only\")\nprint(f\"   Target b = {b_c} IS on the line → Infinite solutions exist\")\n\n# Find one particular solution using pseudoinverse\nsolution_c = np.linalg.pinv(A_c) @ b_c\nprint(f\"   One particular solution: {solution_c}\")\nprint(f\"   Other solutions: {solution_c} + t×[2, -1] for any real number t\")\n\n\n🎯 Case (a) - Unique Solution:\n   Solution: [-4.   4.5]\n   Matrix determinant: -2.000 ≠ 0 → linearly independent columns\n   Column space: ENTIRE 2D plane (any point reachable)\n\n❌ Case (b) - No Solution:\n   Matrix determinant: 0.000 = 0 → linearly dependent columns\n   Column space: 1D line only (most points unreachable)\n   Target b = [5 6] is NOT on the line → No solution exists\n\n♾️  Case (c) - Infinitely Many Solutions:\n   Matrix determinant: 0.000 = 0 → linearly dependent columns\n   Column space: 1D line only\n   Target b = [3 6] IS on the line → Infinite solutions exist\n   One particular solution: [0.6 1.2]\n   Other solutions: [0.6 1.2] + t×[2, -1] for any real number t\n\n\n\n\nCode\n# Visualize all three cases\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Case (a): Unique solution\nax = axes[0]\nax.fill_between([-1, 6], [-1, -1], [7, 7], color='lightblue', alpha=0.2, \n                label='Column space = ENTIRE plane')\n\nax.text(2, 0.5, 'Column space =\\nENTIRE 2D plane\\n(any point reachable)', \n        fontsize=11, fontweight='bold', ha='center',\n        bbox=dict(boxstyle=\"round,pad=0.4\", facecolor='lightblue', alpha=0.8))\n\n# Draw vectors\nax.arrow(0, 0, A_a[0,0], A_a[1,0], head_width=0.15, head_length=0.15,\n         fc='blue', ec='blue', linewidth=3, label='a₁ = [1,3]')\nax.arrow(0, 0, A_a[0,1], A_a[1,1], head_width=0.15, head_length=0.15,\n         fc='green', ec='green', linewidth=3, label='a₂ = [2,4]')\nax.arrow(0, 0, b_a[0], b_a[1], head_width=0.2, head_length=0.2,\n         fc='red', ec='red', linewidth=4, label='b = [5,6]')\n\nax.set_title('✅ Unique Solution\\nLinearly Independent Vectors', fontsize=12, fontweight='bold')\nax.legend(fontsize=10, loc='upper left')\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (b): No solution\nax = axes[1]\n# Show the span line (1D subspace)\nt = np.linspace(-2, 5, 100)\nspan_x, span_y = t * A_b[0,0], t * A_b[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=8, alpha=0.6, \n        label='Column space (1D line)')\n\n# Draw basis vectors\nax.arrow(0, 0, A_b[0,0], A_b[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=3, label='a₁ = [1,2]')\nax.arrow(0, 0, A_b[0,1], A_b[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=3, label='a₂ = [2,4] = 2×a₁')\nax.arrow(0, 0, b_b[0], b_b[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=4, label='b = [5,6] (off line)')\n\nax.text(1, 5, 'b is NOT on\\nthe blue line!\\n→ No solution', \n        fontsize=11, fontweight='bold', ha='center',\n        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='pink', alpha=0.8))\n\nax.set_title('❌ No Solution\\nTarget Not in Column Space', fontsize=12, fontweight='bold')\nax.legend(fontsize=9)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (c): Infinitely many solutions\nax = axes[2]\n# Show the span line\nt = np.linspace(-1, 4, 100)\nspan_x, span_y = t * A_c[0,0], t * A_c[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=8, alpha=0.6,\n        label='Column space (1D line)')\n\n# Draw basis vectors\nax.arrow(0, 0, A_c[0,0], A_c[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=3, label='a₁ = [1,2]')\nax.arrow(0, 0, A_c[0,1], A_c[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=3, label='a₂ = [2,4] = 2×a₁')\nax.arrow(0, 0, b_c[0], b_c[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=4, label='b = [3,6] (on line)')\n\n# Show multiple solutions\nax.arrow(0, 0, 3*A_c[0,0], 3*A_c[1,0], head_width=0.1, head_length=0.1,\n         fc='purple', ec='purple', alpha=0.8, linewidth=2, label='Solution 1: 3×a₁')\nax.arrow(0, 0, 1*A_c[0,0], 1*A_c[1,0], head_width=0.1, head_length=0.1,\n         fc='orange', ec='orange', alpha=0.8, linewidth=2, label='Solution 2: 1×a₁ + 1×a₂')\nax.arrow(1*A_c[0,0], 1*A_c[1,0], 1*A_c[0,1], 1*A_c[1,1], head_width=0.1, head_length=0.1,\n         fc='orange', ec='orange', alpha=0.8, linewidth=2)\n\nax.text(1, 4.5, 'b IS on the line!\\n→ Infinite solutions', \n        fontsize=11, fontweight='bold', ha='center',\n        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightgreen', alpha=0.8))\n\nax.set_title('♾️ Infinite Solutions\\nTarget in Column Space', fontsize=12, fontweight='bold')\nax.legend(fontsize=9)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"🔍 THE KEY INSIGHT: It's All About the Column Space!\")\nprint(\"=\"*70)\nprint(\"🎯 LINEARLY INDEPENDENT columns (det ≠ 0):\")\nprint(\"   → Column space = ENTIRE plane (every b has exactly one solution)\")\nprint(\"\\n❌ LINEARLY DEPENDENT columns (det = 0):\")\nprint(\"   → Column space = limited (line, plane, etc.)\")\nprint(\"   → Solution exists only if b lies in this limited space\")\nprint(\"   → If solution exists, there are infinitely many\")\nprint(\"=\"*70)\n\n\n/var/folders/j9/z2ncdyk14nn09tz526t2p_040000gn/T/ipykernel_48492/3973498423.py:87: UserWarning:\n\nGlyph 9989 (\\N{WHITE HEAVY CHECK MARK}) missing from font(s) DejaVu Sans.\n\n/var/folders/j9/z2ncdyk14nn09tz526t2p_040000gn/T/ipykernel_48492/3973498423.py:87: UserWarning:\n\nGlyph 10060 (\\N{CROSS MARK}) missing from font(s) DejaVu Sans.\n\n/Users/chaoma/miniconda3/envs/research/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning:\n\nGlyph 9989 (\\N{WHITE HEAVY CHECK MARK}) missing from font(s) DejaVu Sans.\n\n/Users/chaoma/miniconda3/envs/research/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning:\n\nGlyph 10060 (\\N{CROSS MARK}) missing from font(s) DejaVu Sans.\n\n\n\n\n\n\n\n\n\n\n\n======================================================================\n🔍 THE KEY INSIGHT: It's All About the Column Space!\n======================================================================\n🎯 LINEARLY INDEPENDENT columns (det ≠ 0):\n   → Column space = ENTIRE plane (every b has exactly one solution)\n\n❌ LINEARLY DEPENDENT columns (det = 0):\n   → Column space = limited (line, plane, etc.)\n   → Solution exists only if b lies in this limited space\n   → If solution exists, there are infinitely many\n======================================================================\n\n\n\nThis covers the core geometric foundations from MIT 18.06 Lecture 1: understanding linear systems through both row and column perspectives."
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#why-these-perspectives-matter",
    "href": "Math/mit1806-lecture1-geometry.html#why-these-perspectives-matter",
    "title": "MIT 18.06 Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "Understanding both the row and column pictures isn’t just academic - it’s foundational for:\n\n\n\nNeural networks: Each layer performs matrix multiplication (column picture)\nPrincipal Component Analysis: Finding the best column space to represent data\nRegression: Fitting hyperplanes (row picture) to data points\n\n\n\n\n\nFeature engineering: Understanding which combinations of features (columns) can represent your target\nDimensionality reduction: Projecting data onto lower-dimensional column spaces\nOptimization: Many algorithms rely on understanding when systems have unique solutions\n\n\n\n\n\nComputer graphics: Transformations as matrix operations\nSignal processing: Decomposing signals into basis functions\nControl systems: Understanding system behavior through linear equations"
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#key-takeaways",
    "href": "Math/mit1806-lecture1-geometry.html#key-takeaways",
    "title": "MIT 18.06 Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "Same equation, different stories: The row picture shows intersections, the column picture shows combinations\nThe column space is king: Whether a system has 0, 1, or ∞ solutions depends entirely on whether your target vector lies in the column space\nLinear independence determines everything: When columns are linearly independent, you can reach any point. When they’re dependent, you’re stuck in a lower-dimensional subspace\nMatrix determinant tells the story: det ≠ 0 means full column space; det = 0 means restricted column space\n\n\nThis covers the core geometric foundations from MIT 18.06 Lecture 1: understanding linear systems through both row and column perspectives."
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#whats-next",
    "href": "Math/mit1806-lecture1-geometry.html#whats-next",
    "title": "MIT 18.06 Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "This geometric foundation sets the stage for everything that follows in linear algebra: - Matrix operations: How they transform geometric spaces - Eigenvalues and eigenvectors: Special directions that matrices prefer - Orthogonality: When vectors are perfectly perpendicular - Least squares: What to do when exact solutions don’t exist\nThe beauty of linear algebra lies in how these simple geometric insights scale to solve problems in hundreds of dimensions - the same intuition applies whether you’re working with 2D plots or 1000-dimensional machine learning models!\n\nThis post is part of my journey through MIT’s 18.06 Linear Algebra course. The geometric perspective transforms abstract equations into intuitive visual understanding - exactly what makes mathematics beautiful and accessible."
  },
  {
    "objectID": "ML/xor-deep-learning.html",
    "href": "ML/xor-deep-learning.html",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.1 shows how ReLU activations let neural networks solve the XOR problem that defeats any linear model.\n📓 For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "ML/xor-deep-learning.html#the-xor-problem-a-challenge-for-linear-models",
    "href": "ML/xor-deep-learning.html#the-xor-problem-a-challenge-for-linear-models",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "The XOR Problem: A Challenge for Linear Models",
    "text": "The XOR Problem: A Challenge for Linear Models\nXOR (Exclusive OR) returns 1 precisely when the two binary inputs differ:\n\\[\\text{XOR}(x_1, x_2) = \\begin{pmatrix}0 & 1\\\\1 & 0\\end{pmatrix}\\]\nThe XOR truth table shows why this is challenging for linear models - the positive class (1) appears at diagonally opposite corners, making it impossible to separate with any single straight line.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Define XOR dataset\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([0, 1, 1, 0])\n\nprint(\"XOR Truth Table:\")\nprint(\"================\")\nprint()\nprint(\"┌─────────┬────────┐\")\nprint(\"│ Input   │ Output │\")\nprint(\"│ (x₁,x₂) │  XOR   │\")\nprint(\"├─────────┼────────┤\")\nfor i in range(4):\n    input_str = f\"({X[i,0]}, {X[i,1]})\"\n    output_str = f\"{y[i]}\"\n    print(f\"│ {input_str:7} │   {output_str:2}   │\")\nprint(\"└─────────┴────────┘\")\nprint()\nprint(\"Notice: XOR = 1 when inputs differ, XOR = 0 when inputs match\")\n\n\nXOR Truth Table:\n================\n\n┌─────────┬────────┐\n│ Input   │ Output │\n│ (x₁,x₂) │  XOR   │\n├─────────┼────────┤\n│ (0, 0)  │   0    │\n│ (0, 1)  │   1    │\n│ (1, 0)  │   1    │\n│ (1, 1)  │   0    │\n└─────────┴────────┘\n\nNotice: XOR = 1 when inputs differ, XOR = 0 when inputs match"
  },
  {
    "objectID": "ML/xor-deep-learning.html#limitation-1-single-layer-linear-model",
    "href": "ML/xor-deep-learning.html#limitation-1-single-layer-linear-model",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Limitation 1: Single Layer Linear Model",
    "text": "Limitation 1: Single Layer Linear Model\nA single layer perceptron can only create linear decision boundaries. Let’s see what happens when we try to solve XOR with logistic regression:\n\n\nShow code\n# Demonstrate single layer linear model failure\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\ncolors = ['red', 'blue']\n\n# Plot XOR data\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'Class {i}', edgecolors='black', linewidth=2)\n\n# Overlay representative linear separators to illustrate the impossibility\nx_line = np.linspace(-0.2, 1.2, 100)\nax.plot(x_line, 0.5 * np.ones_like(x_line), '--', color='gray', alpha=0.7, label='candidate lines')\nax.plot(0.5 * np.ones_like(x_line), x_line, '--', color='orange', alpha=0.7)\nax.plot(x_line, x_line, '--', color='green', alpha=0.7)\nax.plot(x_line, 1 - x_line, '--', color='purple', alpha=0.7)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x₁', fontsize=12)\nax.set_ylabel('x₂', fontsize=12)\nax.set_title('XOR Problem: No Linear Solution', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Fit logistic regression just to report its performance\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\naccuracy = log_reg.score(X, y)\nprint(f'Single layer model accuracy: {accuracy:.1%} - still misclassifies XOR.')\n\n\n\n\n\n\n\n\n\nSingle layer model accuracy: 50.0% - still misclassifies XOR."
  },
  {
    "objectID": "ML/xor-deep-learning.html#limitation-2-multiple-layer-linear-model-without-activation",
    "href": "ML/xor-deep-learning.html#limitation-2-multiple-layer-linear-model-without-activation",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Limitation 2: Multiple Layer Linear Model (Without Activation)",
    "text": "Limitation 2: Multiple Layer Linear Model (Without Activation)\nEven stacking multiple linear layers doesn’t help! Multiple linear transformations are mathematically equivalent to a single linear transformation.\nMathematical proof:\n\\[\\text{Layer 1: } h_1 = W_1 x + b_1\\] \\[\\text{Layer 2: } h_2 = W_2 h_1 + b_2 = W_2(W_1 x + b_1) + b_2 = (W_2W_1)x + (W_2b_1 + b_2)\\]\nResult: Still just \\(Wx + b\\) (a single linear transformation)\nConclusion: Stacking linear layers without activation functions doesn’t increase the model’s expressive power!"
  },
  {
    "objectID": "ML/xor-deep-learning.html#the-solution-relu-activation-function",
    "href": "ML/xor-deep-learning.html#the-solution-relu-activation-function",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "The Solution: ReLU Activation Function",
    "text": "The Solution: ReLU Activation Function\nReLU (Rectified Linear Unit) provides the nonlinearity needed to solve XOR: - ReLU(z) = max(0, z) - Clips negative values to zero, keeping positive values unchanged\nUsing the hand-crafted network from the next code cell, the forward pass can be written compactly in matrix form:\n\\[\nX = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix},\n\\quad\nW_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix},\n\\quad\nb_1 = \\begin{bmatrix} 0 & 0 \\end{bmatrix}\n\\]\n\\[\nZ = X W_1^{\\top} + b_1 = \\begin{bmatrix} 0 & 0 \\\\ -1 & 1 \\\\ 1 & -1 \\\\ 0 & 0 \\end{bmatrix},\n\\qquad\nH = \\text{ReLU}(Z) = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \\end{bmatrix}\n\\]\nWith output parameters \\[\nw_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix},\n\\quad\nb_2 = -0.5\n\\] the final linear scores are \\[\na = H w_2^{\\top} + b_2 = \\begin{bmatrix} -0.5 \\\\ 0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}\n\\Rightarrow\n\\text{sign}_+(a) = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\n\\]\nHere \\(\\text{sign}_+(a)\\) maps non-negative entries to 1 and negative entries to 0. Let’s see how ReLU transforms the XOR problem to make it solvable.\n\n\nShow code\n# Hand-crafted network weights and biases that solve XOR\nfrom IPython.display import display, Math\n\ndef relu(z):\n    return np.maximum(0, z)\n\nW1 = np.array([[1, -1],\n               [-1, 1]])\nb1 = np.array([0, 0])\nw2 = np.array([1, 1])\nb2 = -0.5\n\ndisplay(Math(r\"\\text{Hidden layer: } W_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}\"))\ndisplay(Math(r\"b_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\"))\ndisplay(Math(r\"\\text{Output layer: } w_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix}\"))\ndisplay(Math(r\"b_2 = -0.5\"))\n\ndef forward_pass(X, W1, b1, w2, b2):\n    z1 = X @ W1.T + b1\n    h1 = relu(z1)\n    logits = h1 @ w2 + b2\n    return logits, h1, z1\n\nlogits, hidden_activations, pre_activations = forward_pass(X, W1, b1, w2, b2)\npredictions = (logits &gt;= 0).astype(int)\n\nprint(\"Step-by-step Forward Pass Results:\")\nprint(\"=\" * 80)\nprint()\nprint(\"┌─────────┬──────────────────┬──────────────────┬─────────┬──────────┐\")\nprint(\"│ Input   │  Before ReLU     │  After ReLU      │  Logit  │   Pred   │\")\nprint(\"│ (x₁,x₂) │    (z₁, z₂)      │    (h₁, h₂)      │  score  │  class   │\")\nprint(\"├─────────┼──────────────────┼──────────────────┼─────────┼──────────┤\")\nfor i in range(len(X)):\n    x1, x2 = X[i]\n    z1_vals = pre_activations[i]\n    h1_vals = hidden_activations[i]\n    logit = logits[i]\n    pred = predictions[i]\n    \n    input_str = f\"({x1:.0f}, {x2:.0f})\"\n    pre_relu_str = f\"({z1_vals[0]:4.1f}, {z1_vals[1]:4.1f})\"\n    post_relu_str = f\"({h1_vals[0]:4.1f}, {h1_vals[1]:4.1f})\"\n    logit_str = f\"{logit:6.2f}\"\n    pred_str = f\"{pred:4d}\"\n    \n    print(f\"│ {input_str:7} │ {pre_relu_str:16} │ {post_relu_str:16} │ {logit_str:7} │ {pred_str:8} │\")\nprint(\"└─────────┴──────────────────┴──────────────────┴─────────┴──────────┘\")\n\naccuracy = (predictions == y).mean()\nprint(f\"\\nNetwork Accuracy: {accuracy:.0%} ✅\")\nprint(\"\\nKey transformations:\")\nprint(\"• (-1, 1) → (0, 1) makes XOR(0,1) = 1 separable\")\nprint(\"• ( 1,-1) → (1, 0) makes XOR(1,0) = 1 separable\")\n\n\n\\(\\displaystyle \\text{Hidden layer: } W_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle b_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle \\text{Output layer: } w_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle b_2 = -0.5\\)\n\n\nStep-by-step Forward Pass Results:\n================================================================================\n\n┌─────────┬──────────────────┬──────────────────┬─────────┬──────────┐\n│ Input   │  Before ReLU     │  After ReLU      │  Logit  │   Pred   │\n│ (x₁,x₂) │    (z₁, z₂)      │    (h₁, h₂)      │  score  │  class   │\n├─────────┼──────────────────┼──────────────────┼─────────┼──────────┤\n│ (0, 0)  │ ( 0.0,  0.0)     │ ( 0.0,  0.0)     │  -0.50  │    0     │\n│ (0, 1)  │ (-1.0,  1.0)     │ ( 0.0,  1.0)     │   0.50  │    1     │\n│ (1, 0)  │ ( 1.0, -1.0)     │ ( 1.0,  0.0)     │   0.50  │    1     │\n│ (1, 1)  │ ( 0.0,  0.0)     │ ( 0.0,  0.0)     │  -0.50  │    0     │\n└─────────┴──────────────────┴──────────────────┴─────────┴──────────┘\n\nNetwork Accuracy: 100% ✅\n\nKey transformations:\n• (-1, 1) → (0, 1) makes XOR(0,1) = 1 separable\n• ( 1,-1) → (1, 0) makes XOR(1,0) = 1 separable\n\n\n\nTransformation Table: How ReLU Solves XOR\nLet’s trace through exactly what happens to each input:\n\n\nShow code\n\n# Create detailed transformation table\nprint(\"Complete Transformation Table:\")\nprint(\"=============================\")\nprint()\nprint(\"Input   | Pre-ReLU  | Post-ReLU | Logit | Prediction | Target | Correct?\")\nprint(\"(x₁,x₂) | (z₁, z₂)  | (h₁, h₂)  | score | class      | y      |\")\nprint(\"--------|-----------|-----------|-------|------------|--------|----------\")\n\nfor i in range(4):\n    input_str = f\"({X[i,0]},{X[i,1]})\"\n    pre_relu_str = f\"({pre_activations[i,0]:2.0f},{pre_activations[i,1]:2.0f})\"\n    post_relu_str = f\"({hidden_activations[i,0]:.0f},{hidden_activations[i,1]:.0f})\"\n    logit_str = f\"{logits[i]:.2f}\"\n    pred_str = f\"{predictions[i]}\"\n    target_str = f\"{y[i]}\"\n    correct_str = \"✓\" if predictions[i] == y[i] else \"✗\"\n\n    print(f\"{input_str:7} | {pre_relu_str:9} | {post_relu_str:9} | {logit_str:5} | {pred_str:10} | {target_str:6} | {correct_str}\")\n\nprint()\nprint(\"Key Insight: ReLU transforms (-1,1) → (0,1) and (1,-1) → (1,0)\")\nprint(\"This makes the XOR classes linearly separable in the hidden space!\")\n\n\nComplete Transformation Table:\n=============================\n\nInput   | Pre-ReLU  | Post-ReLU | Logit | Prediction | Target | Correct?\n(x₁,x₂) | (z₁, z₂)  | (h₁, h₂)  | score | class      | y      |\n--------|-----------|-----------|-------|------------|--------|----------\n(0,0)   | ( 0, 0)   | (0,0)     | -0.50 | 0          | 0      | ✓\n(0,1)   | (-1, 1)   | (0,1)     | 0.50  | 1          | 1      | ✓\n(1,0)   | ( 1,-1)   | (1,0)     | 0.50  | 1          | 1      | ✓\n(1,1)   | ( 0, 0)   | (0,0)     | -0.50 | 0          | 0      | ✓\n\nKey Insight: ReLU transforms (-1,1) → (0,1) and (1,-1) → (1,0)\nThis makes the XOR classes linearly separable in the hidden space!\n\n\n\n\nStep 1: Original Input Space\nThe XOR problem in its raw form - notice how no single line can separate the classes:\n\n\nShow code\n# Step 1 visualization: Original Input Space\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'XOR = {i}', edgecolors='black', linewidth=2)\n\n# Annotate each point\nfor i in range(4):\n    ax.annotate(f'({X[i,0]},{X[i,1]})', X[i], xytext=(10, 10), \n                textcoords='offset points', fontsize=10)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x₁', fontsize=12)\nax.set_ylabel('x₂', fontsize=12)\nax.set_title('Step 1: Original Input Space', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Linear Transformation (Before ReLU)\nThe network applies weights W₁ and biases b₁ to transform the input space:\n\n\nShow code\n# Step 2 visualization: Pre-activation space\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\nfor i in range(4):\n    ax.scatter(pre_activations[i, 0], pre_activations[i, 1], \n               c=colors[y[i]], s=200, edgecolors='black', linewidth=2)\n\n# Draw ReLU boundaries\nax.axhline(0, color='gray', linestyle='-', alpha=0.8, linewidth=2)\nax.axvline(0, color='gray', linestyle='-', alpha=0.8, linewidth=2)\n\n# Shade all regions where coordinates turn negative (and thus get clipped by ReLU)\nax.axvspan(-1.2, 0, alpha=0.15, color='red')\nax.axhspan(-1.2, 0, alpha=0.15, color='red')\nax.text(-0.75, 0.85, 'Negative z₁ → ReLU sets to 0', ha='left', fontsize=10,\n        bbox=dict(boxstyle='round', facecolor='red', alpha=0.25))\nax.text(0.95, -0.75, 'Negative z₂ → ReLU sets to 0', ha='right', fontsize=10,\n        bbox=dict(boxstyle='round', facecolor='red', alpha=0.25))\n\n# Annotate points with input labels\nlabels = ['(0,0)', '(0,1)', '(1,0)', '(1,1)']\nfor i, label in enumerate(labels):\n    pre_coord = f'({pre_activations[i,0]:.0f},{pre_activations[i,1]:.0f})'\n    ax.annotate(f'{label}→{pre_coord}', pre_activations[i], xytext=(10, 10), \n                textcoords='offset points', fontsize=9)\n\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1.2, 1.2)\nax.set_xlabel('z₁ (Pre-activation)', fontsize=12)\nax.set_ylabel('z₂ (Pre-activation)', fontsize=12)\nax.set_title('Step 2: Before ReLU (Linear Transform)', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 3: ReLU Transformation\nReLU clips negative values to zero, transforming the space to make it linearly separable:\n\n\nShow code\n# Step 3 visualization: ReLU transformation with arrows\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\n\nfor i in range(4):\n    # Pre-ReLU positions (X marks)\n    ax.scatter(pre_activations[i, 0], pre_activations[i, 1], \n               marker='x', s=150, c=colors[y[i]], alpha=0.5, linewidth=3)\n    # Post-ReLU positions (circles) \n    ax.scatter(hidden_activations[i, 0], hidden_activations[i, 1], \n               marker='o', s=200, c=colors[y[i]], edgecolors='black', linewidth=2)\n    \n    # Draw transformation arrows\n    start = pre_activations[i]\n    end = hidden_activations[i]\n    if not np.array_equal(start, end):\n        ax.annotate('', xy=end, xytext=start,\n                    arrowprops=dict(arrowstyle='-&gt;', lw=2, color=colors[y[i]], alpha=0.8))\n\n\n# Add text box explaining the key transformation\nax.text(0.5, 0.8, 'ReLU clips negative coordinates to zero\\n(-1,1) → (0,1) and (1,-1) → (1,0)', \n        ha='center', va='center', fontsize=11, \n        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n\nax.set_xlim(-1.2, 1.1)\nax.set_ylim(-1.2, 1.1)\nax.set_xlabel('Hidden dimension 1', fontsize=12)\nax.set_ylabel('Hidden dimension 2', fontsize=12)\nax.set_title('Step 3: ReLU Mapping (Before → After)', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Final Classification\nWith the transformed hidden representation, the network can now perfectly classify XOR:\n\n\nShow code\n\n\n# Step 4 visualization: Final classification results\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\n# Create decision boundary\nxx, yy = np.meshgrid(np.linspace(-0.2, 1.2, 100), np.linspace(-0.2, 1.2, 100))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\ngrid_logits, _, _ = forward_pass(grid_points, W1, b1, w2, b2)\ngrid_preds = (grid_logits &gt;= 0).astype(int).reshape(xx.shape)\n\nax.contourf(xx, yy, grid_preds, levels=[-0.5, 0.5, 1.5], \n            colors=['#ffcccc', '#ccccff'], alpha=0.6)\nax.contour(xx, yy, grid_logits.reshape(xx.shape), levels=[0], \n           colors='black', linewidths=2, linestyles='--')\n\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'XOR = {i}', edgecolors='black', linewidth=2)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x₁', fontsize=12)\nax.set_ylabel('x₂', fontsize=12)\nax.set_title('Step 4: Final Classification (100% Accuracy)', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nsample_logits, _, _ = forward_pass(X, W1, b1, w2, b2)\nsample_preds = (sample_logits &gt;= 0).astype(int)\nfor i in range(4):\n    pred_text = f'Pred: {sample_preds[i]}'\n    ax.annotate(pred_text, X[i], xytext=(10, -15), \n                textcoords='offset points', fontsize=9,\n                bbox=dict(boxstyle='round,pad=0.2', facecolor='lightgreen', alpha=0.7))\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ML/xor-deep-learning.html#conclusion",
    "href": "ML/xor-deep-learning.html#conclusion",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Conclusion",
    "text": "Conclusion\nThe XOR problem demonstrates several fundamental principles in deep learning:\n\nNecessity of Nonlinearity: Linear models cannot solve XOR, establishing the critical role of nonlinear activation functions.\nUniversal Approximation: Even simple architectures with sufficient nonlinearity can solve complex classification problems."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ickma.dev",
    "section": "",
    "text": "My learning notes and thoughts on math and machine learning.\nCurrently reading the Deep Learning book.\n\n\n\n\nHow ReLU solves problems that linear models cannot handle.\n\n\n\nThe mathematical connection between probabilistic models and loss functions.\n\n\n\n\n\n\nTwo powerful perspectives that reveal the hidden beauty of linear systems: row picture vs column picture.\n\n\n\n\n\n\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\n\n\nDP Regex"
  },
  {
    "objectID": "index.html#deep-learning-book",
    "href": "index.html#deep-learning-book",
    "title": "ickma.dev",
    "section": "",
    "text": "How ReLU solves problems that linear models cannot handle.\n\n\n\nThe mathematical connection between probabilistic models and loss functions."
  },
  {
    "objectID": "index.html#mathematics",
    "href": "index.html#mathematics",
    "title": "ickma.dev",
    "section": "",
    "text": "Two powerful perspectives that reveal the hidden beauty of linear systems: row picture vs column picture."
  },
  {
    "objectID": "index.html#more",
    "href": "index.html#more",
    "title": "ickma.dev",
    "section": "",
    "text": "K-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\n\n\nDP Regex"
  },
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "",
    "section": "Project Overview",
    "text": "Project Overview\nThis is a Quarto-based technical blog hosted on GitHub Pages (ickma2311.github.io). The site covers machine learning, algorithms, and technical tutorials with a focus on mathematical foundations and practical implementations."
  },
  {
    "objectID": "CLAUDE.html#common-commands",
    "href": "CLAUDE.html#common-commands",
    "title": "",
    "section": "Common Commands",
    "text": "Common Commands\n\nDevelopment Workflow\n\nquarto render - Build the entire website (outputs to docs/ directory)\nquarto preview - Start local development server with live reload\nquarto render &lt;file.qmd&gt; - Render a specific document\nquarto check - Verify Quarto installation and project setup\n\n\n\nContent Management\n\nCreate new ML content in ML/ directory\nCreate new algorithm content in Algorithm/ directory\nUpdate navigation by editing _quarto.yml navbar section\nAdd new content to respective index.qmd files for discoverability"
  },
  {
    "objectID": "CLAUDE.html#project-structure",
    "href": "CLAUDE.html#project-structure",
    "title": "",
    "section": "Project Structure",
    "text": "Project Structure\n├── _quarto.yml          # Main configuration file\n├── docs/                # Generated output (GitHub Pages source)\n├── index.qmd            # Homepage\n├── about.qmd            # About page\n├── ML/                  # Machine Learning content\n│   ├── index.qmd        # ML topics overview\n│   ├── *.qmd            # ML articles\n│   └── *.ipynb          # Jupyter notebooks\n├── Algorithm/           # Algorithm content\n│   ├── index.qmd        # Algorithm topics overview\n│   └── *.qmd            # Algorithm articles\n├── imgs/                # Image assets\n├── media/               # Media files\n└── styles.css           # Custom CSS styles"
  },
  {
    "objectID": "CLAUDE.html#content-organization",
    "href": "CLAUDE.html#content-organization",
    "title": "",
    "section": "Content Organization",
    "text": "Content Organization\nThe site uses a hierarchical navigation structure defined in _quarto.yml: - Two main sections: “ML” and “Algorithm” - Each section has an index page that serves as a directory - Content is categorized by topic (e.g., “NumPy Fundamentals”, “Clustering Algorithms”)\n\nAdding New Content\n\nCreate the content file in the appropriate directory (ML/ or Algorithm/)\nUpdate the corresponding index.qmd file to include the new content\nAdd navigation entry to _quarto.yml if it should appear in the navbar dropdown\nUse consistent frontmatter with title field"
  },
  {
    "objectID": "CLAUDE.html#configuration-notes",
    "href": "CLAUDE.html#configuration-notes",
    "title": "",
    "section": "Configuration Notes",
    "text": "Configuration Notes\n\nOutput directory is set to docs/ for GitHub Pages compatibility\nTheme: Cosmo with custom branding\nAll pages include table of contents (toc: true)\nSite uses custom CSS from styles.css\nJupyter notebooks are supported alongside Quarto markdown"
  },
  {
    "objectID": "CLAUDE.html#github-pages-deployment",
    "href": "CLAUDE.html#github-pages-deployment",
    "title": "",
    "section": "GitHub Pages Deployment",
    "text": "GitHub Pages Deployment\nThe site is automatically deployed from the docs/ directory. After rendering, commit and push the docs/ folder to trigger GitHub Pages rebuild. - Author is Chao Ma"
  },
  {
    "objectID": "ML/logistic_regression.html",
    "href": "ML/logistic_regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "The core idea of logistic regression is to model the probability of a binary outcome.\n\n\nWe model the probability that the target variable (y) is 1, given the features (x), using the sigmoid (or logistic) function, denoted by ().\n\\[\nP(y_i=1 \\mid x_i) = \\hat{y}_i = \\sigma(w^T x_i + b)\n\\]\nThe sigmoid function is defined as: \\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\nSince the outcome is binary, the probability of (y) being 0 is simply: \\[\nP(y_i=0 \\mid x_i) = 1 - \\hat{y}_i\n\\]\nThese two cases can be written compactly as a single equation, which is the probability mass function of a Bernoulli distribution: \\[\nP(y_i \\mid x_i) = \\hat{y}_i^{y_i} (1 - \\hat{y}_i)^{1 - y_i}\n\\]"
  },
  {
    "objectID": "ML/logistic_regression.html#model-formulation",
    "href": "ML/logistic_regression.html#model-formulation",
    "title": "Logistic Regression",
    "section": "",
    "text": "The core idea of logistic regression is to model the probability of a binary outcome.\n\n\nWe model the probability that the target variable (y) is 1, given the features (x), using the sigmoid (or logistic) function, denoted by ().\n\\[\nP(y_i=1 \\mid x_i) = \\hat{y}_i = \\sigma(w^T x_i + b)\n\\]\nThe sigmoid function is defined as: \\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\nSince the outcome is binary, the probability of (y) being 0 is simply: \\[\nP(y_i=0 \\mid x_i) = 1 - \\hat{y}_i\n\\]\nThese two cases can be written compactly as a single equation, which is the probability mass function of a Bernoulli distribution: \\[\nP(y_i \\mid x_i) = \\hat{y}_i^{y_i} (1 - \\hat{y}_i)^{1 - y_i}\n\\]"
  },
  {
    "objectID": "ML/logistic_regression.html#loss-function-binary-cross-entropy",
    "href": "ML/logistic_regression.html#loss-function-binary-cross-entropy",
    "title": "Logistic Regression",
    "section": "Loss Function (Binary Cross-Entropy)",
    "text": "Loss Function (Binary Cross-Entropy)\nTo find the optimal parameters (w) and (b), we use Maximum Likelihood Estimation (MLE). We want to find the parameters that maximize the probability of observing our given dataset.\n\n1. Likelihood\nThe likelihood is the joint probability of observing all (n) data points, assuming they are independent and identically distributed (i.i.d.): \\[\n\\mathcal{L}(w, b) = \\prod_{i=1}^n P(y_i \\mid x_i) = \\prod_{i=1}^n \\hat{y}_i^{y_i} (1 - \\hat{y}_i)^{1 - y_i}\n\\]\n\n\n2. Log-Likelihood\nWorking with products is difficult, so we take the logarithm of the likelihood. Maximizing the log-likelihood is equivalent to maximizing the likelihood.\n\\[\n\\log \\mathcal{L}(w, b) = \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n\\]\n\n\n3. Cost Function\nIn machine learning, we frame problems as minimizing a cost function. The standard convention is to minimize the negative log-likelihood. This gives us the Binary Cross-Entropy loss, (J(w, b)).\n\\[\nJ(w, b) = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n\\] The \\(\\frac{1}{n}\\) term is an average over the training examples and doesn’t change the minimum, but it helps in stabilizing the training process."
  },
  {
    "objectID": "ML/logistic_regression.html#refernce-bernoulli-distribution",
    "href": "ML/logistic_regression.html#refernce-bernoulli-distribution",
    "title": "Logistic Regression",
    "section": "refernce: Bernoulli Distribution",
    "text": "refernce: Bernoulli Distribution\nFormular \\[\nP(y) = p^y (1 - p)^{1 - y}, \\quad y \\in \\{0, 1\\}\n\\] * when y = 1：\\(P(y=1) = p^1 (1 - p)^0 = p\\) * when y=0: $P(y=0) = p^0 (1 - p)^1 = 1 - p $"
  },
  {
    "objectID": "ML/index.html",
    "href": "ML/index.html",
    "title": "Machine Learning Topics",
    "section": "",
    "text": "Understanding Axis(Dim) Operations"
  },
  {
    "objectID": "ML/index.html#numpy-fundamentals",
    "href": "ML/index.html#numpy-fundamentals",
    "title": "Machine Learning Topics",
    "section": "",
    "text": "Understanding Axis(Dim) Operations"
  },
  {
    "objectID": "ML/index.html#clustering-algorithms",
    "href": "ML/index.html#clustering-algorithms",
    "title": "Machine Learning Topics",
    "section": "Clustering Algorithms",
    "text": "Clustering Algorithms\n\nK-Means Clustering"
  },
  {
    "objectID": "ML/index.html#deep-learning-fundamentals",
    "href": "ML/index.html#deep-learning-fundamentals",
    "title": "Machine Learning Topics",
    "section": "Deep Learning Fundamentals",
    "text": "Deep Learning Fundamentals\n\nThe XOR Problem: Nonlinearity in Deep Learning\nLikelihood-Based Loss Functions"
  },
  {
    "objectID": "ML/index.html#classification-algorithms",
    "href": "ML/index.html#classification-algorithms",
    "title": "Machine Learning Topics",
    "section": "Classification Algorithms",
    "text": "Classification Algorithms\n\nLogistic Regression"
  },
  {
    "objectID": "Algorithm/index.html",
    "href": "Algorithm/index.html",
    "title": "Algorithm Topics",
    "section": "",
    "text": "DP: Regular Expression Matching"
  },
  {
    "objectID": "Algorithm/index.html#dynamic-programming",
    "href": "Algorithm/index.html#dynamic-programming",
    "title": "Algorithm Topics",
    "section": "",
    "text": "DP: Regular Expression Matching"
  },
  {
    "objectID": "Math/index.html",
    "href": "Math/index.html",
    "title": "Math",
    "section": "",
    "text": "Mathematical foundations and explorations.\n\n\n\nMIT 18.06 Lecture 1: Geometry of Linear Equations"
  },
  {
    "objectID": "Math/index.html#linear-algebra",
    "href": "Math/index.html#linear-algebra",
    "title": "Math",
    "section": "",
    "text": "MIT 18.06 Lecture 1: Geometry of Linear Equations"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html",
    "href": "ML/likelihood-loss-functions.html",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.2 reveals the fundamental connection between probabilistic assumptions and the loss functions we use to train neural networks.\n📓 For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#the-hidden-connection-why-these-loss-functions",
    "href": "ML/likelihood-loss-functions.html#the-hidden-connection-why-these-loss-functions",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "The Hidden Connection: Why These Loss Functions?",
    "text": "The Hidden Connection: Why These Loss Functions?\nEver wondered why we use mean squared error for regression, cross-entropy for classification, and other specific loss functions? The answer lies in maximum likelihood estimation - each common loss function corresponds to the negative log-likelihood of a specific probabilistic model.\n\n\n\n\n\n\n\n\nProbabilistic Model\nLoss Function\nUse Case\n\n\n\n\nGaussian likelihood\nMean Squared Error\nRegression\n\n\nBernoulli likelihood\nBinary Cross-Entropy\nBinary Classification\n\n\nCategorical likelihood\nSoftmax Cross-Entropy\nMulticlass Classification"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#exploring-the-connection-probabilistic-models-loss-functions",
    "href": "ML/likelihood-loss-functions.html#exploring-the-connection-probabilistic-models-loss-functions",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "🎯 Exploring the Connection: Probabilistic Models → Loss Functions",
    "text": "🎯 Exploring the Connection: Probabilistic Models → Loss Functions\n\n\nShow code\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-1-gaussian-likelihood-mean-squared-error",
    "href": "ML/likelihood-loss-functions.html#connection-1-gaussian-likelihood-mean-squared-error",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 1: Gaussian Likelihood → Mean Squared Error",
    "text": "Connection 1: Gaussian Likelihood → Mean Squared Error\nThe Setup: When we assume our targets have Gaussian noise around our predictions:\n\\[p(y|x) = \\mathcal{N}(y; \\hat{y}, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\hat{y})^2}{2\\sigma^2}\\right)\\]\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = \\frac{(y-\\hat{y})^2}{2\\sigma^2} + \\frac{1}{2}\\log(2\\pi\\sigma^2)\\]\nThe Result: Minimizing this is equivalent to minimizing MSE (the constant term doesn’t affect optimization)!\n\n\nShow code\n# Demonstrate Gaussian likelihood = MSE connection\nnp.random.seed(0)\nx = np.linspace(-1, 1, 20)\ny_true = 2 * x + 1\ny = y_true + np.random.normal(0, 0.1, size=x.shape)  # Gaussian noise\n\n# Simple linear model predictions\nw, b = 1.0, 0.0\ny_pred = w * x + b\n\n# Compute MSE\nmse = np.mean((y - y_pred)**2)\n\n# Compute Gaussian negative log-likelihood\nsigma_squared = 0.1**2\nquadratic_term = 0.5 * np.mean((y - y_pred)**2) / sigma_squared\nconst_term = 0.5 * np.log(2 * np.pi * sigma_squared)\nnll_gaussian = quadratic_term + const_term\n\nprint(\"📊 Gaussian Likelihood ↔ MSE Connection\")\nprint(\"=\" * 45)\nprint(f\"📈 Mean Squared Error:     {mse:.6f}\")\nprint(f\"📊 Gaussian NLL:           {nll_gaussian:.6f}\")\nprint(f\"   ├─ Quadratic term:      {quadratic_term:.6f}\")\nprint(f\"   └─ Constant term:       {const_term:.6f}\")\n\nscaling_factor = 1 / (2 * sigma_squared)\nprint(f\"\\n🔗 Mathematical Connection:\")\nprint(f\"   Quadratic term = {scaling_factor:.1f} × MSE\")\nprint(f\"   {quadratic_term:.6f} = {scaling_factor:.1f} × {mse:.6f}\")\nprint(f\"\\n✅ Minimizing MSE ≡ Maximizing Gaussian likelihood\")\n\n\n📊 Gaussian Likelihood ↔ MSE Connection\n=============================================\n📈 Mean Squared Error:     1.450860\n📊 Gaussian NLL:           71.159339\n   ├─ Quadratic term:      72.542985\n   └─ Constant term:       -1.383647\n\n🔗 Mathematical Connection:\n   Quadratic term = 50.0 × MSE\n   72.542985 = 50.0 × 1.450860\n\n✅ Minimizing MSE ≡ Maximizing Gaussian likelihood"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-2-bernoulli-likelihood-binary-cross-entropy",
    "href": "ML/likelihood-loss-functions.html#connection-2-bernoulli-likelihood-binary-cross-entropy",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 2: Bernoulli Likelihood → Binary Cross-Entropy",
    "text": "Connection 2: Bernoulli Likelihood → Binary Cross-Entropy\nThe Setup: For binary classification, we assume Bernoulli-distributed targets:\n\\[p(y|x) = \\sigma(z)^y (1-\\sigma(z))^{1-y}\\]\nwhere \\(\\sigma(z) = \\frac{1}{1+e^{-z}}\\) is the sigmoid function.\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = -y\\log\\sigma(z) - (1-y)\\log(1-\\sigma(z))\\]\nThe Result: This is exactly binary cross-entropy loss!\n\n\nShow code\n# Demonstrate Bernoulli likelihood = Binary cross-entropy connection\nz = torch.tensor([-0.5, -0.8, 0.0, 0.8, 0.5])  # Model logits\ny = torch.tensor([0.0, 0.0, 1.0, 1.0, 1.0])     # Binary labels\np = torch.sigmoid(z)  # Convert to probabilities\n\nprint(\"🎲 Bernoulli Likelihood ↔ Binary Cross-Entropy\")\nprint(\"=\" * 50)\nprint(\"Input Data:\")\nprint(f\"   Logits:        {z.numpy()}\")\nprint(f\"   Labels:        {y.numpy()}\")\nprint(f\"   Probabilities: {p.numpy()}\")\n\n# Manual Bernoulli NLL computation\nbernoulli_nll = torch.mean(-(y * torch.log(p) + (1 - y) * torch.log(1 - p)))\n\n# PyTorch binary cross-entropy\nbce_loss = F.binary_cross_entropy(p, y)\n\nprint(f\"\\n📊 Loss Function Comparison:\")\nprint(f\"   Manual Bernoulli NLL:  {bernoulli_nll:.6f}\")\nprint(f\"   PyTorch BCE Loss:      {bce_loss:.6f}\")\n\n# Verify they're identical\ndifference = torch.abs(bernoulli_nll - bce_loss)\nprint(f\"\\n🔗 Verification:\")\nprint(f\"   Absolute difference:   {difference:.10f}\")\nprint(f\"\\n✅ Binary cross-entropy IS Bernoulli negative log-likelihood!\")\n\n\n🎲 Bernoulli Likelihood ↔ Binary Cross-Entropy\n==================================================\nInput Data:\n   Logits:        [-0.5 -0.8  0.   0.8  0.5]\n   Labels:        [0. 0. 1. 1. 1.]\n   Probabilities: [0.37754068 0.3100255  0.5        0.6899745  0.62245935]\n\n📊 Loss Function Comparison:\n   Manual Bernoulli NLL:  0.476700\n   PyTorch BCE Loss:      0.476700\n\n🔗 Verification:\n   Absolute difference:   0.0000000000\n\n✅ Binary cross-entropy IS Bernoulli negative log-likelihood!"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-3-categorical-likelihood-softmax-cross-entropy",
    "href": "ML/likelihood-loss-functions.html#connection-3-categorical-likelihood-softmax-cross-entropy",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 3: Categorical Likelihood → Softmax Cross-Entropy",
    "text": "Connection 3: Categorical Likelihood → Softmax Cross-Entropy\nThe Setup: For multiclass classification, we use the categorical distribution:\n\\[p(y=i|x) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} = \\text{softmax}(z)_i\\]\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = -\\log \\frac{e^{z_y}}{\\sum_j e^{z_j}} = -z_y + \\log\\sum_j e^{z_j}\\]\nThe Result: This is exactly softmax cross-entropy loss!\n\n\nShow code\n# Demonstrate Categorical likelihood = Softmax cross-entropy connection\nz = torch.tensor([[0.1, 0.2, 0.7],    # Sample 1: class 2 highest\n                  [0.1, 0.7, 0.2],    # Sample 2: class 1 highest  \n                  [0.7, 0.1, 0.2]])   # Sample 3: class 0 highest\n\ny = torch.tensor([2, 1, 0])           # True class indices\n\nprint(\"🎯 Categorical Likelihood ↔ Softmax Cross-Entropy\")\nprint(\"=\" * 55)\nprint(\"Input Data:\")\nprint(f\"   Logits shape:    {z.shape}\")\nprint(f\"   True classes:    {y.numpy()}\")\n\n# Convert to probabilities\nsoftmax_probs = F.softmax(z, dim=1)\nprint(f\"\\nSoftmax Probabilities:\")\nfor i, (logit_row, prob_row, true_class) in enumerate(zip(z, softmax_probs, y)):\n    print(f\"   Sample {i+1}: {prob_row.numpy()} → Class {true_class}\")\n\n# Manual categorical NLL (using log-softmax for numerical stability)\nlog_softmax = F.log_softmax(z, dim=1)\ncategorical_nll = -torch.mean(log_softmax[range(len(y)), y])\n\n# PyTorch cross-entropy\nce_loss = F.cross_entropy(z, y)\n\nprint(f\"\\n📊 Loss Function Comparison:\")\nprint(f\"   Manual Categorical NLL: {categorical_nll:.6f}\")\nprint(f\"   PyTorch Cross-Entropy:  {ce_loss:.6f}\")\n\n# Verify they're identical\ndifference = torch.abs(categorical_nll - ce_loss)\nprint(f\"\\n🔗 Verification:\")\nprint(f\"   Absolute difference:    {difference:.10f}\")\nprint(f\"\\n✅ Cross-entropy IS categorical negative log-likelihood!\")\n\n\n🎯 Categorical Likelihood ↔ Softmax Cross-Entropy\n=======================================================\nInput Data:\n   Logits shape:    torch.Size([3, 3])\n   True classes:    [2 1 0]\n\nSoftmax Probabilities:\n   Sample 1: [0.25462854 0.28140804 0.46396342] → Class 2\n   Sample 2: [0.25462854 0.46396342 0.28140804] → Class 1\n   Sample 3: [0.46396342 0.25462854 0.28140804] → Class 0\n\n📊 Loss Function Comparison:\n   Manual Categorical NLL: 0.767950\n   PyTorch Cross-Entropy:  0.767950\n\n🔗 Verification:\n   Absolute difference:    0.0000000000\n\n✅ Cross-entropy IS categorical negative log-likelihood!"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#why-this-matters-bce-vs-mse-for-classification",
    "href": "ML/likelihood-loss-functions.html#why-this-matters-bce-vs-mse-for-classification",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Why This Matters: BCE vs MSE for Classification",
    "text": "Why This Matters: BCE vs MSE for Classification\nUnderstanding the probabilistic foundation explains why binary cross-entropy works better than MSE for classification, even though both can theoretically solve binary problems.\nKey Differences: - BCE gradient: \\(\\sigma(z) - y\\) (simple, well-behaved) - MSE gradient: \\(2(\\sigma(z) - y) \\times \\sigma(z) \\times (1 - \\sigma(z))\\) (can vanish!)\nLet’s see this in practice:\n\n\nShow code\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Create synthetic binary classification data\ntorch.manual_seed(0)\nX = torch.randn(100, 2)\ny = (X[:, 0] + X[:, 1] &gt; 0).float()  # Binary labels\n\nprint(\"🔥 BCE vs MSE Training Comparison\")\nprint(\"=\" * 40)\nprint(\"Dataset Information:\")\nprint(f\"   Total samples:     {len(X)}\")\nprint(f\"   Feature dimension: {X.shape[1]}\")\nprint(f\"   Positive class:    {y.sum().item():.0f} samples\")\nprint(f\"   Negative class:    {(1-y).sum().item():.0f} samples\")\n\n# Train with BCE (theoretically correct)\nprint(f\"\\n🎯 Training with Binary Cross-Entropy...\")\nmodel_bce = nn.Linear(2, 1)\nloss_fn_bce = nn.BCEWithLogitsLoss()\noptimizer_bce = optim.SGD(model_bce.parameters(), lr=0.1)\n\nlosses_bce = []\nfor epoch in range(20):\n    optimizer_bce.zero_grad()\n    logits = model_bce(X).squeeze()\n    loss = loss_fn_bce(logits, y)\n    loss.backward()\n    optimizer_bce.step()\n    losses_bce.append(loss.item())\n\n# Train with MSE (suboptimal choice)\nprint(f\"📈 Training with Mean Squared Error...\")\nmodel_mse = nn.Linear(2, 1)\nloss_fn_mse = nn.MSELoss()\noptimizer_mse = optim.SGD(model_mse.parameters(), lr=0.1)\n\nlosses_mse = []\nfor epoch in range(20):\n    optimizer_mse.zero_grad()\n    logits = model_mse(X).squeeze()\n    probs = torch.sigmoid(logits)\n    loss = loss_fn_mse(probs, y)\n    loss.backward()\n    optimizer_mse.step()\n    losses_mse.append(loss.item())\n\n# Compare final performance\nwith torch.no_grad():\n    pred_bce = (torch.sigmoid(model_bce(X)) &gt; 0.5).float()\n    pred_mse = (torch.sigmoid(model_mse(X)) &gt; 0.5).float()\n    \n    acc_bce = (pred_bce.squeeze() == y).float().mean()\n    acc_mse = (pred_mse.squeeze() == y).float().mean()\n\nprint(f\"\\n📊 Final Performance Comparison:\")\nprint(f\"   BCE Accuracy:  {acc_bce:.4f} ({acc_bce*100:.1f}%)\")\nprint(f\"   MSE Accuracy:  {acc_mse:.4f} ({acc_mse*100:.1f}%)\")\n\nimprovement = (acc_bce - acc_mse) / acc_mse * 100 if acc_mse &gt; 0 else float('inf')\nprint(f\"\\n🚀 BCE Performance:\")\nif improvement &gt; 0:\n    print(f\"   {improvement:.1f}% better than MSE\")\nelse:\n    print(f\"   Similar to MSE performance\")\n    \nprint(f\"\\n✅ Key Insight: BCE provides better optimization dynamics for classification!\")\n\n\n🔥 BCE vs MSE Training Comparison\n========================================\nDataset Information:\n   Total samples:     100\n   Feature dimension: 2\n   Positive class:    54 samples\n   Negative class:    46 samples\n\n🎯 Training with Binary Cross-Entropy...\n📈 Training with Mean Squared Error...\n\n📊 Final Performance Comparison:\n   BCE Accuracy:  0.6400 (64.0%)\n   MSE Accuracy:  0.5000 (50.0%)\n\n🚀 BCE Performance:\n   28.0% better than MSE\n\n✅ Key Insight: BCE provides better optimization dynamics for classification!\n\n\n\n\nShow code\n# Visualize training comparison\nplt.figure(figsize=(12, 4))\n\n# Training loss curves\nplt.subplot(1, 2, 1)\nplt.plot(range(20), losses_bce, 'b-o', label='BCE Loss', markersize=4, alpha=0.8)\nplt.plot(range(20), losses_mse, 'r-s', label='MSE Loss', markersize=4, alpha=0.8)\nplt.xlabel('Training Epoch')\nplt.ylabel('Loss Value')\nplt.title('Training Loss Comparison')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Final accuracy comparison\nplt.subplot(1, 2, 2)\nbars = plt.bar(['BCE', 'MSE'], [acc_bce.item(), acc_mse.item()], \n               color=['blue', 'red'], alpha=0.7, edgecolor='black')\nplt.ylabel('Test Accuracy')\nplt.title('Final Model Performance')\nplt.ylim(0, 1.1)\nplt.grid(True, alpha=0.3)\n\n# Add accuracy labels\nfor bar, acc in zip(bars, [acc_bce.item(), acc_mse.item()]):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ML/kmeans.html",
    "href": "ML/kmeans.html",
    "title": "K-means Clustering",
    "section": "",
    "text": "Setup points and K\nwe will implement a KNN algorithm to cluster the points\n\n\nX=[[1,1],[2,2.1],[3,2.5],[6,7],[7,7.1],[9,7.5]]\nk=2\n\nmax_iter=3\n\n\n# Visualize the data\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter([x[0] for x in X],[x[1] for x in X])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pure python implementation of K-means clustering\ndef knn_iter(X,centroids):\n    # set up new clusters\n    new_clusters=[[] for _ in range(len(centroids))]\n    # k=len(centroids)\n    # assign each point to the nearest centroid\n    for x in X:\n        k,distance=0,(x[0]-centroids[0][0])**2+(x[1]-centroids[0][1])**2\n        for i,c in enumerate(centroids[1:],1):\n            if (x[0]-c[0])**2+(x[1]-c[1])**2&lt;distance:\n                k=i\n                distance=(x[0]-c[0])**2+(x[1]-c[1])**2\n        new_clusters[k].append(x)\n    \n    # calculate new centroids\n    new_centroids=[[\n        sum([x[0] for x in cluster])/len(cluster),\n        sum([x[1] for x in cluster])/len(cluster)\n    ] if cluster else centroids[i] for i,cluster in enumerate(new_clusters)]\n    return new_centroids\n\n\n\n\n\n\n\n\ndef iter_and_draw(X,k,max_iter):\n    centroids=X[:k]  # Randomly select 2 centroids\n    fig, axes = plt.subplots(max_iter//3+(1 if max_iter%3!=0 else 0),\n        3, figsize=(15, 10))\n    axes=axes.flatten()\n    for i in range(max_iter):\n        \n        # Plot points and centroids\n\n\n        # Assign each point to nearest centroid and plot with corresponding color\n        colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n        for j, x in enumerate(X):\n            # Find nearest centroid\n            min_dist = float('inf')\n            nearest_centroid = 0\n            for k, c in enumerate(centroids):\n                dist = (x[0]-c[0])**2 + (x[1]-c[1])**2\n                if dist &lt; min_dist:\n                    min_dist = dist\n                    nearest_centroid = k\n            # Plot point with color corresponding to its cluster\n            axes[i].scatter(x[0], x[1], c=colors[nearest_centroid % len(colors)], label=f'Cluster {nearest_centroid+1}' if j==0 else \"\")\n        axes[i].scatter([c[0] for c in centroids], [c[1] for c in centroids], c='red', marker='*', s=200, label='Centroids')\n        axes[i].set_title(f'Iteration {i}')\n        centroids = knn_iter(X, centroids)\n\n    plt.tight_layout()\n    plt.show()\n\niter_and_draw(X,k,max_iter)\n# print(centroids)\n\n\n\n\n\n\n\n\n\n# A 3 clusters example\n\nimport numpy as np\n\nX1=np.random.rand(20,2)+5 # Some points in the upper right corner\nX2=np.random.rand(20,2)+3 # Some points in the middle\nX3=np.random.rand(20,2) # Some points in the lower left corner\n\niter_and_draw(np.concatenate((X1,X2,X3)),3,5)\n\n\n\n\n\n\n\n\n\n\nA question?\n\nWhat to do if one cluster has no assigned points during iteration?\n\n\n\nFormula Derivation\nThe goal is to minimize the loss of inertia which is sum of the points to cluster centroids.\n\\[\nLoss= \\sum_{i=1}^n \\sum_{x \\in C_i} ||x-\\mu_i||^2\n\\]\nTo iter \\(\\mu\\) for each cluster, let us find the derivative of the following function. \\[\nf(\\mu)=\\sum_{i=1}^n ||x_i-\\mu||^2 =\n\\sum_{i=1}^n {x_i}^2+\\mu^2-2x_i\\mu\n\\]\nGiven a \\(\\nabla \\mu\\), \\[\nf(\\mu + \\nabla \\mu)=\\sum_{i=1}^n ||x_i+\\nabla \\mu -\\mu||^2 =\n\\sum_{i=1}^n  {x_i}^2+\\mu^2+{\\nabla \\mu}^2-2{x_i \\mu}-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\nf(\\mu + \\nabla \\mu)-f(\\mu)=\n\\sum_{i=1}^n {\\nabla \\mu}^2-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\n\\frac {f(\\mu + \\nabla \\mu)-f(\\mu)}{\\nabla \\mu}=\\sum_{i=1}^n {\\nabla \\mu} -2 \\mu +2{x_i} = 2\\sum_{i=1}^n x_i - 2n\\mu\n\\]\nNow we can see if \\(n\\mu = \\sum_{i=1}^n x_i\\), then the derivative is 0, this is why in each iteration, we need to set the center of the cluster as centroid."
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#the-example-system",
    "href": "Math/mit1806-lecture1-geometry.html#the-example-system",
    "title": "MIT 18.06 Lecture 1: The Geometry of Linear Equations",
    "section": "The Example System",
    "text": "The Example System\nLet’s work with this concrete example:\n\\[\\begin{align}\nx + 2y &= 5 \\\\\n3x + 4y &= 6\n\\end{align}\\]\nIn matrix form: \\[\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nWe can interpret this system in two completely different ways."
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#row-picture-geometry",
    "href": "Math/mit1806-lecture1-geometry.html#row-picture-geometry",
    "title": "MIT 18.06 Lecture 1: The Geometry of Linear Equations",
    "section": "Row Picture (Geometry)",
    "text": "Row Picture (Geometry)\nIn the row picture, each equation represents a geometric object: - In 2D: each equation is a line - In 3D: each equation is a plane\n- In higher dimensions: each equation is a hyperplane\nThe solution is where all these objects intersect.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the equations in the form y = mx + c\n# Line 1: x + 2y = 5  =&gt;  y = -1/2*x + 5/2\n# Line 2: 3x + 4y = 6  =&gt;  y = -3/4*x + 3/2\nx = np.linspace(-10, 10, 100)\ny1 = -1/2 * x + 5/2\ny2 = -3/4 * x + 3/2\n\n# Solve for intersection point\nA = np.array([[1, 2], [3, 4]])\nb = np.array([5, 6])\nsolution = np.linalg.solve(A, b)\n\n# Plot both lines and intersection\nplt.figure(figsize=(8, 6))\nplt.plot(x, y1, 'b-', label='Line 1: x + 2y = 5', linewidth=2)\nplt.plot(x, y2, 'r-', label='Line 2: 3x + 4y = 6', linewidth=2)\nplt.scatter(solution[0], solution[1], color='green', s=100, zorder=5, \n           label=f'Solution: ({solution[0]:.1f}, {solution[1]:.1f})', edgecolor='white', linewidth=2)\n\nplt.xlim(-8, 8)\nplt.ylim(-1, 8)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Row Picture: Where Lines Meet')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Solution: x = {solution[0]:.3f}, y = {solution[1]:.3f}\")\nprint(f\"Verification: {A @ solution} equals {b}\")\n\n\n\n\n\n\n\n\n\nSolution: x = -4.000, y = 4.500\nVerification: [5. 6.] equals [5 6]"
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#column-picture-algebra",
    "href": "Math/mit1806-lecture1-geometry.html#column-picture-algebra",
    "title": "MIT 18.06 Lecture 1: The Geometry of Linear Equations",
    "section": "Column Picture (Algebra)",
    "text": "Column Picture (Algebra)\nThe column picture reframes the same system as a question about vector combinations:\n\\[x \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} + y \\begin{bmatrix}2 \\\\ 4\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nInstead of asking “where do lines intersect?”, we ask: “Can we combine these vectors to reach our target?”\n\n\nCode\n# Define column vectors and target vector\na1 = np.array([1, 3])\na2 = np.array([2, 4])\nb = np.array([5, 6])\n\n# Solve for coefficients\nA = np.column_stack([a1, a2])\nsolution = np.linalg.solve(A, b)\nx, y = solution[0], solution[1]\n\nprint(f\"Question: Can we write b as a linear combination of a₁ and a₂?\")\nprint(f\"Answer: {x:.3f} × a₁ + {y:.3f} × a₂ = b\")\nprint(f\"Verification: {x*a1} + {y*a2} = {x*a1 + y*a2}\")\n\n# Visualize the vector construction\nplt.figure(figsize=(8, 6))\n\n# Step 1: Draw x*a1 (scaled version)\nplt.arrow(0, 0, x*a1[0], x*a1[1], head_width=0.2, head_length=0.2, \n         fc='blue', ec='blue', linewidth=3,\n         label=f'{x:.2f} × a₁')\n\n# Step 2: Draw y*a2 starting from the tip of x*a1\nplt.arrow(x*a1[0], x*a1[1], y*a2[0], y*a2[1], head_width=0.2, head_length=0.2, \n         fc='green', ec='green', linewidth=3,\n         label=f'{y:.2f} × a₂')\n\n# Show final result vector b\nplt.arrow(0, 0, b[0], b[1], head_width=0.25, head_length=0.25, \n         fc='red', ec='red', linewidth=4, alpha=0.8,\n         label=f'b = [{b[0]}, {b[1]}]')\n\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.xlim(-1, 6)\nplt.ylim(-12, 7)\nplt.xlabel('x-component')\nplt.ylabel('y-component')\nplt.title('Column Picture: Vector Combination')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\n\n\nQuestion: Can we write b as a linear combination of a₁ and a₂?\nAnswer: -4.000 × a₁ + 4.500 × a₂ = b\nVerification: [ -4. -12.] + [ 9. 18.] = [5. 6.]"
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "href": "Math/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "title": "MIT 18.06 Lecture 1: The Geometry of Linear Equations",
    "section": "Three Types of Linear Systems",
    "text": "Three Types of Linear Systems\nLinear systems can have three possible outcomes:\n\nUnique solution - Lines intersect at one point\nNo solution - Lines are parallel (don’t intersect)\nInfinitely many solutions - Lines are the same (overlap completely)\n\n\n\nCode\n# Case (a): Unique solution - non-parallel vectors\nprint(\"🎯 Case (a) - Unique Solution:\")\nA_a = np.array([[1, 2], [3, 4]])\nb_a = np.array([5, 6])\nsolution_a = np.linalg.solve(A_a, b_a)\ndet_a = np.linalg.det(A_a)\nprint(f\"   Solution: {solution_a}\")\nprint(f\"   Matrix determinant: {det_a:.3f} ≠ 0 → linearly independent columns\")\nprint(f\"   Column space: ENTIRE 2D plane (any point reachable)\")\n\n# Case (b): No solution - parallel vectors, b not in span\nprint(f\"\\n❌ Case (b) - No Solution:\")\nA_b = np.array([[1, 2], [2, 4]])  # Columns are parallel\nb_b = np.array([5, 6])            # b not in span\ndet_b = np.linalg.det(A_b)\nprint(f\"   Matrix determinant: {det_b:.3f} = 0 → linearly dependent columns\")\nprint(f\"   Column space: 1D line only (most points unreachable)\")\nprint(f\"   Target b = {b_b} is NOT on the line → No solution exists\")\n\n# Case (c): Infinitely many solutions - parallel vectors, b in span\nprint(f\"\\n♾️  Case (c) - Infinitely Many Solutions:\")\nA_c = np.array([[1, 2], [2, 4]])  # Same parallel columns\nb_c = np.array([3, 6])            # b = 3 * [1, 2], so b is in span\ndet_c = np.linalg.det(A_c)\nprint(f\"   Matrix determinant: {det_c:.3f} = 0 → linearly dependent columns\")\nprint(f\"   Column space: 1D line only\")\nprint(f\"   Target b = {b_c} IS on the line → Infinite solutions exist\")\n\n# Find one particular solution using pseudoinverse\nsolution_c = np.linalg.pinv(A_c) @ b_c\nprint(f\"   One particular solution: {solution_c}\")\nprint(f\"   Other solutions: {solution_c} + t×[2, -1] for any real number t\")\n\n\n🎯 Case (a) - Unique Solution:\n   Solution: [-4.   4.5]\n   Matrix determinant: -2.000 ≠ 0 → linearly independent columns\n   Column space: ENTIRE 2D plane (any point reachable)\n\n❌ Case (b) - No Solution:\n   Matrix determinant: 0.000 = 0 → linearly dependent columns\n   Column space: 1D line only (most points unreachable)\n   Target b = [5 6] is NOT on the line → No solution exists\n\n♾️  Case (c) - Infinitely Many Solutions:\n   Matrix determinant: 0.000 = 0 → linearly dependent columns\n   Column space: 1D line only\n   Target b = [3 6] IS on the line → Infinite solutions exist\n   One particular solution: [0.6 1.2]\n   Other solutions: [0.6 1.2] + t×[2, -1] for any real number t\n\n\n\n\nCode\n# Visualize all three cases\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Case (a): Unique solution\nax = axes[0]\nax.fill_between([-1, 6], [-1, -1], [7, 7], color='lightblue', alpha=0.2, \n                label='Column space = ENTIRE plane')\n\n# Draw vectors\nax.arrow(0, 0, A_a[0,0], A_a[1,0], head_width=0.15, head_length=0.15,\n         fc='blue', ec='blue', linewidth=2, label='a₁ = [1,3]')\nax.arrow(0, 0, A_a[0,1], A_a[1,1], head_width=0.15, head_length=0.15,\n         fc='green', ec='green', linewidth=2, label='a₂ = [2,4]')\nax.arrow(0, 0, b_a[0], b_a[1], head_width=0.2, head_length=0.2,\n         fc='red', ec='red', linewidth=3, label='b = [5,6]')\n\nax.set_title('Unique Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (b): No solution\nax = axes[1]\nt = np.linspace(-2, 5, 100)\nspan_x, span_y = t * A_b[0,0], t * A_b[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6, \n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_b[0,0], A_b[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='a₁ = [1,2]')\nax.arrow(0, 0, A_b[0,1], A_b[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='a₂ = [2,4] = 2×a₁')\nax.arrow(0, 0, b_b[0], b_b[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [5,6] (off line)')\n\nax.set_title('No Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (c): Infinitely many solutions\nax = axes[2]\nt = np.linspace(-1, 4, 100)\nspan_x, span_y = t * A_c[0,0], t * A_c[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6,\n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_c[0,0], A_c[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='a₁ = [1,2]')\nax.arrow(0, 0, A_c[0,1], A_c[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='a₂ = [2,4] = 2×a₁')\nax.arrow(0, 0, b_c[0], b_c[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [3,6] (on line)')\n\nax.set_title('Infinite Solutions')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key insight: Solution depends on whether target vector b lies in the column space\")\n\n\n\n\n\n\n\n\n\nKey insight: Solution depends on whether target vector b lies in the column space\n\n\n\nThis covers the core geometric foundations from MIT 18.06 Lecture 1: understanding linear systems through both row and column perspectives."
  }
]