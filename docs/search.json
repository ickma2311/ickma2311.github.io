[
  {
    "objectID": "ML/hessian-prerequisites.html",
    "href": "ML/hessian-prerequisites.html",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "",
    "text": "My notebook\nBefore diving into optimization algorithms for deep learning (Chapter 7), we need to understand second-order derivatives in multiple dimensions. The Hessian matrix is the key tool that generalizes the concept of curvature to high-dimensional spaces."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#context",
    "href": "ML/hessian-prerequisites.html#context",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "",
    "text": "My notebook\nBefore diving into optimization algorithms for deep learning (Chapter 7), we need to understand second-order derivatives in multiple dimensions. The Hessian matrix is the key tool that generalizes the concept of curvature to high-dimensional spaces."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#why-second-derivatives-matter",
    "href": "ML/hessian-prerequisites.html#why-second-derivatives-matter",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Why Second Derivatives Matter",
    "text": "Why Second Derivatives Matter\nIn one dimension, optimizing \\(f(x)\\) involves:\n\nFirst derivative \\(f'(x) = 0\\) ‚Üí Find critical points\n\nWhy set \\(f'(x) = 0\\)? At a minimum or maximum, the slope is flat (zero)\nThink of a hill: at the very top, you stop going up ‚Üí slope = 0\nAt the bottom of a valley, you stop going down ‚Üí slope = 0\nExample: For \\(f(x) = x^2\\), we have \\(f'(x) = 2x\\). Setting \\(f'(x) = 0\\) gives \\(x = 0\\) (the minimum)\n\nSecond derivative \\(f''(x)\\) ‚Üí Classify the critical point:\n\n\\(f''(x) &gt; 0\\) ‚Üí Local minimum (curves upward like a bowl)\n\\(f''(x) &lt; 0\\) ‚Üí Local maximum (curves downward like a dome)\n\\(f''(x) = 0\\) ‚Üí Inconclusive (could be an inflection point)\nWhy needed? Not all points where \\(f'(x) = 0\\) are minima! For \\(f(x) = x^3\\), we have \\(f'(0) = 0\\) but it‚Äôs neither a min nor max.\n\n\nThe challenge: How do we extend this to functions of many variables \\(f(x_1, x_2, \\ldots, x_n)\\)?\nThe answer: The Hessian matrix captures all second-order information.\n\nVisualizing Second Derivatives\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = np.linspace(-3, 3, 200)\n\n# Three functions with different second derivatives\nf1 = x**2           # f''(x) = 2 (positive, curves up)\nf2 = -x**2          # f''(x) = -2 (negative, curves down)\nf3 = x**3           # f''(x) = 6x (changes sign at x=0)\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\n\n# Function 1: f(x) = x¬≤\naxes[0, 0].plot(x, f1, 'b-', linewidth=2)\naxes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 0].set_title(\"f(x) = x¬≤\\nf''(x) = 2 &gt; 0\\n(Curves UP)\", fontsize=10)\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel('f(x)')\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].annotate('Minimum', xy=(0, 0), xytext=(0.5, 2),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Function 2: f(x) = -x¬≤\naxes[0, 1].plot(x, f2, 'r-', linewidth=2)\naxes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 1].set_title(\"f(x) = -x¬≤\\nf''(x) = -2 &lt; 0\\n(Curves DOWN)\", fontsize=10)\naxes[0, 1].set_xlabel('x')\naxes[0, 1].set_ylabel('f(x)')\naxes[0, 1].grid(True, alpha=0.3)\naxes[0, 1].annotate('Maximum', xy=(0, 0), xytext=(0.5, -2),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Function 3: f(x) = x¬≥\naxes[1, 0].plot(x, f3, 'g-', linewidth=2)\naxes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[1, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[1, 0].set_title(\"f(x) = x¬≥\\nf''(x) = 6x\\n(Changes sign)\", fontsize=10)\naxes[1, 0].set_xlabel('x')\naxes[1, 0].set_ylabel('f(x)')\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].annotate('Inflection point', xy=(0, 0), xytext=(1, -10),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Hide the unused subplot\naxes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nKey observations:\n\n\n\n\n\n\n\n\n\nSecond Derivative\nCurvature\nShape\nPoint Type\n\n\n\n\n\\(f''(x) &gt; 0\\)\nCurves upward\nBowl shape\nPotential minimum\n\n\n\\(f''(x) &lt; 0\\)\nCurves downward\nDome shape\nPotential maximum\n\n\n\\(f''(x) = 0\\) (at critical point)\nChanges sign\nFlat at that point\nInflection point\n\n\n\nNote on the third example: For \\(f(x) = x^3\\), we have \\(f''(x) = 6x\\). At the critical point \\(x = 0\\), \\(f''(0) = 0\\), which is inconclusive. The curvature changes sign: negative for \\(x &lt; 0\\) and positive for \\(x &gt; 0\\)."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#the-hessian-matrix",
    "href": "ML/hessian-prerequisites.html#the-hessian-matrix",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "The Hessian Matrix",
    "text": "The Hessian Matrix\n\nDefinition\nFor a scalar function \\(f(\\mathbf{x}) = f(x_1, x_2, \\ldots, x_n)\\), the Hessian matrix is the square matrix of all second-order partial derivatives:\n\\[\nH(f) =\n\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n\\]\n\n\nKey Properties\n\nSymmetric: If mixed partial derivatives are continuous, then \\(\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}\\), so \\(H = H^T\\).\nShape: Always \\(n \\times n\\) (determined by number of variables, not terms in the function)\nDescribes curvature in all directions simultaneously\nEigenvalue decomposition: Since the Hessian is symmetric, it can be decomposed as \\(H = Q\\Lambda Q^T\\) where \\(Q\\) contains orthonormal eigenvectors and \\(\\Lambda\\) is a diagonal matrix of eigenvalues\n\n\n\nSimple Example\nFor \\(f(x, y) = x^2 + 3y^2\\):\nStep 1: Compute first derivatives \\[\n\\frac{\\partial f}{\\partial x} = 2x, \\quad \\frac{\\partial f}{\\partial y} = 6y\n\\]\nStep 2: Compute second derivatives \\[\n\\frac{\\partial^2 f}{\\partial x^2} = 2, \\quad \\frac{\\partial^2 f}{\\partial y^2} = 6, \\quad \\frac{\\partial^2 f}{\\partial x \\partial y} = 0\n\\]\nStep 3: Build Hessian \\[\nH = \\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}\n\\]\nInterpretation: - Curvature along \\(x\\)-axis: 2 - Curvature along \\(y\\)-axis: 6 - No cross-dependency (off-diagonal = 0)\n\n\nExample with Cross Terms\nFor \\(f(x, y) = x^2 + xy + y^2\\):\n\\[\nH = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\n\\]\nThe off-diagonal term (1) indicates that \\(x\\) and \\(y\\) are coupled‚Äîchanging one affects the rate of change of the other."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#matrix-definiteness",
    "href": "ML/hessian-prerequisites.html#matrix-definiteness",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Matrix Definiteness",
    "text": "Matrix Definiteness\nFor a symmetric matrix \\(A\\), its definiteness is determined by the signs of its eigenvalues.\n\n\n\n\n\n\n\n\n\nType\nEigenvalues\nQuadratic Form \\(x^T A x\\)\nGeometric Shape\n\n\n\n\nPositive definite (PD)\nall \\(&gt; 0\\)\n\\(&gt; 0\\) for all \\(x \\neq 0\\)\nBowl (curves upward)\n\n\nNegative definite (ND)\nall \\(&lt; 0\\)\n\\(&lt; 0\\) for all \\(x \\neq 0\\)\nDome (curves downward)\n\n\nIndefinite\nsome \\(+\\), some \\(-\\)\ndepends on direction\nSaddle\n\n\nPositive semi-definite (PSD)\nall \\(\\geq 0\\)\n\\(\\geq 0\\) for all \\(x\\)\nFlat-bottom bowl\n\n\nNegative semi-definite (NSD)\nall \\(\\leq 0\\)\n\\(\\leq 0\\) for all \\(x\\)\nFlat-top dome\n\n\n\n\nQuick Test (2√ó2 case)\nFor \\(A = \\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}\\):\n\nPositive definite if \\(a &gt; 0\\) and \\(ac - b^2 &gt; 0\\)\nNegative definite if \\(a &lt; 0\\) and \\(ac - b^2 &gt; 0\\)\nIndefinite if \\(ac - b^2 &lt; 0\\)\n\n\n\nExamples\n\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}\\): eigenvalues = [2, 6] ‚Üí Positive definite\n\\(\\begin{bmatrix} -2 & 0 \\\\ 0 & -3 \\end{bmatrix}\\): eigenvalues = [-2, -3] ‚Üí Negative definite\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\\): eigenvalues = [2, -2] ‚Üí Indefinite\n\\(\\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix}\\): eigenvalues = [4, 0] ‚Üí Positive semi-definite"
  },
  {
    "objectID": "ML/hessian-prerequisites.html#interpreting-hessian-at-critical-points",
    "href": "ML/hessian-prerequisites.html#interpreting-hessian-at-critical-points",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Interpreting Hessian at Critical Points",
    "text": "Interpreting Hessian at Critical Points\nAt a critical point where \\(\\nabla f = 0\\), the Hessian determines the nature of the point:\n\n\n\n\n\n\n\n\n\nHessian Type\nEigenvalues\nSurface Shape\nPoint Type\n\n\n\n\nPositive definite\nall positive\nBowl (convex)\nLocal minimum\n\n\nNegative definite\nall negative\nDome (concave)\nLocal maximum\n\n\nIndefinite\nmixed signs\nSaddle\nNeither min nor max\n\n\nSemi-definite\nsome zero\nFlat in some directions\nInconclusive\n\n\n\n\nVisualization: Different Surface Types\n\n\nShow code\n# Create grid for plotting\nx_grid = np.linspace(-2, 2, 100)\ny_grid = np.linspace(-2, 2, 100)\nX, Y = np.meshgrid(x_grid, y_grid)\n\n# Define different functions with different Hessian types\ndef positive_definite(x, y):\n    \"\"\"Minimum: f = x¬≤ + y¬≤\"\"\"\n    return x**2 + y**2\n\ndef negative_definite(x, y):\n    \"\"\"Maximum: f = -x¬≤ - y¬≤\"\"\"\n    return -x**2 - y**2\n\ndef indefinite(x, y):\n    \"\"\"Saddle: f = x¬≤ - y¬≤\"\"\"\n    return x**2 - y**2\n\ndef semi_definite(x, y):\n    \"\"\"Flat direction: f = x¬≤\"\"\"\n    return x**2\n\n# Create 3D surface plots in 2x2 grid\nfig = plt.figure(figsize=(12, 10))\n\nfunctions = [\n    (positive_definite, \"Positive Definite\\n(Bowl - Minimum)\", \"Greens\"),\n    (negative_definite, \"Negative Definite\\n(Dome - Maximum)\", \"Reds\"),\n    (indefinite, \"Indefinite\\n(Saddle Point)\", \"RdBu\"),\n    (semi_definite, \"Semi-Definite\\n(Flat Direction)\", \"YlOrRd\")\n]\n\nfor idx, (func, title, cmap) in enumerate(functions, 1):\n    ax = fig.add_subplot(2, 2, idx, projection='3d')\n    Z = func(X, Y)\n\n    surf = ax.plot_surface(X, Y, Z, cmap=cmap, alpha=0.8,\n                           linewidth=0, antialiased=True)\n\n    ax.set_xlabel('x', fontsize=9)\n    ax.set_ylabel('y', fontsize=9)\n    ax.set_zlabel('f(x,y)', fontsize=9)\n    ax.set_title(title, fontsize=10)\n    ax.view_init(elev=25, azim=45)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nContour plots for better understanding:\n\n\nShow code\n# Contour plots in 2x2 grid\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\naxes = axes.flatten()\n\nfor idx, (func, title, cmap) in enumerate(functions):\n    ax = axes[idx]\n    Z = func(X, Y)\n\n    contour = ax.contour(X, Y, Z, levels=15, cmap=cmap)\n    ax.clabel(contour, inline=True, fontsize=7)\n\n    # Mark the critical point at origin\n    ax.plot(0, 0, 'r*', markersize=12, label='Critical point')\n\n    ax.set_xlabel('x', fontsize=9)\n    ax.set_ylabel('y', fontsize=9)\n    ax.set_title(title, fontsize=10)\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=8)\n    ax.set_aspect('equal')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "This is for MIT 18.06SC Lecture 1, covering how to understand linear systems from two perspectives: geometry (row picture) and algebra (column picture)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#the-example-system",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#the-example-system",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "The Example System",
    "text": "The Example System\nLet‚Äôs work with this concrete example:\n\\[\\begin{align}\nx + 2y &= 5 \\\\\n3x + 4y &= 6\n\\end{align}\\]\nIn matrix form: \\[\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nWe can interpret this system in two completely different ways."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#row-picture-geometry",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#row-picture-geometry",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Row Picture (Geometry)",
    "text": "Row Picture (Geometry)\nIn the row picture, each equation represents a geometric object: - In 2D: each equation is a line - In 3D: each equation is a plane\n- In higher dimensions: each equation is a hyperplane\nThe solution is where all these objects intersect.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the equations in the form y = mx + c\n# Line 1: x + 2y = 5  =&gt;  y = -1/2*x + 5/2\n# Line 2: 3x + 4y = 6  =&gt;  y = -3/4*x + 3/2\nx = np.linspace(-10, 10, 100)\ny1 = -1/2 * x + 5/2\ny2 = -3/4 * x + 3/2\n\n# Solve for intersection point\nA = np.array([[1, 2], [3, 4]])\nb = np.array([5, 6])\nsolution = np.linalg.solve(A, b)\n\n# Plot both lines and intersection\nplt.figure(figsize=(8, 6))\nplt.plot(x, y1, 'b-', label='Line 1: x + 2y = 5', linewidth=2)\nplt.plot(x, y2, 'r-', label='Line 2: 3x + 4y = 6', linewidth=2)\nplt.scatter(solution[0], solution[1], color='green', s=100, zorder=5, \n           label=f'Solution: ({solution[0]:.1f}, {solution[1]:.1f})', edgecolor='white', linewidth=2)\n\nplt.xlim(-8, 8)\nplt.ylim(-1, 8)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Row Picture: Where Lines Meet')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Solution: x = {solution[0]:.3f}, y = {solution[1]:.3f}\")\nprint(f\"Verification: {A @ solution} equals {b}\")\n\n\n\n\n\n\n\n\n\nSolution: x = -4.000, y = 4.500\nVerification: [5. 6.] equals [5 6]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#column-picture-algebra",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#column-picture-algebra",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Column Picture (Algebra)",
    "text": "Column Picture (Algebra)\nThe column picture reframes the same system as a question about vector combinations:\n\\[x \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} + y \\begin{bmatrix}2 \\\\ 4\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nInstead of asking ‚Äúwhere do lines intersect?‚Äù, we ask: ‚ÄúCan we combine these vectors to reach our target?‚Äù\n\n\nCode\n# Define column vectors and target vector\na1 = np.array([1, 3])\na2 = np.array([2, 4])\nb = np.array([5, 6])\n\n# Solve for coefficients\nA = np.column_stack([a1, a2])\nsolution = np.linalg.solve(A, b)\nx, y = solution[0], solution[1]\n\nprint(f\"Question: Can we write b as a linear combination of a‚ÇÅ and a‚ÇÇ?\")\nprint(f\"Answer: {x:.3f} √ó a‚ÇÅ + {y:.3f} √ó a‚ÇÇ = b\")\nprint(f\"Verification: {x*a1} + {y*a2} = {x*a1 + y*a2}\")\n\n# Visualize the vector construction\nplt.figure(figsize=(8, 6))\n\n# Step 1: Draw x*a1 (scaled version)\nplt.arrow(0, 0, x*a1[0], x*a1[1], head_width=0.2, head_length=0.2, \n         fc='blue', ec='blue', linewidth=3,\n         label=f'{x:.2f} √ó a‚ÇÅ')\n\n# Step 2: Draw y*a2 starting from the tip of x*a1\nplt.arrow(x*a1[0], x*a1[1], y*a2[0], y*a2[1], head_width=0.2, head_length=0.2, \n         fc='green', ec='green', linewidth=3,\n         label=f'{y:.2f} √ó a‚ÇÇ')\n\n# Show final result vector b\nplt.arrow(0, 0, b[0], b[1], head_width=0.25, head_length=0.25, \n         fc='red', ec='red', linewidth=4, alpha=0.8,\n         label=f'b = [{b[0]}, {b[1]}]')\n\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.xlim(-1, 6)\nplt.ylim(-12, 7)\nplt.xlabel('x-component')\nplt.ylabel('y-component')\nplt.title('Column Picture: Vector Combination')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\n\n\nQuestion: Can we write b as a linear combination of a‚ÇÅ and a‚ÇÇ?\nAnswer: -4.000 √ó a‚ÇÅ + 4.500 √ó a‚ÇÇ = b\nVerification: [ -4. -12.] + [ 9. 18.] = [5. 6.]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Three Types of Linear Systems",
    "text": "Three Types of Linear Systems\nLinear systems can have three possible outcomes:\n\nUnique solution - Lines intersect at one point\nNo solution - Lines are parallel (don‚Äôt intersect)\nInfinitely many solutions - Lines are the same (overlap completely)\n\n\n\nCode\n# Case (a): Unique solution - non-parallel vectors\nprint(\"üéØ Case (a) - Unique Solution:\")\nA_a = np.array([[1, 2], [3, 4]])\nb_a = np.array([5, 6])\nsolution_a = np.linalg.solve(A_a, b_a)\ndet_a = np.linalg.det(A_a)\nprint(f\"   Solution: {solution_a}\")\nprint(f\"   Matrix determinant: {det_a:.3f} ‚â† 0 ‚Üí linearly independent columns\")\nprint(f\"   Column space: ENTIRE 2D plane (any point reachable)\")\n\n# Case (b): No solution - parallel vectors, b not in span\nprint(f\"\\n‚ùå Case (b) - No Solution:\")\nA_b = np.array([[1, 2], [2, 4]])  # Columns are parallel\nb_b = np.array([5, 6])            # b not in span\ndet_b = np.linalg.det(A_b)\nprint(f\"   Matrix determinant: {det_b:.3f} = 0 ‚Üí linearly dependent columns\")\nprint(f\"   Column space: 1D line only (most points unreachable)\")\nprint(f\"   Target b = {b_b} is NOT on the line ‚Üí No solution exists\")\n\n# Case (c): Infinitely many solutions - parallel vectors, b in span\nprint(f\"\\n‚ôæÔ∏è  Case (c) - Infinitely Many Solutions:\")\nA_c = np.array([[1, 2], [2, 4]])  # Same parallel columns\nb_c = np.array([3, 6])            # b = 3 * [1, 2], so b is in span\ndet_c = np.linalg.det(A_c)\nprint(f\"   Matrix determinant: {det_c:.3f} = 0 ‚Üí linearly dependent columns\")\nprint(f\"   Column space: 1D line only\")\nprint(f\"   Target b = {b_c} IS on the line ‚Üí Infinite solutions exist\")\n\n# Find one particular solution using pseudoinverse\nsolution_c = np.linalg.pinv(A_c) @ b_c\nprint(f\"   One particular solution: {solution_c}\")\nprint(f\"   Other solutions: {solution_c} + t√ó[2, -1] for any real number t\")\n\n\nüéØ Case (a) - Unique Solution:\n   Solution: [-4.   4.5]\n   Matrix determinant: -2.000 ‚â† 0 ‚Üí linearly independent columns\n   Column space: ENTIRE 2D plane (any point reachable)\n\n‚ùå Case (b) - No Solution:\n   Matrix determinant: 0.000 = 0 ‚Üí linearly dependent columns\n   Column space: 1D line only (most points unreachable)\n   Target b = [5 6] is NOT on the line ‚Üí No solution exists\n\n‚ôæÔ∏è  Case (c) - Infinitely Many Solutions:\n   Matrix determinant: 0.000 = 0 ‚Üí linearly dependent columns\n   Column space: 1D line only\n   Target b = [3 6] IS on the line ‚Üí Infinite solutions exist\n   One particular solution: [0.6 1.2]\n   Other solutions: [0.6 1.2] + t√ó[2, -1] for any real number t\n\n\n\n\nCode\n# Visualize all three cases\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Case (a): Unique solution\nax = axes[0]\nax.fill_between([-1, 6], [-1, -1], [7, 7], color='lightblue', alpha=0.2, \n                label='Column space = ENTIRE plane')\n\n# Draw vectors\nax.arrow(0, 0, A_a[0,0], A_a[1,0], head_width=0.15, head_length=0.15,\n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,3]')\nax.arrow(0, 0, A_a[0,1], A_a[1,1], head_width=0.15, head_length=0.15,\n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4]')\nax.arrow(0, 0, b_a[0], b_a[1], head_width=0.2, head_length=0.2,\n         fc='red', ec='red', linewidth=3, label='b = [5,6]')\n\nax.set_title('Unique Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (b): No solution\nax = axes[1]\nt = np.linspace(-2, 5, 100)\nspan_x, span_y = t * A_b[0,0], t * A_b[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6, \n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_b[0,0], A_b[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,2]')\nax.arrow(0, 0, A_b[0,1], A_b[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4] = 2√óa‚ÇÅ')\nax.arrow(0, 0, b_b[0], b_b[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [5,6] (off line)')\n\nax.set_title('No Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (c): Infinitely many solutions\nax = axes[2]\nt = np.linspace(-1, 4, 100)\nspan_x, span_y = t * A_c[0,0], t * A_c[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6,\n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_c[0,0], A_c[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,2]')\nax.arrow(0, 0, A_c[0,1], A_c[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4] = 2√óa‚ÇÅ')\nax.arrow(0, 0, b_c[0], b_c[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [3,6] (on line)')\n\nax.set_title('Infinite Solutions')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key insight: Solution depends on whether target vector b lies in the column space\")\n\n\n\n\n\n\n\n\n\nKey insight: Solution depends on whether target vector b lies in the column space\n\n\n\nThis covers the core geometric foundations from MIT 18.06SC Lecture 1: understanding linear systems through both row and column perspectives."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ickma.dev",
    "section": "",
    "text": "A growing collection of structured study notes and visual explanations ‚Äî written for clarity, reproducibility, and long-term memory."
  },
  {
    "objectID": "index.html#latest-updates",
    "href": "index.html#latest-updates",
    "title": "ickma.dev",
    "section": "Latest Updates",
    "text": "Latest Updates\n\n‚àá Deep Learning Book 62 chapters\nMy notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\nChapter 18: Confronting the Partition Function Energy-based models require a partition function for normalization. This chapter follows how \\(\\nabla_\\theta \\log Z(\\theta)\\) enters the log-likelihood gradient and surveys training strategies like contrastive divergence, pseudolikelihood, score matching, NCE, and AIS that avoid or estimate \\(Z\\).\n\n\nChapter 17: Monte Carlo Methods Monte Carlo estimation replaces intractable expectations with sample averages. Importance sampling reweights proposal draws to reduce variance, while Markov chain methods like Gibbs sampling generate dependent samples when direct sampling is infeasible. Tempering improves mixing across multimodal landscapes.\n\n\nChapter 16: Structured Probabilistic Models for Deep Learning Structured probabilistic models use graphs to factorize high-dimensional distributions into tractable components by encoding conditional independence. Directed models represent causal relationships and support efficient ancestral sampling. Undirected models capture symmetric dependencies through clique potentials normalized by partition function. Energy-based models express potentials via Boltzmann distribution. Deep learning embraces approximate inference with large latent variable models, prioritizing scalability over exact computations through distributed representations.\n\n\nChapter 15: Representation Learning Greedy layer-wise pretraining learns meaningful representations through unsupervised learning of hierarchical features, providing better initialization than random weights. Transfer learning enables knowledge sharing across tasks by reusing learned representations‚Äîgeneric early-layer features transfer well while late layers adapt to task-specific patterns. Semi-supervised learning leverages both labeled and unlabeled data to discover disentangled causal factors that generate observations. Distributed representations enable exponential gains in capacity through shared features rather than symbolic codes.\n\n\n\nSee all Deep Learning chapters ‚Üí\n\n\n\nüìê MIT 18.06SC Linear Algebra 36 lectures\nMy journey through MIT‚Äôs Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\n\nSee all MIT 18.06SC lectures ‚Üí\n\n\n\nüìê MIT 18.065: Linear Algebra Applications 14 lectures\nMy notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning‚Äîexploring how linear algebra powers modern applications.\n\n\nLecture 14: Low Rank Changes and Their Inverse Low-rank updates reuse an existing inverse via the Sherman‚ÄìMorrison‚ÄìWoodbury identity. Rank-1 updates power recursive least squares; the full SMW formula updates \\((A-UV^\\top)^{-1}\\) in terms of \\(A^{-1}\\), avoiding a full recomputation.\n\n\nLecture 13: Randomized Matrix Multiplication Randomized matrix multiplication approximates \\(AB\\) by sampling rank-1 outer products. The estimator remains unbiased, and the variance is minimized by sampling indices proportional to \\(\\|a_j\\|\\,\\|b_j\\|\\). This yields a low-rank approximation with predictable error decay in \\(1/\\sqrt{s}\\).\n\n\nLecture 12: Computing Eigenvalues and Eigenvectors Eigenvalues are defined by \\(\\det(A-\\lambda I)=0\\) but computed by stable similarity transformations. QR iteration transforms matrices toward triangular form where eigenvalues appear on the diagonal; shifted QR with Wilkinson shifts accelerates convergence. Reduction to Hessenberg (or tridiagonal for symmetric matrices) enables efficient iteration. For large sparse problems, Krylov subspace methods (Arnoldi/Lanczos) approximate eigenvalues using only matrix-vector products.\n\n\nLecture 11: Minimize ||x|| subject to Ax=b Three norms produce different solution geometries: L1 promotes sparsity (diamond touches constraint at corner), L2 yields smooth isotropic solutions (circle tangency), and L‚àû balances components (square equalization). QR factorization via Gram‚ÄìSchmidt computes minimum-norm solutions; column pivoting improves numerical stability. Krylov subspace methods solve large sparse systems via matrix‚Äìvector products.\n\n\n\nSee all MIT 18.065 lectures ‚Üí\n\n\n\nüìê Stanford EE 364A: Convex Optimization 13 lectures\nMy notes from Stanford EE 364A: Convex Optimization‚Äîtheory and applications of optimization problems.\n\n\nChapter 4.6: Generalized Inequality Constraints Generalized inequalities replace componentwise order with cone-induced order. Cone programs unify LP and SOCP, while SDPs use positive semidefinite cones to express linear matrix inequalities.\n\n\nChapter 4.5: Geometric Programming Geometric programming uses monomials and posynomials over positive variables; a log change of variables turns constraints into convex log-sum-exp and affine forms, enabling global optimization with applications like cantilever beam design.\n\n\nChapter 4.4: Quadratic Optimization Problems Quadratic programs minimize convex quadratic objectives with affine constraints; QCQPs allow quadratic inequalities. SOCPs encode norm constraints and unify LP/QP/QCQP formulations. Robust LP handles uncertainty using ellipsoidal sets or Gaussian chance constraints, producing SOCP reformulations.\n\n\nChapter 4.3: Linear Optimization Linear programs (LP) minimize linear objectives subject to affine constraints. Geometric interpretation: level curves orthogonal to objective vector \\(c\\) touch polyhedron feasible set. Applications: diet problem (minimize cost meeting nutritional requirements), piecewise-linear minimization (epigraph formulation \\(\\min t\\) s.t. \\(a_i^\\top x+b_i\\le t\\)), Chebyshev center (largest inscribed ball), linear fractional programs (convert to LP via \\(y=xz\\), \\(z=1/(e^\\top x+f)\\)). Von Neumann growth model: maximize minimum sectoral growth rate using quasiconvex bisection.\n\n\n\nSee all EE 364A lectures ‚Üí"
  },
  {
    "objectID": "index.html#more-topics",
    "href": "index.html#more-topics",
    "title": "ickma.dev",
    "section": "More Topics",
    "text": "More Topics\n\n\nMachine Learning\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\nAlgorithms\n\nDP Regex"
  },
  {
    "objectID": "Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.html",
    "href": "Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.html",
    "title": "EE 364A (Convex Optimization): Chapter 4.6 - Generalized Inequality Constraints",
    "section": "",
    "text": "Standard convex optimization uses componentwise inequalities (e.g., \\(f_i(x) \\le 0\\)). A powerful generalization replaces componentwise order with a cone-induced order, letting inequality constraints be vector-valued and expressed with respect to a proper cone. This unifies LP, SOCP, and SDP under one framework.\n\n\nLet \\(K \\subseteq \\mathbb{R}^k\\) be a proper cone (closed, convex, solid, pointed). Define the generalized inequality \\[\nu \\preceq_K w \\quad \\Longleftrightarrow \\quad w-\\nu \\in K,\\] with strict inequality \\(\\nu \\prec_K w\\) meaning \\(w-\\nu \\in \\mathrm{int}\\,K\\).\nWhen \\(K = \\mathbb{R}_+^k\\), this reduces to componentwise inequality.\n\n\n\nA standard-form convex problem with generalized inequalities is \\[\n\\begin{aligned}\n\\text{minimize} &\\quad f_0(x) \\\\\n\\text{subject to} &\\quad f_i(x) \\preceq_{K_i} 0,\\quad i=1,\\dots,m \\\\\n&\\quad Ax=b,\n\\end{aligned}\n\\] where each \\(K_i\\) is a proper cone and each \\(f_i: \\mathbb{R}^n \\to \\mathbb{R}^{k_i}\\) is \\(K_i\\)-convex, i.e. \\[\nf_i(\\theta x + (1-\\theta)y) \\preceq_{K_i} \\theta f_i(x) + (1-\\theta) f_i(y),\\quad \\theta\\in[0,1].\n\\]\nKey properties carry over from ordinary convex problems: - Feasible set, sublevel sets, and optimal set are convex. - Any local optimum is global. - First-order optimality conditions for differentiable \\(f_0\\) remain valid.\n\n\n\nA basic generalized-inequality problem is the conic form: \\[\n\\begin{aligned}\n\\text{minimize} &\\quad c^\\top x \\\\\n\\text{subject to} &\\quad Fx+g \\preceq_K 0 \\\\\n&\\quad Ax=b.\n\\end{aligned}\n\\]\nIf \\(K=\\mathbb{R}_+^m\\), this reduces to a linear program. The standard form is \\[\n\\begin{aligned}\n\\text{minimize} &\\quad c^\\top x \\\\\n\\text{subject to} &\\quad x \\succeq_K 0 \\\\\n&\\quad Ax=b,\n\\end{aligned}\n\\] which generalizes LP standard form by replacing \\(x\\ge 0\\) with \\(x\\in K\\).\n\n\n\nWhen \\(K = S_+^k\\) (the cone of positive semidefinite matrices), conic form becomes an SDP. A common formulation is \\[\n\\begin{aligned}\n\\text{minimize} &\\quad c^\\top x \\\\\n\\text{subject to} &\\quad x_1 F_1 + \\cdots + x_n F_n + G \\succeq 0 \\\\\n&\\quad Ax=b,\n\\end{aligned}\n\\] where \\(F_i, G \\in S^k\\).\nAn equivalent standard SDP form uses a matrix variable \\(X \\in S^k\\): \\[\n\\begin{aligned}\n\\text{minimize} &\\quad \\mathrm{tr}(CX) \\\\\n\\text{subject to} &\\quad \\mathrm{tr}(A_i X)=b_i,\\quad i=1,\\dots,p \\\\\n&\\quad X \\succeq 0.\n\\end{aligned}\n\\] If all \\(F_i\\) and \\(G\\) are diagonal, the LMI reduces to componentwise inequalities, and the SDP collapses to an LP.\n\n\n\nGeneralized inequalities let us express constraints in the right geometry for the problem. Conic form captures many convex models, and specialized cones (second-order cone, semidefinite cone, etc.) give efficient algorithms without sacrificing convexity or global optimality."
  },
  {
    "objectID": "Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.html#generalized-inequality",
    "href": "Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.html#generalized-inequality",
    "title": "EE 364A (Convex Optimization): Chapter 4.6 - Generalized Inequality Constraints",
    "section": "",
    "text": "Let \\(K \\subseteq \\mathbb{R}^k\\) be a proper cone (closed, convex, solid, pointed). Define the generalized inequality \\[\nu \\preceq_K w \\quad \\Longleftrightarrow \\quad w-\\nu \\in K,\\] with strict inequality \\(\\nu \\prec_K w\\) meaning \\(w-\\nu \\in \\mathrm{int}\\,K\\).\nWhen \\(K = \\mathbb{R}_+^k\\), this reduces to componentwise inequality."
  },
  {
    "objectID": "Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.html#convex-problems-with-generalized-inequalities",
    "href": "Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.html#convex-problems-with-generalized-inequalities",
    "title": "EE 364A (Convex Optimization): Chapter 4.6 - Generalized Inequality Constraints",
    "section": "",
    "text": "A standard-form convex problem with generalized inequalities is \\[\n\\begin{aligned}\n\\text{minimize} &\\quad f_0(x) \\\\\n\\text{subject to} &\\quad f_i(x) \\preceq_{K_i} 0,\\quad i=1,\\dots,m \\\\\n&\\quad Ax=b,\n\\end{aligned}\n\\] where each \\(K_i\\) is a proper cone and each \\(f_i: \\mathbb{R}^n \\to \\mathbb{R}^{k_i}\\) is \\(K_i\\)-convex, i.e. \\[\nf_i(\\theta x + (1-\\theta)y) \\preceq_{K_i} \\theta f_i(x) + (1-\\theta) f_i(y),\\quad \\theta\\in[0,1].\n\\]\nKey properties carry over from ordinary convex problems: - Feasible set, sublevel sets, and optimal set are convex. - Any local optimum is global. - First-order optimality conditions for differentiable \\(f_0\\) remain valid."
  },
  {
    "objectID": "Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.html#conic-form-problems-cone-programs",
    "href": "Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.html#conic-form-problems-cone-programs",
    "title": "EE 364A (Convex Optimization): Chapter 4.6 - Generalized Inequality Constraints",
    "section": "",
    "text": "A basic generalized-inequality problem is the conic form: \\[\n\\begin{aligned}\n\\text{minimize} &\\quad c^\\top x \\\\\n\\text{subject to} &\\quad Fx+g \\preceq_K 0 \\\\\n&\\quad Ax=b.\n\\end{aligned}\n\\]\nIf \\(K=\\mathbb{R}_+^m\\), this reduces to a linear program. The standard form is \\[\n\\begin{aligned}\n\\text{minimize} &\\quad c^\\top x \\\\\n\\text{subject to} &\\quad x \\succeq_K 0 \\\\\n&\\quad Ax=b,\n\\end{aligned}\n\\] which generalizes LP standard form by replacing \\(x\\ge 0\\) with \\(x\\in K\\)."
  },
  {
    "objectID": "Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.html#semidefinite-programming-sdp",
    "href": "Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.html#semidefinite-programming-sdp",
    "title": "EE 364A (Convex Optimization): Chapter 4.6 - Generalized Inequality Constraints",
    "section": "",
    "text": "When \\(K = S_+^k\\) (the cone of positive semidefinite matrices), conic form becomes an SDP. A common formulation is \\[\n\\begin{aligned}\n\\text{minimize} &\\quad c^\\top x \\\\\n\\text{subject to} &\\quad x_1 F_1 + \\cdots + x_n F_n + G \\succeq 0 \\\\\n&\\quad Ax=b,\n\\end{aligned}\n\\] where \\(F_i, G \\in S^k\\).\nAn equivalent standard SDP form uses a matrix variable \\(X \\in S^k\\): \\[\n\\begin{aligned}\n\\text{minimize} &\\quad \\mathrm{tr}(CX) \\\\\n\\text{subject to} &\\quad \\mathrm{tr}(A_i X)=b_i,\\quad i=1,\\dots,p \\\\\n&\\quad X \\succeq 0.\n\\end{aligned}\n\\] If all \\(F_i\\) and \\(G\\) are diagonal, the LMI reduces to componentwise inequalities, and the SDP collapses to an LP."
  },
  {
    "objectID": "Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.html#why-this-matters",
    "href": "Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.html#why-this-matters",
    "title": "EE 364A (Convex Optimization): Chapter 4.6 - Generalized Inequality Constraints",
    "section": "",
    "text": "Generalized inequalities let us express constraints in the right geometry for the problem. Conic form captures many convex models, and specialized cones (second-order cone, semidefinite cone, etc.) give efficient algorithms without sacrificing convexity or global optimality."
  },
  {
    "objectID": "Math/index.html",
    "href": "Math/index.html",
    "title": "Math",
    "section": "",
    "text": "Mathematical foundations and explorations.\n\n\n\nThe Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots\n\n\n\n\n\nLecture 1: Geometry of Linear Equations\nLecture 2: Elimination with Matrices\nLecture 3: Matrix Multiplication and Inverse\nLecture 4: LU Decomposition\nLecture 5.1: Permutation Matrices\nLecture 5.2: Transpose\nLecture 5.3: Vector Spaces\nLecture 6: Column Space and Null Space\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nLecture 9: Independence, Basis, and Dimension\nLecture 10: Four Fundamental Subspaces\nLecture 11: Matrix Spaces, Rank-1, and Graphs\nLecture 12: Graphs, Networks, and Incidence Matrices\nLecture 13: Quiz 1 Review\nLecture 14: Orthogonal Vectors and Subspaces\nLecture 15: Projection onto Subspaces\nLecture 16: Projection Matrices and Least Squares\nLecture 17: Orthogonal Matrices and Gram-Schmidt\nLecture 18: Properties of Determinants\nLecture 19: Determinant Formulas and Cofactors\nLecture 20: Cramer‚Äôs Rule, Inverse Matrix, and Volume\nLecture 21: Eigenvalues and Eigenvectors\nLecture 22: Diagonalization and Powers of A\nLecture 23: Differential Equations and exp(At)\n\n\n\n\n\nLecture 1: The Column Space of A Contains All Vectors Ax\nLecture 2: Multiplying and Factoring Matrices\nLecture 8: Norms of Vectors and Matrices\nLecture 9: Four Ways to Solve Least Squares Problems"
  },
  {
    "objectID": "Math/index.html#reflections-synthesis",
    "href": "Math/index.html#reflections-synthesis",
    "title": "Math",
    "section": "",
    "text": "The Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots"
  },
  {
    "objectID": "Math/index.html#mit-18.06sc-linear-algebra",
    "href": "Math/index.html#mit-18.06sc-linear-algebra",
    "title": "Math",
    "section": "",
    "text": "Lecture 1: Geometry of Linear Equations\nLecture 2: Elimination with Matrices\nLecture 3: Matrix Multiplication and Inverse\nLecture 4: LU Decomposition\nLecture 5.1: Permutation Matrices\nLecture 5.2: Transpose\nLecture 5.3: Vector Spaces\nLecture 6: Column Space and Null Space\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nLecture 9: Independence, Basis, and Dimension\nLecture 10: Four Fundamental Subspaces\nLecture 11: Matrix Spaces, Rank-1, and Graphs\nLecture 12: Graphs, Networks, and Incidence Matrices\nLecture 13: Quiz 1 Review\nLecture 14: Orthogonal Vectors and Subspaces\nLecture 15: Projection onto Subspaces\nLecture 16: Projection Matrices and Least Squares\nLecture 17: Orthogonal Matrices and Gram-Schmidt\nLecture 18: Properties of Determinants\nLecture 19: Determinant Formulas and Cofactors\nLecture 20: Cramer‚Äôs Rule, Inverse Matrix, and Volume\nLecture 21: Eigenvalues and Eigenvectors\nLecture 22: Diagonalization and Powers of A\nLecture 23: Differential Equations and exp(At)"
  },
  {
    "objectID": "Math/index.html#mit-18.065-linear-algebra-applications",
    "href": "Math/index.html#mit-18.065-linear-algebra-applications",
    "title": "Math",
    "section": "",
    "text": "Lecture 1: The Column Space of A Contains All Vectors Ax\nLecture 2: Multiplying and Factoring Matrices\nLecture 8: Norms of Vectors and Matrices\nLecture 9: Four Ways to Solve Least Squares Problems"
  }
]