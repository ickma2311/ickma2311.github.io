[
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strang‚Äôs fourth lecture introduces one of the most important matrix factorizations: LU decomposition, which factors any invertible matrix \\(A\\) into the product of a Lower triangular matrix and an Upper triangular matrix. This factorization is the foundation of efficient numerical linear algebra."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#context",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#context",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strang‚Äôs fourth lecture introduces one of the most important matrix factorizations: LU decomposition, which factors any invertible matrix \\(A\\) into the product of a Lower triangular matrix and an Upper triangular matrix. This factorization is the foundation of efficient numerical linear algebra."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#what-is-lu-decomposition",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#what-is-lu-decomposition",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "What is LU Decomposition?",
    "text": "What is LU Decomposition?\nGoal: Factor any invertible matrix \\(A\\) as the product of: - \\(L\\) = Lower triangular matrix (with 1‚Äôs on diagonal) - \\(U\\) = Upper triangular matrix (the result of elimination)\n\\[\nA = LU\n\\]\n\nWhy is this useful?\n\nEfficient solving: \\(Ax = b\\) becomes two simpler triangular solves:\nStep 1 - Forward substitution: Solve \\(Lc = b\\) for \\(c\\)\nStep 2 - Back substitution: Solve \\(Ux = c\\) for \\(x\\)\nHow this works:\nSince \\(A = LU\\), we have \\(Ax = LUx = b\\). Let \\(Ux = c\\), then: \\[\nLUx = Lc = b\n\\]\nForward substitution (solving \\(Lc = b\\)):\nSince \\(L\\) is lower triangular with 1‚Äôs on the diagonal, we can solve for \\(c\\) step by step: \\[\n\\begin{aligned}\nc_1 &= b_1 \\\\\nc_2 &= b_2 - m_{21}c_1 \\\\\nc_3 &= b_3 - m_{31}c_1 - m_{32}c_2 \\\\\n&\\vdots\n\\end{aligned}\n\\]\nEach \\(c_i\\) depends only on previously computed values, so we solve forward from \\(c_1\\) to \\(c_n\\).\nBack substitution (solving \\(Ux = c\\)):\nSince \\(U\\) is upper triangular, we solve backward from \\(x_n\\) to \\(x_1\\): \\[\n\\begin{aligned}\nx_n &= \\frac{c_n}{u_{nn}} \\\\\nx_{n-1} &= \\frac{c_{n-1} - u_{n-1,n}x_n}{u_{n-1,n-1}} \\\\\n&\\vdots\n\\end{aligned}\n\\]\nResult: We‚Äôve solved \\(Ax = b\\) without ever explicitly computing \\(A^{-1}\\)!\nReusable factorization: When \\(A\\) is fixed but \\(b\\) changes, we can reuse \\(L\\) and \\(U\\)\n\nFactorization: \\(O(n^3)\\) operations (done once)\nEach solve: \\(O(n^2)\\) operations\nHuge savings for multiple right-hand sides!\n\nFoundation of numerical computing: Used in MATLAB, NumPy, and all scientific computing libraries"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#how-elimination-creates-u",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#how-elimination-creates-u",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "How Elimination Creates U",
    "text": "How Elimination Creates U\n\nThe Elimination Process\nStarting with \\(A\\), we apply elimination matrices \\(E_{21}, E_{31}, E_{32}, \\ldots\\) to get upper triangular \\(U\\):\n\\[\nE_{32} E_{31} E_{21} A = U\n\\]\nExample (3√ó3 case):\n\\[\nA = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\n\\]\nStep 1: Eliminate below first pivot (rows 2 and 3)\n\\[\nE_{21} = \\begin{bmatrix} 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad\nE_{31} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix}\n\\]\nStep 2: Eliminate below second pivot (row 3)\n\\[\nE_{32} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -1 & 1 \\end{bmatrix}\n\\]\n\n\nStructure of Elimination Matrices\nAn elimination matrix \\(E_{ij}\\) eliminates the entry at position \\((i,j)\\) by subtracting a multiple of row \\(j\\) from row \\(i\\).\nGeneral form: \\[\nE_{ij} = I - m_{ij} \\mathbf{e}_i \\mathbf{e}_j^T\n\\]\nwhere: - \\(m_{ij}\\) = multiplier = \\(\\frac{A_{ij}}{\\text{pivot at } (j,j)}\\) - \\(\\mathbf{e}_i\\) = \\(i\\)-th standard basis vector - The \\((i,j)\\) entry of \\(E_{ij}\\) is \\(-m_{ij}\\)\nKey properties: 1. Lower triangular (operates below diagonal) 2. Determinant = 1 (doesn‚Äôt change volume) 3. Easy to invert: \\(E_{ij}^{-1} = I + m_{ij} \\mathbf{e}_i \\mathbf{e}_j^T\\) (just flip the sign!)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#inverting-to-get-l-the-key-insight",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#inverting-to-get-l-the-key-insight",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Inverting to Get L: The Key Insight",
    "text": "Inverting to Get L: The Key Insight\nFrom elimination, we have:\n\\[\nE_{32} E_{31} E_{21} A = U\n\\]\nMultiply both sides by the inverses (in reverse order):\n\\[\nA = E_{21}^{-1} E_{31}^{-1} E_{32}^{-1} U = LU\n\\]\nwhere: \\[\nL = E_{21}^{-1} E_{31}^{-1} E_{32}^{-1}\n\\]\n\nThe Beautiful Result\nWhen elimination matrices are multiplied in the right order, their inverses combine beautifully:\n\\[\nL = \\begin{bmatrix}\n1 & 0 & 0 & \\cdots \\\\\nm_{21} & 1 & 0 & \\cdots \\\\\nm_{31} & m_{32} & 1 & \\cdots \\\\\n\\vdots & \\vdots & \\ddots & \\ddots\n\\end{bmatrix}\n\\]\nThe multipliers \\(m_{ij}\\) (used during elimination) directly fill in the entries of \\(L\\) below the diagonal!\nNo extra computation needed ‚Äî just save the multipliers as you eliminate."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#computational-complexity",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#computational-complexity",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Computational Complexity",
    "text": "Computational Complexity\n\nOperation Counts\nFor an \\(n \\times n\\) matrix:\n\n\n\nStep\nOperations\nOrder\n\n\n\n\nElimination (find U)\n\\(\\frac{n^3}{3} + O(n^2)\\)\n\\(O(n^3)\\)\n\n\nForward substitution \\((Lc = b)\\)\n\\(\\frac{n^2}{2}\\)\n\\(O(n^2)\\)\n\n\nBack substitution \\((Ux = c)\\)\n\\(\\frac{n^2}{2}\\)\n\\(O(n^2)\\)\n\n\n\n\n\nWhy \\(\\frac{n^3}{3}\\)?\nAt step \\(k\\), we update an \\((n-k) \\times (n-k)\\) submatrix:\n\\[\n\\text{Total operations} = \\sum_{k=1}^{n-1} (n-k)^2 \\approx \\int_0^n x^2 \\, dx = \\frac{n^3}{3}\n\\]\n\n\nWhen is LU Worth It?\nSingle solve: \\(Ax = b\\) costs \\(O(n^3)\\) either way\nMultiple solves: If solving \\(Ax = b_1, Ax = b_2, \\ldots, Ax = b_m\\): - Without LU: \\(m \\times O(n^3)\\) - With LU: \\(O(n^3)\\) (once) + \\(m \\times O(n^2)\\) ‚úÖ\nHuge savings when \\(A\\) is fixed but \\(b\\) changes!"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#hands-on-exercises",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#hands-on-exercises",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Hands-On Exercises",
    "text": "Hands-On Exercises\nLet‚Äôs practice LU decomposition with concrete examples.\n\n\nShow code\nimport numpy as np\n\nprint(\"‚úì Libraries imported successfully\")\n\n\n‚úì Libraries imported successfully\n\n\n\nExercise 1: Manual LU Decomposition (2√ó2)\nCompute the LU decomposition of \\(A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}\\) by hand.\nSteps: 1. Perform elimination to get \\(U\\) 2. Record the multiplier \\(m_{21}\\) to build \\(L\\) 3. Verify \\(A = LU\\)\n\n\nShow code\nfrom IPython.display import display, Markdown, Latex\n\n# Original matrix\nA = np.array([[2, 3],\n              [4, 7]])\n\ndisplay(Markdown(\"**Original matrix A:**\"))\ndisplay(Latex(r\"$$A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}$$\"))\n\n# Compute multiplier m21\nm21 = 4/2  # row2[0] / row1[0]\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 1: Compute multiplier**\"))\ndisplay(Latex(f\"$$m_{{21}} = \\\\frac{{4}}{{2}} = {m21}$$\"))\n\n# Build L matrix\nL = np.array([[1, 0],\n              [m21, 1]])\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 2: Build L matrix**\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 \\\\ m_{21} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}$$\"))\n\n# Build U matrix (result after elimination)\n# After: row2 = row2 - m21*row1\n# [2, 3]        [2, 3]\n# [4, 7]  --&gt;   [0, 1]  (because 7 - 2*3 = 1)\nU = np.array([[2, 3],\n              [0, 1]])\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 3: Build U matrix (after elimination)**\"))\ndisplay(Markdown(\"Row 2 ‚Üí Row 2 - 2 √ó Row 1\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Verification: $A = LU$**\"))\ndisplay(Latex(r\"$$LU = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix} = A \\quad \\checkmark$$\"))\n\n\nOriginal matrix A:\n\n\n\\[A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}\\]\n\n\n\n\n\nStep 1: Compute multiplier\n\n\n\\[m_{21} = \\frac{4}{2} = 2.0\\]\n\n\n\n\n\nStep 2: Build L matrix\n\n\n\\[L = \\begin{bmatrix} 1 & 0 \\\\ m_{21} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nStep 3: Build U matrix (after elimination)\n\n\nRow 2 ‚Üí Row 2 - 2 √ó Row 1\n\n\n\\[U = \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(A = LU\\)\n\n\n\\[LU = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix} = A \\quad \\checkmark\\]\n\n\nKey observation: The multiplier \\(m_{21} = 2\\) goes directly into position \\((2,1)\\) of \\(L\\)!\n\n\nExercise 2: LU Decomposition (3√ó3)\nPerform LU decomposition on:\n\\[\nA = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\n\\]\nGoal: Find \\(L\\) and \\(U\\) such that \\(A = LU\\)\n\n\nShow code\nfrom IPython.display import display, Markdown, Latex\n\nA = np.array([[2, 1, 1],\n              [4, -6, 0],\n              [-2, 7, 2]])\n\ndisplay(Markdown(\"**Original matrix A:**\"))\ndisplay(Latex(r\"$$A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Step 1: Eliminate column 1\"))\n\n# Calculate multipliers for column 1\nm21 = A[1, 0] / A[0, 0]  # 4/2 = 2\nm31 = A[2, 0] / A[0, 0]  # -2/2 = -1\n\ndisplay(Markdown(\"**Multipliers:**\"))\ndisplay(Latex(f\"$$m_{{21}} = \\\\frac{{4}}{{2}} = {m21}, \\\\quad m_{{31}} = \\\\frac{{-2}}{{2}} = {m31}$$\"))\n\n# Create A1 after first elimination\nA1 = A.copy().astype(float)\nA1[1] = A1[1] - m21 * A1[0]  # row2 - 2*row1\nA1[2] = A1[2] - m31 * A1[0]  # row3 - (-1)*row1\n\ndisplay(Markdown(\"**After eliminating column 1:**\"))\ndisplay(Markdown(\"- Row 2 ‚Üí Row 2 - 2 √ó Row 1\"))\ndisplay(Markdown(\"- Row 3 ‚Üí Row 3 - (-1) √ó Row 1\"))\ndisplay(Latex(r\"$$A^{(1)} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 8 & 3 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Step 2: Eliminate column 2\"))\n\n# Calculate multiplier for column 2\nm32 = A1[2, 1] / A1[1, 1]  # 8/(-8) = -1\n\ndisplay(Markdown(\"**Multiplier:**\"))\ndisplay(Latex(f\"$$m_{{32}} = \\\\frac{{8}}{{-8}} = {m32}$$\"))\n\n# Create U (final upper triangular)\nU = A1.copy()\nU[2] = U[2] - m32 * U[1]  # row3 - (-1)*row2\n\ndisplay(Markdown(\"**After eliminating column 2:**\"))\ndisplay(Markdown(\"- Row 3 ‚Üí Row 3 - (-1) √ó Row 2\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Build L from multipliers\"))\n\n# Build L from multipliers\nL = np.array([[1, 0, 0],\n              [m21, 1, 0],\n              [m31, m32, 1]])\n\ndisplay(Markdown(\"The multipliers directly fill in $L$:\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 & 0 \\\\ m_{21} & 1 & 0 \\\\ m_{31} & m_{32} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Verification: $A = LU$\"))\n\ndisplay(Latex(r\"$$LU = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix} = A \\quad \\checkmark$$\"))\n\n\nOriginal matrix A:\n\n\n\\[A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\\]\n\n\n\n\n\nStep 1: Eliminate column 1\n\n\nMultipliers:\n\n\n\\[m_{21} = \\frac{4}{2} = 2.0, \\quad m_{31} = \\frac{-2}{2} = -1.0\\]\n\n\nAfter eliminating column 1:\n\n\n\nRow 2 ‚Üí Row 2 - 2 √ó Row 1\n\n\n\n\nRow 3 ‚Üí Row 3 - (-1) √ó Row 1\n\n\n\n\\[A^{(1)} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 8 & 3 \\end{bmatrix}\\]\n\n\n\n\n\nStep 2: Eliminate column 2\n\n\nMultiplier:\n\n\n\\[m_{32} = \\frac{8}{-8} = -1.0\\]\n\n\nAfter eliminating column 2:\n\n\n\nRow 3 ‚Üí Row 3 - (-1) √ó Row 2\n\n\n\n\\[U = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nBuild L from multipliers\n\n\nThe multipliers directly fill in \\(L\\):\n\n\n\\[L = \\begin{bmatrix} 1 & 0 & 0 \\\\ m_{21} & 1 & 0 \\\\ m_{31} & m_{32} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(A = LU\\)\n\n\n\\[LU = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix} = A \\quad \\checkmark\\]\n\n\nKey observation: All three multipliers \\((m_{21}, m_{31}, m_{32})\\) go directly into their corresponding positions in \\(L\\):\n\\[\nL = \\begin{bmatrix}\n1 & 0 & 0 \\\\\nm_{21} & 1 & 0 \\\\\nm_{31} & m_{32} & 1\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n-1 & -1 & 1\n\\end{bmatrix}\n\\]\n\nNote: In practice, numerical libraries like SciPy provide scipy.linalg.lu() which computes LU decomposition efficiently and includes automatic row permutation (pivoting) for numerical stability.\n\n\nShow code\nfrom scipy.linalg import lu\nfrom IPython.display import display, Markdown, Latex\n\nA = np.array([[2, 1, 1],\n              [4, -6, 0],\n              [-2, 7, 2]], dtype=float)\n\n# SciPy returns P, L, U where PA = LU (P is permutation matrix)\nP, L_scipy, U_scipy = lu(A)\n\ndisplay(Markdown(\"**SciPy's LU decomposition:**\"))\ndisplay(Markdown(\"SciPy returns $P$, $L$, $U$ where $PA = LU$ ($P$ is a permutation matrix)\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Permutation matrix P:**\"))\ndisplay(Latex(r\"$$P = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\ndisplay(Markdown(\"(This swaps rows 1 and 2 for numerical stability)\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Lower triangular L:**\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0.5 & 1 & 0 \\\\ -0.5 & 1 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Upper triangular U:**\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 4 & -6 & 0 \\\\ 0 & 4 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Verification: $PA = LU$**\"))\n\n# Note: If P = I (identity), then our manual L and U should match\nif np.allclose(P, np.eye(3)):\n    display(Markdown(\"‚úì No row swaps needed! Our manual $L$ and $U$ match SciPy.\"))\nelse:\n    display(Markdown(\"‚ö† **Row swaps were performed** (pivot strategy for numerical stability).\"))\n    display(Markdown(\"SciPy chose the largest pivot to minimize rounding errors.\"))\n    display(Markdown(\"Our manual decomposition is valid but uses a different pivot order.\"))\n\n\nSciPy‚Äôs LU decomposition:\n\n\nSciPy returns \\(P\\), \\(L\\), \\(U\\) where \\(PA = LU\\) (\\(P\\) is a permutation matrix)\n\n\n\n\n\nPermutation matrix P:\n\n\n\\[P = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n(This swaps rows 1 and 2 for numerical stability)\n\n\n\n\n\nLower triangular L:\n\n\n\\[L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0.5 & 1 & 0 \\\\ -0.5 & 1 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nUpper triangular U:\n\n\n\\[U = \\begin{bmatrix} 4 & -6 & 0 \\\\ 0 & 4 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(PA = LU\\)\n\n\n‚ö† Row swaps were performed (pivot strategy for numerical stability).\n\n\nSciPy chose the largest pivot to minimize rounding errors.\n\n\nOur manual decomposition is valid but uses a different pivot order."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "This is for MIT 18.06SC Lecture 1, covering how to understand linear systems from two perspectives: geometry (row picture) and algebra (column picture)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#the-example-system",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#the-example-system",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "The Example System",
    "text": "The Example System\nLet‚Äôs work with this concrete example:\n\\[\\begin{align}\nx + 2y &= 5 \\\\\n3x + 4y &= 6\n\\end{align}\\]\nIn matrix form: \\[\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nWe can interpret this system in two completely different ways."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#row-picture-geometry",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#row-picture-geometry",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Row Picture (Geometry)",
    "text": "Row Picture (Geometry)\nIn the row picture, each equation represents a geometric object: - In 2D: each equation is a line - In 3D: each equation is a plane\n- In higher dimensions: each equation is a hyperplane\nThe solution is where all these objects intersect.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the equations in the form y = mx + c\n# Line 1: x + 2y = 5  =&gt;  y = -1/2*x + 5/2\n# Line 2: 3x + 4y = 6  =&gt;  y = -3/4*x + 3/2\nx = np.linspace(-10, 10, 100)\ny1 = -1/2 * x + 5/2\ny2 = -3/4 * x + 3/2\n\n# Solve for intersection point\nA = np.array([[1, 2], [3, 4]])\nb = np.array([5, 6])\nsolution = np.linalg.solve(A, b)\n\n# Plot both lines and intersection\nplt.figure(figsize=(8, 6))\nplt.plot(x, y1, 'b-', label='Line 1: x + 2y = 5', linewidth=2)\nplt.plot(x, y2, 'r-', label='Line 2: 3x + 4y = 6', linewidth=2)\nplt.scatter(solution[0], solution[1], color='green', s=100, zorder=5, \n           label=f'Solution: ({solution[0]:.1f}, {solution[1]:.1f})', edgecolor='white', linewidth=2)\n\nplt.xlim(-8, 8)\nplt.ylim(-1, 8)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Row Picture: Where Lines Meet')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Solution: x = {solution[0]:.3f}, y = {solution[1]:.3f}\")\nprint(f\"Verification: {A @ solution} equals {b}\")\n\n\n\n\n\n\n\n\n\nSolution: x = -4.000, y = 4.500\nVerification: [5. 6.] equals [5 6]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#column-picture-algebra",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#column-picture-algebra",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Column Picture (Algebra)",
    "text": "Column Picture (Algebra)\nThe column picture reframes the same system as a question about vector combinations:\n\\[x \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} + y \\begin{bmatrix}2 \\\\ 4\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nInstead of asking ‚Äúwhere do lines intersect?‚Äù, we ask: ‚ÄúCan we combine these vectors to reach our target?‚Äù\n\n\nCode\n# Define column vectors and target vector\na1 = np.array([1, 3])\na2 = np.array([2, 4])\nb = np.array([5, 6])\n\n# Solve for coefficients\nA = np.column_stack([a1, a2])\nsolution = np.linalg.solve(A, b)\nx, y = solution[0], solution[1]\n\nprint(f\"Question: Can we write b as a linear combination of a‚ÇÅ and a‚ÇÇ?\")\nprint(f\"Answer: {x:.3f} √ó a‚ÇÅ + {y:.3f} √ó a‚ÇÇ = b\")\nprint(f\"Verification: {x*a1} + {y*a2} = {x*a1 + y*a2}\")\n\n# Visualize the vector construction\nplt.figure(figsize=(8, 6))\n\n# Step 1: Draw x*a1 (scaled version)\nplt.arrow(0, 0, x*a1[0], x*a1[1], head_width=0.2, head_length=0.2, \n         fc='blue', ec='blue', linewidth=3,\n         label=f'{x:.2f} √ó a‚ÇÅ')\n\n# Step 2: Draw y*a2 starting from the tip of x*a1\nplt.arrow(x*a1[0], x*a1[1], y*a2[0], y*a2[1], head_width=0.2, head_length=0.2, \n         fc='green', ec='green', linewidth=3,\n         label=f'{y:.2f} √ó a‚ÇÇ')\n\n# Show final result vector b\nplt.arrow(0, 0, b[0], b[1], head_width=0.25, head_length=0.25, \n         fc='red', ec='red', linewidth=4, alpha=0.8,\n         label=f'b = [{b[0]}, {b[1]}]')\n\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.xlim(-1, 6)\nplt.ylim(-12, 7)\nplt.xlabel('x-component')\nplt.ylabel('y-component')\nplt.title('Column Picture: Vector Combination')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\n\n\nQuestion: Can we write b as a linear combination of a‚ÇÅ and a‚ÇÇ?\nAnswer: -4.000 √ó a‚ÇÅ + 4.500 √ó a‚ÇÇ = b\nVerification: [ -4. -12.] + [ 9. 18.] = [5. 6.]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Three Types of Linear Systems",
    "text": "Three Types of Linear Systems\nLinear systems can have three possible outcomes:\n\nUnique solution - Lines intersect at one point\nNo solution - Lines are parallel (don‚Äôt intersect)\nInfinitely many solutions - Lines are the same (overlap completely)\n\n\n\nCode\n# Case (a): Unique solution - non-parallel vectors\nprint(\"üéØ Case (a) - Unique Solution:\")\nA_a = np.array([[1, 2], [3, 4]])\nb_a = np.array([5, 6])\nsolution_a = np.linalg.solve(A_a, b_a)\ndet_a = np.linalg.det(A_a)\nprint(f\"   Solution: {solution_a}\")\nprint(f\"   Matrix determinant: {det_a:.3f} ‚â† 0 ‚Üí linearly independent columns\")\nprint(f\"   Column space: ENTIRE 2D plane (any point reachable)\")\n\n# Case (b): No solution - parallel vectors, b not in span\nprint(f\"\\n‚ùå Case (b) - No Solution:\")\nA_b = np.array([[1, 2], [2, 4]])  # Columns are parallel\nb_b = np.array([5, 6])            # b not in span\ndet_b = np.linalg.det(A_b)\nprint(f\"   Matrix determinant: {det_b:.3f} = 0 ‚Üí linearly dependent columns\")\nprint(f\"   Column space: 1D line only (most points unreachable)\")\nprint(f\"   Target b = {b_b} is NOT on the line ‚Üí No solution exists\")\n\n# Case (c): Infinitely many solutions - parallel vectors, b in span\nprint(f\"\\n‚ôæÔ∏è  Case (c) - Infinitely Many Solutions:\")\nA_c = np.array([[1, 2], [2, 4]])  # Same parallel columns\nb_c = np.array([3, 6])            # b = 3 * [1, 2], so b is in span\ndet_c = np.linalg.det(A_c)\nprint(f\"   Matrix determinant: {det_c:.3f} = 0 ‚Üí linearly dependent columns\")\nprint(f\"   Column space: 1D line only\")\nprint(f\"   Target b = {b_c} IS on the line ‚Üí Infinite solutions exist\")\n\n# Find one particular solution using pseudoinverse\nsolution_c = np.linalg.pinv(A_c) @ b_c\nprint(f\"   One particular solution: {solution_c}\")\nprint(f\"   Other solutions: {solution_c} + t√ó[2, -1] for any real number t\")\n\n\nüéØ Case (a) - Unique Solution:\n   Solution: [-4.   4.5]\n   Matrix determinant: -2.000 ‚â† 0 ‚Üí linearly independent columns\n   Column space: ENTIRE 2D plane (any point reachable)\n\n‚ùå Case (b) - No Solution:\n   Matrix determinant: 0.000 = 0 ‚Üí linearly dependent columns\n   Column space: 1D line only (most points unreachable)\n   Target b = [5 6] is NOT on the line ‚Üí No solution exists\n\n‚ôæÔ∏è  Case (c) - Infinitely Many Solutions:\n   Matrix determinant: 0.000 = 0 ‚Üí linearly dependent columns\n   Column space: 1D line only\n   Target b = [3 6] IS on the line ‚Üí Infinite solutions exist\n   One particular solution: [0.6 1.2]\n   Other solutions: [0.6 1.2] + t√ó[2, -1] for any real number t\n\n\n\n\nCode\n# Visualize all three cases\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Case (a): Unique solution\nax = axes[0]\nax.fill_between([-1, 6], [-1, -1], [7, 7], color='lightblue', alpha=0.2, \n                label='Column space = ENTIRE plane')\n\n# Draw vectors\nax.arrow(0, 0, A_a[0,0], A_a[1,0], head_width=0.15, head_length=0.15,\n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,3]')\nax.arrow(0, 0, A_a[0,1], A_a[1,1], head_width=0.15, head_length=0.15,\n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4]')\nax.arrow(0, 0, b_a[0], b_a[1], head_width=0.2, head_length=0.2,\n         fc='red', ec='red', linewidth=3, label='b = [5,6]')\n\nax.set_title('Unique Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (b): No solution\nax = axes[1]\nt = np.linspace(-2, 5, 100)\nspan_x, span_y = t * A_b[0,0], t * A_b[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6, \n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_b[0,0], A_b[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,2]')\nax.arrow(0, 0, A_b[0,1], A_b[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4] = 2√óa‚ÇÅ')\nax.arrow(0, 0, b_b[0], b_b[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [5,6] (off line)')\n\nax.set_title('No Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (c): Infinitely many solutions\nax = axes[2]\nt = np.linspace(-1, 4, 100)\nspan_x, span_y = t * A_c[0,0], t * A_c[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6,\n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_c[0,0], A_c[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,2]')\nax.arrow(0, 0, A_c[0,1], A_c[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4] = 2√óa‚ÇÅ')\nax.arrow(0, 0, b_c[0], b_c[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [3,6] (on line)')\n\nax.set_title('Infinite Solutions')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key insight: Solution depends on whether target vector b lies in the column space\")\n\n\n\n\n\n\n\n\n\nKey insight: Solution depends on whether target vector b lies in the column space\n\n\n\nThis covers the core geometric foundations from MIT 18.06SC Lecture 1: understanding linear systems through both row and column perspectives."
  },
  {
    "objectID": "ML/hessian-prerequisites.html",
    "href": "ML/hessian-prerequisites.html",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "",
    "text": "My notebook\nBefore diving into optimization algorithms for deep learning (Chapter 7), we need to understand second-order derivatives in multiple dimensions. The Hessian matrix is the key tool that generalizes the concept of curvature to high-dimensional spaces."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#context",
    "href": "ML/hessian-prerequisites.html#context",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "",
    "text": "My notebook\nBefore diving into optimization algorithms for deep learning (Chapter 7), we need to understand second-order derivatives in multiple dimensions. The Hessian matrix is the key tool that generalizes the concept of curvature to high-dimensional spaces."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#why-second-derivatives-matter",
    "href": "ML/hessian-prerequisites.html#why-second-derivatives-matter",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Why Second Derivatives Matter",
    "text": "Why Second Derivatives Matter\nIn one dimension, optimizing \\(f(x)\\) involves:\n\nFirst derivative \\(f'(x) = 0\\) ‚Üí Find critical points\n\nWhy set \\(f'(x) = 0\\)? At a minimum or maximum, the slope is flat (zero)\nThink of a hill: at the very top, you stop going up ‚Üí slope = 0\nAt the bottom of a valley, you stop going down ‚Üí slope = 0\nExample: For \\(f(x) = x^2\\), we have \\(f'(x) = 2x\\). Setting \\(f'(x) = 0\\) gives \\(x = 0\\) (the minimum)\n\nSecond derivative \\(f''(x)\\) ‚Üí Classify the critical point:\n\n\\(f''(x) &gt; 0\\) ‚Üí Local minimum (curves upward like a bowl)\n\\(f''(x) &lt; 0\\) ‚Üí Local maximum (curves downward like a dome)\n\\(f''(x) = 0\\) ‚Üí Inconclusive (could be an inflection point)\nWhy needed? Not all points where \\(f'(x) = 0\\) are minima! For \\(f(x) = x^3\\), we have \\(f'(0) = 0\\) but it‚Äôs neither a min nor max.\n\n\nThe challenge: How do we extend this to functions of many variables \\(f(x_1, x_2, \\ldots, x_n)\\)?\nThe answer: The Hessian matrix captures all second-order information.\n\nVisualizing Second Derivatives\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = np.linspace(-3, 3, 200)\n\n# Three functions with different second derivatives\nf1 = x**2           # f''(x) = 2 (positive, curves up)\nf2 = -x**2          # f''(x) = -2 (negative, curves down)\nf3 = x**3           # f''(x) = 6x (changes sign at x=0)\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\n\n# Function 1: f(x) = x¬≤\naxes[0, 0].plot(x, f1, 'b-', linewidth=2)\naxes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 0].set_title(\"f(x) = x¬≤\\nf''(x) = 2 &gt; 0\\n(Curves UP)\", fontsize=10)\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel('f(x)')\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].annotate('Minimum', xy=(0, 0), xytext=(0.5, 2),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Function 2: f(x) = -x¬≤\naxes[0, 1].plot(x, f2, 'r-', linewidth=2)\naxes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 1].set_title(\"f(x) = -x¬≤\\nf''(x) = -2 &lt; 0\\n(Curves DOWN)\", fontsize=10)\naxes[0, 1].set_xlabel('x')\naxes[0, 1].set_ylabel('f(x)')\naxes[0, 1].grid(True, alpha=0.3)\naxes[0, 1].annotate('Maximum', xy=(0, 0), xytext=(0.5, -2),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Function 3: f(x) = x¬≥\naxes[1, 0].plot(x, f3, 'g-', linewidth=2)\naxes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[1, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[1, 0].set_title(\"f(x) = x¬≥\\nf''(x) = 6x\\n(Changes sign)\", fontsize=10)\naxes[1, 0].set_xlabel('x')\naxes[1, 0].set_ylabel('f(x)')\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].annotate('Inflection point', xy=(0, 0), xytext=(1, -10),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Hide the unused subplot\naxes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nKey observations:\n\n\n\n\n\n\n\n\n\nSecond Derivative\nCurvature\nShape\nPoint Type\n\n\n\n\n\\(f''(x) &gt; 0\\)\nCurves upward\nBowl shape\nPotential minimum\n\n\n\\(f''(x) &lt; 0\\)\nCurves downward\nDome shape\nPotential maximum\n\n\n\\(f''(x) = 0\\) (at critical point)\nChanges sign\nFlat at that point\nInflection point\n\n\n\nNote on the third example: For \\(f(x) = x^3\\), we have \\(f''(x) = 6x\\). At the critical point \\(x = 0\\), \\(f''(0) = 0\\), which is inconclusive. The curvature changes sign: negative for \\(x &lt; 0\\) and positive for \\(x &gt; 0\\)."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#the-hessian-matrix",
    "href": "ML/hessian-prerequisites.html#the-hessian-matrix",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "The Hessian Matrix",
    "text": "The Hessian Matrix\n\nDefinition\nFor a scalar function \\(f(\\mathbf{x}) = f(x_1, x_2, \\ldots, x_n)\\), the Hessian matrix is the square matrix of all second-order partial derivatives:\n\\[\nH(f) =\n\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n\\]\n\n\nKey Properties\n\nSymmetric: If mixed partial derivatives are continuous, then \\(\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}\\), so \\(H = H^T\\).\nShape: Always \\(n \\times n\\) (determined by number of variables, not terms in the function)\nDescribes curvature in all directions simultaneously\nEigenvalue decomposition: Since the Hessian is symmetric, it can be decomposed as \\(H = Q\\Lambda Q^T\\) where \\(Q\\) contains orthonormal eigenvectors and \\(\\Lambda\\) is a diagonal matrix of eigenvalues\n\n\n\nSimple Example\nFor \\(f(x, y) = x^2 + 3y^2\\):\nStep 1: Compute first derivatives \\[\n\\frac{\\partial f}{\\partial x} = 2x, \\quad \\frac{\\partial f}{\\partial y} = 6y\n\\]\nStep 2: Compute second derivatives \\[\n\\frac{\\partial^2 f}{\\partial x^2} = 2, \\quad \\frac{\\partial^2 f}{\\partial y^2} = 6, \\quad \\frac{\\partial^2 f}{\\partial x \\partial y} = 0\n\\]\nStep 3: Build Hessian \\[\nH = \\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}\n\\]\nInterpretation: - Curvature along \\(x\\)-axis: 2 - Curvature along \\(y\\)-axis: 6 - No cross-dependency (off-diagonal = 0)\n\n\nExample with Cross Terms\nFor \\(f(x, y) = x^2 + xy + y^2\\):\n\\[\nH = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\n\\]\nThe off-diagonal term (1) indicates that \\(x\\) and \\(y\\) are coupled‚Äîchanging one affects the rate of change of the other."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#matrix-definiteness",
    "href": "ML/hessian-prerequisites.html#matrix-definiteness",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Matrix Definiteness",
    "text": "Matrix Definiteness\nFor a symmetric matrix \\(A\\), its definiteness is determined by the signs of its eigenvalues.\n\n\n\n\n\n\n\n\n\nType\nEigenvalues\nQuadratic Form \\(x^T A x\\)\nGeometric Shape\n\n\n\n\nPositive definite (PD)\nall \\(&gt; 0\\)\n\\(&gt; 0\\) for all \\(x \\neq 0\\)\nBowl (curves upward)\n\n\nNegative definite (ND)\nall \\(&lt; 0\\)\n\\(&lt; 0\\) for all \\(x \\neq 0\\)\nDome (curves downward)\n\n\nIndefinite\nsome \\(+\\), some \\(-\\)\ndepends on direction\nSaddle\n\n\nPositive semi-definite (PSD)\nall \\(\\geq 0\\)\n\\(\\geq 0\\) for all \\(x\\)\nFlat-bottom bowl\n\n\nNegative semi-definite (NSD)\nall \\(\\leq 0\\)\n\\(\\leq 0\\) for all \\(x\\)\nFlat-top dome\n\n\n\n\nQuick Test (2√ó2 case)\nFor \\(A = \\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}\\):\n\nPositive definite if \\(a &gt; 0\\) and \\(ac - b^2 &gt; 0\\)\nNegative definite if \\(a &lt; 0\\) and \\(ac - b^2 &gt; 0\\)\nIndefinite if \\(ac - b^2 &lt; 0\\)\n\n\n\nExamples\n\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}\\): eigenvalues = [2, 6] ‚Üí Positive definite\n\\(\\begin{bmatrix} -2 & 0 \\\\ 0 & -3 \\end{bmatrix}\\): eigenvalues = [-2, -3] ‚Üí Negative definite\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\\): eigenvalues = [2, -2] ‚Üí Indefinite\n\\(\\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix}\\): eigenvalues = [4, 0] ‚Üí Positive semi-definite"
  },
  {
    "objectID": "ML/hessian-prerequisites.html#interpreting-hessian-at-critical-points",
    "href": "ML/hessian-prerequisites.html#interpreting-hessian-at-critical-points",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Interpreting Hessian at Critical Points",
    "text": "Interpreting Hessian at Critical Points\nAt a critical point where \\(\\nabla f = 0\\), the Hessian determines the nature of the point:\n\n\n\n\n\n\n\n\n\nHessian Type\nEigenvalues\nSurface Shape\nPoint Type\n\n\n\n\nPositive definite\nall positive\nBowl (convex)\nLocal minimum\n\n\nNegative definite\nall negative\nDome (concave)\nLocal maximum\n\n\nIndefinite\nmixed signs\nSaddle\nNeither min nor max\n\n\nSemi-definite\nsome zero\nFlat in some directions\nInconclusive\n\n\n\n\nVisualization: Different Surface Types\n\n\nShow code\n# Create grid for plotting\nx_grid = np.linspace(-2, 2, 100)\ny_grid = np.linspace(-2, 2, 100)\nX, Y = np.meshgrid(x_grid, y_grid)\n\n# Define different functions with different Hessian types\ndef positive_definite(x, y):\n    \"\"\"Minimum: f = x¬≤ + y¬≤\"\"\"\n    return x**2 + y**2\n\ndef negative_definite(x, y):\n    \"\"\"Maximum: f = -x¬≤ - y¬≤\"\"\"\n    return -x**2 - y**2\n\ndef indefinite(x, y):\n    \"\"\"Saddle: f = x¬≤ - y¬≤\"\"\"\n    return x**2 - y**2\n\ndef semi_definite(x, y):\n    \"\"\"Flat direction: f = x¬≤\"\"\"\n    return x**2\n\n# Create 3D surface plots in 2x2 grid\nfig = plt.figure(figsize=(12, 10))\n\nfunctions = [\n    (positive_definite, \"Positive Definite\\n(Bowl - Minimum)\", \"Greens\"),\n    (negative_definite, \"Negative Definite\\n(Dome - Maximum)\", \"Reds\"),\n    (indefinite, \"Indefinite\\n(Saddle Point)\", \"RdBu\"),\n    (semi_definite, \"Semi-Definite\\n(Flat Direction)\", \"YlOrRd\")\n]\n\nfor idx, (func, title, cmap) in enumerate(functions, 1):\n    ax = fig.add_subplot(2, 2, idx, projection='3d')\n    Z = func(X, Y)\n\n    surf = ax.plot_surface(X, Y, Z, cmap=cmap, alpha=0.8,\n                           linewidth=0, antialiased=True)\n\n    ax.set_xlabel('x', fontsize=9)\n    ax.set_ylabel('y', fontsize=9)\n    ax.set_zlabel('f(x,y)', fontsize=9)\n    ax.set_title(title, fontsize=10)\n    ax.view_init(elev=25, azim=45)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nContour plots for better understanding:\n\n\nShow code\n# Contour plots in 2x2 grid\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\naxes = axes.flatten()\n\nfor idx, (func, title, cmap) in enumerate(functions):\n    ax = axes[idx]\n    Z = func(X, Y)\n\n    contour = ax.contour(X, Y, Z, levels=15, cmap=cmap)\n    ax.clabel(contour, inline=True, fontsize=7)\n\n    # Mark the critical point at origin\n    ax.plot(0, 0, 'r*', markersize=12, label='Critical point')\n\n    ax.set_xlabel('x', fontsize=9)\n    ax.set_ylabel('y', fontsize=9)\n    ax.set_title(title, fontsize=10)\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=8)\n    ax.set_aspect('equal')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ickma.dev",
    "section": "",
    "text": "A growing collection of structured study notes and visual explanations ‚Äî written for clarity, reproducibility, and long-term memory."
  },
  {
    "objectID": "index.html#latest-updates",
    "href": "index.html#latest-updates",
    "title": "ickma.dev",
    "section": "Latest Updates",
    "text": "Latest Updates\n\n‚àá Deep Learning Book 39 chapters\nMy notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\nChapter 10.1: Unfold Computation Graph Unfolding computation graphs in RNNs enables parameter sharing across time steps. The same function with fixed parameters processes sequences of any length, compressing input history into fixed-size hidden states that retain only task-relevant information for predictions.\n\n\nChapter 9.10: Neuroscientific Basis for Convolutional Networks V1 simple cells act like Gabor filters detecting oriented edges, complex cells pool for translation invariance‚Äîinspiring CNNs. But biological vision includes saccadic attention, multisensory integration, top-down feedback, and adaptive receptive fields that CNNs lack, making animal vision far richer than feed-forward object recognition.\n\n\nChapter 9.9: Unsupervised or Semi-Supervised Feature Learning Before CNNs, computer vision relied on hand-crafted kernels (Sobel, Laplacian) and unsupervised methods (sparse coding, autoencoders, k-means). Modern CNNs surpass both by learning hierarchical features end-to-end‚Äîfrom edges to semantic concepts‚Äîoptimized for the task, making hand-crafted filters largely obsolete.\n\n\n\nSee all Deep Learning chapters ‚Üí\n\n\n\nüìê MIT 18.06SC Linear Algebra 36 lectures\nMy journey through MIT‚Äôs Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\n\nSee all MIT 18.06SC lectures ‚Üí\n\n\n\nüìê MIT 18.065: Linear Algebra Applications 2 lectures\nMy notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning‚Äîexploring how linear algebra powers modern applications.\n\n\nLecture 9: Four Ways to Solve Least Squares Problems Four equivalent methods for solving \\(Ax = b\\) when \\(A\\) has no inverse: pseudo-inverse, normal equations, algebraic minimization, and geometric projection‚Äîall converging to the same optimal solution.\n\n\nLecture 8: Norms of Vectors and Matrices Understanding vector p-norms (\\(\\|v\\|_p\\)) and when they satisfy the triangle inequality (only for \\(p \\geq 1\\)). The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù creates non-convex unit balls for strong sparsity.\n\n\n\nSee all MIT 18.065 lectures ‚Üí\n\n\n\nüìê Stanford EE 364A: Convex Optimization 3 lectures\nMy notes from Stanford EE 364A: Convex Optimization‚Äîtheory and applications of optimization problems.\n\n\nLecture 3: Convex Functions Separating and supporting hyperplane theorems, dual cones, convex function definition and examples, first-order condition (tangent underestimates), second-order condition (Hessian PSD), epigraph and sublevel sets, Jensen‚Äôs inequality, and operations preserving convexity.\n\n\nLecture 2: Convex Sets Convex sets (affine combinations, convex hull, convex cone), hyperplanes and halfspaces, Euclidean balls and ellipsoids, norm balls and norm cones, polyhedra, positive semidefinite cone. Operations that preserve convexity: intersection, affine functions, perspective, linear-fractional.\n\n\nLecture 1: Introduction to Convex Optimization Introduction to constraint optimization: least squares (\\(\\|Ax-b\\|_2^2\\)), linear programming, and convex optimization. Convex problems generalize both while maintaining polynomial-time solvability through interior-point methods.\n\n\n\nSee all EE 364A lectures ‚Üí"
  },
  {
    "objectID": "index.html#more-topics",
    "href": "index.html#more-topics",
    "title": "ickma.dev",
    "section": "More Topics",
    "text": "More Topics\n\n\nMachine Learning\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\nAlgorithms\n\nDP Regex"
  },
  {
    "objectID": "ML/xor-deep-learning.html",
    "href": "ML/xor-deep-learning.html",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.1 shows how ReLU activations let neural networks solve the XOR problem that defeats any linear model.\nüìì For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "ML/xor-deep-learning.html#the-xor-problem-a-challenge-for-linear-models",
    "href": "ML/xor-deep-learning.html#the-xor-problem-a-challenge-for-linear-models",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "The XOR Problem: A Challenge for Linear Models",
    "text": "The XOR Problem: A Challenge for Linear Models\nXOR (Exclusive OR) returns 1 precisely when the two binary inputs differ:\n\\[\\text{XOR}(x_1, x_2) = \\begin{pmatrix}0 & 1\\\\1 & 0\\end{pmatrix}\\]\nThe XOR truth table shows why this is challenging for linear models - the positive class (1) appears at diagonally opposite corners, making it impossible to separate with any single straight line.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Define XOR dataset\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([0, 1, 1, 0])\n\nprint(\"XOR Truth Table:\")\nprint(\"================\")\nprint()\nprint(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\nprint(\"‚îÇ Input   ‚îÇ Output ‚îÇ\")\nprint(\"‚îÇ (x‚ÇÅ,x‚ÇÇ) ‚îÇ  XOR   ‚îÇ\")\nprint(\"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\nfor i in range(4):\n    input_str = f\"({X[i,0]}, {X[i,1]})\"\n    output_str = f\"{y[i]}\"\n    print(f\"‚îÇ {input_str:7} ‚îÇ   {output_str:2}   ‚îÇ\")\nprint(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\nprint()\nprint(\"Notice: XOR = 1 when inputs differ, XOR = 0 when inputs match\")\n\n\nXOR Truth Table:\n================\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Input   ‚îÇ Output ‚îÇ\n‚îÇ (x‚ÇÅ,x‚ÇÇ) ‚îÇ  XOR   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ (0, 0)  ‚îÇ   0    ‚îÇ\n‚îÇ (0, 1)  ‚îÇ   1    ‚îÇ\n‚îÇ (1, 0)  ‚îÇ   1    ‚îÇ\n‚îÇ (1, 1)  ‚îÇ   0    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nNotice: XOR = 1 when inputs differ, XOR = 0 when inputs match"
  },
  {
    "objectID": "ML/xor-deep-learning.html#limitation-1-single-layer-linear-model",
    "href": "ML/xor-deep-learning.html#limitation-1-single-layer-linear-model",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Limitation 1: Single Layer Linear Model",
    "text": "Limitation 1: Single Layer Linear Model\nA single layer perceptron can only create linear decision boundaries. Let‚Äôs see what happens when we try to solve XOR with logistic regression:\n\n\nShow code\n# Demonstrate single layer linear model failure\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\ncolors = ['red', 'blue']\n\n# Plot XOR data\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'Class {i}', edgecolors='black', linewidth=2)\n\n# Overlay representative linear separators to illustrate the impossibility\nx_line = np.linspace(-0.2, 1.2, 100)\nax.plot(x_line, 0.5 * np.ones_like(x_line), '--', color='gray', alpha=0.7, label='candidate lines')\nax.plot(0.5 * np.ones_like(x_line), x_line, '--', color='orange', alpha=0.7)\nax.plot(x_line, x_line, '--', color='green', alpha=0.7)\nax.plot(x_line, 1 - x_line, '--', color='purple', alpha=0.7)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x‚ÇÅ', fontsize=12)\nax.set_ylabel('x‚ÇÇ', fontsize=12)\nax.set_title('XOR Problem: No Linear Solution', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Fit logistic regression just to report its performance\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\naccuracy = log_reg.score(X, y)\nprint(f'Single layer model accuracy: {accuracy:.1%} - still misclassifies XOR.')\n\n\n\n\n\n\n\n\n\nSingle layer model accuracy: 50.0% - still misclassifies XOR."
  },
  {
    "objectID": "ML/xor-deep-learning.html#limitation-2-multiple-layer-linear-model-without-activation",
    "href": "ML/xor-deep-learning.html#limitation-2-multiple-layer-linear-model-without-activation",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Limitation 2: Multiple Layer Linear Model (Without Activation)",
    "text": "Limitation 2: Multiple Layer Linear Model (Without Activation)\nEven stacking multiple linear layers doesn‚Äôt help! Multiple linear transformations are mathematically equivalent to a single linear transformation.\nMathematical proof:\n\\[\\text{Layer 1: } h_1 = W_1 x + b_1\\] \\[\\text{Layer 2: } h_2 = W_2 h_1 + b_2 = W_2(W_1 x + b_1) + b_2 = (W_2W_1)x + (W_2b_1 + b_2)\\]\nResult: Still just \\(Wx + b\\) (a single linear transformation)\nConclusion: Stacking linear layers without activation functions doesn‚Äôt increase the model‚Äôs expressive power!"
  },
  {
    "objectID": "ML/xor-deep-learning.html#the-solution-relu-activation-function",
    "href": "ML/xor-deep-learning.html#the-solution-relu-activation-function",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "The Solution: ReLU Activation Function",
    "text": "The Solution: ReLU Activation Function\nReLU (Rectified Linear Unit) provides the nonlinearity needed to solve XOR: - ReLU(z) = max(0, z) - Clips negative values to zero, keeping positive values unchanged\nUsing the hand-crafted network from the next code cell, the forward pass can be written compactly in matrix form:\n\\[\nX = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix},\n\\quad\nW_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix},\n\\quad\nb_1 = \\begin{bmatrix} 0 & 0 \\end{bmatrix}\n\\]\n\\[\nZ = X W_1^{\\top} + b_1 = \\begin{bmatrix} 0 & 0 \\\\ -1 & 1 \\\\ 1 & -1 \\\\ 0 & 0 \\end{bmatrix},\n\\qquad\nH = \\text{ReLU}(Z) = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \\end{bmatrix}\n\\]\nWith output parameters \\[\nw_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix},\n\\quad\nb_2 = -0.5\n\\] the final linear scores are \\[\na = H w_2^{\\top} + b_2 = \\begin{bmatrix} -0.5 \\\\ 0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}\n\\Rightarrow\n\\text{sign}_+(a) = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\n\\]\nHere \\(\\text{sign}_+(a)\\) maps non-negative entries to 1 and negative entries to 0. Let‚Äôs see how ReLU transforms the XOR problem to make it solvable.\n\n\nShow code\n# Hand-crafted network weights and biases that solve XOR\nfrom IPython.display import display, Math\n\ndef relu(z):\n    return np.maximum(0, z)\n\nW1 = np.array([[1, -1],\n               [-1, 1]])\nb1 = np.array([0, 0])\nw2 = np.array([1, 1])\nb2 = -0.5\n\ndisplay(Math(r\"\\text{Hidden layer: } W_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}\"))\ndisplay(Math(r\"b_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\"))\ndisplay(Math(r\"\\text{Output layer: } w_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix}\"))\ndisplay(Math(r\"b_2 = -0.5\"))\n\ndef forward_pass(X, W1, b1, w2, b2):\n    z1 = X @ W1.T + b1\n    h1 = relu(z1)\n    logits = h1 @ w2 + b2\n    return logits, h1, z1\n\nlogits, hidden_activations, pre_activations = forward_pass(X, W1, b1, w2, b2)\npredictions = (logits &gt;= 0).astype(int)\n\nprint(\"Step-by-step Forward Pass Results:\")\nprint(\"=\" * 80)\nprint()\nprint(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\nprint(\"‚îÇ Input   ‚îÇ  Before ReLU     ‚îÇ  After ReLU      ‚îÇ  Logit  ‚îÇ   Pred   ‚îÇ\")\nprint(\"‚îÇ (x‚ÇÅ,x‚ÇÇ) ‚îÇ    (z‚ÇÅ, z‚ÇÇ)      ‚îÇ    (h‚ÇÅ, h‚ÇÇ)      ‚îÇ  score  ‚îÇ  class   ‚îÇ\")\nprint(\"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\nfor i in range(len(X)):\n    x1, x2 = X[i]\n    z1_vals = pre_activations[i]\n    h1_vals = hidden_activations[i]\n    logit = logits[i]\n    pred = predictions[i]\n    \n    input_str = f\"({x1:.0f}, {x2:.0f})\"\n    pre_relu_str = f\"({z1_vals[0]:4.1f}, {z1_vals[1]:4.1f})\"\n    post_relu_str = f\"({h1_vals[0]:4.1f}, {h1_vals[1]:4.1f})\"\n    logit_str = f\"{logit:6.2f}\"\n    pred_str = f\"{pred:4d}\"\n    \n    print(f\"‚îÇ {input_str:7} ‚îÇ {pre_relu_str:16} ‚îÇ {post_relu_str:16} ‚îÇ {logit_str:7} ‚îÇ {pred_str:8} ‚îÇ\")\nprint(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n\naccuracy = (predictions == y).mean()\nprint(f\"\\nNetwork Accuracy: {accuracy:.0%} ‚úÖ\")\nprint(\"\\nKey transformations:\")\nprint(\"‚Ä¢ (-1, 1) ‚Üí (0, 1) makes XOR(0,1) = 1 separable\")\nprint(\"‚Ä¢ ( 1,-1) ‚Üí (1, 0) makes XOR(1,0) = 1 separable\")\n\n\n\\(\\displaystyle \\text{Hidden layer: } W_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle b_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle \\text{Output layer: } w_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle b_2 = -0.5\\)\n\n\nStep-by-step Forward Pass Results:\n================================================================================\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Input   ‚îÇ  Before ReLU     ‚îÇ  After ReLU      ‚îÇ  Logit  ‚îÇ   Pred   ‚îÇ\n‚îÇ (x‚ÇÅ,x‚ÇÇ) ‚îÇ    (z‚ÇÅ, z‚ÇÇ)      ‚îÇ    (h‚ÇÅ, h‚ÇÇ)      ‚îÇ  score  ‚îÇ  class   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ (0, 0)  ‚îÇ ( 0.0,  0.0)     ‚îÇ ( 0.0,  0.0)     ‚îÇ  -0.50  ‚îÇ    0     ‚îÇ\n‚îÇ (0, 1)  ‚îÇ (-1.0,  1.0)     ‚îÇ ( 0.0,  1.0)     ‚îÇ   0.50  ‚îÇ    1     ‚îÇ\n‚îÇ (1, 0)  ‚îÇ ( 1.0, -1.0)     ‚îÇ ( 1.0,  0.0)     ‚îÇ   0.50  ‚îÇ    1     ‚îÇ\n‚îÇ (1, 1)  ‚îÇ ( 0.0,  0.0)     ‚îÇ ( 0.0,  0.0)     ‚îÇ  -0.50  ‚îÇ    0     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nNetwork Accuracy: 100% ‚úÖ\n\nKey transformations:\n‚Ä¢ (-1, 1) ‚Üí (0, 1) makes XOR(0,1) = 1 separable\n‚Ä¢ ( 1,-1) ‚Üí (1, 0) makes XOR(1,0) = 1 separable\n\n\n\nTransformation Table: How ReLU Solves XOR\nLet‚Äôs trace through exactly what happens to each input:\n\n\nShow code\n\n# Create detailed transformation table\nprint(\"Complete Transformation Table:\")\nprint(\"=============================\")\nprint()\nprint(\"Input   | Pre-ReLU  | Post-ReLU | Logit | Prediction | Target | Correct?\")\nprint(\"(x‚ÇÅ,x‚ÇÇ) | (z‚ÇÅ, z‚ÇÇ)  | (h‚ÇÅ, h‚ÇÇ)  | score | class      | y      |\")\nprint(\"--------|-----------|-----------|-------|------------|--------|----------\")\n\nfor i in range(4):\n    input_str = f\"({X[i,0]},{X[i,1]})\"\n    pre_relu_str = f\"({pre_activations[i,0]:2.0f},{pre_activations[i,1]:2.0f})\"\n    post_relu_str = f\"({hidden_activations[i,0]:.0f},{hidden_activations[i,1]:.0f})\"\n    logit_str = f\"{logits[i]:.2f}\"\n    pred_str = f\"{predictions[i]}\"\n    target_str = f\"{y[i]}\"\n    correct_str = \"‚úì\" if predictions[i] == y[i] else \"‚úó\"\n\n    print(f\"{input_str:7} | {pre_relu_str:9} | {post_relu_str:9} | {logit_str:5} | {pred_str:10} | {target_str:6} | {correct_str}\")\n\nprint()\nprint(\"Key Insight: ReLU transforms (-1,1) ‚Üí (0,1) and (1,-1) ‚Üí (1,0)\")\nprint(\"This makes the XOR classes linearly separable in the hidden space!\")\n\n\nComplete Transformation Table:\n=============================\n\nInput   | Pre-ReLU  | Post-ReLU | Logit | Prediction | Target | Correct?\n(x‚ÇÅ,x‚ÇÇ) | (z‚ÇÅ, z‚ÇÇ)  | (h‚ÇÅ, h‚ÇÇ)  | score | class      | y      |\n--------|-----------|-----------|-------|------------|--------|----------\n(0,0)   | ( 0, 0)   | (0,0)     | -0.50 | 0          | 0      | ‚úì\n(0,1)   | (-1, 1)   | (0,1)     | 0.50  | 1          | 1      | ‚úì\n(1,0)   | ( 1,-1)   | (1,0)     | 0.50  | 1          | 1      | ‚úì\n(1,1)   | ( 0, 0)   | (0,0)     | -0.50 | 0          | 0      | ‚úì\n\nKey Insight: ReLU transforms (-1,1) ‚Üí (0,1) and (1,-1) ‚Üí (1,0)\nThis makes the XOR classes linearly separable in the hidden space!\n\n\n\n\nStep 1: Original Input Space\nThe XOR problem in its raw form - notice how no single line can separate the classes:\n\n\nShow code\n# Step 1 visualization: Original Input Space\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'XOR = {i}', edgecolors='black', linewidth=2)\n\n# Annotate each point\nfor i in range(4):\n    ax.annotate(f'({X[i,0]},{X[i,1]})', X[i], xytext=(10, 10), \n                textcoords='offset points', fontsize=10)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x‚ÇÅ', fontsize=12)\nax.set_ylabel('x‚ÇÇ', fontsize=12)\nax.set_title('Step 1: Original Input Space', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Linear Transformation (Before ReLU)\nThe network applies weights W‚ÇÅ and biases b‚ÇÅ to transform the input space:\n\n\nShow code\n# Step 2 visualization: Pre-activation space\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\nfor i in range(4):\n    ax.scatter(pre_activations[i, 0], pre_activations[i, 1], \n               c=colors[y[i]], s=200, edgecolors='black', linewidth=2)\n\n# Draw ReLU boundaries\nax.axhline(0, color='gray', linestyle='-', alpha=0.8, linewidth=2)\nax.axvline(0, color='gray', linestyle='-', alpha=0.8, linewidth=2)\n\n# Shade all regions where coordinates turn negative (and thus get clipped by ReLU)\nax.axvspan(-1.2, 0, alpha=0.15, color='red')\nax.axhspan(-1.2, 0, alpha=0.15, color='red')\nax.text(-0.75, 0.85, 'Negative z‚ÇÅ ‚Üí ReLU sets to 0', ha='left', fontsize=10,\n        bbox=dict(boxstyle='round', facecolor='red', alpha=0.25))\nax.text(0.95, -0.75, 'Negative z‚ÇÇ ‚Üí ReLU sets to 0', ha='right', fontsize=10,\n        bbox=dict(boxstyle='round', facecolor='red', alpha=0.25))\n\n# Annotate points with input labels\nlabels = ['(0,0)', '(0,1)', '(1,0)', '(1,1)']\nfor i, label in enumerate(labels):\n    pre_coord = f'({pre_activations[i,0]:.0f},{pre_activations[i,1]:.0f})'\n    ax.annotate(f'{label}‚Üí{pre_coord}', pre_activations[i], xytext=(10, 10), \n                textcoords='offset points', fontsize=9)\n\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1.2, 1.2)\nax.set_xlabel('z‚ÇÅ (Pre-activation)', fontsize=12)\nax.set_ylabel('z‚ÇÇ (Pre-activation)', fontsize=12)\nax.set_title('Step 2: Before ReLU (Linear Transform)', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 3: ReLU Transformation\nReLU clips negative values to zero, transforming the space to make it linearly separable:\n\n\nShow code\n# Step 3 visualization: ReLU transformation with arrows\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\n\nfor i in range(4):\n    # Pre-ReLU positions (X marks)\n    ax.scatter(pre_activations[i, 0], pre_activations[i, 1], \n               marker='x', s=150, c=colors[y[i]], alpha=0.5, linewidth=3)\n    # Post-ReLU positions (circles) \n    ax.scatter(hidden_activations[i, 0], hidden_activations[i, 1], \n               marker='o', s=200, c=colors[y[i]], edgecolors='black', linewidth=2)\n    \n    # Draw transformation arrows\n    start = pre_activations[i]\n    end = hidden_activations[i]\n    if not np.array_equal(start, end):\n        ax.annotate('', xy=end, xytext=start,\n                    arrowprops=dict(arrowstyle='-&gt;', lw=2, color=colors[y[i]], alpha=0.8))\n\n\n# Add text box explaining the key transformation\nax.text(0.5, 0.8, 'ReLU clips negative coordinates to zero\\n(-1,1) ‚Üí (0,1) and (1,-1) ‚Üí (1,0)', \n        ha='center', va='center', fontsize=11, \n        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n\nax.set_xlim(-1.2, 1.1)\nax.set_ylim(-1.2, 1.1)\nax.set_xlabel('Hidden dimension 1', fontsize=12)\nax.set_ylabel('Hidden dimension 2', fontsize=12)\nax.set_title('Step 3: ReLU Mapping (Before ‚Üí After)', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Final Classification\nWith the transformed hidden representation, the network can now perfectly classify XOR:\n\n\nShow code\n\n\n# Step 4 visualization: Final classification results\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\n# Create decision boundary\nxx, yy = np.meshgrid(np.linspace(-0.2, 1.2, 100), np.linspace(-0.2, 1.2, 100))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\ngrid_logits, _, _ = forward_pass(grid_points, W1, b1, w2, b2)\ngrid_preds = (grid_logits &gt;= 0).astype(int).reshape(xx.shape)\n\nax.contourf(xx, yy, grid_preds, levels=[-0.5, 0.5, 1.5], \n            colors=['#ffcccc', '#ccccff'], alpha=0.6)\nax.contour(xx, yy, grid_logits.reshape(xx.shape), levels=[0], \n           colors='black', linewidths=2, linestyles='--')\n\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'XOR = {i}', edgecolors='black', linewidth=2)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x‚ÇÅ', fontsize=12)\nax.set_ylabel('x‚ÇÇ', fontsize=12)\nax.set_title('Step 4: Final Classification (100% Accuracy)', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nsample_logits, _, _ = forward_pass(X, W1, b1, w2, b2)\nsample_preds = (sample_logits &gt;= 0).astype(int)\nfor i in range(4):\n    pred_text = f'Pred: {sample_preds[i]}'\n    ax.annotate(pred_text, X[i], xytext=(10, -15), \n                textcoords='offset points', fontsize=9,\n                bbox=dict(boxstyle='round,pad=0.2', facecolor='lightgreen', alpha=0.7))\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ML/xor-deep-learning.html#conclusion",
    "href": "ML/xor-deep-learning.html#conclusion",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Conclusion",
    "text": "Conclusion\nThe XOR problem demonstrates several fundamental principles in deep learning:\n\nNecessity of Nonlinearity: Linear models cannot solve XOR, establishing the critical role of nonlinear activation functions.\nUniversal Approximation: Even simple architectures with sufficient nonlinearity can solve complex classification problems."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html",
    "title": "Lecture 3: Convex Functions",
    "section": "",
    "text": "If C and D are nonempty disjoint convex sets, then there exists \\(a \\neq 0, b\\) such that:\n\\[\na^\\top x \\leq b \\quad \\text{for } x \\in C, \\quad a^\\top x \\geq b \\quad \\text{for } x \\in D\n\\]\nThe hyperplane \\(a^\\top x = b\\) separates C and D.\n\n\n\nSeparating hyperplane between two convex sets"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#separating-hyperplane-theorem",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#separating-hyperplane-theorem",
    "title": "Lecture 3: Convex Functions",
    "section": "Separating Hyperplane Theorem",
    "text": "Separating Hyperplane Theorem\nIf C and D are nonempty disjoint convex sets, then there exists \\(a \\neq 0, b\\) such that:\n\\[\na^\\top x \\leq b \\quad \\text{for } x \\in C, \\quad a^\\top x \\geq b \\quad \\text{for } x \\in D\n\\]\nThe hyperplane \\(a^\\top x = b\\) separates C and D.\n\n\n\nSeparating hyperplane between two convex sets"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#supporting-hyperplane-theorem",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#supporting-hyperplane-theorem",
    "title": "Lecture 3: Convex Functions",
    "section": "Supporting Hyperplane Theorem",
    "text": "Supporting Hyperplane Theorem\nSupporting hyperplane to set C (doesn‚Äôt have to be a convex set) at boundary point \\(x_0\\):\n\\[\n\\{x \\mid a^\\top x = a^\\top x_0\\}\n\\]\nwhere \\(a\\) is the normal vector that determines the hyperplane tangent to C at point \\(x_0\\), with \\(a \\neq 0\\) and \\(a^\\top x \\leq a^\\top x_0\\) for \\(x \\in C\\).\n\n\n\nSupporting hyperplane at boundary point\n\n\nIf C is convex, then there is a supporting hyperplane at every point of its boundary.\n\n\n\nSupporting hyperplanes exist at every boundary point for convex sets"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#dual-cones-and-generalized-inequalities",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#dual-cones-and-generalized-inequalities",
    "title": "Lecture 3: Convex Functions",
    "section": "Dual Cones and Generalized Inequalities",
    "text": "Dual Cones and Generalized Inequalities\nDual cone of a cone K:\n\\[\nK^* = \\{y \\mid y^\\top x \\geq 0 \\quad \\forall x \\in K\\}\n\\]\nThe inner product of any vector in \\(K^*\\) and K is nonnegative.\n\n\n\nDual cone visualization\n\n\n\nExamples\n\n\\(K = \\{0\\}\\): the dual cone is \\(K^* = \\mathbb{R}^n\\)\nK is a ray: the dual cone is a half space\nSelf-dual cones:\n\n\\(K = \\mathbb{R}_+^n\\): dual cone \\(K^* = K\\)\n\\(K = S_+^n\\) (positive semidefinite cone)\n\\(K = \\{(x,t) \\mid \\|x\\|_2 \\leq t\\}\\) (second-order cone)\n\n\\(K = \\{(x,t) \\mid \\|x\\|_\\infty \\leq t\\}\\): dual cone \\(K^* = \\{(y,\\tau) \\mid \\|y\\|_1 \\leq \\tau\\}\\)\n\nThe dual cone of a proper cone is proper."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#minimum-and-minimal-elements-via-dual-inequalities",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#minimum-and-minimal-elements-via-dual-inequalities",
    "title": "Lecture 3: Convex Functions",
    "section": "Minimum and Minimal Elements via Dual Inequalities",
    "text": "Minimum and Minimal Elements via Dual Inequalities\n\nMinimum Element\n\\(x\\) is the minimum element of S if and only if for all \\(\\lambda \\succ_{K^*} 0\\), \\(x\\) is the unique minimizer of \\(\\lambda^\\top z\\) over S.\n\n\nMinimal Element\nIf \\(x\\) minimizes \\(\\lambda^\\top z\\) over S for some \\(\\lambda \\succ_{K^*} 0\\), then \\(x\\) is minimal.\nIf \\(x\\) is a minimal element of a convex set S, then there exists a nonzero \\(\\lambda \\succ_{K^*} 0\\) such that \\(x\\) minimizes \\(\\lambda^\\top z\\) over S."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#examples-on-mathbbr",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#examples-on-mathbbr",
    "title": "Lecture 3: Convex Functions",
    "section": "Examples on \\(\\mathbb{R}\\)",
    "text": "Examples on \\(\\mathbb{R}\\)\nConvex:\n\nAffine: \\(ax + b\\) for any \\(a, b \\in \\mathbb{R}\\)\nExponential: \\(e^{ax}\\) for any \\(a \\in \\mathbb{R}\\)\nPowers: \\(x^a\\) on \\(\\mathbb{R}_{++}\\) for \\(a \\geq 1\\) or \\(a \\leq 0\\)\nPowers of absolute value: \\(|x|^p\\) on \\(\\mathbb{R}\\) for \\(p \\geq 1\\)\nNegative entropy: \\(x \\log x\\) on \\(\\mathbb{R}_{++}\\)\n\nConcave:\n\nAffine: \\(ax + b\\)\nPowers: \\(x^a\\) on \\(\\mathbb{R}_{++}\\) for \\(0 \\leq a \\leq 1\\)\nLogarithm: \\(\\log x\\) on \\(\\mathbb{R}_{++}\\)"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#examples-on-mathbbrn-and-mathbbrm-times-n",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#examples-on-mathbbrn-and-mathbbrm-times-n",
    "title": "Lecture 3: Convex Functions",
    "section": "Examples on \\(\\mathbb{R}^n\\) and \\(\\mathbb{R}^{m \\times n}\\)",
    "text": "Examples on \\(\\mathbb{R}^n\\) and \\(\\mathbb{R}^{m \\times n}\\)\nOn \\(\\mathbb{R}^n\\):\n\nAffine function: \\(a^\\top x + b\\)\nNorms: \\(\\|x\\|_p = \\sqrt[p]{\\sum_i |x_i|^p}\\); \\(\\|x\\|_\\infty = \\max_k |x_k|\\)\n\nOn \\(\\mathbb{R}^{m \\times n}\\):\n\nAffine function: \\(f(X) = \\text{tr}(A^\\top X) + b = \\sum_{i,j} A_{ij} X_{ij} + b\\)\nSpectral norm: \\(f(X) = \\|X\\|_2 = \\sigma_{\\max}(X)\\)"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#restriction-of-a-convex-function-to-a-line",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#restriction-of-a-convex-function-to-a-line",
    "title": "Lecture 3: Convex Functions",
    "section": "Restriction of a Convex Function to a Line",
    "text": "Restriction of a Convex Function to a Line\n\\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is convex if and only if the function \\(g: \\mathbb{R} \\rightarrow \\mathbb{R}\\):\n\\[\ng(t) = f(x + tv), \\quad \\text{dom } g = \\{t \\mid x + tv \\in \\text{dom } f\\}\n\\]\nis convex for any \\(x \\in \\text{dom } f\\), \\(v \\in \\mathbb{R}^n\\).\nThis allows checking convexity of \\(f\\) by checking convexity of functions of one variable.\n\nExample\n\\(f: S^n \\rightarrow \\mathbb{R}\\) with \\(f(X) = \\log \\det X\\), \\(\\text{dom } f = S_{++}^n\\)\n\\[\ng(t) = \\log \\det(X + tV) = \\log \\det X + \\log \\det(I + tX^{-1/2}VX^{-1/2}) = \\log \\det X + \\sum_{i=1}^n \\log(1 + t\\lambda_i)\n\\]\nwhere \\(\\lambda_i\\) are the eigenvalues of \\(X^{-1/2}VX^{-1/2}\\). Since \\(g\\) is concave, \\(f\\) is concave."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#extended-value-extension",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#extended-value-extension",
    "title": "Lecture 3: Convex Functions",
    "section": "Extended Value Extension",
    "text": "Extended Value Extension\n\\[\n\\tilde{f}(x) = \\begin{cases} f(x) & x \\in \\text{dom } f \\\\ +\\infty & x \\notin \\text{dom } f \\end{cases}\n\\]\nThis extends the output domain of \\(f\\) to \\(\\mathbb{R} \\cup \\{+\\infty\\}\\).\nConvexity of \\(\\tilde{f}(\\theta x + (1-\\theta)y) \\leq \\theta \\tilde{f}(x) + (1-\\theta)\\tilde{f}(y)\\) still holds because:\n\nFor \\(x, y \\in \\text{dom } f\\): \\(f(x)\\) is convex, and \\(\\theta x + (1-\\theta)y \\in \\text{dom } f\\) (domain of convex function is convex)\nFor \\(x \\notin \\text{dom } f\\) or \\(y \\notin \\text{dom } f\\): \\(\\tilde{f}(\\theta x + (1-\\theta)y) \\leq +\\infty\\)\n\n\n\n\nExtended value extension"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#first-order-condition",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#first-order-condition",
    "title": "Lecture 3: Convex Functions",
    "section": "First-Order Condition",
    "text": "First-Order Condition\n\\(f\\) is differentiable if \\(\\text{dom } f\\) is open and the gradient exists at each \\(x \\in \\text{dom } f\\):\n\\[\n\\nabla f(x) = \\left(\\frac{\\partial f(x)}{\\partial x_1}, \\frac{\\partial f(x)}{\\partial x_2}, \\ldots, \\frac{\\partial f(x)}{\\partial x_n}\\right)\n\\]\nFirst-order condition: Differentiable \\(f\\) with convex domain is convex if and only if:\n\\[\nf(y) \\geq f(x) + \\nabla f(x)^\\top (y - x) \\quad \\forall x, y \\in \\text{dom } f\n\\]\n\n\n\nFirst-order condition: tangent always underestimates\n\n\nKey insight: For a convex function, the tangent (first-order linear approximation) always underestimates the function."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#second-order-conditions",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#second-order-conditions",
    "title": "Lecture 3: Convex Functions",
    "section": "Second-Order Conditions",
    "text": "Second-Order Conditions\n\\(f\\) is twice differentiable if \\(\\text{dom } f\\) is open and the Hessian \\(\\nabla^2 f(x) \\in S^n\\) exists at each \\(x \\in \\text{dom } f\\):\n\\[\n\\nabla^2 f(x)_{ij} = \\frac{\\partial^2 f(x)}{\\partial x_i \\partial x_j}, \\quad i, j = 1, \\ldots, n\n\\]\nSecond-order conditions: For twice differentiable \\(f\\) with convex domain:\n\n\\(f\\) is convex if and only if \\(\\nabla^2 f(x) \\succeq 0\\) for all \\(x \\in \\text{dom } f\\)\nIf \\(\\nabla^2 f(x) \\succ 0\\), then \\(f\\) is strictly convex\n\n\n\n\nSecond-order condition visualization\n\n\n\nExamples\nQuadratic function: \\(f(x) = \\frac{1}{2}x^\\top P x + q^\\top x + r\\) (with \\(P \\in S^n\\))\n\\[\n\\nabla f(x) = Px + q, \\quad \\nabla^2 f(x) = P\n\\]\nIf \\(P \\succeq 0\\), then \\(f(x)\\) is convex.\nLeast-squares objective: \\(f(x) = \\|Ax - b\\|_2^2\\)\n\\[\n\\nabla f(x) = 2A^\\top(Ax - b), \\quad \\nabla^2 f(x) = 2A^\\top A\n\\]\nConvex for any \\(A\\), because \\(A^\\top A\\) is always positive semidefinite.\nQuadratic-over-linear: \\(f(x, y) = x^2/y\\) is convex if \\(y &gt; 0\\)\n\\[\n\\nabla^2 f(x, y) = \\frac{2}{y^3} \\begin{bmatrix} y \\\\ -x \\end{bmatrix} \\begin{bmatrix} y & -x \\end{bmatrix} \\succeq 0\n\\]\nLog-sum-exp: \\(f(x) = \\log \\sum_{k=1}^n \\exp x_k\\) is convex\nLet \\(T = \\sum_{k=1}^n \\exp x_k\\) and \\(z_k = \\exp x_k\\):\n\\[\nH = \\nabla^2 f(x) = \\frac{1}{T}\\text{diag}(z) - \\frac{1}{T^2}zz^\\top\n\\]\nTo verify H is PSD, check \\(v^\\top H v \\geq 0\\) for all \\(v\\):\n\\[\nv^\\top \\nabla^2 f(x) v = \\frac{\\sum_k z_k v_k^2}{\\sum_k z_k} - \\frac{(\\sum_k v_k z_k)^2}{(\\sum_k z_k)^2}\n\\]\nBy Cauchy-Schwarz inequality: \\((\\sum_k v_k z_k)^2 \\leq (\\sum_k z_k v_k^2)(\\sum_k z_k)\\), so \\(v^\\top \\nabla^2 f(x) v \\geq 0\\).\nGeometric mean: \\(f(x) = \\sqrt[n]{\\prod_{k=1}^n x_k}\\) on \\(\\mathbb{R}_{++}^n\\) is concave."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#epigraph-and-sub-level-sets",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#epigraph-and-sub-level-sets",
    "title": "Lecture 3: Convex Functions",
    "section": "Epigraph and Sub-level Sets",
    "text": "Epigraph and Sub-level Sets\n\nSub-level Sets\n\\(\\alpha\\)-sublevel set of \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\):\n\\[\nC_\\alpha = \\{x \\in \\text{dom } f \\mid f(x) \\leq \\alpha\\}\n\\]\nSub-level sets of convex functions are convex (converse is false).\n\n\nEpigraph\nEpigraph of \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\):\n\\[\n\\text{epi } f = \\{(x, t) \\in \\mathbb{R}^{n+1} \\mid f(x) \\leq t, \\; x \\in \\text{dom } f\\}\n\\]\n\\(f\\) is convex if and only if \\(\\text{epi } f\\) is a convex set.\n\n\n\nEpigraph of a convex function"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#jensens-inequality",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#jensens-inequality",
    "title": "Lecture 3: Convex Functions",
    "section": "Jensen‚Äôs Inequality",
    "text": "Jensen‚Äôs Inequality\nIf \\(f\\) is convex:\n\\[\nf(\\mathbb{E}[z]) \\leq \\mathbb{E}[f(z)]\n\\]\nfor any random variable \\(z\\).\nThe function of the expectation is less than or equal to the expectation of the function.\nThe basic convexity inequality is a special case with discrete distribution:\n\n\\(\\text{prob}(z = x) = \\theta\\)\n\\(\\text{prob}(z = y) = 1 - \\theta\\)\n\n\n\n\nJensen‚Äôs inequality visualization"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#operations-that-preserve-convexity",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#operations-that-preserve-convexity",
    "title": "Lecture 3: Convex Functions",
    "section": "Operations that Preserve Convexity",
    "text": "Operations that Preserve Convexity\nWays to establish convexity:\n\nVerify definition directly\nFor twice differentiable functions, show \\(\\nabla^2 f(x) \\succeq 0\\)\nCombine simple convex functions using operations that preserve convexity:\n\nNonnegative weighted sum: \\(f_1 + f_2\\) is convex if \\(f_1, f_2\\) are convex\nNonnegative multiplication: \\(\\alpha f\\) is convex if \\(f\\) is convex and \\(\\alpha &gt; 0\\)\nComposition with affine function: \\(f(Ax + b)\\) is convex if \\(f\\) is convex\nPointwise maximum and supremum\nComposition\nMinimization\nPerspective\n\nExamples\nLog barrier for linear inequalities: \\[\nf(x) = -\\sum_{i=1}^m \\log(b_i - a_i^\\top x), \\quad \\text{dom } f = \\{x \\mid a_i^\\top x &lt; b_i, \\; i = 1, \\ldots, m\\}\n\\]\nNorm of affine function: \\[\nf(x) = \\|Ax + b\\|\n\\]"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture3-convex-functions.html#key-insights",
    "href": "Math/EE364A/ee364a-lecture3-convex-functions.html#key-insights",
    "title": "Lecture 3: Convex Functions",
    "section": "Key Insights",
    "text": "Key Insights\nConvex functions are fundamental to optimization. The first-order condition shows that local information (gradient) provides a global lower bound. The second-order condition connects convexity to positive semidefiniteness of the Hessian. Jensen‚Äôs inequality generalizes the basic convexity definition to expectations, with applications throughout probability and machine learning. Operations preserving convexity allow building complex convex functions from simpler ones."
  },
  {
    "objectID": "ML/likelihood-loss-functions.html",
    "href": "ML/likelihood-loss-functions.html",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.2 reveals the fundamental connection between probabilistic assumptions and the loss functions we use to train neural networks.\nüìì For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#the-hidden-connection-why-these-loss-functions",
    "href": "ML/likelihood-loss-functions.html#the-hidden-connection-why-these-loss-functions",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "The Hidden Connection: Why These Loss Functions?",
    "text": "The Hidden Connection: Why These Loss Functions?\nEver wondered why we use mean squared error for regression, cross-entropy for classification, and other specific loss functions? The answer lies in maximum likelihood estimation - each common loss function corresponds to the negative log-likelihood of a specific probabilistic model.\n\n\n\n\n\n\n\n\nProbabilistic Model\nLoss Function\nUse Case\n\n\n\n\nGaussian likelihood\nMean Squared Error\nRegression\n\n\nBernoulli likelihood\nBinary Cross-Entropy\nBinary Classification\n\n\nCategorical likelihood\nSoftmax Cross-Entropy\nMulticlass Classification"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#exploring-the-connection-probabilistic-models-loss-functions",
    "href": "ML/likelihood-loss-functions.html#exploring-the-connection-probabilistic-models-loss-functions",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "üéØ Exploring the Connection: Probabilistic Models ‚Üí Loss Functions",
    "text": "üéØ Exploring the Connection: Probabilistic Models ‚Üí Loss Functions\n\n\nShow code\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-1-gaussian-likelihood-mean-squared-error",
    "href": "ML/likelihood-loss-functions.html#connection-1-gaussian-likelihood-mean-squared-error",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 1: Gaussian Likelihood ‚Üí Mean Squared Error",
    "text": "Connection 1: Gaussian Likelihood ‚Üí Mean Squared Error\nThe Setup: When we assume our targets have Gaussian noise around our predictions:\n\\[p(y|x) = \\mathcal{N}(y; \\hat{y}, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\hat{y})^2}{2\\sigma^2}\\right)\\]\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = \\frac{(y-\\hat{y})^2}{2\\sigma^2} + \\frac{1}{2}\\log(2\\pi\\sigma^2)\\]\nThe Result: Minimizing this is equivalent to minimizing MSE (the constant term doesn‚Äôt affect optimization)!\n\n\nShow code\n# Demonstrate Gaussian likelihood = MSE connection\nnp.random.seed(0)\nx = np.linspace(-1, 1, 20)\ny_true = 2 * x + 1\ny = y_true + np.random.normal(0, 0.1, size=x.shape)  # Gaussian noise\n\n# Simple linear model predictions\nw, b = 1.0, 0.0\ny_pred = w * x + b\n\n# Compute MSE\nmse = np.mean((y - y_pred)**2)\n\n# Compute Gaussian negative log-likelihood\nsigma_squared = 0.1**2\nquadratic_term = 0.5 * np.mean((y - y_pred)**2) / sigma_squared\nconst_term = 0.5 * np.log(2 * np.pi * sigma_squared)\nnll_gaussian = quadratic_term + const_term\n\nprint(\"üìä Gaussian Likelihood ‚Üî MSE Connection\")\nprint(\"=\" * 45)\nprint(f\"üìà Mean Squared Error:     {mse:.6f}\")\nprint(f\"üìä Gaussian NLL:           {nll_gaussian:.6f}\")\nprint(f\"   ‚îú‚îÄ Quadratic term:      {quadratic_term:.6f}\")\nprint(f\"   ‚îî‚îÄ Constant term:       {const_term:.6f}\")\n\nscaling_factor = 1 / (2 * sigma_squared)\nprint(f\"\\nüîó Mathematical Connection:\")\nprint(f\"   Quadratic term = {scaling_factor:.1f} √ó MSE\")\nprint(f\"   {quadratic_term:.6f} = {scaling_factor:.1f} √ó {mse:.6f}\")\nprint(f\"\\n‚úÖ Minimizing MSE ‚â° Maximizing Gaussian likelihood\")\n\n\nüìä Gaussian Likelihood ‚Üî MSE Connection\n=============================================\nüìà Mean Squared Error:     1.450860\nüìä Gaussian NLL:           71.159339\n   ‚îú‚îÄ Quadratic term:      72.542985\n   ‚îî‚îÄ Constant term:       -1.383647\n\nüîó Mathematical Connection:\n   Quadratic term = 50.0 √ó MSE\n   72.542985 = 50.0 √ó 1.450860\n\n‚úÖ Minimizing MSE ‚â° Maximizing Gaussian likelihood"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-2-bernoulli-likelihood-binary-cross-entropy",
    "href": "ML/likelihood-loss-functions.html#connection-2-bernoulli-likelihood-binary-cross-entropy",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 2: Bernoulli Likelihood ‚Üí Binary Cross-Entropy",
    "text": "Connection 2: Bernoulli Likelihood ‚Üí Binary Cross-Entropy\nThe Setup: For binary classification, we assume Bernoulli-distributed targets:\n\\[p(y|x) = \\sigma(z)^y (1-\\sigma(z))^{1-y}\\]\nwhere \\(\\sigma(z) = \\frac{1}{1+e^{-z}}\\) is the sigmoid function.\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = -y\\log\\sigma(z) - (1-y)\\log(1-\\sigma(z))\\]\nThe Result: This is exactly binary cross-entropy loss!\n\n\nShow code\n# Demonstrate Bernoulli likelihood = Binary cross-entropy connection\nz = torch.tensor([-0.5, -0.8, 0.0, 0.8, 0.5])  # Model logits\ny = torch.tensor([0.0, 0.0, 1.0, 1.0, 1.0])     # Binary labels\np = torch.sigmoid(z)  # Convert to probabilities\n\nprint(\"üé≤ Bernoulli Likelihood ‚Üî Binary Cross-Entropy\")\nprint(\"=\" * 50)\nprint(\"Input Data:\")\nprint(f\"   Logits:        {z.numpy()}\")\nprint(f\"   Labels:        {y.numpy()}\")\nprint(f\"   Probabilities: {p.numpy()}\")\n\n# Manual Bernoulli NLL computation\nbernoulli_nll = torch.mean(-(y * torch.log(p) + (1 - y) * torch.log(1 - p)))\n\n# PyTorch binary cross-entropy\nbce_loss = F.binary_cross_entropy(p, y)\n\nprint(f\"\\nüìä Loss Function Comparison:\")\nprint(f\"   Manual Bernoulli NLL:  {bernoulli_nll:.6f}\")\nprint(f\"   PyTorch BCE Loss:      {bce_loss:.6f}\")\n\n# Verify they're identical\ndifference = torch.abs(bernoulli_nll - bce_loss)\nprint(f\"\\nüîó Verification:\")\nprint(f\"   Absolute difference:   {difference:.10f}\")\nprint(f\"\\n‚úÖ Binary cross-entropy IS Bernoulli negative log-likelihood!\")\n\n\nüé≤ Bernoulli Likelihood ‚Üî Binary Cross-Entropy\n==================================================\nInput Data:\n   Logits:        [-0.5 -0.8  0.   0.8  0.5]\n   Labels:        [0. 0. 1. 1. 1.]\n   Probabilities: [0.37754068 0.3100255  0.5        0.6899745  0.62245935]\n\nüìä Loss Function Comparison:\n   Manual Bernoulli NLL:  0.476700\n   PyTorch BCE Loss:      0.476700\n\nüîó Verification:\n   Absolute difference:   0.0000000000\n\n‚úÖ Binary cross-entropy IS Bernoulli negative log-likelihood!"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-3-categorical-likelihood-softmax-cross-entropy",
    "href": "ML/likelihood-loss-functions.html#connection-3-categorical-likelihood-softmax-cross-entropy",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 3: Categorical Likelihood ‚Üí Softmax Cross-Entropy",
    "text": "Connection 3: Categorical Likelihood ‚Üí Softmax Cross-Entropy\nThe Setup: For multiclass classification, we use the categorical distribution:\n\\[p(y=i|x) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} = \\text{softmax}(z)_i\\]\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = -\\log \\frac{e^{z_y}}{\\sum_j e^{z_j}} = -z_y + \\log\\sum_j e^{z_j}\\]\nThe Result: This is exactly softmax cross-entropy loss!\n\n\nShow code\n# Demonstrate Categorical likelihood = Softmax cross-entropy connection\nz = torch.tensor([[0.1, 0.2, 0.7],    # Sample 1: class 2 highest\n                  [0.1, 0.7, 0.2],    # Sample 2: class 1 highest  \n                  [0.7, 0.1, 0.2]])   # Sample 3: class 0 highest\n\ny = torch.tensor([2, 1, 0])           # True class indices\n\nprint(\"üéØ Categorical Likelihood ‚Üî Softmax Cross-Entropy\")\nprint(\"=\" * 55)\nprint(\"Input Data:\")\nprint(f\"   Logits shape:    {z.shape}\")\nprint(f\"   True classes:    {y.numpy()}\")\n\n# Convert to probabilities\nsoftmax_probs = F.softmax(z, dim=1)\nprint(f\"\\nSoftmax Probabilities:\")\nfor i, (logit_row, prob_row, true_class) in enumerate(zip(z, softmax_probs, y)):\n    print(f\"   Sample {i+1}: {prob_row.numpy()} ‚Üí Class {true_class}\")\n\n# Manual categorical NLL (using log-softmax for numerical stability)\nlog_softmax = F.log_softmax(z, dim=1)\ncategorical_nll = -torch.mean(log_softmax[range(len(y)), y])\n\n# PyTorch cross-entropy\nce_loss = F.cross_entropy(z, y)\n\nprint(f\"\\nüìä Loss Function Comparison:\")\nprint(f\"   Manual Categorical NLL: {categorical_nll:.6f}\")\nprint(f\"   PyTorch Cross-Entropy:  {ce_loss:.6f}\")\n\n# Verify they're identical\ndifference = torch.abs(categorical_nll - ce_loss)\nprint(f\"\\nüîó Verification:\")\nprint(f\"   Absolute difference:    {difference:.10f}\")\nprint(f\"\\n‚úÖ Cross-entropy IS categorical negative log-likelihood!\")\n\n\nüéØ Categorical Likelihood ‚Üî Softmax Cross-Entropy\n=======================================================\nInput Data:\n   Logits shape:    torch.Size([3, 3])\n   True classes:    [2 1 0]\n\nSoftmax Probabilities:\n   Sample 1: [0.25462854 0.28140804 0.46396342] ‚Üí Class 2\n   Sample 2: [0.25462854 0.46396342 0.28140804] ‚Üí Class 1\n   Sample 3: [0.46396342 0.25462854 0.28140804] ‚Üí Class 0\n\nüìä Loss Function Comparison:\n   Manual Categorical NLL: 0.767950\n   PyTorch Cross-Entropy:  0.767950\n\nüîó Verification:\n   Absolute difference:    0.0000000000\n\n‚úÖ Cross-entropy IS categorical negative log-likelihood!"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#why-this-matters-bce-vs-mse-for-classification",
    "href": "ML/likelihood-loss-functions.html#why-this-matters-bce-vs-mse-for-classification",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Why This Matters: BCE vs MSE for Classification",
    "text": "Why This Matters: BCE vs MSE for Classification\nUnderstanding the probabilistic foundation explains why binary cross-entropy works better than MSE for classification, even though both can theoretically solve binary problems.\nKey Differences: - BCE gradient: \\(\\sigma(z) - y\\) (simple, well-behaved) - MSE gradient: \\(2(\\sigma(z) - y) \\times \\sigma(z) \\times (1 - \\sigma(z))\\) (can vanish!)\nLet‚Äôs see this in practice:"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#key-takeaways",
    "href": "ML/likelihood-loss-functions.html#key-takeaways",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nUnderstanding the probabilistic foundation of loss functions reveals:\n\nMSE = Gaussian NLL: Mean squared error emerges from assuming Gaussian noise\nBCE = Bernoulli NLL: Binary cross-entropy is exactly Bernoulli negative log-likelihood\n\nCross-entropy = Categorical NLL: Softmax cross-entropy corresponds to categorical distributions\nBetter gradients: Probabilistically-motivated loss functions provide better optimization dynamics\n\nThis connection between probability theory and optimization is fundamental to understanding why certain loss functions work well for specific tasks.\n\nThis mathematical foundation helps explain not just which loss function to use, but why it works so effectively for the given problem type."
  },
  {
    "objectID": "ML/k_means_clustering.html",
    "href": "ML/k_means_clustering.html",
    "title": "K-means Clustering",
    "section": "",
    "text": "Setup points and K\nwe will implement a KNN algorithm to cluster the points\n\n\nX=[[1,1],[2,2.1],[3,2.5],[6,7],[7,7.1],[9,7.5]]\nk=2\n\nmax_iter=3\n\n\n# Visualize the data\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter([x[0] for x in X],[x[1] for x in X])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pure python implementation of K-means clustering\ndef knn_iter(X,centroids):\n    # set up new clusters\n    new_clusters=[[] for _ in range(len(centroids))]\n    # k=len(centroids)\n    # assign each point to the nearest centroid\n    for x in X:\n        k,distance=0,(x[0]-centroids[0][0])**2+(x[1]-centroids[0][1])**2\n        for i,c in enumerate(centroids[1:],1):\n            if (x[0]-c[0])**2+(x[1]-c[1])**2&lt;distance:\n                k=i\n                distance=(x[0]-c[0])**2+(x[1]-c[1])**2\n        new_clusters[k].append(x)\n    \n    # calculate new centroids\n    new_centroids=[[\n        sum([x[0] for x in cluster])/len(cluster),\n        sum([x[1] for x in cluster])/len(cluster)\n    ] if cluster else centroids[i] for i,cluster in enumerate(new_clusters)]\n    return new_centroids\n\n\n\n\n\n\n\n\ndef iter_and_draw(X,k,max_iter):\n    centroids=X[:k]  # Randomly select 2 centroids\n    fig, axes = plt.subplots(max_iter//3+(1 if max_iter%3!=0 else 0),\n        3, figsize=(15, 10))\n    axes=axes.flatten()\n    for i in range(max_iter):\n        \n        # Plot points and centroids\n\n\n        # Assign each point to nearest centroid and plot with corresponding color\n        colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n        for j, x in enumerate(X):\n            # Find nearest centroid\n            min_dist = float('inf')\n            nearest_centroid = 0\n            for k, c in enumerate(centroids):\n                dist = (x[0]-c[0])**2 + (x[1]-c[1])**2\n                if dist &lt; min_dist:\n                    min_dist = dist\n                    nearest_centroid = k\n            # Plot point with color corresponding to its cluster\n            axes[i].scatter(x[0], x[1], c=colors[nearest_centroid % len(colors)], label=f'Cluster {nearest_centroid+1}' if j==0 else \"\")\n        axes[i].scatter([c[0] for c in centroids], [c[1] for c in centroids], c='red', marker='*', s=200, label='Centroids')\n        axes[i].set_title(f'Iteration {i}')\n        centroids = knn_iter(X, centroids)\n\n    plt.tight_layout()\n    plt.show()\n\niter_and_draw(X,k,max_iter)\n# print(centroids)\n\n\n\n\n\n\n\n\n\n# A 3 clusters example\n\nimport numpy as np\n\nX1=np.random.rand(20,2)+5 # Some points in the upper right corner\nX2=np.random.rand(20,2)+3 # Some points in the middle\nX3=np.random.rand(20,2) # Some points in the lower left corner\n\niter_and_draw(np.concatenate((X1,X2,X3)),3,5)\n\n\n\n\n\n\n\n\n\n\nA question?\n\nWhat to do if one cluster has no assigned points during iteration?\n\n\n\nFormula Derivation\nThe goal is to minimize the loss of inertia which is sum of the points to cluster centroids.\n\\[\nLoss= \\sum_{i=1}^n \\sum_{x \\in C_i} ||x-\\mu_i||^2\n\\]\nTo iter \\(\\mu\\) for each cluster, let us find the derivative of the following function. \\[\nf(\\mu)=\\sum_{i=1}^n ||x_i-\\mu||^2 =\n\\sum_{i=1}^n {x_i}^2+\\mu^2-2x_i\\mu\n\\]\nGiven a \\(\\nabla \\mu\\), \\[\nf(\\mu + \\nabla \\mu)=\\sum_{i=1}^n ||x_i+\\nabla \\mu -\\mu||^2 =\n\\sum_{i=1}^n  {x_i}^2+\\mu^2+{\\nabla \\mu}^2-2{x_i \\mu}-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\nf(\\mu + \\nabla \\mu)-f(\\mu)=\n\\sum_{i=1}^n {\\nabla \\mu}^2-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\n\\frac {f(\\mu + \\nabla \\mu)-f(\\mu)}{\\nabla \\mu}=\\sum_{i=1}^n {\\nabla \\mu} -2 \\mu +2{x_i} = 2\\sum_{i=1}^n x_i - 2n\\mu\n\\]\nNow we can see if \\(n\\mu = \\sum_{i=1}^n x_i\\), then the derivative is 0, this is why in each iteration, we need to set the center of the cluster as centroid."
  },
  {
    "objectID": "ML/kmeans.html",
    "href": "ML/kmeans.html",
    "title": "Redirecting‚Ä¶",
    "section": "",
    "text": "Click here if you are not redirected"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html",
    "title": "Lecture 2: Convex Sets",
    "section": "",
    "text": "\\[x = \\theta x_1 + (1-\\theta)x_2, \\quad \\theta \\in \\mathbb{R}\\]\nAffine combination is the special form of linear combination where the sum of coefficients equals 1.\nExample: Solution set \\(\\{x \\mid Ax = b\\}\\)\nFor any \\(x_1, x_2\\) which both are solutions of \\(Ax=b\\):\n\\[A(\\theta x_1 + (1-\\theta)x_2) = \\theta A x_1 + (1-\\theta) A x_2 = \\theta b + (1-\\theta)b = b\\]\n Figure: Affine combinations form a line through two points. Unlike linear combinations, affine combinations require coefficients to sum to 1, making them translation-invariant‚Äîthe solution set of Ax=b forms an affine subspace.\n\n\n\n\nLinear segment: \\(x = \\theta x_1 + (1-\\theta)x_2, \\quad 0 \\leq \\theta \\leq 1\\)\nConvex set: A set \\(C\\) is convex if for all \\(x_1, x_2 \\in C\\) and \\(0 \\leq \\theta \\leq 1\\):\n\\[\\theta x_1 + (1-\\theta)x_2 \\in C\\]\n Figure: A convex set contains all line segments between any two points in the set. The left shape is convex (any line segment between two points stays inside), while the right shape is non-convex (some line segments exit the set).\n\n\n\n\nConvex combination:\n\\[x = \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k\\]\nwhere \\(\\theta_1 + \\theta_2 + \\ldots + \\theta_k = 1\\) and \\(\\theta_i \\geq 0\\)\nConvex hull: The set of all convex combinations of points in \\(S\\)‚Äîthe smallest convex set containing \\(S\\).\n Figure: The convex hull of a finite set of points forms a convex polygon (or polyhedron in higher dimensions) that encloses all the points. It‚Äôs like stretching a rubber band around the outermost points.\n\n\n\n\n\\[x = \\theta_1 x_1 + \\theta_2 x_2, \\quad \\theta_1 \\geq 0, \\theta_2 \\geq 0\\]\n Figure: A convex cone is closed under positive linear combinations. If you take any two vectors in the cone and scale them by non-negative coefficients, their sum remains in the cone. The cone extends infinitely from the origin."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#convex-sets",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#convex-sets",
    "title": "Lecture 2: Convex Sets",
    "section": "Convex Sets",
    "text": "Convex Sets\n\nAffine Combination\n\\[x = \\theta x_1 + (1-\\theta)x_2, \\quad \\theta \\in \\mathbb{R}\\]\nAffine combination is the special form of linear combination where the sum of coefficients equals 1.\nExample: Solution set \\(\\{x \\mid Ax = b\\}\\)\nFor any \\(x_1, x_2\\) which both are solutions of \\(Ax=b\\):\n\\[A(\\theta x_1 + (1-\\theta)x_2) = \\theta A x_1 + (1-\\theta) A x_2 = \\theta b + (1-\\theta)b = b\\]\n Figure: Affine combinations form a line through two points. Unlike linear combinations, affine combinations require coefficients to sum to 1, making them translation-invariant‚Äîthe solution set of Ax=b forms an affine subspace.\n\n\n\nConvex Set\nLinear segment: \\(x = \\theta x_1 + (1-\\theta)x_2, \\quad 0 \\leq \\theta \\leq 1\\)\nConvex set: A set \\(C\\) is convex if for all \\(x_1, x_2 \\in C\\) and \\(0 \\leq \\theta \\leq 1\\):\n\\[\\theta x_1 + (1-\\theta)x_2 \\in C\\]\n Figure: A convex set contains all line segments between any two points in the set. The left shape is convex (any line segment between two points stays inside), while the right shape is non-convex (some line segments exit the set).\n\n\n\nConvex Combination and Convex Hull\nConvex combination:\n\\[x = \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k\\]\nwhere \\(\\theta_1 + \\theta_2 + \\ldots + \\theta_k = 1\\) and \\(\\theta_i \\geq 0\\)\nConvex hull: The set of all convex combinations of points in \\(S\\)‚Äîthe smallest convex set containing \\(S\\).\n Figure: The convex hull of a finite set of points forms a convex polygon (or polyhedron in higher dimensions) that encloses all the points. It‚Äôs like stretching a rubber band around the outermost points.\n\n\n\nConvex Cone\n\\[x = \\theta_1 x_1 + \\theta_2 x_2, \\quad \\theta_1 \\geq 0, \\theta_2 \\geq 0\\]\n Figure: A convex cone is closed under positive linear combinations. If you take any two vectors in the cone and scale them by non-negative coefficients, their sum remains in the cone. The cone extends infinitely from the origin."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#hyperplanes-and-halfspaces",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#hyperplanes-and-halfspaces",
    "title": "Lecture 2: Convex Sets",
    "section": "Hyperplanes and Halfspaces",
    "text": "Hyperplanes and Halfspaces\n\nHyperplane\nSet of the form \\(a^\\top x = b\\) where \\(a \\neq 0\\)\n\n\\(a\\) is the normal vector\nHyperplanes are both affine and convex\n\n\n\nHalfspaces\n\\[a^\\top x \\leq b\\]\n\nHalfspaces are convex\n\n Figure: A hyperplane (a^x = b) divides space into two halfspaces: a^x ‚â§ b and a^x ‚â• b. The normal vector a points perpendicular to the hyperplane, and b controls the offset from the origin."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#euclidean-balls-and-ellipsoids",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#euclidean-balls-and-ellipsoids",
    "title": "Lecture 2: Convex Sets",
    "section": "Euclidean Balls and Ellipsoids",
    "text": "Euclidean Balls and Ellipsoids\n\nEuclidean Ball\n\\[B(x_c, r) = \\{x \\mid \\|x - x_c\\|_2 \\leq r\\} = \\{x_c + ru \\mid \\|u\\|_2 \\leq 1\\}\\]\n\n\nEllipsoid\n\\[\\{x \\mid (x - x_c)^\\top P^{-1}(x - x_c) \\leq 1\\}\\]\nwhere \\(P \\in \\mathbb{S}_{++}^n\\) (P is positive definite symmetric)\nAlternative representation: \\(x_c + Au\\) where \\(A\\) is an invertible square matrix\n Figure: Euclidean balls have uniform radius in all directions, while ellipsoids stretch the ball along different axes. The matrix P in the ellipsoid formula determines the shape and orientation of the stretching."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#norm-balls-and-norm-cones",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#norm-balls-and-norm-cones",
    "title": "Lecture 2: Convex Sets",
    "section": "Norm Balls and Norm Cones",
    "text": "Norm Balls and Norm Cones\n\nNorm Ball\nA norm function \\(\\|\\cdot\\|\\) satisfies:\n\n\\(\\|x\\| \\geq 0\\), and \\(\\|x\\| = 0\\) only if \\(x = 0\\)\n\\(\\|tx\\| = |t| \\cdot \\|x\\|\\) for \\(t \\in \\mathbb{R}\\)\n\\(\\|x + y\\| \\leq \\|x\\| + \\|y\\|\\) (triangle inequality)\n\nNorm ball: \\(\\{x \\mid \\|x - x_c\\| \\leq r\\}\\)\n Figure: Different norms create different ball shapes. L1 norm creates a diamond, L2 norm creates a circle, and L‚àû norm creates a square. All satisfy the norm axioms but measure distance differently.\n\n\n\nNorm Cone\n\\[\\{(x, t) \\mid \\|x\\| \\leq t\\}\\]\nThe Euclidean norm cone is called the second-order cone.\n Figure: A norm cone extends infinitely upward, containing all points (x,t) where the norm of x is bounded by t. The second-order cone (using L2 norm) is fundamental in conic optimization and has special properties for efficient optimization."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#polyhedra",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#polyhedra",
    "title": "Lecture 2: Convex Sets",
    "section": "Polyhedra",
    "text": "Polyhedra\nDefined by inequality and equality constraints:\n\\[Ax \\preceq b, \\quad Cx = d\\]\nwhere \\(A \\in \\mathbb{R}^{m \\times n}\\) and \\(C \\in \\mathbb{R}^{p \\times n}\\)\nHere \\(Ax \\preceq b\\) denotes componentwise inequality, i.e., \\((Ax)_i \\leq b_i\\) for all \\(i\\).\n Figure: A polyhedron is the intersection of finitely many halfspaces and hyperplanes. Linear programming optimizes over polyhedra, making them fundamental to optimization theory. Every vertex represents a basic feasible solution."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#positive-semidefinite-cone",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#positive-semidefinite-cone",
    "title": "Lecture 2: Convex Sets",
    "section": "Positive Semidefinite Cone",
    "text": "Positive Semidefinite Cone\nLet \\(\\mathbb{S}^n\\) denote the set of \\(n \\times n\\) symmetric matrices.\n\n\\(\\mathbb{S}_+^n = \\{X \\in \\mathbb{S}^n \\mid X \\succeq 0\\}\\) ‚Äî positive semidefinite\n\n\\(\\mathbb{S}_+^n\\) is a convex cone\n\n\\(\\mathbb{S}_{++}^n = \\{X \\in \\mathbb{S}^n \\mid X \\succ 0\\}\\) ‚Äî positive definite\n\n Figure: The positive semidefinite cone contains all symmetric matrices with non-negative eigenvalues. It‚Äôs a convex cone in the space of symmetric matrices, fundamental to semidefinite programming (SDP)."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#preserving-convexity",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#preserving-convexity",
    "title": "Lecture 2: Convex Sets",
    "section": "Preserving Convexity",
    "text": "Preserving Convexity\n\nTesting Convexity\nTo determine if a set \\(C\\) is convex, check whether every convex combination of any two points in \\(C\\) is also in \\(C\\):\n\\[x_1, x_2 \\in C, \\quad 0 \\leq \\theta \\leq 1 \\implies \\theta x_1 + (1-\\theta)x_2 \\in C\\]\n\n\nOperations That Preserve Convexity\n1. Intersection: The intersection of convex sets is convex.\n Figure: Intersecting convex sets always produces a convex set. This property is crucial because it allows us to build complex convex sets by intersecting simpler ones, like defining polyhedra as intersections of halfspaces.\n2. Affine functions: \\(f(x) = Ax + b\\)\nIf \\(C\\) is convex, then both \\(f(C)\\) and \\(f^{-1}(C)\\) are convex.\n Figure: Affine transformations (scaling, rotation, translation) preserve convexity. If you take a convex set and apply an affine function, the image is still convex. Similarly, the pre-image of a convex set under an affine function is convex.\n3. Perspective functions: \\(P: \\mathbb{R}^{n+1} \\to \\mathbb{R}^n\\)\n\\[P(x, t) = x/t, \\quad \\text{dom}\\,P = \\{(x, t) \\mid t &gt; 0\\}\\]\n4. Linear-fractional functions:\n\\[f(x) = \\frac{Ax + b}{c^\\top x + d}, \\quad \\text{dom}\\,f = \\{x \\mid c^\\top x + d &gt; 0\\}\\]\nExample: \\(f(x) = \\frac{1}{x_1 + x_2 + 1}x\\)"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#generalized-inequalities",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#generalized-inequalities",
    "title": "Lecture 2: Convex Sets",
    "section": "Generalized Inequalities",
    "text": "Generalized Inequalities\nA convex cone \\(K \\subseteq \\mathbb{R}^n\\) is a proper cone if:\n\n\\(K\\) is closed (contains its boundary)\n\\(K\\) is solid (has nonempty interior)\n\\(K\\) is pointed (contains no line)\n\n\nExamples of Proper Cones\n\nNonnegative orthant: \\(K = \\mathbb{R}_+^n = \\{x \\in \\mathbb{R}^n \\mid x_i \\geq 0, \\, i=1,\\ldots,n\\}\\)\nPositive semidefinite cone: \\(\\mathbb{S}_+^n\\)\nNonnegative polynomials on [0,1]: \\[K = \\{x \\in \\mathbb{R}^n \\mid x_1 + x_2t + x_3t^2 + \\ldots + x_nt^{n-1} \\geq 0 \\text{ for } t \\in [0,1]\\}\\]\n\n\n\nGeneralized Inequality Notation\n\\[x \\preceq_K y \\Longleftrightarrow y - x \\in K\\] \\[x \\prec_K y \\Longleftrightarrow y - x \\in \\text{int}\\,K\\]\n\n\nExamples\n\nComponentwise inequality (\\(K = \\mathbb{R}_+^n\\)): \\[x \\preceq_{\\mathbb{R}_+^n} y \\Longleftrightarrow x_i \\leq y_i \\text{ for } i=1,2,\\ldots,n\\]\nMatrix inequality (\\(K = \\mathbb{S}_+^n\\)): \\[X \\preceq_{\\mathbb{S}_+^n} Y \\Longleftrightarrow Y - X \\text{ is positive semidefinite}\\]\n\n\n\nProperties Similar to \\(\\leq\\) in \\(\\mathbb{R}\\)\n\\[x \\preceq_K y, \\, u \\preceq_K v \\implies x + u \\preceq_K y + v\\]"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#minimum-and-minimal-elements",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#minimum-and-minimal-elements",
    "title": "Lecture 2: Convex Sets",
    "section": "Minimum and Minimal Elements",
    "text": "Minimum and Minimal Elements\nImportant: \\(\\preceq_K\\) is not a total ordering‚Äîwe can have \\(x \\not\\preceq_K y\\) and \\(y \\not\\preceq_K x\\) simultaneously.\n\nMinimum: \\(x\\) is the minimum element of \\(S\\) with respect to \\(\\preceq_K\\) if: \\[y \\in S \\implies x \\preceq_K y\\]\nMinimal: \\(x\\) is a minimal element of \\(S\\) with respect to \\(\\preceq_K\\) if: \\[y \\in S, \\, y \\preceq_K x \\implies y = x\\]\n\n Figure: The minimum element is unique and comparable to all other elements (if it exists). Minimal elements are not dominated by any other element in the set, but there can be multiple minimal elements that are incomparable to each other."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture2-math-foundations.html#key-insight",
    "href": "Math/EE364A/ee364a-lecture2-math-foundations.html#key-insight",
    "title": "Lecture 2: Convex Sets",
    "section": "Key Insight",
    "text": "Key Insight\nMathematical foundations of convex optimization rest on convex sets and generalized inequalities. Convex sets (balls, cones, polyhedra, semidefinite cones) form the geometric foundation. Affine functions, intersections, and perspective operations preserve convexity, allowing us to construct complex convex sets from simpler building blocks. Generalized inequalities (\\(\\preceq_K\\)) extend scalar ordering to vectors and matrices, enabling matrix inequalities (semidefinite constraints) and componentwise constraints. While generalized orderings lack total comparability, they retain key properties like additivity, making them powerful tools for formulating optimization problems over cones. These mathematical structures‚Äîconvex sets, proper cones, and their preservation under operations‚Äîform the bedrock of convex optimization theory."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#mathematical-definition",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#mathematical-definition",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Mathematical Definition",
    "text": "Mathematical Definition\nConstraint Optimization Problems\nMinimize \\(f_0(x)\\), subject to \\(f_i(x) \\leq b_i\\), \\(i=1,2,...,m\\)\n\n\\(x=(x_1,...,x_n)\\): optimization variable\n\\(f_0: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\): objective function\n\\(f_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), \\(i=1,2,...,m\\): constraint functions\n\nSolution or optimal point \\(x^*\\) has the smallest value of \\(f_0\\) among all vectors that satisfy the constraints."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#examples",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#examples",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Examples",
    "text": "Examples\n\nProfit Optimization\n\nVariables: amount invested in different assets\nConstraints: budget, max/min per asset, minimum return\nObjective: overall risk or return variance\n\n\n\nDevice Sizing in Electronic Circuits\n\nVariables: device width and lengths\nConstraints: manufacturing limits, timing requirements, maximum area\nObjective: power consumption\n\n\n\nData Fitting\n\nVariables: model parameters\nConstraints: prior information, parameter limits\nObjective: measure of misfit or prediction error, plus regularization term"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#solving-optimization-problems",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#solving-optimization-problems",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Solving Optimization Problems",
    "text": "Solving Optimization Problems\n\nGeneral Optimization Problems\n\nDifficult to solve\nSome compromise: very long computation time, or not always finding the solution\n\n\n\nExceptions\n\nLeast square problems\nLinear programming\nConvex optimization"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#least-squares",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#least-squares",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Least Squares",
    "text": "Least Squares\nMinimize \\(\\|Ax-b\\|_2^2\\)\n\nAnalytical solution: \\(x^* = (A^\\top A)^{-1}A^\\top b\\)\nComputation time: proportional to \\(n^2k\\) (where \\(A \\in \\mathbb{R}^{k \\times n}\\))"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#linear-programming",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#linear-programming",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Linear Programming",
    "text": "Linear Programming\nMinimize \\(c^\\top x\\)\nSubject to \\(a_i^\\top x \\leq b_i\\), \\(i=1,2,...,m\\)\n\nNo analytical solution\nReliable and efficient algorithms, software available\nComputation time: proportional to \\(n^2m\\) (if \\(m \\geq n\\))"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#convex-optimization",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#convex-optimization",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Convex Optimization",
    "text": "Convex Optimization\nMinimize \\(f_0(x)\\)\nSubject to \\(f_i(x) \\leq b_i\\)\nConvexity requirement: objective and constraint functions are convex:\n\\[f(\\alpha x + \\beta y) \\leq \\alpha f(x) + \\beta f(y)\\]\nwhere \\(\\alpha &gt; 0\\), \\(\\beta &gt; 0\\), \\(\\alpha + \\beta = 1\\)\n\nLeast squares and linear programming are special cases\nNo analytical solution\nComputation time: proportional to \\(\\max(n^3, n^2m, F)\\), where \\(F\\) is the cost of evaluating \\(f_i\\)‚Äôs and their first and second derivatives\n\n Figure: Comparing least squares, linear programming, and convex optimization in terms of complexity and solution methods. Least squares has analytical solutions, linear programming has polynomial-time algorithms, and convex optimization generalizes both while maintaining tractability."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#example-lamp-illumination-optimization",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#example-lamp-illumination-optimization",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Example: Lamp Illumination Optimization",
    "text": "Example: Lamp Illumination Optimization\nProblem: There are \\(m\\) lamps illuminating \\(n\\) patches. The goal is to choose lamp powers so that all patches have illumination \\(I_k\\) close to a desired value \\(I_{\\text{des}}\\), by minimizing the maximum log-illumination error, subject to power constraints.\n Figure: Lamp illumination optimization problem. Given m lamps and n patches, find lamp powers that achieve uniform illumination across all patches while respecting power constraints. This is a convex optimization problem that can be solved efficiently."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#key-insight",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#key-insight",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Key Insight",
    "text": "Key Insight\nConvex optimization bridges the gap between tractable special cases and general nonlinear programming. Least squares problems have closed-form solutions but are limited in expressiveness. General optimization is flexible but computationally intractable. Convex optimization occupies a sweet spot: it generalizes least squares and linear programming while maintaining polynomial-time solvability through interior-point methods. This makes convex optimization the foundation for practical applications in machine learning, control systems, signal processing, and engineering design."
  }
]