[
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "",
    "section": "Project Overview",
    "text": "Project Overview\nThis is a Quarto-based technical blog hosted on GitHub Pages (ickma2311.github.io). The site covers machine learning, algorithms, and technical tutorials with a focus on mathematical foundations and practical implementations."
  },
  {
    "objectID": "CLAUDE.html#common-commands",
    "href": "CLAUDE.html#common-commands",
    "title": "",
    "section": "Common Commands",
    "text": "Common Commands\n\nDevelopment Workflow\n\nquarto render - Build the entire website (outputs to docs/ directory)\nquarto preview - Start local development server with live reload\nquarto render &lt;file.qmd&gt; - Render a specific document\nquarto check - Verify Quarto installation and project setup\n\n\n\nContent Management\n\nCreate new ML content in ML/ directory\nCreate new algorithm content in Algorithm/ directory\nUpdate navigation by editing _quarto.yml navbar section\nAdd new content to respective index.qmd files for discoverability"
  },
  {
    "objectID": "CLAUDE.html#project-structure",
    "href": "CLAUDE.html#project-structure",
    "title": "",
    "section": "Project Structure",
    "text": "Project Structure\nâ”œâ”€â”€ _quarto.yml          # Main configuration file\nâ”œâ”€â”€ docs/                # Generated output (GitHub Pages source)\nâ”œâ”€â”€ index.qmd            # Homepage\nâ”œâ”€â”€ about.qmd            # About page\nâ”œâ”€â”€ ML/                  # Machine Learning content\nâ”‚   â”œâ”€â”€ index.qmd        # ML topics overview\nâ”‚   â”œâ”€â”€ *.qmd            # ML articles\nâ”‚   â””â”€â”€ *.ipynb          # Jupyter notebooks\nâ”œâ”€â”€ Algorithm/           # Algorithm content\nâ”‚   â”œâ”€â”€ index.qmd        # Algorithm topics overview\nâ”‚   â””â”€â”€ *.qmd            # Algorithm articles\nâ”œâ”€â”€ imgs/                # Image assets\nâ”œâ”€â”€ media/               # Media files\nâ””â”€â”€ styles.css           # Custom CSS styles"
  },
  {
    "objectID": "CLAUDE.html#content-organization",
    "href": "CLAUDE.html#content-organization",
    "title": "",
    "section": "Content Organization",
    "text": "Content Organization\nThe site uses a hierarchical navigation structure defined in _quarto.yml: - Two main sections: â€œMLâ€ and â€œAlgorithmâ€ - Each section has an index page that serves as a directory - Content is categorized by topic (e.g., â€œNumPy Fundamentalsâ€, â€œClustering Algorithmsâ€)\n\nAdding New Content\n\nCreate the content file in the appropriate directory (ML/ or Algorithm/)\nUpdate the corresponding index.qmd file to include the new content\nAdd navigation entry to _quarto.yml if it should appear in the navbar dropdown\nUse consistent frontmatter with title field"
  },
  {
    "objectID": "CLAUDE.html#configuration-notes",
    "href": "CLAUDE.html#configuration-notes",
    "title": "",
    "section": "Configuration Notes",
    "text": "Configuration Notes\n\nOutput directory is set to docs/ for GitHub Pages compatibility\nTheme: Cosmo with custom branding\nAll pages include table of contents (toc: true)\nSite uses custom CSS from styles.css\nJupyter notebooks are supported alongside Quarto markdown"
  },
  {
    "objectID": "CLAUDE.html#github-pages-deployment",
    "href": "CLAUDE.html#github-pages-deployment",
    "title": "",
    "section": "GitHub Pages Deployment",
    "text": "GitHub Pages Deployment\nThe site is automatically deployed from the docs/ directory. After rendering, commit and push the docs/ folder to trigger GitHub Pages rebuild. - Author is Chao Ma"
  },
  {
    "objectID": "CLAUDE.html#linkedin-post-guidelines",
    "href": "CLAUDE.html#linkedin-post-guidelines",
    "title": "",
    "section": "LinkedIn Post Guidelines",
    "text": "LinkedIn Post Guidelines\nWhen drafting LinkedIn posts for blog content, use these emojis: - Deep Learning topics: âˆ‡ (delta/nabla symbol) - represents gradients and optimization - Linear Algebra topics: ğŸ“ (triangle/ruler) - represents geometric and matrix concepts"
  },
  {
    "objectID": "ML/logistic_regression.html",
    "href": "ML/logistic_regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "The core idea of logistic regression is to model the probability of a binary outcome.\n\n\nWe model the probability that the target variable (y) is 1, given the features (x), using the sigmoid (or logistic) function, denoted by ().\n\\[\nP(y_i=1 \\mid x_i) = \\hat{y}_i = \\sigma(w^T x_i + b)\n\\]\nThe sigmoid function is defined as: \\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\nSince the outcome is binary, the probability of (y) being 0 is simply: \\[\nP(y_i=0 \\mid x_i) = 1 - \\hat{y}_i\n\\]\nThese two cases can be written compactly as a single equation, which is the probability mass function of a Bernoulli distribution: \\[\nP(y_i \\mid x_i) = \\hat{y}_i^{y_i} (1 - \\hat{y}_i)^{1 - y_i}\n\\]"
  },
  {
    "objectID": "ML/logistic_regression.html#model-formulation",
    "href": "ML/logistic_regression.html#model-formulation",
    "title": "Logistic Regression",
    "section": "",
    "text": "The core idea of logistic regression is to model the probability of a binary outcome.\n\n\nWe model the probability that the target variable (y) is 1, given the features (x), using the sigmoid (or logistic) function, denoted by ().\n\\[\nP(y_i=1 \\mid x_i) = \\hat{y}_i = \\sigma(w^T x_i + b)\n\\]\nThe sigmoid function is defined as: \\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\nSince the outcome is binary, the probability of (y) being 0 is simply: \\[\nP(y_i=0 \\mid x_i) = 1 - \\hat{y}_i\n\\]\nThese two cases can be written compactly as a single equation, which is the probability mass function of a Bernoulli distribution: \\[\nP(y_i \\mid x_i) = \\hat{y}_i^{y_i} (1 - \\hat{y}_i)^{1 - y_i}\n\\]"
  },
  {
    "objectID": "ML/logistic_regression.html#loss-function-binary-cross-entropy",
    "href": "ML/logistic_regression.html#loss-function-binary-cross-entropy",
    "title": "Logistic Regression",
    "section": "Loss Function (Binary Cross-Entropy)",
    "text": "Loss Function (Binary Cross-Entropy)\nTo find the optimal parameters (w) and (b), we use Maximum Likelihood Estimation (MLE). We want to find the parameters that maximize the probability of observing our given dataset.\n\n1. Likelihood\nThe likelihood is the joint probability of observing all (n) data points, assuming they are independent and identically distributed (i.i.d.): \\[\n\\mathcal{L}(w, b) = \\prod_{i=1}^n P(y_i \\mid x_i) = \\prod_{i=1}^n \\hat{y}_i^{y_i} (1 - \\hat{y}_i)^{1 - y_i}\n\\]\n\n\n2. Log-Likelihood\nWorking with products is difficult, so we take the logarithm of the likelihood. Maximizing the log-likelihood is equivalent to maximizing the likelihood.\n\\[\n\\log \\mathcal{L}(w, b) = \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n\\]\n\n\n3. Cost Function\nIn machine learning, we frame problems as minimizing a cost function. The standard convention is to minimize the negative log-likelihood. This gives us the Binary Cross-Entropy loss, (J(w, b)).\n\\[\nJ(w, b) = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n\\] The \\(\\frac{1}{n}\\) term is an average over the training examples and doesnâ€™t change the minimum, but it helps in stabilizing the training process."
  },
  {
    "objectID": "ML/logistic_regression.html#refernce-bernoulli-distribution",
    "href": "ML/logistic_regression.html#refernce-bernoulli-distribution",
    "title": "Logistic Regression",
    "section": "refernce: Bernoulli Distribution",
    "text": "refernce: Bernoulli Distribution\nFormular \\[\nP(y) = p^y (1 - p)^{1 - y}, \\quad y \\in \\{0, 1\\}\n\\] * when y = 1ï¼š\\(P(y=1) = p^1 (1 - p)^0 = p\\) * when y=0: $P(y=0) = p^0 (1 - p)^1 = 1 - p $"
  },
  {
    "objectID": "ML/axis.html",
    "href": "ML/axis.html",
    "title": "Understanding Axis(dim) Operations Smartly",
    "section": "",
    "text": "import numpy as np\n\nx = np.array([[1, 2, 3], [4, 5, 6]])\nThe 2D array is: \\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\]\n\n\nprint(x.sum(axis=0))\nThe result is:\narray([5, 7, 9])\nWhen axis(dim) is 0, it means the operation is performed along 0 dimension. Items along 0 dimension are each sub-array. Then the result is just two vectors added together.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n4 & 5 & 6\n\\end{bmatrix}\n\\]\nx.sum(axis=1)==x[0]+x[1]\nOperations along axis 0 is just operate on all sub-arrays. For example,\nsum(x,axis=0) is just \\(\\vec{x[0]}+\\vec{x[1]}+...+\\vec{x[n]}\\)\n\n\n\nprint(x.sum(axis=1))\nThe result is:\narray([6, 15])\nWhen axis(dim) is 1, it means the operation is performed along 1 dimension.\n\\[\n\\begin{bmatrix}\n1+2+3 \\\\\n4+5+6\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "ML/axis.html#a-2d-example",
    "href": "ML/axis.html#a-2d-example",
    "title": "Understanding Axis(dim) Operations Smartly",
    "section": "",
    "text": "import numpy as np\n\nx = np.array([[1, 2, 3], [4, 5, 6]])\nThe 2D array is: \\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\]\n\n\nprint(x.sum(axis=0))\nThe result is:\narray([5, 7, 9])\nWhen axis(dim) is 0, it means the operation is performed along 0 dimension. Items along 0 dimension are each sub-array. Then the result is just two vectors added together.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n4 & 5 & 6\n\\end{bmatrix}\n\\]\nx.sum(axis=1)==x[0]+x[1]\nOperations along axis 0 is just operate on all sub-arrays. For example,\nsum(x,axis=0) is just \\(\\vec{x[0]}+\\vec{x[1]}+...+\\vec{x[n]}\\)\n\n\n\nprint(x.sum(axis=1))\nThe result is:\narray([6, 15])\nWhen axis(dim) is 1, it means the operation is performed along 1 dimension.\n\\[\n\\begin{bmatrix}\n1+2+3 \\\\\n4+5+6\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "ML/axis.html#a-3d-example",
    "href": "ML/axis.html#a-3d-example",
    "title": "Understanding Axis(dim) Operations Smartly",
    "section": "A 3D example",
    "text": "A 3D example\nx_3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nThe 3D array looks like:\n\\[\nX = \\left[\\begin{array}{c|c}\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix} &\n\\begin{bmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{bmatrix}\n\\end{array}\\right]\n\\]\n\nsum along axis 0\nprint(x_3d.sum(axis=0))\nThe result is:\narray([[8, 10, 12], [18, 20, 22]])\nThe result is the sum of two matrices.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n8 & 10 & 12 \\\\\n18 & 20 & 22\n\\end{bmatrix}\n\\]\nWhen axis(dim) is 0, given each element in this dimension is the matrix, so the sum is the sum of two matrices.\n\n\nsum along axis 1\nprint(x_3d.sum(axis=1))\nThe result is:\n\nWhen axis(dim) is 1, given each element in this dimension is the rows of the matrix, so the sum is the sum of all the rows in each matrix.\n\\[\n[\\begin{array}{c|c}\n\\begin{bmatrix}\n1 & 2 & 3\n\\end{bmatrix} +\n\\begin{bmatrix}\n4 & 5 & 6\n\\end{bmatrix} &\n\\begin{bmatrix}\n7 & 8 & 9\n\\end{bmatrix} +\n\\begin{bmatrix}\n10 & 11 & 12\n\\end{bmatrix}\n\\end{array}]\n\\]\nso the result is:\narray([[5,7,9], [17, 19, 21]])\n\n\nsum along axis 2\nWhen axis(dim) is 2, given each element in this dimension is the elements of the matrix, so the sum is the sum of the elements.\n\\[\n\\begin{array}{c|c}\n\\begin{bmatrix}\n1+2+3,4+5+6\n\\end{bmatrix} &\n\\begin{bmatrix}\n7+8+9,10+11+12\n\\end{bmatrix}\n\\end{array}\n\\]\nso the result is:\narray([[6, 15], [24, 33]])\nAso, when axis(dim) is -1, it means the operation is performed along the last dimension. So for 2d array, axis -1 is the same as axis 1."
  },
  {
    "objectID": "ML/axis.html#rules",
    "href": "ML/axis.html#rules",
    "title": "Understanding Axis(dim) Operations Smartly",
    "section": "Rules",
    "text": "Rules\n\nwhen operate on axis(dim) N, it means the operation is performed along the elements of dimension [0â€¦N].\nfor 2d array, axis 0 is the sum of vectors, because each element of the array is a vector, computer sees m vectors at once for a (m,n) shape array.\nfor 3d array, axis 0 is the sum of matrices, because each element of the array is a matrix. Computer sees m matrices at once for a (m,n,p) shape array.\nfor 2d array, axis 1 is the sum of elements in each vector and merge back to (m,1) shape array. Computer sees 1 vector with n elements at once for m times.\nfor 3d array, axis 1 is the sum of all vectors in each matrix and merge back to (m,1,p) shape array. Computer sees n vectors at once for m times.\nfor 3d array, axis 2 is the sum of each vector in each matrix and merge back to (m,n,1) shape array. Computer sees p vectors for m*n times."
  },
  {
    "objectID": "ML/xor-deep-learning.html",
    "href": "ML/xor-deep-learning.html",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.1 shows how ReLU activations let neural networks solve the XOR problem that defeats any linear model.\nğŸ““ For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "ML/xor-deep-learning.html#the-xor-problem-a-challenge-for-linear-models",
    "href": "ML/xor-deep-learning.html#the-xor-problem-a-challenge-for-linear-models",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "The XOR Problem: A Challenge for Linear Models",
    "text": "The XOR Problem: A Challenge for Linear Models\nXOR (Exclusive OR) returns 1 precisely when the two binary inputs differ:\n\\[\\text{XOR}(x_1, x_2) = \\begin{pmatrix}0 & 1\\\\1 & 0\\end{pmatrix}\\]\nThe XOR truth table shows why this is challenging for linear models - the positive class (1) appears at diagonally opposite corners, making it impossible to separate with any single straight line.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Define XOR dataset\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([0, 1, 1, 0])\n\nprint(\"XOR Truth Table:\")\nprint(\"================\")\nprint()\nprint(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\")\nprint(\"â”‚ Input   â”‚ Output â”‚\")\nprint(\"â”‚ (xâ‚,xâ‚‚) â”‚  XOR   â”‚\")\nprint(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\nfor i in range(4):\n    input_str = f\"({X[i,0]}, {X[i,1]})\"\n    output_str = f\"{y[i]}\"\n    print(f\"â”‚ {input_str:7} â”‚   {output_str:2}   â”‚\")\nprint(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\nprint()\nprint(\"Notice: XOR = 1 when inputs differ, XOR = 0 when inputs match\")\n\n\nXOR Truth Table:\n================\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Input   â”‚ Output â”‚\nâ”‚ (xâ‚,xâ‚‚) â”‚  XOR   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ (0, 0)  â”‚   0    â”‚\nâ”‚ (0, 1)  â”‚   1    â”‚\nâ”‚ (1, 0)  â”‚   1    â”‚\nâ”‚ (1, 1)  â”‚   0    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nNotice: XOR = 1 when inputs differ, XOR = 0 when inputs match"
  },
  {
    "objectID": "ML/xor-deep-learning.html#limitation-1-single-layer-linear-model",
    "href": "ML/xor-deep-learning.html#limitation-1-single-layer-linear-model",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Limitation 1: Single Layer Linear Model",
    "text": "Limitation 1: Single Layer Linear Model\nA single layer perceptron can only create linear decision boundaries. Letâ€™s see what happens when we try to solve XOR with logistic regression:\n\n\nShow code\n# Demonstrate single layer linear model failure\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\ncolors = ['red', 'blue']\n\n# Plot XOR data\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'Class {i}', edgecolors='black', linewidth=2)\n\n# Overlay representative linear separators to illustrate the impossibility\nx_line = np.linspace(-0.2, 1.2, 100)\nax.plot(x_line, 0.5 * np.ones_like(x_line), '--', color='gray', alpha=0.7, label='candidate lines')\nax.plot(0.5 * np.ones_like(x_line), x_line, '--', color='orange', alpha=0.7)\nax.plot(x_line, x_line, '--', color='green', alpha=0.7)\nax.plot(x_line, 1 - x_line, '--', color='purple', alpha=0.7)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('xâ‚', fontsize=12)\nax.set_ylabel('xâ‚‚', fontsize=12)\nax.set_title('XOR Problem: No Linear Solution', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Fit logistic regression just to report its performance\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\naccuracy = log_reg.score(X, y)\nprint(f'Single layer model accuracy: {accuracy:.1%} - still misclassifies XOR.')\n\n\n\n\n\n\n\n\n\nSingle layer model accuracy: 50.0% - still misclassifies XOR."
  },
  {
    "objectID": "ML/xor-deep-learning.html#limitation-2-multiple-layer-linear-model-without-activation",
    "href": "ML/xor-deep-learning.html#limitation-2-multiple-layer-linear-model-without-activation",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Limitation 2: Multiple Layer Linear Model (Without Activation)",
    "text": "Limitation 2: Multiple Layer Linear Model (Without Activation)\nEven stacking multiple linear layers doesnâ€™t help! Multiple linear transformations are mathematically equivalent to a single linear transformation.\nMathematical proof:\n\\[\\text{Layer 1: } h_1 = W_1 x + b_1\\] \\[\\text{Layer 2: } h_2 = W_2 h_1 + b_2 = W_2(W_1 x + b_1) + b_2 = (W_2W_1)x + (W_2b_1 + b_2)\\]\nResult: Still just \\(Wx + b\\) (a single linear transformation)\nConclusion: Stacking linear layers without activation functions doesnâ€™t increase the modelâ€™s expressive power!"
  },
  {
    "objectID": "ML/xor-deep-learning.html#the-solution-relu-activation-function",
    "href": "ML/xor-deep-learning.html#the-solution-relu-activation-function",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "The Solution: ReLU Activation Function",
    "text": "The Solution: ReLU Activation Function\nReLU (Rectified Linear Unit) provides the nonlinearity needed to solve XOR: - ReLU(z) = max(0, z) - Clips negative values to zero, keeping positive values unchanged\nUsing the hand-crafted network from the next code cell, the forward pass can be written compactly in matrix form:\n\\[\nX = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix},\n\\quad\nW_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix},\n\\quad\nb_1 = \\begin{bmatrix} 0 & 0 \\end{bmatrix}\n\\]\n\\[\nZ = X W_1^{\\top} + b_1 = \\begin{bmatrix} 0 & 0 \\\\ -1 & 1 \\\\ 1 & -1 \\\\ 0 & 0 \\end{bmatrix},\n\\qquad\nH = \\text{ReLU}(Z) = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \\end{bmatrix}\n\\]\nWith output parameters \\[\nw_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix},\n\\quad\nb_2 = -0.5\n\\] the final linear scores are \\[\na = H w_2^{\\top} + b_2 = \\begin{bmatrix} -0.5 \\\\ 0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}\n\\Rightarrow\n\\text{sign}_+(a) = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\n\\]\nHere \\(\\text{sign}_+(a)\\) maps non-negative entries to 1 and negative entries to 0. Letâ€™s see how ReLU transforms the XOR problem to make it solvable.\n\n\nShow code\n# Hand-crafted network weights and biases that solve XOR\nfrom IPython.display import display, Math\n\ndef relu(z):\n    return np.maximum(0, z)\n\nW1 = np.array([[1, -1],\n               [-1, 1]])\nb1 = np.array([0, 0])\nw2 = np.array([1, 1])\nb2 = -0.5\n\ndisplay(Math(r\"\\text{Hidden layer: } W_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}\"))\ndisplay(Math(r\"b_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\"))\ndisplay(Math(r\"\\text{Output layer: } w_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix}\"))\ndisplay(Math(r\"b_2 = -0.5\"))\n\ndef forward_pass(X, W1, b1, w2, b2):\n    z1 = X @ W1.T + b1\n    h1 = relu(z1)\n    logits = h1 @ w2 + b2\n    return logits, h1, z1\n\nlogits, hidden_activations, pre_activations = forward_pass(X, W1, b1, w2, b2)\npredictions = (logits &gt;= 0).astype(int)\n\nprint(\"Step-by-step Forward Pass Results:\")\nprint(\"=\" * 80)\nprint()\nprint(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\nprint(\"â”‚ Input   â”‚  Before ReLU     â”‚  After ReLU      â”‚  Logit  â”‚   Pred   â”‚\")\nprint(\"â”‚ (xâ‚,xâ‚‚) â”‚    (zâ‚, zâ‚‚)      â”‚    (hâ‚, hâ‚‚)      â”‚  score  â”‚  class   â”‚\")\nprint(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\nfor i in range(len(X)):\n    x1, x2 = X[i]\n    z1_vals = pre_activations[i]\n    h1_vals = hidden_activations[i]\n    logit = logits[i]\n    pred = predictions[i]\n    \n    input_str = f\"({x1:.0f}, {x2:.0f})\"\n    pre_relu_str = f\"({z1_vals[0]:4.1f}, {z1_vals[1]:4.1f})\"\n    post_relu_str = f\"({h1_vals[0]:4.1f}, {h1_vals[1]:4.1f})\"\n    logit_str = f\"{logit:6.2f}\"\n    pred_str = f\"{pred:4d}\"\n    \n    print(f\"â”‚ {input_str:7} â”‚ {pre_relu_str:16} â”‚ {post_relu_str:16} â”‚ {logit_str:7} â”‚ {pred_str:8} â”‚\")\nprint(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n\naccuracy = (predictions == y).mean()\nprint(f\"\\nNetwork Accuracy: {accuracy:.0%} âœ…\")\nprint(\"\\nKey transformations:\")\nprint(\"â€¢ (-1, 1) â†’ (0, 1) makes XOR(0,1) = 1 separable\")\nprint(\"â€¢ ( 1,-1) â†’ (1, 0) makes XOR(1,0) = 1 separable\")\n\n\n\\(\\displaystyle \\text{Hidden layer: } W_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle b_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle \\text{Output layer: } w_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle b_2 = -0.5\\)\n\n\nStep-by-step Forward Pass Results:\n================================================================================\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Input   â”‚  Before ReLU     â”‚  After ReLU      â”‚  Logit  â”‚   Pred   â”‚\nâ”‚ (xâ‚,xâ‚‚) â”‚    (zâ‚, zâ‚‚)      â”‚    (hâ‚, hâ‚‚)      â”‚  score  â”‚  class   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ (0, 0)  â”‚ ( 0.0,  0.0)     â”‚ ( 0.0,  0.0)     â”‚  -0.50  â”‚    0     â”‚\nâ”‚ (0, 1)  â”‚ (-1.0,  1.0)     â”‚ ( 0.0,  1.0)     â”‚   0.50  â”‚    1     â”‚\nâ”‚ (1, 0)  â”‚ ( 1.0, -1.0)     â”‚ ( 1.0,  0.0)     â”‚   0.50  â”‚    1     â”‚\nâ”‚ (1, 1)  â”‚ ( 0.0,  0.0)     â”‚ ( 0.0,  0.0)     â”‚  -0.50  â”‚    0     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nNetwork Accuracy: 100% âœ…\n\nKey transformations:\nâ€¢ (-1, 1) â†’ (0, 1) makes XOR(0,1) = 1 separable\nâ€¢ ( 1,-1) â†’ (1, 0) makes XOR(1,0) = 1 separable\n\n\n\nTransformation Table: How ReLU Solves XOR\nLetâ€™s trace through exactly what happens to each input:\n\n\nShow code\n\n# Create detailed transformation table\nprint(\"Complete Transformation Table:\")\nprint(\"=============================\")\nprint()\nprint(\"Input   | Pre-ReLU  | Post-ReLU | Logit | Prediction | Target | Correct?\")\nprint(\"(xâ‚,xâ‚‚) | (zâ‚, zâ‚‚)  | (hâ‚, hâ‚‚)  | score | class      | y      |\")\nprint(\"--------|-----------|-----------|-------|------------|--------|----------\")\n\nfor i in range(4):\n    input_str = f\"({X[i,0]},{X[i,1]})\"\n    pre_relu_str = f\"({pre_activations[i,0]:2.0f},{pre_activations[i,1]:2.0f})\"\n    post_relu_str = f\"({hidden_activations[i,0]:.0f},{hidden_activations[i,1]:.0f})\"\n    logit_str = f\"{logits[i]:.2f}\"\n    pred_str = f\"{predictions[i]}\"\n    target_str = f\"{y[i]}\"\n    correct_str = \"âœ“\" if predictions[i] == y[i] else \"âœ—\"\n\n    print(f\"{input_str:7} | {pre_relu_str:9} | {post_relu_str:9} | {logit_str:5} | {pred_str:10} | {target_str:6} | {correct_str}\")\n\nprint()\nprint(\"Key Insight: ReLU transforms (-1,1) â†’ (0,1) and (1,-1) â†’ (1,0)\")\nprint(\"This makes the XOR classes linearly separable in the hidden space!\")\n\n\nComplete Transformation Table:\n=============================\n\nInput   | Pre-ReLU  | Post-ReLU | Logit | Prediction | Target | Correct?\n(xâ‚,xâ‚‚) | (zâ‚, zâ‚‚)  | (hâ‚, hâ‚‚)  | score | class      | y      |\n--------|-----------|-----------|-------|------------|--------|----------\n(0,0)   | ( 0, 0)   | (0,0)     | -0.50 | 0          | 0      | âœ“\n(0,1)   | (-1, 1)   | (0,1)     | 0.50  | 1          | 1      | âœ“\n(1,0)   | ( 1,-1)   | (1,0)     | 0.50  | 1          | 1      | âœ“\n(1,1)   | ( 0, 0)   | (0,0)     | -0.50 | 0          | 0      | âœ“\n\nKey Insight: ReLU transforms (-1,1) â†’ (0,1) and (1,-1) â†’ (1,0)\nThis makes the XOR classes linearly separable in the hidden space!\n\n\n\n\nStep 1: Original Input Space\nThe XOR problem in its raw form - notice how no single line can separate the classes:\n\n\nShow code\n# Step 1 visualization: Original Input Space\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'XOR = {i}', edgecolors='black', linewidth=2)\n\n# Annotate each point\nfor i in range(4):\n    ax.annotate(f'({X[i,0]},{X[i,1]})', X[i], xytext=(10, 10), \n                textcoords='offset points', fontsize=10)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('xâ‚', fontsize=12)\nax.set_ylabel('xâ‚‚', fontsize=12)\nax.set_title('Step 1: Original Input Space', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Linear Transformation (Before ReLU)\nThe network applies weights Wâ‚ and biases bâ‚ to transform the input space:\n\n\nShow code\n# Step 2 visualization: Pre-activation space\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\nfor i in range(4):\n    ax.scatter(pre_activations[i, 0], pre_activations[i, 1], \n               c=colors[y[i]], s=200, edgecolors='black', linewidth=2)\n\n# Draw ReLU boundaries\nax.axhline(0, color='gray', linestyle='-', alpha=0.8, linewidth=2)\nax.axvline(0, color='gray', linestyle='-', alpha=0.8, linewidth=2)\n\n# Shade all regions where coordinates turn negative (and thus get clipped by ReLU)\nax.axvspan(-1.2, 0, alpha=0.15, color='red')\nax.axhspan(-1.2, 0, alpha=0.15, color='red')\nax.text(-0.75, 0.85, 'Negative zâ‚ â†’ ReLU sets to 0', ha='left', fontsize=10,\n        bbox=dict(boxstyle='round', facecolor='red', alpha=0.25))\nax.text(0.95, -0.75, 'Negative zâ‚‚ â†’ ReLU sets to 0', ha='right', fontsize=10,\n        bbox=dict(boxstyle='round', facecolor='red', alpha=0.25))\n\n# Annotate points with input labels\nlabels = ['(0,0)', '(0,1)', '(1,0)', '(1,1)']\nfor i, label in enumerate(labels):\n    pre_coord = f'({pre_activations[i,0]:.0f},{pre_activations[i,1]:.0f})'\n    ax.annotate(f'{label}â†’{pre_coord}', pre_activations[i], xytext=(10, 10), \n                textcoords='offset points', fontsize=9)\n\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1.2, 1.2)\nax.set_xlabel('zâ‚ (Pre-activation)', fontsize=12)\nax.set_ylabel('zâ‚‚ (Pre-activation)', fontsize=12)\nax.set_title('Step 2: Before ReLU (Linear Transform)', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 3: ReLU Transformation\nReLU clips negative values to zero, transforming the space to make it linearly separable:\n\n\nShow code\n# Step 3 visualization: ReLU transformation with arrows\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\n\nfor i in range(4):\n    # Pre-ReLU positions (X marks)\n    ax.scatter(pre_activations[i, 0], pre_activations[i, 1], \n               marker='x', s=150, c=colors[y[i]], alpha=0.5, linewidth=3)\n    # Post-ReLU positions (circles) \n    ax.scatter(hidden_activations[i, 0], hidden_activations[i, 1], \n               marker='o', s=200, c=colors[y[i]], edgecolors='black', linewidth=2)\n    \n    # Draw transformation arrows\n    start = pre_activations[i]\n    end = hidden_activations[i]\n    if not np.array_equal(start, end):\n        ax.annotate('', xy=end, xytext=start,\n                    arrowprops=dict(arrowstyle='-&gt;', lw=2, color=colors[y[i]], alpha=0.8))\n\n\n# Add text box explaining the key transformation\nax.text(0.5, 0.8, 'ReLU clips negative coordinates to zero\\n(-1,1) â†’ (0,1) and (1,-1) â†’ (1,0)', \n        ha='center', va='center', fontsize=11, \n        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n\nax.set_xlim(-1.2, 1.1)\nax.set_ylim(-1.2, 1.1)\nax.set_xlabel('Hidden dimension 1', fontsize=12)\nax.set_ylabel('Hidden dimension 2', fontsize=12)\nax.set_title('Step 3: ReLU Mapping (Before â†’ After)', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Final Classification\nWith the transformed hidden representation, the network can now perfectly classify XOR:\n\n\nShow code\n\n\n# Step 4 visualization: Final classification results\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\n# Create decision boundary\nxx, yy = np.meshgrid(np.linspace(-0.2, 1.2, 100), np.linspace(-0.2, 1.2, 100))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\ngrid_logits, _, _ = forward_pass(grid_points, W1, b1, w2, b2)\ngrid_preds = (grid_logits &gt;= 0).astype(int).reshape(xx.shape)\n\nax.contourf(xx, yy, grid_preds, levels=[-0.5, 0.5, 1.5], \n            colors=['#ffcccc', '#ccccff'], alpha=0.6)\nax.contour(xx, yy, grid_logits.reshape(xx.shape), levels=[0], \n           colors='black', linewidths=2, linestyles='--')\n\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'XOR = {i}', edgecolors='black', linewidth=2)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('xâ‚', fontsize=12)\nax.set_ylabel('xâ‚‚', fontsize=12)\nax.set_title('Step 4: Final Classification (100% Accuracy)', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nsample_logits, _, _ = forward_pass(X, W1, b1, w2, b2)\nsample_preds = (sample_logits &gt;= 0).astype(int)\nfor i in range(4):\n    pred_text = f'Pred: {sample_preds[i]}'\n    ax.annotate(pred_text, X[i], xytext=(10, -15), \n                textcoords='offset points', fontsize=9,\n                bbox=dict(boxstyle='round,pad=0.2', facecolor='lightgreen', alpha=0.7))\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ML/xor-deep-learning.html#conclusion",
    "href": "ML/xor-deep-learning.html#conclusion",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Conclusion",
    "text": "Conclusion\nThe XOR problem demonstrates several fundamental principles in deep learning:\n\nNecessity of Nonlinearity: Linear models cannot solve XOR, establishing the critical role of nonlinear activation functions.\nUniversal Approximation: Even simple architectures with sufficient nonlinearity can solve complex classification problems."
  },
  {
    "objectID": "ML/hessian-prerequisites.html",
    "href": "ML/hessian-prerequisites.html",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "",
    "text": "My notebook\nBefore diving into optimization algorithms for deep learning (Chapter 7), we need to understand second-order derivatives in multiple dimensions. The Hessian matrix is the key tool that generalizes the concept of curvature to high-dimensional spaces."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#context",
    "href": "ML/hessian-prerequisites.html#context",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "",
    "text": "My notebook\nBefore diving into optimization algorithms for deep learning (Chapter 7), we need to understand second-order derivatives in multiple dimensions. The Hessian matrix is the key tool that generalizes the concept of curvature to high-dimensional spaces."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#why-second-derivatives-matter",
    "href": "ML/hessian-prerequisites.html#why-second-derivatives-matter",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Why Second Derivatives Matter",
    "text": "Why Second Derivatives Matter\nIn one dimension, optimizing \\(f(x)\\) involves:\n\nFirst derivative \\(f'(x) = 0\\) â†’ Find critical points\n\nWhy set \\(f'(x) = 0\\)? At a minimum or maximum, the slope is flat (zero)\nThink of a hill: at the very top, you stop going up â†’ slope = 0\nAt the bottom of a valley, you stop going down â†’ slope = 0\nExample: For \\(f(x) = x^2\\), we have \\(f'(x) = 2x\\). Setting \\(f'(x) = 0\\) gives \\(x = 0\\) (the minimum)\n\nSecond derivative \\(f''(x)\\) â†’ Classify the critical point:\n\n\\(f''(x) &gt; 0\\) â†’ Local minimum (curves upward like a bowl)\n\\(f''(x) &lt; 0\\) â†’ Local maximum (curves downward like a dome)\n\\(f''(x) = 0\\) â†’ Inconclusive (could be an inflection point)\nWhy needed? Not all points where \\(f'(x) = 0\\) are minima! For \\(f(x) = x^3\\), we have \\(f'(0) = 0\\) but itâ€™s neither a min nor max.\n\n\nThe challenge: How do we extend this to functions of many variables \\(f(x_1, x_2, \\ldots, x_n)\\)?\nThe answer: The Hessian matrix captures all second-order information.\n\nVisualizing Second Derivatives\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = np.linspace(-3, 3, 200)\n\n# Three functions with different second derivatives\nf1 = x**2           # f''(x) = 2 (positive, curves up)\nf2 = -x**2          # f''(x) = -2 (negative, curves down)\nf3 = x**3           # f''(x) = 6x (changes sign at x=0)\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\n\n# Function 1: f(x) = xÂ²\naxes[0, 0].plot(x, f1, 'b-', linewidth=2)\naxes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 0].set_title(\"f(x) = xÂ²\\nf''(x) = 2 &gt; 0\\n(Curves UP)\", fontsize=10)\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel('f(x)')\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].annotate('Minimum', xy=(0, 0), xytext=(0.5, 2),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Function 2: f(x) = -xÂ²\naxes[0, 1].plot(x, f2, 'r-', linewidth=2)\naxes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 1].set_title(\"f(x) = -xÂ²\\nf''(x) = -2 &lt; 0\\n(Curves DOWN)\", fontsize=10)\naxes[0, 1].set_xlabel('x')\naxes[0, 1].set_ylabel('f(x)')\naxes[0, 1].grid(True, alpha=0.3)\naxes[0, 1].annotate('Maximum', xy=(0, 0), xytext=(0.5, -2),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Function 3: f(x) = xÂ³\naxes[1, 0].plot(x, f3, 'g-', linewidth=2)\naxes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[1, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[1, 0].set_title(\"f(x) = xÂ³\\nf''(x) = 6x\\n(Changes sign)\", fontsize=10)\naxes[1, 0].set_xlabel('x')\naxes[1, 0].set_ylabel('f(x)')\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].annotate('Inflection point', xy=(0, 0), xytext=(1, -10),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Hide the unused subplot\naxes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nKey observations:\n\n\n\n\n\n\n\n\n\nSecond Derivative\nCurvature\nShape\nPoint Type\n\n\n\n\n\\(f''(x) &gt; 0\\)\nCurves upward\nBowl shape\nPotential minimum\n\n\n\\(f''(x) &lt; 0\\)\nCurves downward\nDome shape\nPotential maximum\n\n\n\\(f''(x) = 0\\) (at critical point)\nChanges sign\nFlat at that point\nInflection point\n\n\n\nNote on the third example: For \\(f(x) = x^3\\), we have \\(f''(x) = 6x\\). At the critical point \\(x = 0\\), \\(f''(0) = 0\\), which is inconclusive. The curvature changes sign: negative for \\(x &lt; 0\\) and positive for \\(x &gt; 0\\)."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#the-hessian-matrix",
    "href": "ML/hessian-prerequisites.html#the-hessian-matrix",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "The Hessian Matrix",
    "text": "The Hessian Matrix\n\nDefinition\nFor a scalar function \\(f(\\mathbf{x}) = f(x_1, x_2, \\ldots, x_n)\\), the Hessian matrix is the square matrix of all second-order partial derivatives:\n\\[\nH(f) =\n\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n\\]\n\n\nKey Properties\n\nSymmetric: If mixed partial derivatives are continuous, then \\(\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}\\), so \\(H = H^T\\).\nShape: Always \\(n \\times n\\) (determined by number of variables, not terms in the function)\nDescribes curvature in all directions simultaneously\nEigenvalue decomposition: Since the Hessian is symmetric, it can be decomposed as \\(H = Q\\Lambda Q^T\\) where \\(Q\\) contains orthonormal eigenvectors and \\(\\Lambda\\) is a diagonal matrix of eigenvalues\n\n\n\nSimple Example\nFor \\(f(x, y) = x^2 + 3y^2\\):\nStep 1: Compute first derivatives \\[\n\\frac{\\partial f}{\\partial x} = 2x, \\quad \\frac{\\partial f}{\\partial y} = 6y\n\\]\nStep 2: Compute second derivatives \\[\n\\frac{\\partial^2 f}{\\partial x^2} = 2, \\quad \\frac{\\partial^2 f}{\\partial y^2} = 6, \\quad \\frac{\\partial^2 f}{\\partial x \\partial y} = 0\n\\]\nStep 3: Build Hessian \\[\nH = \\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}\n\\]\nInterpretation: - Curvature along \\(x\\)-axis: 2 - Curvature along \\(y\\)-axis: 6 - No cross-dependency (off-diagonal = 0)\n\n\nExample with Cross Terms\nFor \\(f(x, y) = x^2 + xy + y^2\\):\n\\[\nH = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\n\\]\nThe off-diagonal term (1) indicates that \\(x\\) and \\(y\\) are coupledâ€”changing one affects the rate of change of the other."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#matrix-definiteness",
    "href": "ML/hessian-prerequisites.html#matrix-definiteness",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Matrix Definiteness",
    "text": "Matrix Definiteness\nFor a symmetric matrix \\(A\\), its definiteness is determined by the signs of its eigenvalues.\n\n\n\n\n\n\n\n\n\nType\nEigenvalues\nQuadratic Form \\(x^T A x\\)\nGeometric Shape\n\n\n\n\nPositive definite (PD)\nall \\(&gt; 0\\)\n\\(&gt; 0\\) for all \\(x \\neq 0\\)\nBowl (curves upward)\n\n\nNegative definite (ND)\nall \\(&lt; 0\\)\n\\(&lt; 0\\) for all \\(x \\neq 0\\)\nDome (curves downward)\n\n\nIndefinite\nsome \\(+\\), some \\(-\\)\ndepends on direction\nSaddle\n\n\nPositive semi-definite (PSD)\nall \\(\\geq 0\\)\n\\(\\geq 0\\) for all \\(x\\)\nFlat-bottom bowl\n\n\nNegative semi-definite (NSD)\nall \\(\\leq 0\\)\n\\(\\leq 0\\) for all \\(x\\)\nFlat-top dome\n\n\n\n\nQuick Test (2Ã—2 case)\nFor \\(A = \\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}\\):\n\nPositive definite if \\(a &gt; 0\\) and \\(ac - b^2 &gt; 0\\)\nNegative definite if \\(a &lt; 0\\) and \\(ac - b^2 &gt; 0\\)\nIndefinite if \\(ac - b^2 &lt; 0\\)\n\n\n\nExamples\n\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}\\): eigenvalues = [2, 6] â†’ Positive definite\n\\(\\begin{bmatrix} -2 & 0 \\\\ 0 & -3 \\end{bmatrix}\\): eigenvalues = [-2, -3] â†’ Negative definite\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\\): eigenvalues = [2, -2] â†’ Indefinite\n\\(\\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix}\\): eigenvalues = [4, 0] â†’ Positive semi-definite"
  },
  {
    "objectID": "ML/hessian-prerequisites.html#interpreting-hessian-at-critical-points",
    "href": "ML/hessian-prerequisites.html#interpreting-hessian-at-critical-points",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Interpreting Hessian at Critical Points",
    "text": "Interpreting Hessian at Critical Points\nAt a critical point where \\(\\nabla f = 0\\), the Hessian determines the nature of the point:\n\n\n\n\n\n\n\n\n\nHessian Type\nEigenvalues\nSurface Shape\nPoint Type\n\n\n\n\nPositive definite\nall positive\nBowl (convex)\nLocal minimum\n\n\nNegative definite\nall negative\nDome (concave)\nLocal maximum\n\n\nIndefinite\nmixed signs\nSaddle\nNeither min nor max\n\n\nSemi-definite\nsome zero\nFlat in some directions\nInconclusive\n\n\n\n\nVisualization: Different Surface Types\n\n\nShow code\n# Create grid for plotting\nx_grid = np.linspace(-2, 2, 100)\ny_grid = np.linspace(-2, 2, 100)\nX, Y = np.meshgrid(x_grid, y_grid)\n\n# Define different functions with different Hessian types\ndef positive_definite(x, y):\n    \"\"\"Minimum: f = xÂ² + yÂ²\"\"\"\n    return x**2 + y**2\n\ndef negative_definite(x, y):\n    \"\"\"Maximum: f = -xÂ² - yÂ²\"\"\"\n    return -x**2 - y**2\n\ndef indefinite(x, y):\n    \"\"\"Saddle: f = xÂ² - yÂ²\"\"\"\n    return x**2 - y**2\n\ndef semi_definite(x, y):\n    \"\"\"Flat direction: f = xÂ²\"\"\"\n    return x**2\n\n# Create 3D surface plots in 2x2 grid\nfig = plt.figure(figsize=(12, 10))\n\nfunctions = [\n    (positive_definite, \"Positive Definite\\n(Bowl - Minimum)\", \"Greens\"),\n    (negative_definite, \"Negative Definite\\n(Dome - Maximum)\", \"Reds\"),\n    (indefinite, \"Indefinite\\n(Saddle Point)\", \"RdBu\"),\n    (semi_definite, \"Semi-Definite\\n(Flat Direction)\", \"YlOrRd\")\n]\n\nfor idx, (func, title, cmap) in enumerate(functions, 1):\n    ax = fig.add_subplot(2, 2, idx, projection='3d')\n    Z = func(X, Y)\n\n    surf = ax.plot_surface(X, Y, Z, cmap=cmap, alpha=0.8,\n                           linewidth=0, antialiased=True)\n\n    ax.set_xlabel('x', fontsize=9)\n    ax.set_ylabel('y', fontsize=9)\n    ax.set_zlabel('f(x,y)', fontsize=9)\n    ax.set_title(title, fontsize=10)\n    ax.view_init(elev=25, azim=45)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nContour plots for better understanding:\n\n\nShow code\n# Contour plots in 2x2 grid\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\naxes = axes.flatten()\n\nfor idx, (func, title, cmap) in enumerate(functions):\n    ax = axes[idx]\n    Z = func(X, Y)\n\n    contour = ax.contour(X, Y, Z, levels=15, cmap=cmap)\n    ax.clabel(contour, inline=True, fontsize=7)\n\n    # Mark the critical point at origin\n    ax.plot(0, 0, 'r*', markersize=12, label='Critical point')\n\n    ax.set_xlabel('x', fontsize=9)\n    ax.set_ylabel('y', fontsize=9)\n    ax.set_title(title, fontsize=10)\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=8)\n    ax.set_aspect('equal')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ML/architecture-design.html",
    "href": "ML/architecture-design.html",
    "title": "Deep Learning Book 6.4: Architecture Design - Depth vs Width",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.4 explores how network architectureâ€”depth versus widthâ€”fundamentally shapes what neural networks can learn and how efficiently they learn it.\nğŸ““ For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "ML/architecture-design.html#the-architecture-question-deep-or-wide",
    "href": "ML/architecture-design.html#the-architecture-question-deep-or-wide",
    "title": "Deep Learning Book 6.4: Architecture Design - Depth vs Width",
    "section": "The Architecture Question: Deep or Wide?",
    "text": "The Architecture Question: Deep or Wide?\nWhen designing a neural network, one of the most fundamental decisions is choosing between depth (many layers) and width (many units per layer). Should you build a shallow network with many units, or a deep network with fewer units per layer?\nThe answer reveals something profound about how neural networks represent functions: deep networks can achieve exponentially greater expressiveness than shallow networks with the same number of parameters. This isnâ€™t just theoreticalâ€”it has practical implications for model efficiency and performance.\n\nQuick Reference: Understanding Depth vs Width\nFor context on the fundamental concepts of network architecture, see the Architecture Design summary.\nKey insight: A deep ReLU network with \\(n\\) units per layer and depth \\(L\\) can create \\(\\mathcal{O}(n^L)\\) distinct linear regions in the input space. A shallow network would need exponentially many units (\\(\\mathcal{O}(n^L)\\) units in a single layer) to achieve the same expressiveness.\n\n\n\n\n\n\n\n\n\nArchitecture\nCharacteristic\nAdvantage\nChallenge\n\n\n\n\nDeep (many layers)\nHierarchical feature reuse\nExponential expressiveness with fewer parameters\nHarder to optimize (vanishing/exploding gradients)\n\n\nWide (many units per layer)\nIncreased capacity per layer\nEasier optimization\nParameter inefficient; requires exponentially more units"
  },
  {
    "objectID": "ML/architecture-design.html#experiment-shallow-vs-deep-network-comparison",
    "href": "ML/architecture-design.html#experiment-shallow-vs-deep-network-comparison",
    "title": "Deep Learning Book 6.4: Architecture Design - Depth vs Width",
    "section": "ğŸ”¬ Experiment: Shallow vs Deep Network Comparison",
    "text": "ğŸ”¬ Experiment: Shallow vs Deep Network Comparison\nLetâ€™s explore whether depth provides an advantage in practice by comparing two networks: - Shallow Network: 1 hidden layer with 128 units - Deep Network: 3 hidden layers (16 â†’ 8 â†’ output)\nBoth networks are trained on the same regression task: \\(y = \\sin^2(x) + x^3\\)\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Configure plotting\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['axes.grid'] = True\nplt.rcParams['grid.alpha'] = 0.3\n\nprint(\"âœ“ Setup complete\")\n\n\nâœ“ Setup complete\n\n\n\nStep 1: Generate Training and Test Data\n\n\nShow code\n# Training data\nx_train = np.random.rand(200, 1)\ny_train = np.square(np.sin(x_train)) + np.power(x_train, 3)\n\n# Test data\nx_test = np.random.rand(100, 1)\ny_test = np.square(np.sin(x_test)) + np.power(x_test, 3)\n\n# Convert to PyTorch tensors\nx_train_tensor = torch.FloatTensor(x_train)\ny_train_tensor = torch.FloatTensor(y_train)\nx_test_tensor = torch.FloatTensor(x_test)\ny_test_tensor = torch.FloatTensor(y_test)\n\nprint(f\"Training samples: {len(x_train)}\")\nprint(f\"Test samples: {len(x_test)}\")\nprint(f\"Input range: [{x_train.min():.2f}, {x_train.max():.2f}]\")\nprint(f\"Target range: [{y_train.min():.2f}, {y_train.max():.2f}]\")\n\n\nTraining samples: 200\nTest samples: 100\nInput range: [0.01, 0.99]\nTarget range: [0.00, 1.66]\n\n\n\n\nStep 2: Define Model Architectures\n\n\nShow code\n# Shallow model: 1 hidden layer with 128 units\nshallow_model = nn.Sequential(\n    nn.Linear(1, 128),\n    nn.ReLU(),\n    nn.Linear(128, 1)\n)\n\n# Deep model: 3 hidden layers (16 â†’ 8 â†’ output)\ndeep_model = nn.Sequential(\n    nn.Linear(1, 16),\n    nn.ReLU(),\n    nn.Linear(16, 8),\n    nn.ReLU(),\n    nn.Linear(8, 1)\n)\n\nprint(\"âœ“ Models created\")\nprint(f\"\\nShallow model architecture:\")\nprint(shallow_model)\nprint(f\"\\nDeep model architecture:\")\nprint(deep_model)\n\n\nâœ“ Models created\n\nShallow model architecture:\nSequential(\n  (0): Linear(in_features=1, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=1, bias=True)\n)\n\nDeep model architecture:\nSequential(\n  (0): Linear(in_features=1, out_features=16, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=16, out_features=8, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=8, out_features=1, bias=True)\n)\n\n\n\n\nStep 3: Count Parameters\nHow many trainable parameters does each architecture use?\n\n\nShow code\ndef count_parameters(model):\n    \"\"\"Count total trainable parameters in a model\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nshallow_params = count_parameters(shallow_model)\ndeep_params = count_parameters(deep_model)\n\nprint(\"Parameter Counts:\")\nprint(\"-\" * 50)\nprint(f\"Shallow model (1 layer Ã— 128 units): {shallow_params:,} parameters\")\nprint(f\"Deep model (3 layers):                {deep_params:,} parameters\")\nprint(\"-\" * 50)\nprint(f\"Ratio (shallow/deep): {shallow_params/deep_params:.2f}x\")\n\n# Visualize parameter counts\nfig, ax = plt.subplots(figsize=(8, 5))\nmodels = ['Shallow\\n(1Ã—128)', 'Deep\\n(3 layers)']\nparams = [shallow_params, deep_params]\ncolors = ['#ff7f0e', '#1f77b4']\n\nbars = ax.bar(models, params, color=colors, alpha=0.7, edgecolor='black')\nax.set_ylabel('Number of Parameters', fontsize=12)\nax.set_title('Model Parameter Comparison', fontsize=14, fontweight='bold')\nax.grid(axis='y', alpha=0.3)\n\n# Add value labels on bars\nfor bar, param in zip(bars, params):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{param:,}',\n            ha='center', va='bottom', fontsize=11, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\nParameter Counts:\n--------------------------------------------------\nShallow model (1 layer Ã— 128 units): 385 parameters\nDeep model (3 layers):                177 parameters\n--------------------------------------------------\nRatio (shallow/deep): 2.18x\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Train Both Models\n\n\nShow code\n# Training configuration\nn_epochs = 500\nlearning_rate = 0.01\nloss_fn = nn.MSELoss()\n\n# Track training history\nhistory = {\n    'Shallow': {'train_loss': [], 'test_loss': []},\n    'Deep': {'train_loss': [], 'test_loss': []}\n}\n\nmodels = {\n    'Shallow': shallow_model,\n    'Deep': deep_model\n}\n\n# Train each model\nfor name, model in models.items():\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    for epoch in range(n_epochs):\n        # Training\n        model.train()\n        y_pred = model(x_train_tensor)\n        loss = loss_fn(y_pred, y_train_tensor)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        history[name]['train_loss'].append(loss.item())\n\n        # Evaluation on test set\n        model.eval()\n        with torch.no_grad():\n            y_test_pred = model(x_test_tensor)\n            test_loss = loss_fn(y_test_pred, y_test_tensor).item()\n            history[name]['test_loss'].append(test_loss)\n\n    print(f\"âœ“ {name} model trained\")\n\nprint(\"\\nâœ“ Training complete\")\n\n\nâœ“ Shallow model trained\nâœ“ Deep model trained\n\nâœ“ Training complete\n\n\n\n\nStep 5: Compare Model Performance\n\n\nShow code\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\ncolors = {'Shallow': '#ff7f0e', 'Deep': '#1f77b4'}\n\n# Plot training loss\nfor name, data in history.items():\n    axes[0].plot(data['train_loss'], label=name, linewidth=2, color=colors[name])\n\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Training Loss (MSE)', fontsize=12)\naxes[0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\naxes[0].set_yscale('log')\n\n# Plot test loss\nfor name, data in history.items():\n    axes[1].plot(data['test_loss'], label=name, linewidth=2, color=colors[name])\n\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Test Loss (MSE)', fontsize=12)\naxes[1].set_title('Test Loss Comparison', fontsize=14, fontweight='bold')\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\naxes[1].set_yscale('log')\n\nplt.tight_layout()\nplt.show()\n\n# Print final metrics\nprint(\"\\nFinal Performance (after {} epochs):\".format(n_epochs))\nprint(\"-\" * 70)\nprint(f\"{'Model':&lt;15} {'Parameters':&lt;15} {'Train Loss':&lt;15} {'Test Loss':&lt;15}\")\nprint(\"-\" * 70)\nfor name in models.keys():\n    params = count_parameters(models[name])\n    train_loss = history[name]['train_loss'][-1]\n    test_loss = history[name]['test_loss'][-1]\n    print(f\"{name:&lt;15} {params:&lt;15,} {train_loss:&lt;15.6f} {test_loss:&lt;15.6f}\")\n\n\n\n\n\n\n\n\n\n\nFinal Performance (after 500 epochs):\n----------------------------------------------------------------------\nModel           Parameters      Train Loss      Test Loss      \n----------------------------------------------------------------------\nShallow         385             0.000017        0.000022       \nDeep            177             0.000012        0.000013       \n\n\n\nThis experiment demonstrates the practical implications of depth versus width in neural network architecture design, showing how deeper networks can achieve competitive performance with fewer parameters."
  },
  {
    "objectID": "Math/mit1806-lecture3-multiplication.html",
    "href": "Math/mit1806-lecture3-multiplication.html",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strangâ€™s third lecture reveals a profound insight: matrix multiplication isnâ€™t just one operationâ€”itâ€™s five different perspectives on the same computation, each revealing different structural properties. Then we explore matrix inverses and why some matrices fundamentally cannot be inverted."
  },
  {
    "objectID": "Math/mit1806-lecture3-multiplication.html#context",
    "href": "Math/mit1806-lecture3-multiplication.html#context",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strangâ€™s third lecture reveals a profound insight: matrix multiplication isnâ€™t just one operationâ€”itâ€™s five different perspectives on the same computation, each revealing different structural properties. Then we explore matrix inverses and why some matrices fundamentally cannot be inverted."
  },
  {
    "objectID": "Math/mit1806-lecture3-multiplication.html#the-five-ways-of-matrix-multiplication",
    "href": "Math/mit1806-lecture3-multiplication.html#the-five-ways-of-matrix-multiplication",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "The Five Ways of Matrix Multiplication",
    "text": "The Five Ways of Matrix Multiplication\nGiven \\(C = AB\\) where \\(A\\) is \\(m \\times n\\) and \\(B\\) is \\(n \\times p\\):\n\n1. Element-by-Element View (Row Ã— Column)\nThe standard definition: each entry \\(C_{ij}\\) is the dot product of row \\(i\\) of \\(A\\) with column \\(j\\) of \\(B\\):\n\\[\nC_{ij} = \\sum_{k=1}^n A_{ik} B_{kj}\n\\]\nExample: \\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n=\n\\begin{bmatrix} 1 \\cdot 5+2 \\cdot 7 & 1 \\cdot 6+2 \\cdot 8 \\\\ 3 \\cdot 5+4 \\cdot 7 & 3 \\cdot 6+4 \\cdot 8 \\end{bmatrix}\n=\n\\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n\\]\n\n\n2. Column View\nKey insight: Each column of \\(C\\) is a linear combination of the columns of \\(A\\).\n\\[\nC = A \\begin{bmatrix} | & | & & | \\\\ b_1 & b_2 & \\cdots & b_p \\\\ | & | & & | \\end{bmatrix}\n= \\begin{bmatrix} | & | & & | \\\\ Ab_1 & Ab_2 & \\cdots & Ab_p \\\\ | & | & & | \\end{bmatrix}\n\\]\nInterpretation: Multiply \\(A\\) by each column of \\(B\\) to get each column of \\(C\\).\nThis perspective is crucial for understanding: - The column space of \\(C\\) lies in the column space of \\(A\\) - Matrix-vector multiplication \\(Ax\\) as a linear combination of \\(A\\)â€™s columns\n\n\n3. Row View\nKey insight: Each row of \\(C\\) is a linear combination of the rows of \\(B\\).\n\\[\nC = \\begin{bmatrix} â€” & a_1 & â€” \\\\ â€” & a_2 & â€” \\\\ & \\vdots & \\\\ â€” & a_m & â€” \\end{bmatrix} B\n= \\begin{bmatrix} â€” & a_1B & â€” \\\\ â€” & a_2B & â€” \\\\ & \\vdots & \\\\ â€” & a_mB & â€” \\end{bmatrix}\n\\]\nInterpretation: Each row of \\(A\\) multiplies the entire matrix \\(B\\) to produce a row of \\(C\\).\nThis shows that the row space of \\(C\\) lies in the row space of \\(B\\).\n\n\n4. Column Ã— Row View (Sum of Rank-1 Matrices)\nThe most powerful perspective:\n\\[\nAB = \\sum_{k=1}^n (\\text{column } k \\text{ of } A) \\times (\\text{row } k \\text{ of } B)\n\\]\nEach term is a rank-1 matrix (outer product of a column vector with a row vector).\nExample: \\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n=\n\\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\\begin{bmatrix} 5 & 6 \\end{bmatrix}\n+\n\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\n\\begin{bmatrix} 7 & 8 \\end{bmatrix}\n\\]\n\\[\n=\n\\begin{bmatrix} 5 & 6 \\\\ 15 & 18 \\end{bmatrix}\n+\n\\begin{bmatrix} 14 & 16 \\\\ 28 & 32 \\end{bmatrix}\n=\n\\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n\\]\nWhy this matters: - Each rank-1 matrix has all rows as multiples of each other - Matrix multiplication is a sum of simple, structured pieces - Foundation for understanding matrix rank and decompositions (SVD, eigendecomposition)\n\n\n5. Block Multiplication\nMatrices can be partitioned into blocks and multiplied block-wise:\n\\[\n\\begin{bmatrix} A_1 & A_2 \\\\ A_3 & A_4 \\end{bmatrix}\n\\begin{bmatrix} B_1 & B_2 \\\\ B_3 & B_4 \\end{bmatrix}\n=\n\\begin{bmatrix} A_1B_1 + A_2B_3 & A_1B_2 + A_2B_4 \\\\ A_3B_1 + A_4B_3 & A_3B_2 + A_4B_4 \\end{bmatrix}\n\\]\nCondition: Block dimensions must be compatible for multiplication.\nApplications: - Efficient computation for sparse or structured matrices - Recursive algorithms for large matrices - Parallel computing strategies"
  },
  {
    "objectID": "Math/mit1806-lecture3-multiplication.html#matrix-inverse",
    "href": "Math/mit1806-lecture3-multiplication.html#matrix-inverse",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "Matrix Inverse",
    "text": "Matrix Inverse\n\nDefinition\nFor a square matrix \\(A\\), if there exists a matrix \\(A^{-1}\\) such that:\n\\[\nA^{-1}A = I \\quad \\text{and} \\quad AA^{-1} = I\n\\]\nthen \\(A\\) is invertible (or non-singular).\n\n\nWhen Does an Inverse NOT Exist?\nA matrix \\(A\\) has no inverse if any of these equivalent conditions hold:\n\nZero determinant: \\(\\det(A) = 0\\)\nDependent columns: Some column is a linear combination of others\nNon-trivial null space: \\(Ax = 0\\) for some non-zero \\(x\\)\n\n\n\nWhy Singular Matrices Have No Inverse: A Proof\nConsider the singular matrix:\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix}\n\\]\nObservation: The second column is 2Ã— the first (columns are dependent).\nWe can find \\(x = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} \\neq 0\\) such that:\n\\[\nAx = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n\\]\nProof by contradiction:\nSuppose \\(A^{-1}\\) exists. Then:\n\nWe have \\(Ax = 0\\) where \\(x \\neq 0\\)\nMultiply both sides by \\(A^{-1}\\): \\(A^{-1}(Ax) = A^{-1} \\cdot 0\\)\nLeft side simplifies: \\(A^{-1}(Ax) = (A^{-1}A)x = Ix = x\\)\nRight side: \\(A^{-1} \\cdot 0 = 0\\)\nTherefore: \\(x = 0\\)\n\nContradiction! We started with \\(x = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} \\neq 0\\), but our logic says \\(x = 0\\).\nThis proves \\(A^{-1}\\) cannot exist. The fundamental issue: if \\(A\\) maps different inputs to the same output (like both \\(x\\) and \\(0\\) to \\(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)), we cannot uniquely reverse the operation.\n\n\nComputing the Inverse\nFor 2Ã—2 matrices:\n\\[\nA = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n\\quad \\Rightarrow \\quad\nA^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\\]\nNote: \\(ad - bc\\) is the determinant. If \\(\\det(A) = 0\\), no inverse exists.\nFor larger matrices, use Gauss-Jordan elimination:\n\\[\n[A | I] \\xrightarrow{\\text{row operations}} [I | A^{-1}]\n\\]\nStart with \\(A\\) augmented with the identity matrix, then use row operations to transform \\(A\\) into \\(I\\). The same operations transform \\(I\\) into \\(A^{-1}\\)."
  },
  {
    "objectID": "Math/mit1806-lecture3-multiplication.html#key-takeaways",
    "href": "Math/mit1806-lecture3-multiplication.html#key-takeaways",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nMatrix multiplication has five perspectives:\n\nElement-by-element (computational)\nColumn view (output columns from input columns)\nRow view (output rows from input rows)\nSum of rank-1 matrices (most insightful)\nBlock multiplication (practical for large matrices)\n\nMatrix inverse exists if and only if:\n\nDeterminant is non-zero\nColumns are independent\n\\(Ax = 0\\) only when \\(x = 0\\)\n\nWhy this matters:\n\nSolving \\(Ax = b\\): if \\(A^{-1}\\) exists, then \\(x = A^{-1}b\\)\nHowever, Gaussian elimination is usually more efficient than computing \\(A^{-1}\\)\nUnderstanding when inverses donâ€™t exist reveals the structure of linear transformations\n\n\nThe rank-1 decomposition (view 4) is particularly powerfulâ€”itâ€™s the foundation for understanding matrix rank, the Four Fundamental Subspaces, and advanced topics like Singular Value Decomposition."
  },
  {
    "objectID": "Math/mit1806-lecture3-multiplication.html#exercises",
    "href": "Math/mit1806-lecture3-multiplication.html#exercises",
    "title": "MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse",
    "section": "Exercises",
    "text": "Exercises\n\nExample 1: Matrix Multiplication (All Five Perspectives)\n\n\nShow code\nimport numpy as np\nfrom IPython.display import display, Markdown, Latex\n\ndef matrix_to_latex(M, name=\"\"):\n    \"\"\"Convert numpy matrix to LaTeX bmatrix format\"\"\"\n    if len(M.shape) == 1:\n        M = M.reshape(-1, 1)\n    rows = [\" & \".join([f\"{x:.0f}\" if x == int(x) else f\"{x:.2f}\" for x in row]) for row in M]\n    latex_str = r\"\\begin{bmatrix}\" + r\" \\\\ \".join(rows) + r\"\\end{bmatrix}\"\n    if name:\n        latex_str = f\"{name} = \" + latex_str\n    return latex_str\n\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\nC = A @ B\n\ndisplay(Markdown(\"**Given matrices:**\"))\ndisplay(Latex(f\"$${matrix_to_latex(A, 'A')}$$\"))\ndisplay(Latex(f\"$${matrix_to_latex(B, 'B')}$$\"))\n\n\nGiven matrices:\n\n\n\\[A = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}\\]\n\n\n\\[B = \\begin{bmatrix}5 & 6 \\\\ 7 & 8\\end{bmatrix}\\]\n\n\nMethod 1: Element-by-element\n\n\nShow code\ndisplay(Latex(f\"$${matrix_to_latex(C, 'C = AB')}$$\"))\ndisplay(Latex(f\"$$C_{{11}} = (1)(5) + (2)(7) = {C[0,0]}$$\"))\ndisplay(Latex(f\"$$C_{{12}} = (1)(6) + (2)(8) = {C[0,1]}$$\"))\ndisplay(Latex(f\"$$C_{{21}} = (3)(5) + (4)(7) = {C[1,0]}$$\"))\ndisplay(Latex(f\"$$C_{{22}} = (3)(6) + (4)(8) = {C[1,1]}$$\"))\n\n\n\\[C = AB = \\begin{bmatrix}19 & 22 \\\\ 43 & 50\\end{bmatrix}\\]\n\n\n\\[C_{11} = (1)(5) + (2)(7) = 19\\]\n\n\n\\[C_{12} = (1)(6) + (2)(8) = 22\\]\n\n\n\\[C_{21} = (3)(5) + (4)(7) = 43\\]\n\n\n\\[C_{22} = (3)(6) + (4)(8) = 50\\]\n\n\nMethod 2: Column view\n\n\nShow code\ncol1 = A @ B[:, 0]\ncol2 = A @ B[:, 1]\n\ndisplay(Markdown(\"Column 1 of C is a linear combination of A's columns:\"))\ndisplay(Latex(f\"$$A \\\\begin{{bmatrix}} 5 \\\\\\\\ 7 \\\\end{{bmatrix}} = {matrix_to_latex(col1)}$$\"))\n\ndisplay(Markdown(\"Column 2 of C is a linear combination of A's columns:\"))\ndisplay(Latex(f\"$$A \\\\begin{{bmatrix}} 6 \\\\\\\\ 8 \\\\end{{bmatrix}} = {matrix_to_latex(col2)}$$\"))\n\n\nColumn 1 of C is a linear combination of Aâ€™s columns:\n\n\n\\[A \\begin{bmatrix} 5 \\\\ 7 \\end{bmatrix} = \\begin{bmatrix}19 \\\\ 43\\end{bmatrix}\\]\n\n\nColumn 2 of C is a linear combination of Aâ€™s columns:\n\n\n\\[A \\begin{bmatrix} 6 \\\\ 8 \\end{bmatrix} = \\begin{bmatrix}22 \\\\ 50\\end{bmatrix}\\]\n\n\nMethod 3: Row view\n\n\nShow code\nrow1 = A[0, :] @ B\nrow2 = A[1, :] @ B\n\ndisplay(Markdown(\"Row 1 of C is a linear combination of B's rows:\"))\ndisplay(Latex(f\"$$\\\\begin{{bmatrix}} 1 & 2 \\\\end{{bmatrix}} B = {matrix_to_latex(row1.reshape(1, -1))}$$\"))\n\ndisplay(Markdown(\"Row 2 of C is a linear combination of B's rows:\"))\ndisplay(Latex(f\"$$\\\\begin{{bmatrix}} 3 & 4 \\\\end{{bmatrix}} B = {matrix_to_latex(row2.reshape(1, -1))}$$\"))\n\n\nRow 1 of C is a linear combination of Bâ€™s rows:\n\n\n\\[\\begin{bmatrix} 1 & 2 \\end{bmatrix} B = \\begin{bmatrix}19 & 22\\end{bmatrix}\\]\n\n\nRow 2 of C is a linear combination of Bâ€™s rows:\n\n\n\\[\\begin{bmatrix} 3 & 4 \\end{bmatrix} B = \\begin{bmatrix}43 & 50\\end{bmatrix}\\]\n\n\nMethod 4: Sum of rank-1 matrices (most powerful!)\n\n\nShow code\nrank1_1 = np.outer(A[:, 0], B[0, :])\nrank1_2 = np.outer(A[:, 1], B[1, :])\n\ndisplay(Markdown(\"**Rank-1 term 1:**\"))\ndisplay(Latex(f\"$$\\\\begin{{bmatrix}} 1 \\\\\\\\ 3 \\\\end{{bmatrix}} \\\\begin{{bmatrix}} 5 & 6 \\\\end{{bmatrix}} = {matrix_to_latex(rank1_1)}$$\"))\n\ndisplay(Markdown(\"**Rank-1 term 2:**\"))\ndisplay(Latex(f\"$$\\\\begin{{bmatrix}} 2 \\\\\\\\ 4 \\\\end{{bmatrix}} \\\\begin{{bmatrix}} 7 & 8 \\\\end{{bmatrix}} = {matrix_to_latex(rank1_2)}$$\"))\n\ndisplay(Markdown(\"**Sum of rank-1 matrices:**\"))\ndisplay(Latex(f\"$${matrix_to_latex(rank1_1)} + {matrix_to_latex(rank1_2)} = {matrix_to_latex(rank1_1 + rank1_2)}$$\"))\n\ndisplay(Markdown(\"âœ“ **All methods give the same result!**\"))\n\n\nRank-1 term 1:\n\n\n\\[\\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} \\begin{bmatrix} 5 & 6 \\end{bmatrix} = \\begin{bmatrix}5 & 6 \\\\ 15 & 18\\end{bmatrix}\\]\n\n\nRank-1 term 2:\n\n\n\\[\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} \\begin{bmatrix} 7 & 8 \\end{bmatrix} = \\begin{bmatrix}14 & 16 \\\\ 28 & 32\\end{bmatrix}\\]\n\n\nSum of rank-1 matrices:\n\n\n\\[\\begin{bmatrix}5 & 6 \\\\ 15 & 18\\end{bmatrix} + \\begin{bmatrix}14 & 16 \\\\ 28 & 32\\end{bmatrix} = \\begin{bmatrix}19 & 22 \\\\ 43 & 50\\end{bmatrix}\\]\n\n\nâœ“ All methods give the same result!\n\n\n\n\nExample 2: Matrix Inverse\nInvertible matrix:\n\n\nShow code\nA_inv = np.array([[2, 1], [5, 3]])\nA_inv_inverse = np.linalg.inv(A_inv)\ndet_A = np.linalg.det(A_inv)\n\ndisplay(Latex(f\"$${matrix_to_latex(A_inv, 'A')}$$\"))\ndisplay(Latex(f\"$$\\\\det(A) = {det_A:.0f} \\\\neq 0 \\\\quad \\\\checkmark \\\\text{{ Invertible!}}$$\"))\ndisplay(Latex(f\"$${matrix_to_latex(A_inv_inverse, 'A^{-1}')}$$\"))\n\ndisplay(Markdown(\"**Verification:**\"))\nidentity = A_inv @ A_inv_inverse\ndisplay(Latex(f\"$$A A^{{-1}} = {matrix_to_latex(np.round(identity, 10), '')} = I$$\"))\n\n\n\\[A = \\begin{bmatrix}2 & 1 \\\\ 5 & 3\\end{bmatrix}\\]\n\n\n\\[\\det(A) = 1 \\neq 0 \\quad \\checkmark \\text{ Invertible!}\\]\n\n\n\\[A^{-1} = \\begin{bmatrix}3.00 & -1.00 \\\\ -5.00 & 2.00\\end{bmatrix}\\]\n\n\nVerification:\n\n\n\\[A A^{-1} = \\begin{bmatrix}1 & 0 \\\\ -0 & 1\\end{bmatrix} = I\\]\n\n\nSingular matrix (no inverse):\n\n\nShow code\nA_singular = np.array([[1, 2], [2, 4]])\ndet_singular = np.linalg.det(A_singular)\n\ndisplay(Latex(f\"$${matrix_to_latex(A_singular, 'A')}$$\"))\ndisplay(Latex(f\"$$\\\\det(A) = 0 \\\\quad \\\\times \\\\text{{ Singular!}}$$\"))\n\ndisplay(Markdown(\"**Observation:** Column 2 = 2 Ã— Column 1 (linearly dependent)\"))\ndisplay(Latex(f\"$${matrix_to_latex(A_singular[:, 1])} = 2 \\\\times {matrix_to_latex(A_singular[:, 0])}$$\"))\n\nx = np.array([2, -1])\nAx = A_singular @ x\n\ndisplay(Markdown(f\"**Non-zero vector satisfying $Ax = 0$:**\"))\ndisplay(Latex(f\"$$A {matrix_to_latex(x, 'x')} = {matrix_to_latex(Ax)} = 0$$\"))\ndisplay(Markdown(\"âŸ¹ **No inverse exists** (proven by contradiction earlier)\"))\n\n\n\\[A = \\begin{bmatrix}1 & 2 \\\\ 2 & 4\\end{bmatrix}\\]\n\n\n\\[\\det(A) = 0 \\quad \\times \\text{ Singular!}\\]\n\n\nObservation: Column 2 = 2 Ã— Column 1 (linearly dependent)\n\n\n\\[\\begin{bmatrix}2 \\\\ 4\\end{bmatrix} = 2 \\times \\begin{bmatrix}1 \\\\ 2\\end{bmatrix}\\]\n\n\nNon-zero vector satisfying \\(Ax = 0\\):\n\n\n\\[A x = \\begin{bmatrix}2 \\\\ -1\\end{bmatrix} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix} = 0\\]\n\n\nâŸ¹ No inverse exists (proven by contradiction earlier)"
  },
  {
    "objectID": "Math/mit1806-lecture2-elimination.html",
    "href": "Math/mit1806-lecture2-elimination.html",
    "title": "MIT 18.06SC Lecture 2: Elimination with Matrices",
    "section": "",
    "text": "This recap of MIT 18.06SC Lecture 2 explores Gaussian eliminationâ€”the systematic algorithm that transforms any linear system into an easily solvable upper triangular form.\nğŸ““ For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "Math/mit1806-lecture2-elimination.html#from-linear-systems-to-upper-triangular-form",
    "href": "Math/mit1806-lecture2-elimination.html#from-linear-systems-to-upper-triangular-form",
    "title": "MIT 18.06SC Lecture 2: Elimination with Matrices",
    "section": "From Linear Systems to Upper Triangular Form",
    "text": "From Linear Systems to Upper Triangular Form\nHow do we actually solve a system of linear equations like this?\n\\[\\begin{cases}\n2u + v + w = 5 \\\\\n4u - 6v = -2 \\\\\n-2u + 7v + 2w = 9\n\\end{cases}\\]\nThe answer is Gaussian eliminationâ€”a systematic process that transforms the coefficient matrix into an upper triangular form, where solutions can be read off directly through back substitution. This fundamental algorithm underpins much of numerical linear algebra and is essential for understanding how computers solve linear systems.\n\nQuick Reference: Understanding Elimination\nFor context on the mathematical foundations of Gaussian elimination, see the Lecture 2 summary.\nThe Two-Step Process:\n\nForward Elimination: Transform \\(A\\) into upper triangular \\(U\\) using row operations\n\nFor each pivot position, eliminate all entries below it\nRow operation: \\(\\text{Row}_i \\leftarrow \\text{Row}_i - m_{ij} \\cdot \\text{Row}_j\\) where \\(m_{ij} = a_{ij}/\\text{pivot}\\)\n\nBack Substitution: Solve \\(Ux = c\\) from bottom to top\n\nStart with the last equation (only one unknown)\nWork upward, substituting known values\n\n\nKey insight: Each elimination step can be represented as multiplication by an elimination matrix \\(E_{ij}\\), connecting row operations to matrix multiplication."
  },
  {
    "objectID": "Math/mit1806-lecture2-elimination.html#exercise-implement-gaussian-elimination-from-scratch",
    "href": "Math/mit1806-lecture2-elimination.html#exercise-implement-gaussian-elimination-from-scratch",
    "title": "MIT 18.06SC Lecture 2: Elimination with Matrices",
    "section": "ğŸ”¬ Exercise: Implement Gaussian Elimination from Scratch",
    "text": "ğŸ”¬ Exercise: Implement Gaussian Elimination from Scratch\nLetâ€™s implement the complete Gaussian elimination algorithm and verify it works correctly.\n\n\nShow code\nimport numpy as np\nfrom IPython.display import display, Markdown\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\ndef matrix_to_latex(matrix, precision=2):\n    \"\"\"Convert numpy matrix to LaTeX format.\"\"\"\n    if len(matrix.shape) == 1:\n        # Vector\n        elements = \" \\\\\\\\ \".join([f\"{x:.{precision}f}\" for x in matrix])\n        return f\"$$\\\\begin{{bmatrix}}{elements}\\\\end{{bmatrix}}$$\"\n    else:\n        # Matrix\n        rows = []\n        for row in matrix:\n            row_str = \" & \".join([f\"{x:.{precision}f}\" for x in row])\n            rows.append(row_str)\n        matrix_str = \" \\\\\\\\ \".join(rows)\n        return f\"$$\\\\begin{{bmatrix}}{matrix_str}\\\\end{{bmatrix}}$$\"\n\nprint(\"âœ“ Setup complete\")\n\n\nâœ“ Setup complete\n\n\n\nAlgorithm Overview\nThe algorithm consists of two phases:\n\nForward Elimination: Transform \\(A\\) into upper triangular form \\(U\\)\n\nFor each column \\(j\\) from 0 to \\(n-1\\):\n\nPivot = \\(A[j, j]\\)\nFor each row \\(i\\) below pivot (\\(i &gt; j\\)):\n\nCompute multiplier: \\(m_{ij} = A[i, j] / \\text{pivot}\\)\nRow operation: \\(A[i, :] = A[i, :] - m_{ij} \\cdot A[j, :]\\)\nUpdate RHS: \\(b[i] = b[i] - m_{ij} \\cdot b[j]\\)\n\n\n\nBack Substitution: Solve \\(Ux = c\\) from bottom to top\n\nStart from last row: \\(x[n-1] = c[n-1] / U[n-1, n-1]\\)\nFor each row \\(i\\) from \\(n-2\\) down to \\(0\\):\n\n\\(x[i] = (c[i] - \\sum_{j=i+1}^{n-1} U[i,j] \\cdot x[j]) / U[i,i]\\)\n\n\n\n\n\nImplementation\n\n\nShow code\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using Gaussian elimination.\n\n    Parameters:\n    -----------\n    A : numpy.ndarray, shape (n, n)\n        Coefficient matrix\n    b : numpy.ndarray, shape (n,)\n        Right-hand side vector\n\n    Returns:\n    --------\n    x : numpy.ndarray, shape (n,)\n        Solution vector\n    \"\"\"\n    A_copy = A.copy()\n    b_copy = b.copy()\n\n    rows, cols = A.shape\n    assert rows == cols, \"Matrix must be square\"\n\n    for i in range(rows - 1):\n        multiplier = A_copy[i + 1:, i] / A_copy[i, i]\n        for index, m in enumerate(multiplier):\n            A_copy[i + 1 + index] = A_copy[i + 1 + index] - m * A_copy[i]\n            assert abs(A_copy[i + 1 + index, i]) &lt; 1e-10, \"Upper triangular matrix expected\"\n            b_copy[i + 1 + index] = b_copy[i + 1 + index] - m * b_copy[i]\n\n    U = A_copy\n    print(\"Upper triangular matrix U:\")\n    display(Markdown(matrix_to_latex(U)))\n\n    # Now our matrix is upper triangular\n    # Solve for x from bottom to top\n    x = np.zeros(rows)\n    for i in range(rows - 1, -1, -1):\n        x[i] = (b_copy[i] - np.dot(A_copy[i][i + 1:], x[i + 1:])) / A_copy[i][i]\n\n    return x\n\nprint(\"âœ“ Function defined\")\n\n\nâœ“ Function defined\n\n\n\n\nTest on Lecture Example\nTest on the system from the lecture:\n\\[\\begin{cases}\n2u + v + w = 5 \\\\\n4u - 6v = -2 \\\\\n-2u + 7v + 2w = 9\n\\end{cases}\\]\nExpected solution: \\(u = 1, v = 1, w = 2\\)\n\n\nShow code\n# Define the system from lecture\nA = np.array([\n    [2, 1, 1],\n    [4, -6, 0],\n    [-2, 7, 2]\n], dtype=float)\n\nb = np.array([5, -2, 9], dtype=float)\n\nprint(\"System to solve:\")\nprint(\"A =\")\nprint(A)\nprint(\"\\nb =\")\nprint(b)\n\n# Solve using our implementation\nx_our = gaussian_elimination(A, b)\n\n# Solve using NumPy\nx_numpy = np.linalg.solve(A, b)\n\n# Compare results\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Results:\")\nprint(\"=\" * 60)\nprint(f\"Our implementation:      x = {x_our}\")\nprint(f\"NumPy (np.linalg.solve): x = {x_numpy}\")\nprint(\"=\" * 60)\n\n# Verify solution\nresidual_our = np.linalg.norm(A @ x_our - b)\nresidual_numpy = np.linalg.norm(A @ x_numpy - b)\n\nprint(f\"\\nResidual (our):   ||Ax - b|| = {residual_our:.2e}\")\nprint(f\"Residual (NumPy): ||Ax - b|| = {residual_numpy:.2e}\")\n\n# Check difference\ndifference = np.linalg.norm(x_our - x_numpy)\nprint(f\"\\nDifference: ||x_our - x_numpy|| = {difference:.2e}\")\n\nif difference &lt; 1e-10:\n    print(\"\\nâœ… Solutions match! Implementation is correct.\")\nelse:\n    print(\"\\nâŒ Solutions differ. Check implementation.\")\n\n\nSystem to solve:\nA =\n[[ 2.  1.  1.]\n [ 4. -6.  0.]\n [-2.  7.  2.]]\n\nb =\n[ 5. -2.  9.]\nUpper triangular matrix U:\n\n\n\\[\\begin{bmatrix}2.00 & 1.00 & 1.00 \\\\ 0.00 & -8.00 & -2.00 \\\\ 0.00 & 0.00 & 1.00\\end{bmatrix}\\]\n\n\n\n============================================================\nResults:\n============================================================\nOur implementation:      x = [1. 1. 2.]\nNumPy (np.linalg.solve): x = [1. 1. 2.]\n============================================================\n\nResidual (our):   ||Ax - b|| = 0.00e+00\nResidual (NumPy): ||Ax - b|| = 0.00e+00\n\nDifference: ||x_our - x_numpy|| = 0.00e+00\n\nâœ… Solutions match! Implementation is correct.\n\n\n\n\nVisualize Upper Triangular Matrix\nLetâ€™s test on a larger 10Ã—10 system to see the upper triangular structure more clearly.\n\n\nShow code\n# Create a 10x10 random system\nnp.random.seed(123)\nA_large = np.random.randn(10, 10)\nb_large = np.random.randn(10)\n\nprint(\"Original 10Ã—10 matrix A:\")\ndisplay(Markdown(matrix_to_latex(A_large)))\n\nprint(\"\\n\" + \"=\" * 70)\n\n# Solve using our implementation\nx_large = gaussian_elimination(A_large, b_large)\n\nprint(\"=\" * 70)\nprint(\"\\nâœ“ Upper triangular structure achieved!\")\n\n\nOriginal 10Ã—10 matrix A:\n\n\n\\[\\begin{bmatrix}-1.09 & 1.00 & 0.28 & -1.51 & -0.58 & 1.65 & -2.43 & -0.43 & 1.27 & -0.87 \\\\ -0.68 & -0.09 & 1.49 & -0.64 & -0.44 & -0.43 & 2.21 & 2.19 & 1.00 & 0.39 \\\\ 0.74 & 1.49 & -0.94 & 1.18 & -1.25 & -0.64 & 0.91 & -1.43 & -0.14 & -0.86 \\\\ -0.26 & -2.80 & -1.77 & -0.70 & 0.93 & -0.17 & 0.00 & 0.69 & -0.88 & 0.28 \\\\ -0.81 & -1.73 & -0.39 & 0.57 & 0.34 & -0.01 & 2.39 & 0.41 & 0.98 & 2.24 \\\\ -1.29 & -1.04 & 1.74 & -0.80 & 0.03 & 1.07 & 0.89 & 1.75 & 1.50 & 1.07 \\\\ -0.77 & 0.79 & 0.31 & -1.33 & 1.42 & 0.81 & 0.05 & -0.23 & -1.20 & 0.20 \\\\ 0.47 & -0.83 & 1.16 & -1.10 & -2.12 & 1.04 & -0.40 & -0.13 & -0.84 & -1.61 \\\\ 1.26 & -0.69 & 1.66 & 0.81 & -0.31 & -1.09 & -0.73 & -1.21 & 2.09 & 0.16 \\\\ 1.15 & -1.27 & 0.18 & 1.18 & -0.34 & 1.03 & -1.08 & -1.36 & 0.38 & -0.38\\end{bmatrix}\\]\n\n\n\n======================================================================\nUpper triangular matrix U:\n\n\n\\[\\begin{bmatrix}-1.09 & 1.00 & 0.28 & -1.51 & -0.58 & 1.65 & -2.43 & -0.43 & 1.27 & -0.87 \\\\ -0.00 & -0.72 & 1.31 & 0.30 & -0.08 & -1.47 & 3.72 & 2.46 & 0.21 & 0.93 \\\\ -0.00 & 0.00 & 3.22 & 1.07 & -1.89 & -3.94 & 10.50 & 5.69 & 1.36 & 1.35 \\\\ -0.00 & 0.00 & 0.00 & 0.82 & -2.93 & -3.41 & 8.91 & 3.46 & 1.04 & -0.34 \\\\ 0.00 & 0.00 & 0.00 & 0.00 & 6.41 & 7.26 & -17.36 & -8.55 & -1.51 & 2.79 \\\\ 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 1.15 & -1.79 & -0.91 & -0.09 & -0.48 \\\\ 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 3.20 & 0.39 & -1.78 & -0.72 \\\\ -0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -2.27 & 0.17 & 2.74 \\\\ 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 1.73 & -2.90 \\\\ -0.00 & -0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 3.24\\end{bmatrix}\\]\n\n\n======================================================================\n\nâœ“ Upper triangular structure achieved!\n\n\n\nThis implementation demonstrates the fundamental algorithm for solving linear systems, connecting the geometric view from Lecture 1 with the algebraic machinery of row operations and matrix elimination."
  },
  {
    "objectID": "Math/mit1806-lecture4-lu-decomposition.html",
    "href": "Math/mit1806-lecture4-lu-decomposition.html",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strangâ€™s fourth lecture introduces one of the most important matrix factorizations: LU decomposition, which factors any invertible matrix \\(A\\) into the product of a Lower triangular matrix and an Upper triangular matrix. This factorization is the foundation of efficient numerical linear algebra."
  },
  {
    "objectID": "Math/mit1806-lecture4-lu-decomposition.html#context",
    "href": "Math/mit1806-lecture4-lu-decomposition.html#context",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strangâ€™s fourth lecture introduces one of the most important matrix factorizations: LU decomposition, which factors any invertible matrix \\(A\\) into the product of a Lower triangular matrix and an Upper triangular matrix. This factorization is the foundation of efficient numerical linear algebra."
  },
  {
    "objectID": "Math/mit1806-lecture4-lu-decomposition.html#what-is-lu-decomposition",
    "href": "Math/mit1806-lecture4-lu-decomposition.html#what-is-lu-decomposition",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "What is LU Decomposition?",
    "text": "What is LU Decomposition?\nGoal: Factor any invertible matrix \\(A\\) as the product of: - \\(L\\) = Lower triangular matrix (with 1â€™s on diagonal) - \\(U\\) = Upper triangular matrix (the result of elimination)\n\\[\nA = LU\n\\]\n\nWhy is this useful?\n\nEfficient solving: \\(Ax = b\\) becomes two simpler triangular solves:\nStep 1 - Forward substitution: Solve \\(Lc = b\\) for \\(c\\)\nStep 2 - Back substitution: Solve \\(Ux = c\\) for \\(x\\)\nHow this works:\nSince \\(A = LU\\), we have \\(Ax = LUx = b\\). Let \\(Ux = c\\), then: \\[\nLUx = Lc = b\n\\]\nForward substitution (solving \\(Lc = b\\)):\nSince \\(L\\) is lower triangular with 1â€™s on the diagonal, we can solve for \\(c\\) step by step: \\[\n\\begin{aligned}\nc_1 &= b_1 \\\\\nc_2 &= b_2 - m_{21}c_1 \\\\\nc_3 &= b_3 - m_{31}c_1 - m_{32}c_2 \\\\\n&\\vdots\n\\end{aligned}\n\\]\nEach \\(c_i\\) depends only on previously computed values, so we solve forward from \\(c_1\\) to \\(c_n\\).\nBack substitution (solving \\(Ux = c\\)):\nSince \\(U\\) is upper triangular, we solve backward from \\(x_n\\) to \\(x_1\\): \\[\n\\begin{aligned}\nx_n &= \\frac{c_n}{u_{nn}} \\\\\nx_{n-1} &= \\frac{c_{n-1} - u_{n-1,n}x_n}{u_{n-1,n-1}} \\\\\n&\\vdots\n\\end{aligned}\n\\]\nResult: Weâ€™ve solved \\(Ax = b\\) without ever explicitly computing \\(A^{-1}\\)!\nReusable factorization: When \\(A\\) is fixed but \\(b\\) changes, we can reuse \\(L\\) and \\(U\\)\n\nFactorization: \\(O(n^3)\\) operations (done once)\nEach solve: \\(O(n^2)\\) operations\nHuge savings for multiple right-hand sides!\n\nFoundation of numerical computing: Used in MATLAB, NumPy, and all scientific computing libraries"
  },
  {
    "objectID": "Math/mit1806-lecture4-lu-decomposition.html#how-elimination-creates-u",
    "href": "Math/mit1806-lecture4-lu-decomposition.html#how-elimination-creates-u",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "How Elimination Creates U",
    "text": "How Elimination Creates U\n\nThe Elimination Process\nStarting with \\(A\\), we apply elimination matrices \\(E_{21}, E_{31}, E_{32}, \\ldots\\) to get upper triangular \\(U\\):\n\\[\nE_{32} E_{31} E_{21} A = U\n\\]\nExample (3Ã—3 case):\n\\[\nA = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\n\\]\nStep 1: Eliminate below first pivot (rows 2 and 3)\n\\[\nE_{21} = \\begin{bmatrix} 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad\nE_{31} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix}\n\\]\nStep 2: Eliminate below second pivot (row 3)\n\\[\nE_{32} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -1 & 1 \\end{bmatrix}\n\\]\n\n\nStructure of Elimination Matrices\nAn elimination matrix \\(E_{ij}\\) eliminates the entry at position \\((i,j)\\) by subtracting a multiple of row \\(j\\) from row \\(i\\).\nGeneral form: \\[\nE_{ij} = I - m_{ij} \\mathbf{e}_i \\mathbf{e}_j^T\n\\]\nwhere: - \\(m_{ij}\\) = multiplier = \\(\\frac{A_{ij}}{\\text{pivot at } (j,j)}\\) - \\(\\mathbf{e}_i\\) = \\(i\\)-th standard basis vector - The \\((i,j)\\) entry of \\(E_{ij}\\) is \\(-m_{ij}\\)\nKey properties: 1. Lower triangular (operates below diagonal) 2. Determinant = 1 (doesnâ€™t change volume) 3. Easy to invert: \\(E_{ij}^{-1} = I + m_{ij} \\mathbf{e}_i \\mathbf{e}_j^T\\) (just flip the sign!)"
  },
  {
    "objectID": "Math/mit1806-lecture4-lu-decomposition.html#inverting-to-get-l-the-key-insight",
    "href": "Math/mit1806-lecture4-lu-decomposition.html#inverting-to-get-l-the-key-insight",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Inverting to Get L: The Key Insight",
    "text": "Inverting to Get L: The Key Insight\nFrom elimination, we have:\n\\[\nE_{32} E_{31} E_{21} A = U\n\\]\nMultiply both sides by the inverses (in reverse order):\n\\[\nA = E_{21}^{-1} E_{31}^{-1} E_{32}^{-1} U = LU\n\\]\nwhere: \\[\nL = E_{21}^{-1} E_{31}^{-1} E_{32}^{-1}\n\\]\n\nThe Beautiful Result\nWhen elimination matrices are multiplied in the right order, their inverses combine beautifully:\n\\[\nL = \\begin{bmatrix}\n1 & 0 & 0 & \\cdots \\\\\nm_{21} & 1 & 0 & \\cdots \\\\\nm_{31} & m_{32} & 1 & \\cdots \\\\\n\\vdots & \\vdots & \\ddots & \\ddots\n\\end{bmatrix}\n\\]\nThe multipliers \\(m_{ij}\\) (used during elimination) directly fill in the entries of \\(L\\) below the diagonal!\nNo extra computation needed â€” just save the multipliers as you eliminate."
  },
  {
    "objectID": "Math/mit1806-lecture4-lu-decomposition.html#computational-complexity",
    "href": "Math/mit1806-lecture4-lu-decomposition.html#computational-complexity",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Computational Complexity",
    "text": "Computational Complexity\n\nOperation Counts\nFor an \\(n \\times n\\) matrix:\n\n\n\nStep\nOperations\nOrder\n\n\n\n\nElimination (find U)\n\\(\\frac{n^3}{3} + O(n^2)\\)\n\\(O(n^3)\\)\n\n\nForward substitution \\((Lc = b)\\)\n\\(\\frac{n^2}{2}\\)\n\\(O(n^2)\\)\n\n\nBack substitution \\((Ux = c)\\)\n\\(\\frac{n^2}{2}\\)\n\\(O(n^2)\\)\n\n\n\n\n\nWhy \\(\\frac{n^3}{3}\\)?\nAt step \\(k\\), we update an \\((n-k) \\times (n-k)\\) submatrix:\n\\[\n\\text{Total operations} = \\sum_{k=1}^{n-1} (n-k)^2 \\approx \\int_0^n x^2 \\, dx = \\frac{n^3}{3}\n\\]\n\n\nWhen is LU Worth It?\nSingle solve: \\(Ax = b\\) costs \\(O(n^3)\\) either way\nMultiple solves: If solving \\(Ax = b_1, Ax = b_2, \\ldots, Ax = b_m\\): - Without LU: \\(m \\times O(n^3)\\) - With LU: \\(O(n^3)\\) (once) + \\(m \\times O(n^2)\\) âœ…\nHuge savings when \\(A\\) is fixed but \\(b\\) changes!"
  },
  {
    "objectID": "Math/mit1806-lecture4-lu-decomposition.html#hands-on-exercises",
    "href": "Math/mit1806-lecture4-lu-decomposition.html#hands-on-exercises",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Hands-On Exercises",
    "text": "Hands-On Exercises\nLetâ€™s practice LU decomposition with concrete examples.\n\n\nShow code\nimport numpy as np\n\nprint(\"âœ“ Libraries imported successfully\")\n\n\nâœ“ Libraries imported successfully\n\n\n\nExercise 1: Manual LU Decomposition (2Ã—2)\nCompute the LU decomposition of \\(A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}\\) by hand.\nSteps: 1. Perform elimination to get \\(U\\) 2. Record the multiplier \\(m_{21}\\) to build \\(L\\) 3. Verify \\(A = LU\\)\n\n\nShow code\nfrom IPython.display import display, Markdown, Latex\n\n# Original matrix\nA = np.array([[2, 3],\n              [4, 7]])\n\ndisplay(Markdown(\"**Original matrix A:**\"))\ndisplay(Latex(r\"$$A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}$$\"))\n\n# Compute multiplier m21\nm21 = 4/2  # row2[0] / row1[0]\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 1: Compute multiplier**\"))\ndisplay(Latex(f\"$$m_{{21}} = \\\\frac{{4}}{{2}} = {m21}$$\"))\n\n# Build L matrix\nL = np.array([[1, 0],\n              [m21, 1]])\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 2: Build L matrix**\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 \\\\ m_{21} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}$$\"))\n\n# Build U matrix (result after elimination)\n# After: row2 = row2 - m21*row1\n# [2, 3]        [2, 3]\n# [4, 7]  --&gt;   [0, 1]  (because 7 - 2*3 = 1)\nU = np.array([[2, 3],\n              [0, 1]])\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 3: Build U matrix (after elimination)**\"))\ndisplay(Markdown(\"Row 2 â†’ Row 2 - 2 Ã— Row 1\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Verification: $A = LU$**\"))\ndisplay(Latex(r\"$$LU = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix} = A \\quad \\checkmark$$\"))\n\n\nOriginal matrix A:\n\n\n\\[A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}\\]\n\n\n\n\n\nStep 1: Compute multiplier\n\n\n\\[m_{21} = \\frac{4}{2} = 2.0\\]\n\n\n\n\n\nStep 2: Build L matrix\n\n\n\\[L = \\begin{bmatrix} 1 & 0 \\\\ m_{21} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nStep 3: Build U matrix (after elimination)\n\n\nRow 2 â†’ Row 2 - 2 Ã— Row 1\n\n\n\\[U = \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(A = LU\\)\n\n\n\\[LU = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix} = A \\quad \\checkmark\\]\n\n\nKey observation: The multiplier \\(m_{21} = 2\\) goes directly into position \\((2,1)\\) of \\(L\\)!\n\n\nExercise 2: LU Decomposition (3Ã—3)\nPerform LU decomposition on:\n\\[\nA = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\n\\]\nGoal: Find \\(L\\) and \\(U\\) such that \\(A = LU\\)\n\n\nShow code\nfrom IPython.display import display, Markdown, Latex\n\nA = np.array([[2, 1, 1],\n              [4, -6, 0],\n              [-2, 7, 2]])\n\ndisplay(Markdown(\"**Original matrix A:**\"))\ndisplay(Latex(r\"$$A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Step 1: Eliminate column 1\"))\n\n# Calculate multipliers for column 1\nm21 = A[1, 0] / A[0, 0]  # 4/2 = 2\nm31 = A[2, 0] / A[0, 0]  # -2/2 = -1\n\ndisplay(Markdown(\"**Multipliers:**\"))\ndisplay(Latex(f\"$$m_{{21}} = \\\\frac{{4}}{{2}} = {m21}, \\\\quad m_{{31}} = \\\\frac{{-2}}{{2}} = {m31}$$\"))\n\n# Create A1 after first elimination\nA1 = A.copy().astype(float)\nA1[1] = A1[1] - m21 * A1[0]  # row2 - 2*row1\nA1[2] = A1[2] - m31 * A1[0]  # row3 - (-1)*row1\n\ndisplay(Markdown(\"**After eliminating column 1:**\"))\ndisplay(Markdown(\"- Row 2 â†’ Row 2 - 2 Ã— Row 1\"))\ndisplay(Markdown(\"- Row 3 â†’ Row 3 - (-1) Ã— Row 1\"))\ndisplay(Latex(r\"$$A^{(1)} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 8 & 3 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Step 2: Eliminate column 2\"))\n\n# Calculate multiplier for column 2\nm32 = A1[2, 1] / A1[1, 1]  # 8/(-8) = -1\n\ndisplay(Markdown(\"**Multiplier:**\"))\ndisplay(Latex(f\"$$m_{{32}} = \\\\frac{{8}}{{-8}} = {m32}$$\"))\n\n# Create U (final upper triangular)\nU = A1.copy()\nU[2] = U[2] - m32 * U[1]  # row3 - (-1)*row2\n\ndisplay(Markdown(\"**After eliminating column 2:**\"))\ndisplay(Markdown(\"- Row 3 â†’ Row 3 - (-1) Ã— Row 2\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Build L from multipliers\"))\n\n# Build L from multipliers\nL = np.array([[1, 0, 0],\n              [m21, 1, 0],\n              [m31, m32, 1]])\n\ndisplay(Markdown(\"The multipliers directly fill in $L$:\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 & 0 \\\\ m_{21} & 1 & 0 \\\\ m_{31} & m_{32} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Verification: $A = LU$\"))\n\ndisplay(Latex(r\"$$LU = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix} = A \\quad \\checkmark$$\"))\n\n\nOriginal matrix A:\n\n\n\\[A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\\]\n\n\n\n\n\nStep 1: Eliminate column 1\n\n\nMultipliers:\n\n\n\\[m_{21} = \\frac{4}{2} = 2.0, \\quad m_{31} = \\frac{-2}{2} = -1.0\\]\n\n\nAfter eliminating column 1:\n\n\n\nRow 2 â†’ Row 2 - 2 Ã— Row 1\n\n\n\n\nRow 3 â†’ Row 3 - (-1) Ã— Row 1\n\n\n\n\\[A^{(1)} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 8 & 3 \\end{bmatrix}\\]\n\n\n\n\n\nStep 2: Eliminate column 2\n\n\nMultiplier:\n\n\n\\[m_{32} = \\frac{8}{-8} = -1.0\\]\n\n\nAfter eliminating column 2:\n\n\n\nRow 3 â†’ Row 3 - (-1) Ã— Row 2\n\n\n\n\\[U = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nBuild L from multipliers\n\n\nThe multipliers directly fill in \\(L\\):\n\n\n\\[L = \\begin{bmatrix} 1 & 0 & 0 \\\\ m_{21} & 1 & 0 \\\\ m_{31} & m_{32} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(A = LU\\)\n\n\n\\[LU = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix} = A \\quad \\checkmark\\]\n\n\nKey observation: All three multipliers \\((m_{21}, m_{31}, m_{32})\\) go directly into their corresponding positions in \\(L\\):\n\\[\nL = \\begin{bmatrix}\n1 & 0 & 0 \\\\\nm_{21} & 1 & 0 \\\\\nm_{31} & m_{32} & 1\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n-1 & -1 & 1\n\\end{bmatrix}\n\\]\n\nNote: In practice, numerical libraries like SciPy provide scipy.linalg.lu() which computes LU decomposition efficiently and includes automatic row permutation (pivoting) for numerical stability.\n\n\nShow code\nfrom scipy.linalg import lu\nfrom IPython.display import display, Markdown, Latex\n\nA = np.array([[2, 1, 1],\n              [4, -6, 0],\n              [-2, 7, 2]], dtype=float)\n\n# SciPy returns P, L, U where PA = LU (P is permutation matrix)\nP, L_scipy, U_scipy = lu(A)\n\ndisplay(Markdown(\"**SciPy's LU decomposition:**\"))\ndisplay(Markdown(\"SciPy returns $P$, $L$, $U$ where $PA = LU$ ($P$ is a permutation matrix)\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Permutation matrix P:**\"))\ndisplay(Latex(r\"$$P = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\ndisplay(Markdown(\"(This swaps rows 1 and 2 for numerical stability)\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Lower triangular L:**\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0.5 & 1 & 0 \\\\ -0.5 & 1 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Upper triangular U:**\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 4 & -6 & 0 \\\\ 0 & 4 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Verification: $PA = LU$**\"))\n\n# Note: If P = I (identity), then our manual L and U should match\nif np.allclose(P, np.eye(3)):\n    display(Markdown(\"âœ“ No row swaps needed! Our manual $L$ and $U$ match SciPy.\"))\nelse:\n    display(Markdown(\"âš  **Row swaps were performed** (pivot strategy for numerical stability).\"))\n    display(Markdown(\"SciPy chose the largest pivot to minimize rounding errors.\"))\n    display(Markdown(\"Our manual decomposition is valid but uses a different pivot order.\"))\n\n\nSciPyâ€™s LU decomposition:\n\n\nSciPy returns \\(P\\), \\(L\\), \\(U\\) where \\(PA = LU\\) (\\(P\\) is a permutation matrix)\n\n\n\n\n\nPermutation matrix P:\n\n\n\\[P = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n(This swaps rows 1 and 2 for numerical stability)\n\n\n\n\n\nLower triangular L:\n\n\n\\[L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0.5 & 1 & 0 \\\\ -0.5 & 1 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nUpper triangular U:\n\n\n\\[U = \\begin{bmatrix} 4 & -6 & 0 \\\\ 0 & 4 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(PA = LU\\)\n\n\nâš  Row swaps were performed (pivot strategy for numerical stability).\n\n\nSciPy chose the largest pivot to minimize rounding errors.\n\n\nOur manual decomposition is valid but uses a different pivot order."
  },
  {
    "objectID": "Algorithm/dp_regex.html",
    "href": "Algorithm/dp_regex.html",
    "title": "DP: Regular Expression Matching",
    "section": "",
    "text": "Dynamic programming is a technique for solving problems by breaking them down into smaller sub-problems and solving each subproblem only once."
  },
  {
    "objectID": "Algorithm/dp_regex.html#example-of-regular-expression-matching",
    "href": "Algorithm/dp_regex.html#example-of-regular-expression-matching",
    "title": "DP: Regular Expression Matching",
    "section": "Example of Regular Expression Matching",
    "text": "Example of Regular Expression Matching\nA problem from Leetcode 10:\nYou are given a string s and a pattern p, implement regular expression matching with support for â€˜.â€™ and â€™*â€™ where:\nâ€˜.â€™ Matches any single character. â€™*â€™ Matches zero or more of the preceding element. The matching should cover the entire input string (not partial).\ns = \"abcabc\"    \np1 = \".*c\"    \np2 = \".*d\""
  },
  {
    "objectID": "Algorithm/dp_regex.html#dp-table",
    "href": "Algorithm/dp_regex.html#dp-table",
    "title": "DP: Regular Expression Matching",
    "section": "1. DP Table",
    "text": "1. DP Table\nLook at the following case.\n\nCase 1\np1 is valid if we have a table like this:\nwe can see that the last cell is T, so p1 is valid.\n\n\n\n\n\n.\n*\nc\n\n\n\n\n\nT\nF\nT\nF\n\n\na\nF\nT\nT\nF\n\n\nb\nF\nF\nT\nF\n\n\nc\nF\nF\nT\nT\n\n\na\nF\nF\nT\nF\n\n\nb\nF\nF\nT\nF\n\n\nc\nF\nF\nT\nT\n\n\n\nThe table is the match result of s[0:i] and p[0:j],\nso the last cell is the match result of s[0:6](the entire string) and p[0:3](the entire pattern). If the result is T, then the entire string matches the entire pattern.\n\n\nHow does each cell is calculated?\n\nthe last cell, p[:3] matches s[:6], also p[:2] matches s[:5]\n\nit is now a dp problem, the cellâ€™s value is the match result of p[:i] and s[:j] and the match result of p[:i-1] and s[:j-1],meaning both should be T.\n\n\n\nCase 2\nNow look at an invalid case:\np2 is invalid because .* can match abcab but d cannot match c\n\n\n\n\n\n.\n*\nd\n\n\n\n\n\nT\nF\nT\nF\n\n\na\nF\nT\nT\nF\n\n\nb\nF\nF\nT\nF\n\n\nc\nF\nF\nT\nF\n\n\na\nF\nF\nT\nF\n\n\nb\nF\nF\nT\nF\n\n\nc\nF\nF\nT\nF\n\n\n\nLook at the last cell, p[:3] matches s[:6], but p[2] does not match s[5], so the last cell is F."
  },
  {
    "objectID": "Algorithm/dp_regex.html#formula-derivation",
    "href": "Algorithm/dp_regex.html#formula-derivation",
    "title": "DP: Regular Expression Matching",
    "section": "2. Formula Derivation",
    "text": "2. Formula Derivation\n\nTwo rules\n\nwe can compare single character of the string s[i] with 1 or 2 characters of the pattern p[j],p[j-2]....,\nwe can query the previous results from the DP table dp[i-1][j-1], dp[i][j-2], dp[i-1][j].\n\n\n\nThe flow\nThe diagramm below shows how can we calculate the match result of s[0...i] and p[0...j].\n\n\n\nalt\n\n\nNow the formula seems to be: \\[\n\\text{dp}[i][j] =\n\\begin{cases}\n\\text{true} & \\text{if } p[i] \\neq '*'  \\land s[i] \\text{ matches } p[j] \\land \\text{dp}[i-1][j-1] = \\text{true} \\\\\n\\text{true} & \\text{if } p[i] = '*'  \\land dp[i][j-2] = \\text{true} \\\\\n\\text{true} & \\text{if } p[i] = '*'  \\land s[i] \\text{ matches } p[j-1] \\land \\text{dp}[i-1][j-2] = \\text{true} \\\\\n\\text{true} & \\text{if } p[i] = '*'  \\land s[i] \\text{ matches } p[j-1] \\land \\text{dp}[i-1][j] = \\text{true} \\\\\n\\text{false} & \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "Algorithm/dp_regex.html#code-example",
    "href": "Algorithm/dp_regex.html#code-example",
    "title": "DP: Regular Expression Matching",
    "section": "3. Code Example",
    "text": "3. Code Example\nPlease not that in the code, when we retrieve character from the string or pattern, we need to use s[i-1] and p[j-1] instead of s[i] and p[j] as the index of the string and pattern is 0-based.\nfrom collections import defaultdict\nclass Solution:\n    def isMatch(self,s, p):\n        m, n = len(s), len(p)\n        dp = [[False] * (n + 1) for _ in range(m + 1)]\n     # DP is a table with m+1 rows and n+1 columns\n     # we retrieve dp[i][k], i is the index of s, k is the index of p\n        dp[0][0] = True\n        for j in range(2,n+1):\n            if p[j-1]=='*':\n                dp[0][j]=dp[0][j-2]\n            \n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if p[j-1] == '*':\n                    dp[i][j] = dp[i][j-2] # zero occurrence\n                    if s[i-1]==p[j-2] or p[j-2]=='.':\n                        dp[i][j]|=dp[i-1][j] or dp[i-1][j-2] # one or more occurrence\n                else:\n                    dp[i][j] = dp[i-1][j-1] and (s[i-1] == p[j-1] or p[j-1] == '.')\n        return dp[m][n]"
  },
  {
    "objectID": "Algorithm/index.html",
    "href": "Algorithm/index.html",
    "title": "Algorithm Topics",
    "section": "",
    "text": "DP: Regular Expression Matching"
  },
  {
    "objectID": "Algorithm/index.html#dynamic-programming",
    "href": "Algorithm/index.html#dynamic-programming",
    "title": "Algorithm Topics",
    "section": "",
    "text": "DP: Regular Expression Matching"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ickma.dev",
    "section": "",
    "text": "My learning notes and thoughts on math and machine learning.\nCurrently reading the Deep Learning book.\n\n\n\n\nHow ReLU solves problems that linear models cannot handle.\n\n\n\nThe mathematical connection between probabilistic models and loss functions.\n\n\n\nExploring activation functions and their impact on neural network learning.\n\n\n\nHow depth enables hierarchical feature reuse and exponential expressiveness with fewer parameters.\n\n\n\nThe algorithm that makes training deep networks computationally feasible through efficient gradient computation.\n\n\n\nEssential second-order calculus concepts needed before Chapter 7 on optimization algorithms.\n\n\n\n\n\n\nTwo powerful perspectives that reveal the hidden beauty of linear systems: row picture vs column picture.\n\n\n\nThe systematic algorithm that transforms linear systems into upper triangular form for easy solution.\n\n\n\nFive different perspectives on matrix multiplication, from element-wise computation to rank-1 decomposition, plus understanding when matrices canâ€™t be inverted.\n\n\n\n\n\n\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\n\n\nDP Regex"
  },
  {
    "objectID": "index.html#deep-learning-book",
    "href": "index.html#deep-learning-book",
    "title": "ickma.dev",
    "section": "",
    "text": "How ReLU solves problems that linear models cannot handle.\n\n\n\nThe mathematical connection between probabilistic models and loss functions.\n\n\n\nExploring activation functions and their impact on neural network learning.\n\n\n\nHow depth enables hierarchical feature reuse and exponential expressiveness with fewer parameters.\n\n\n\nThe algorithm that makes training deep networks computationally feasible through efficient gradient computation.\n\n\n\nEssential second-order calculus concepts needed before Chapter 7 on optimization algorithms."
  },
  {
    "objectID": "index.html#mathematics",
    "href": "index.html#mathematics",
    "title": "ickma.dev",
    "section": "",
    "text": "Two powerful perspectives that reveal the hidden beauty of linear systems: row picture vs column picture.\n\n\n\nThe systematic algorithm that transforms linear systems into upper triangular form for easy solution.\n\n\n\nFive different perspectives on matrix multiplication, from element-wise computation to rank-1 decomposition, plus understanding when matrices canâ€™t be inverted."
  },
  {
    "objectID": "index.html#more",
    "href": "index.html#more",
    "title": "ickma.dev",
    "section": "",
    "text": "K-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\n\n\nDP Regex"
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html",
    "href": "Math/mit1806-lecture1-geometry.html",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "This is for MIT 18.06SC Lecture 1, covering how to understand linear systems from two perspectives: geometry (row picture) and algebra (column picture)."
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#the-example-system",
    "href": "Math/mit1806-lecture1-geometry.html#the-example-system",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "The Example System",
    "text": "The Example System\nLetâ€™s work with this concrete example:\n\\[\\begin{align}\nx + 2y &= 5 \\\\\n3x + 4y &= 6\n\\end{align}\\]\nIn matrix form: \\[\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nWe can interpret this system in two completely different ways."
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#row-picture-geometry",
    "href": "Math/mit1806-lecture1-geometry.html#row-picture-geometry",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Row Picture (Geometry)",
    "text": "Row Picture (Geometry)\nIn the row picture, each equation represents a geometric object: - In 2D: each equation is a line - In 3D: each equation is a plane\n- In higher dimensions: each equation is a hyperplane\nThe solution is where all these objects intersect.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the equations in the form y = mx + c\n# Line 1: x + 2y = 5  =&gt;  y = -1/2*x + 5/2\n# Line 2: 3x + 4y = 6  =&gt;  y = -3/4*x + 3/2\nx = np.linspace(-10, 10, 100)\ny1 = -1/2 * x + 5/2\ny2 = -3/4 * x + 3/2\n\n# Solve for intersection point\nA = np.array([[1, 2], [3, 4]])\nb = np.array([5, 6])\nsolution = np.linalg.solve(A, b)\n\n# Plot both lines and intersection\nplt.figure(figsize=(8, 6))\nplt.plot(x, y1, 'b-', label='Line 1: x + 2y = 5', linewidth=2)\nplt.plot(x, y2, 'r-', label='Line 2: 3x + 4y = 6', linewidth=2)\nplt.scatter(solution[0], solution[1], color='green', s=100, zorder=5, \n           label=f'Solution: ({solution[0]:.1f}, {solution[1]:.1f})', edgecolor='white', linewidth=2)\n\nplt.xlim(-8, 8)\nplt.ylim(-1, 8)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Row Picture: Where Lines Meet')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Solution: x = {solution[0]:.3f}, y = {solution[1]:.3f}\")\nprint(f\"Verification: {A @ solution} equals {b}\")\n\n\n\n\n\n\n\n\n\nSolution: x = -4.000, y = 4.500\nVerification: [5. 6.] equals [5 6]"
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#column-picture-algebra",
    "href": "Math/mit1806-lecture1-geometry.html#column-picture-algebra",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Column Picture (Algebra)",
    "text": "Column Picture (Algebra)\nThe column picture reframes the same system as a question about vector combinations:\n\\[x \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} + y \\begin{bmatrix}2 \\\\ 4\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nInstead of asking â€œwhere do lines intersect?â€, we ask: â€œCan we combine these vectors to reach our target?â€\n\n\nCode\n# Define column vectors and target vector\na1 = np.array([1, 3])\na2 = np.array([2, 4])\nb = np.array([5, 6])\n\n# Solve for coefficients\nA = np.column_stack([a1, a2])\nsolution = np.linalg.solve(A, b)\nx, y = solution[0], solution[1]\n\nprint(f\"Question: Can we write b as a linear combination of aâ‚ and aâ‚‚?\")\nprint(f\"Answer: {x:.3f} Ã— aâ‚ + {y:.3f} Ã— aâ‚‚ = b\")\nprint(f\"Verification: {x*a1} + {y*a2} = {x*a1 + y*a2}\")\n\n# Visualize the vector construction\nplt.figure(figsize=(8, 6))\n\n# Step 1: Draw x*a1 (scaled version)\nplt.arrow(0, 0, x*a1[0], x*a1[1], head_width=0.2, head_length=0.2, \n         fc='blue', ec='blue', linewidth=3,\n         label=f'{x:.2f} Ã— aâ‚')\n\n# Step 2: Draw y*a2 starting from the tip of x*a1\nplt.arrow(x*a1[0], x*a1[1], y*a2[0], y*a2[1], head_width=0.2, head_length=0.2, \n         fc='green', ec='green', linewidth=3,\n         label=f'{y:.2f} Ã— aâ‚‚')\n\n# Show final result vector b\nplt.arrow(0, 0, b[0], b[1], head_width=0.25, head_length=0.25, \n         fc='red', ec='red', linewidth=4, alpha=0.8,\n         label=f'b = [{b[0]}, {b[1]}]')\n\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.xlim(-1, 6)\nplt.ylim(-12, 7)\nplt.xlabel('x-component')\nplt.ylabel('y-component')\nplt.title('Column Picture: Vector Combination')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\n\n\nQuestion: Can we write b as a linear combination of aâ‚ and aâ‚‚?\nAnswer: -4.000 Ã— aâ‚ + 4.500 Ã— aâ‚‚ = b\nVerification: [ -4. -12.] + [ 9. 18.] = [5. 6.]"
  },
  {
    "objectID": "Math/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "href": "Math/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Three Types of Linear Systems",
    "text": "Three Types of Linear Systems\nLinear systems can have three possible outcomes:\n\nUnique solution - Lines intersect at one point\nNo solution - Lines are parallel (donâ€™t intersect)\nInfinitely many solutions - Lines are the same (overlap completely)\n\n\n\nCode\n# Case (a): Unique solution - non-parallel vectors\nprint(\"ğŸ¯ Case (a) - Unique Solution:\")\nA_a = np.array([[1, 2], [3, 4]])\nb_a = np.array([5, 6])\nsolution_a = np.linalg.solve(A_a, b_a)\ndet_a = np.linalg.det(A_a)\nprint(f\"   Solution: {solution_a}\")\nprint(f\"   Matrix determinant: {det_a:.3f} â‰  0 â†’ linearly independent columns\")\nprint(f\"   Column space: ENTIRE 2D plane (any point reachable)\")\n\n# Case (b): No solution - parallel vectors, b not in span\nprint(f\"\\nâŒ Case (b) - No Solution:\")\nA_b = np.array([[1, 2], [2, 4]])  # Columns are parallel\nb_b = np.array([5, 6])            # b not in span\ndet_b = np.linalg.det(A_b)\nprint(f\"   Matrix determinant: {det_b:.3f} = 0 â†’ linearly dependent columns\")\nprint(f\"   Column space: 1D line only (most points unreachable)\")\nprint(f\"   Target b = {b_b} is NOT on the line â†’ No solution exists\")\n\n# Case (c): Infinitely many solutions - parallel vectors, b in span\nprint(f\"\\nâ™¾ï¸  Case (c) - Infinitely Many Solutions:\")\nA_c = np.array([[1, 2], [2, 4]])  # Same parallel columns\nb_c = np.array([3, 6])            # b = 3 * [1, 2], so b is in span\ndet_c = np.linalg.det(A_c)\nprint(f\"   Matrix determinant: {det_c:.3f} = 0 â†’ linearly dependent columns\")\nprint(f\"   Column space: 1D line only\")\nprint(f\"   Target b = {b_c} IS on the line â†’ Infinite solutions exist\")\n\n# Find one particular solution using pseudoinverse\nsolution_c = np.linalg.pinv(A_c) @ b_c\nprint(f\"   One particular solution: {solution_c}\")\nprint(f\"   Other solutions: {solution_c} + tÃ—[2, -1] for any real number t\")\n\n\nğŸ¯ Case (a) - Unique Solution:\n   Solution: [-4.   4.5]\n   Matrix determinant: -2.000 â‰  0 â†’ linearly independent columns\n   Column space: ENTIRE 2D plane (any point reachable)\n\nâŒ Case (b) - No Solution:\n   Matrix determinant: 0.000 = 0 â†’ linearly dependent columns\n   Column space: 1D line only (most points unreachable)\n   Target b = [5 6] is NOT on the line â†’ No solution exists\n\nâ™¾ï¸  Case (c) - Infinitely Many Solutions:\n   Matrix determinant: 0.000 = 0 â†’ linearly dependent columns\n   Column space: 1D line only\n   Target b = [3 6] IS on the line â†’ Infinite solutions exist\n   One particular solution: [0.6 1.2]\n   Other solutions: [0.6 1.2] + tÃ—[2, -1] for any real number t\n\n\n\n\nCode\n# Visualize all three cases\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Case (a): Unique solution\nax = axes[0]\nax.fill_between([-1, 6], [-1, -1], [7, 7], color='lightblue', alpha=0.2, \n                label='Column space = ENTIRE plane')\n\n# Draw vectors\nax.arrow(0, 0, A_a[0,0], A_a[1,0], head_width=0.15, head_length=0.15,\n         fc='blue', ec='blue', linewidth=2, label='aâ‚ = [1,3]')\nax.arrow(0, 0, A_a[0,1], A_a[1,1], head_width=0.15, head_length=0.15,\n         fc='green', ec='green', linewidth=2, label='aâ‚‚ = [2,4]')\nax.arrow(0, 0, b_a[0], b_a[1], head_width=0.2, head_length=0.2,\n         fc='red', ec='red', linewidth=3, label='b = [5,6]')\n\nax.set_title('Unique Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (b): No solution\nax = axes[1]\nt = np.linspace(-2, 5, 100)\nspan_x, span_y = t * A_b[0,0], t * A_b[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6, \n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_b[0,0], A_b[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='aâ‚ = [1,2]')\nax.arrow(0, 0, A_b[0,1], A_b[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='aâ‚‚ = [2,4] = 2Ã—aâ‚')\nax.arrow(0, 0, b_b[0], b_b[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [5,6] (off line)')\n\nax.set_title('No Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (c): Infinitely many solutions\nax = axes[2]\nt = np.linspace(-1, 4, 100)\nspan_x, span_y = t * A_c[0,0], t * A_c[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6,\n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_c[0,0], A_c[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='aâ‚ = [1,2]')\nax.arrow(0, 0, A_c[0,1], A_c[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='aâ‚‚ = [2,4] = 2Ã—aâ‚')\nax.arrow(0, 0, b_c[0], b_c[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [3,6] (on line)')\n\nax.set_title('Infinite Solutions')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key insight: Solution depends on whether target vector b lies in the column space\")\n\n\n\n\n\n\n\n\n\nKey insight: Solution depends on whether target vector b lies in the column space\n\n\n\nThis covers the core geometric foundations from MIT 18.06SC Lecture 1: understanding linear systems through both row and column perspectives."
  },
  {
    "objectID": "Math/index.html",
    "href": "Math/index.html",
    "title": "Math",
    "section": "",
    "text": "Mathematical foundations and explorations.\n\n\n\nMIT 18.06SC Lecture 1: Geometry of Linear Equations\nMIT 18.06SC Lecture 2: Elimination with Matrices\nMIT 18.06SC Lecture 3: Matrix Multiplication and Inverse\nMIT 18.06SC Lecture 4: LU Decomposition\nMIT 18.06SC Lecture 5.1: Permutation Matrices"
  },
  {
    "objectID": "Math/index.html#linear-algebra",
    "href": "Math/index.html#linear-algebra",
    "title": "Math",
    "section": "",
    "text": "MIT 18.06SC Lecture 1: Geometry of Linear Equations\nMIT 18.06SC Lecture 2: Elimination with Matrices\nMIT 18.06SC Lecture 3: Matrix Multiplication and Inverse\nMIT 18.06SC Lecture 4: LU Decomposition\nMIT 18.06SC Lecture 5.1: Permutation Matrices"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, Iâ€™m Chao Ma (aka ickma), a passionate developer and researcher focused on machine learning, algorithms, and problem-solving.\n\n\nI enjoy exploring the intersection of mathematics and computer science, with a particular interest in:\n\nğŸ¤– Machine Learning & AI - From fundamental concepts to practical implementations\nğŸ§® Algorithms & Data Structures - Solving complex problems efficiently\n\nğŸ“Š Data Science - Extracting insights from data using Python and NumPy\nğŸ’» Software Development - Building robust, scalable solutions\n\n\n\n\nThis site serves as my digital notebook where I share:\n\nAlgorithm explanations with visual examples and code implementations\nProblem-solving approaches for coding challenges and mathematical concepts\nTechnical insights from my learning journey\n\nI believe in learning by doing and explaining concepts clearly with code examples, visualizations, and mathematical foundations.\n\n\n\nMy content covers:\n\nAlgorithms - Dynamic programming, optimization, data structures, and algorithmic problem solving\nMathematics - Linear algebra, calculus, statistics, and mathematical foundations for CS\nReinforcement Learning & Deep Learning - Neural networks, policy optimization, and AI agents\nParallel Computation - Distributed systems, GPU computing, and performance optimization\n\n\n\n\nIâ€™m always excited to discuss technology, collaborate on projects, or help fellow learners!\n\nğŸ“§ Email: ickma2311@gmail.com\nğŸ’» GitHub: @ickma2311\nğŸ¦ Twitter: @ickma2311\n\nFeel free to reach out if you have questions about any of my posts, want to collaborate, or just want to chat about machine learning and algorithms!\n\nThis blog is built with Quarto and hosted on GitHub Pages. All code examples are available in my repositories."
  },
  {
    "objectID": "about.html#what-i-do",
    "href": "about.html#what-i-do",
    "title": "About",
    "section": "",
    "text": "I enjoy exploring the intersection of mathematics and computer science, with a particular interest in:\n\nğŸ¤– Machine Learning & AI - From fundamental concepts to practical implementations\nğŸ§® Algorithms & Data Structures - Solving complex problems efficiently\n\nğŸ“Š Data Science - Extracting insights from data using Python and NumPy\nğŸ’» Software Development - Building robust, scalable solutions"
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "This site serves as my digital notebook where I share:\n\nAlgorithm explanations with visual examples and code implementations\nProblem-solving approaches for coding challenges and mathematical concepts\nTechnical insights from my learning journey\n\nI believe in learning by doing and explaining concepts clearly with code examples, visualizations, and mathematical foundations."
  },
  {
    "objectID": "about.html#technical-focus",
    "href": "about.html#technical-focus",
    "title": "About",
    "section": "",
    "text": "My content covers:\n\nAlgorithms - Dynamic programming, optimization, data structures, and algorithmic problem solving\nMathematics - Linear algebra, calculus, statistics, and mathematical foundations for CS\nReinforcement Learning & Deep Learning - Neural networks, policy optimization, and AI agents\nParallel Computation - Distributed systems, GPU computing, and performance optimization"
  },
  {
    "objectID": "about.html#connect-with-me",
    "href": "about.html#connect-with-me",
    "title": "About",
    "section": "",
    "text": "Iâ€™m always excited to discuss technology, collaborate on projects, or help fellow learners!\n\nğŸ“§ Email: ickma2311@gmail.com\nğŸ’» GitHub: @ickma2311\nğŸ¦ Twitter: @ickma2311\n\nFeel free to reach out if you have questions about any of my posts, want to collaborate, or just want to chat about machine learning and algorithms!\n\nThis blog is built with Quarto and hosted on GitHub Pages. All code examples are available in my repositories."
  },
  {
    "objectID": "ML/activation-functions.html",
    "href": "ML/activation-functions.html",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "",
    "text": "This exploration of Deep Learning Chapter 6.3 reveals how activation functions shape the behavior of hidden units in neural networks - and why choosing the right one matters.\nğŸ““ For the complete implementation with additional exercises, see the notebook on GitHub.\nğŸ“š For theoretical background and summary, see the chapter summary."
  },
  {
    "objectID": "ML/activation-functions.html#why-activation-functions-matter",
    "href": "ML/activation-functions.html#why-activation-functions-matter",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "Why Activation Functions Matter",
    "text": "Why Activation Functions Matter\nLinear transformations alone can only represent linear relationships. No matter how many layers you stack, \\(W_3(W_2(W_1x))\\) is still just a linear function. Activation functions introduce the non-linearity that makes deep learning powerful.\nBut which activation function should you use? The answer depends on understanding their mathematical properties and how they affect gradient flow during training.\n\n\n\n\n\n\n\n\n\nActivation\nOutput Range\nKey Property\nBest For\n\n\n\n\nReLU\n\\([0, \\infty)\\)\nZero for negatives\nHidden layers (default choice)\n\n\nSigmoid\n\\((0, 1)\\)\nSquashing, smooth\nBinary classification output\n\n\nTanh\n\\((-1, 1)\\)\nZero-centered\nHidden layers (when centering helps)"
  },
  {
    "objectID": "ML/activation-functions.html#exploring-activation-functions-shape-and-derivatives",
    "href": "ML/activation-functions.html#exploring-activation-functions-shape-and-derivatives",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "ğŸ¯ Exploring Activation Functions: Shape and Derivatives",
    "text": "ğŸ¯ Exploring Activation Functions: Shape and Derivatives\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Configure plotting\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['axes.grid'] = True\nplt.rcParams['grid.alpha'] = 0.3\n\n\nThe behavior of an activation function is determined by two things: 1. Its shape - how it transforms inputs 2. Its derivative - how gradients flow backward during training\n\nDefine Activation Functions\n\n\nShow code\ndef relu(x):\n    return np.clip(x, 0, np.inf)\n\ndef relu_derivative(x):\n    return np.where(x &gt; 0, 1, 0)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    return 1 - np.tanh(x)**2\n\n\n\n\nPlot Functions and Derivatives\n\n\nShow code\nx = np.linspace(-5, 5, 1000)\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 8))\nfig.suptitle('Common Activation Functions and Their Derivatives', fontsize=16)\n\n# ReLU\naxes[0, 0].plot(x, relu(x), linewidth=2, color='blue')\naxes[0, 0].set_title('ReLU', fontsize=12)\naxes[0, 0].set_ylabel('f(x)', fontsize=11)\naxes[1, 0].plot(x, relu_derivative(x), linewidth=2, color='blue')\naxes[1, 0].set_title('ReLU Derivative', fontsize=12)\naxes[1, 0].set_ylabel(\"f'(x)\", fontsize=11)\naxes[1, 0].set_xlabel('x', fontsize=11)\n\n# Sigmoid\naxes[0, 1].plot(x, sigmoid(x), linewidth=2, color='red')\naxes[0, 1].set_title('Sigmoid', fontsize=12)\naxes[1, 1].plot(x, sigmoid_derivative(x), linewidth=2, color='red')\naxes[1, 1].set_title('Sigmoid Derivative', fontsize=12)\naxes[1, 1].set_xlabel('x', fontsize=11)\n\n# Tanh\naxes[0, 2].plot(x, tanh(x), linewidth=2, color='green')\naxes[0, 2].set_title('Tanh', fontsize=12)\naxes[1, 2].plot(x, tanh_derivative(x), linewidth=2, color='green')\naxes[1, 2].set_title('Tanh Derivative', fontsize=12)\naxes[1, 2].set_xlabel('x', fontsize=11)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nKey observations:\n\nReLU: \\(f(x) = \\max(0, x)\\) - Zero for negative inputs, identity for positive. Derivative is 0 or 1 (simple!).\nSigmoid: \\(f(x) = \\frac{1}{1+e^{-x}}\\) - Squashes inputs to \\((0, 1)\\). Derivative peaks at 0, vanishes at extremes (gradient vanishing problem).\nTanh: \\(f(x) = \\tanh(x)\\) - Similar to sigmoid but outputs in \\((-1, 1)\\). Zero-centered with stronger gradients than sigmoid."
  },
  {
    "objectID": "ML/activation-functions.html#the-dead-relu-problem-when-neurons-stop-learning",
    "href": "ML/activation-functions.html#the-dead-relu-problem-when-neurons-stop-learning",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "The Dead ReLU Problem: When Neurons Stop Learning",
    "text": "The Dead ReLU Problem: When Neurons Stop Learning\nReLUâ€™s simplicity is its strength, but also its weakness. A ReLU neuron can â€œdieâ€ - permanently outputting zero and never learning again.\nWhy does this happen?\nWhen a neuronâ€™s pre-activation values are consistently negative (due to poor initialization, high learning rate, or bad gradients), ReLU outputs zero. Since the derivative is also zero for negative inputs, no gradient flows backward. The neuron is stuck forever.\n\n\nShow code\n# Generate input data\nx = torch.randn(1000, 10)  # 1000 samples, 10 features\nlinear = nn.Linear(10, 5)   # 5 hidden units\n\n# Set bias to large negative values to \"kill\" neurons\nwith torch.no_grad():\n    linear.bias.fill_(-10.0)\n\n# Forward pass\npre_activation = linear(x)\npost_activation = torch.relu(pre_activation)\n\n# Calculate statistics\ndead_percentage = (post_activation == 0).float().mean() * 100\nprint(f\"Percentage of dead neurons: {dead_percentage:.2f}%\\n\")\n\n# Display table showing ReLU input vs output\nprint(\"ReLU Input vs Output (first 10 samples, neuron 0):\")\nprint(\"-\" * 50)\nprint(f\"{'Sample':&lt;10} {'Pre-Activation':&lt;20} {'Post-Activation':&lt;20}\")\nprint(\"-\" * 50)\n\nfor i in range(10):\n    pre_val = pre_activation[i, 0].item()\n    post_val = post_activation[i, 0].item()\n    print(f\"{i:&lt;10} {pre_val:&lt;20.4f} {post_val:&lt;20.4f}\")\n\nprint(\"\\nObservation: All negative inputs become 0 after ReLU â†’ Dead neuron!\")\n\n\nPercentage of dead neurons: 100.00%\n\nReLU Input vs Output (first 10 samples, neuron 0):\n--------------------------------------------------\nSample     Pre-Activation       Post-Activation     \n--------------------------------------------------\n0          -9.7837              0.0000              \n1          -10.0322             0.0000              \n2          -10.4466             0.0000              \n3          -10.3243             0.0000              \n4          -10.5448             0.0000              \n5          -9.7712              0.0000              \n6          -10.8104             0.0000              \n7          -11.3418             0.0000              \n8          -9.8559              0.0000              \n9          -8.6873              0.0000              \n\nObservation: All negative inputs become 0 after ReLU â†’ Dead neuron!\n\n\nWith a large negative bias, every input becomes negative after the linear transformation. ReLU zeros them all out. The gradient is zero everywhere. The neuron never updates. Itâ€™s dead."
  },
  {
    "objectID": "ML/activation-functions.html#experiment-do-different-activations-make-a-difference",
    "href": "ML/activation-functions.html#experiment-do-different-activations-make-a-difference",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "Experiment: Do Different Activations Make a Difference?",
    "text": "Experiment: Do Different Activations Make a Difference?\nTheory is nice, but letâ€™s see activation functions in action. Weâ€™ll train three identical networks with different activations on a simple regression task: \\(y = \\sin(x) + x^2 + 1\\).\n\nGenerate Data\n\n\nShow code\n# Training data\nx_train = np.random.rand(200, 1)\ny_train = np.sin(x_train) + np.power(x_train, 2) + 1\n\n# Test data\nx_test = np.random.rand(50, 1)\ny_test = np.sin(x_test) + np.power(x_test, 2) + 1\n\n# Convert to PyTorch tensors\nx_train_tensor = torch.FloatTensor(x_train)\ny_train_tensor = torch.FloatTensor(y_train)\nx_test_tensor = torch.FloatTensor(x_test)\ny_test_tensor = torch.FloatTensor(y_test)\n\n\n\n\nCreate and Train Models\n\n\nShow code\ndef create_regression_model(activation_fn):\n    \"\"\"Create a 2-layer network with specified activation\"\"\"\n    return nn.Sequential(\n        nn.Linear(1, 20),\n        activation_fn,\n        nn.Linear(20, 1)\n    )\n\n# Create 3 models with different activations\nmodels = {\n    'ReLU': create_regression_model(nn.ReLU()),\n    'Sigmoid': create_regression_model(nn.Sigmoid()),\n    'Tanh': create_regression_model(nn.Tanh())\n}\n\n# Training configuration\nn_epochs = 100\nlearning_rate = 0.01\nloss_fn = nn.MSELoss()\n\n# Track metrics\nloss_history = {name: [] for name in models.keys()}\ntest_mse_history = {name: [] for name in models.keys()}\n\n# Train each model\nfor name, model in models.items():\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    for epoch in range(n_epochs):\n        # Training\n        model.train()\n        y_pred = model(x_train_tensor)\n        loss = loss_fn(y_pred, y_train_tensor)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loss_history[name].append(loss.item())\n\n        # Evaluation on test set\n        model.eval()\n        with torch.no_grad():\n            y_test_pred = model(x_test_tensor)\n            test_mse = loss_fn(y_test_pred, y_test_tensor).item()\n            test_mse_history[name].append(test_mse)\n\n\n\n\nCompare Learning Curves\n\n\nShow code\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\ncolors = {'ReLU': 'blue', 'Sigmoid': 'red', 'Tanh': 'green'}\n\n# Plot training loss\nfor name, losses in loss_history.items():\n    axes[0].plot(losses, label=name, linewidth=2, color=colors[name])\n\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Training Loss (MSE)', fontsize=12)\naxes[0].set_title('Training Loss Over Time', fontsize=14)\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\naxes[0].set_yscale('log')\n\n# Plot test MSE\nfor name, test_mse in test_mse_history.items():\n    axes[1].plot(test_mse, label=name, linewidth=2, color=colors[name])\n\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Test Loss (MSE)', fontsize=12)\naxes[1].set_title('Test Loss Over Time', fontsize=14)\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\naxes[1].set_yscale('log')\n\nplt.tight_layout()\nplt.show()\n\n# Print final metrics\nprint(\"\\nFinal Metrics after {} epochs:\".format(n_epochs))\nprint(\"-\" * 60)\nprint(f\"{'Activation':&lt;15} {'Train Loss':&lt;15} {'Test Loss':&lt;15}\")\nprint(\"-\" * 60)\nfor name in models.keys():\n    train_loss = loss_history[name][-1]\n    test_loss = test_mse_history[name][-1]\n    print(f\"{name:&lt;15} {train_loss:&lt;15.6f} {test_loss:&lt;15.6f}\")\n\n\n\n\n\n\n\n\n\n\nFinal Metrics after 100 epochs:\n------------------------------------------------------------\nActivation      Train Loss      Test Loss      \n------------------------------------------------------------\nReLU            0.007420        0.008211       \nSigmoid         0.227441        0.247947       \nTanh            0.035384        0.038743"
  },
  {
    "objectID": "ML/backpropagation.html",
    "href": "ML/backpropagation.html",
    "title": "Deep Learning Book 6.5: Back-Propagation and Other Differentiation Algorithms",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.5 explores backpropagationâ€”the algorithm that makes training deep neural networks computationally feasible by efficiently computing gradients through the chain rule.\nğŸ““ For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub.\nğŸš€ Want to see backpropagation in action? Check out picogradâ€”a minimal automatic differentiation engine I built (inspired by Andrej Karpathyâ€™s micrograd) that implements reverse-mode autodiff from scratch. It demonstrates how computational graphs enable automatic gradient computation through backpropagation!"
  },
  {
    "objectID": "ML/backpropagation.html#the-gradient-problem-why-backpropagation",
    "href": "ML/backpropagation.html#the-gradient-problem-why-backpropagation",
    "title": "Deep Learning Book 6.5: Back-Propagation and Other Differentiation Algorithms",
    "section": "The Gradient Problem: Why Backpropagation?",
    "text": "The Gradient Problem: Why Backpropagation?\nTraining a neural network requires computing gradients of the loss function with respect to every parameterâ€”potentially millions of them. The naive approach of computing each gradient independently would be computationally intractable.\nBackpropagation solves this by exploiting the chain rule in a clever way: it reuses intermediate computations to calculate all gradients in a single backward pass through the network. This transforms an exponentially expensive problem into a linear one.\n\nQuick Reference: Understanding Backpropagation\nFor context on the mathematical foundations of backpropagation, see the Backpropagation summary.\nKey Concepts:\n\nChain Rule in Vector Form: For composite mapping \\(z = f(y), y = g(x)\\): \\[\\nabla_x z = \\left( \\frac{\\partial y}{\\partial x} \\right)^{\\top} \\nabla_y z\\]\nForward Pass: Cache all activations \\(h^{(l)} = f^{(l)}(W^{(l)} h^{(l-1)} + b^{(l)})\\)\nBackward Pass: Propagate gradients layer by layer: \\[\\nabla_{h^{(l-1)}} L = (W^{(l)})^{\\top} (\\nabla_{h^{(l)}} L \\odot f'^{(l)}(z^{(l)}))\\]\nParameter Gradients: \\[\\frac{\\partial L}{\\partial W^{(l)}} = (\\nabla_{h^{(l)}} L \\odot f'^{(l)}(z^{(l)})) (h^{(l-1)})^{\\top}\\]\n\n\n\n\n\n\n\n\n\nConcept\nDescription\nKey Insight\n\n\n\n\nComputational Graph\nDAG representing operations\nEnables reverse-mode automatic differentiation\n\n\nGradient Reuse\nShare intermediate computations\nReduces complexity from exponential to linear\n\n\nLocal Gradients\nEach operation computes local derivative\nChain rule combines them for global gradient"
  },
  {
    "objectID": "ML/backpropagation.html#exercise-1-understanding-jacobian-matrices",
    "href": "ML/backpropagation.html#exercise-1-understanding-jacobian-matrices",
    "title": "Deep Learning Book 6.5: Back-Propagation and Other Differentiation Algorithms",
    "section": "ğŸ”¬ Exercise 1: Understanding Jacobian Matrices",
    "text": "ğŸ”¬ Exercise 1: Understanding Jacobian Matrices\nHow do gradients flow through multiple layers? Letâ€™s trace the chain rule through a 2-layer linear network.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\nprint(\"âœ“ Setup complete\")\n\n\nâœ“ Setup complete\n\n\n\nThe 2-Layer Network\nLayer 1 (Hidden layer): \\[\n\\begin{align}\nh_1 &= 2x_1 + x_2 \\\\\nh_2 &= x_1 + 3x_2 \\\\\nh_3 &= -x_1 + x_2\n\\end{align}\n\\]\nLayer 2 (Output layer): \\[\n\\begin{align}\ny_1 &= h_1 + 2h_2 - h_3 \\\\\ny_2 &= 3h_1 - h_2 + h_3\n\\end{align}\n\\]\n\n\nForward Pass Implementation\n\n\nShow code\ndef forward_pass(x: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute forward pass through the 2-layer network.\n\n    Args:\n        x: Input vector [x1, x2]\n\n    Returns:\n        h: Hidden layer output [h1, h2, h3]\n        y: Output layer [y1, y2]\n    \"\"\"\n    x1, x2 = x[0], x[1]\n\n    # Layer 1 (linear)\n    h1 = 2*x1 + x2\n    h2 = x1 + 3*x2\n    h3 = -x1 + x2\n    h = np.array([h1, h2, h3])\n\n    # Layer 2 (linear)\n    y1 = h1 + 2*h2 - h3\n    y2 = 3*h1 - h2 + h3\n    y = np.array([y1, y2])\n\n    return h, y\n\nprint(\"âœ“ Forward pass defined\")\n\n\nâœ“ Forward pass defined\n\n\n\n\nComputing Jacobian Matrices\nThe chain rule states: \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{h}} \\cdot \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{x}}\\)\n\n\nShow code\ndef compute_jacobian_h_x(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute âˆ‚h/âˆ‚x analytically.\n\n    Args:\n        x: Input vector [x1, x2]\n\n    Returns:\n        Jacobian matrix (3x2)\n    \"\"\"\n    J = np.array([[2, 1], [1, 3], [-1, 1]])\n    return J\n\ndef compute_jacobian_y_h(x: np.ndarray, h: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute âˆ‚y/âˆ‚h analytically.\n\n    Args:\n        x: Input vector [x1, x2]\n        h: Hidden layer [h1, h2, h3]\n\n    Returns:\n        Jacobian matrix (2x3)\n    \"\"\"\n    J = np.array([[1, 2, -1], [3, -1, 1]])\n    return J\n\ndef compute_jacobian_y_x(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute âˆ‚y/âˆ‚x using chain rule.\n\n    Args:\n        x: Input vector [x1, x2]\n\n    Returns:\n        Jacobian matrix (2x2)\n    \"\"\"\n    return compute_jacobian_y_h(x, forward_pass(x)[0]) @ compute_jacobian_h_x(x)\n\nprint(\"âœ“ Jacobian functions defined\")\n\n\nâœ“ Jacobian functions defined\n\n\n\n\nVerification with Numerical Gradients\n\n\nShow code\ndef numerical_gradient(func, x, epsilon=1e-7):\n    \"\"\"\n    Compute numerical gradient using finite differences.\n    \"\"\"\n    grad = np.zeros((len(func(x)), len(x)))\n    for i in range(len(x)):\n        x_plus = x.copy()\n        x_minus = x.copy()\n        x_plus[i] += epsilon\n        x_minus[i] -= epsilon\n        grad[:, i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n    return grad\n\n# Test at point (1.0, 2.0)\nx_test = np.array([1.0, 2.0])\nh_test, y_test = forward_pass(x_test)\n\nprint(f\"Test point: x = {x_test}\")\nprint(f\"Hidden layer: h = {h_test}\")\nprint(f\"Output: y = {y_test}\")\nprint(\"\\n\" + \"=\"*50)\n\n# Compute analytical Jacobians\nJ_h_x_analytical = compute_jacobian_h_x(x_test)\nJ_y_h_analytical = compute_jacobian_y_h(x_test, h_test)\nJ_y_x_analytical = compute_jacobian_y_x(x_test)\n\n# Compute numerical Jacobians\nJ_h_x_numerical = numerical_gradient(lambda x: forward_pass(x)[0], x_test)\nJ_y_x_numerical = numerical_gradient(lambda x: forward_pass(x)[1], x_test)\n\nprint(\"\\nâˆ‚h/âˆ‚x (Analytical):\")\nprint(J_h_x_analytical)\nprint(\"\\nâˆ‚h/âˆ‚x (Numerical):\")\nprint(J_h_x_numerical)\nprint(f\"\\nDifference: {np.max(np.abs(J_h_x_analytical - J_h_x_numerical)):.2e}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"\\nâˆ‚y/âˆ‚h (Analytical):\")\nprint(J_y_h_analytical)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"\\nâˆ‚y/âˆ‚x (Analytical):\")\nprint(J_y_x_analytical)\nprint(\"\\nâˆ‚y/âˆ‚x (Numerical):\")\nprint(J_y_x_numerical)\nprint(f\"\\nDifference: {np.max(np.abs(J_y_x_analytical - J_y_x_numerical)):.2e}\")\n\n\nTest point: x = [1. 2.]\nHidden layer: h = [4. 7. 1.]\nOutput: y = [17.  6.]\n\n==================================================\n\nâˆ‚h/âˆ‚x (Analytical):\n[[ 2  1]\n [ 1  3]\n [-1  1]]\n\nâˆ‚h/âˆ‚x (Numerical):\n[[ 2.  1.]\n [ 1.  3.]\n [-1.  1.]]\n\nDifference: 2.80e-09\n\n==================================================\n\nâˆ‚y/âˆ‚h (Analytical):\n[[ 1  2 -1]\n [ 3 -1  1]]\n\n==================================================\n\nâˆ‚y/âˆ‚x (Analytical):\n[[5 6]\n [4 1]]\n\nâˆ‚y/âˆ‚x (Numerical):\n[[5.00000002 6.        ]\n [4.         0.99999999]]\n\nDifference: 2.29e-08"
  },
  {
    "objectID": "ML/backpropagation.html#exercise-2-gradient-descent-with-backpropagation",
    "href": "ML/backpropagation.html#exercise-2-gradient-descent-with-backpropagation",
    "title": "Deep Learning Book 6.5: Back-Propagation and Other Differentiation Algorithms",
    "section": "ğŸ”¬ Exercise 2: Gradient Descent with Backpropagation",
    "text": "ğŸ”¬ Exercise 2: Gradient Descent with Backpropagation\nLetâ€™s implement a complete 3-layer network with backpropagation and watch how the parameters evolve during training.\n\nNetwork Architecture\nLayer 1: \\(h^{(1)}_i = \\tanh(w^{(1)}_{i1} x_1 + w^{(1)}_{i2} x_2 + b^{(1)}_i)\\)\nLayer 2: \\(h^{(2)}_i = \\tanh(w^{(2)}_{i1} a^{(1)}_1 + w^{(2)}_{i2} a^{(1)}_2 + b^{(2)}_i)\\)\nOutput: \\(\\hat{y} = w^{(3)}_1 a^{(2)}_1 + w^{(3)}_2 a^{(2)}_2 + b^{(3)}\\)\nLoss: \\(L = \\frac{1}{2}(\\hat{y} - y_{\\text{target}})^2\\)\n\n\nImplementation\n\n\nShow code\nclass ThreeLayerNetwork:\n    def __init__(self, seed=42):\n        \"\"\"\n        Initialize a 3-layer neural network with random weights.\n        \"\"\"\n        np.random.seed(seed)\n\n        # Layer 1: 2 inputs -&gt; 2 hidden units\n        self.W1 = np.random.randn(2, 2) * 0.5\n        self.b1 = np.random.randn(2) * 0.5\n\n        # Layer 2: 2 hidden -&gt; 2 hidden units\n        self.W2 = np.random.randn(2, 2) * 0.5\n        self.b2 = np.random.randn(2) * 0.5\n\n        # Layer 3: 2 hidden -&gt; 1 output\n        self.W3 = np.random.randn(2) * 0.5\n        self.b3 = np.random.randn() * 0.5\n\n        # For storing intermediate values during forward pass\n        self.cache = {}\n\n    def forward(self, x: np.ndarray) -&gt; float:\n        \"\"\"\n        Forward propagation through the network.\n\n        Args:\n            x: Input vector [x1, x2]\n\n        Returns:\n            y_hat: Predicted output (scalar)\n        \"\"\"\n        # Store input\n        self.cache['x'] = x\n\n        # Layer 1: h1 = W1 @ x + b1, a1 = tanh(h1)\n        self.cache['h1'] = self.W1 @ x + self.b1\n        self.cache['a1'] = np.tanh(self.cache['h1'])\n\n        # Layer 2: h2 = W2 @ a1 + b2, a2 = tanh(h2)\n        self.cache['h2'] = self.W2 @ self.cache['a1'] + self.b2\n        self.cache['a2'] = np.tanh(self.cache['h2'])\n\n        # Layer 3: y_hat = W3 @ a2 + b3 (linear output)\n        y_hat = self.W3 @ self.cache['a2'] + self.b3\n        self.cache['y_hat'] = y_hat\n\n        return y_hat\n\n    def backward(self, y_target: float, learning_rate: float) -&gt; dict:\n        \"\"\"\n        Backpropagation to compute gradients.\n\n        Args:\n            y_target: Target output (scalar)\n            learning_rate: Learning rate for gradient descent\n\n        Returns:\n            grads: Dictionary containing gradients for all parameters\n        \"\"\"\n        # Get cached values\n        x = self.cache['x']\n        a1 = self.cache['a1']\n        a2 = self.cache['a2']\n        y_hat = self.cache['y_hat']\n\n        # Output gradient: dL/dy_hat = y_hat - y_target\n        dL_dy = y_hat - y_target\n\n        # Layer 3 gradients\n        dL_dW3 = dL_dy * a2  # shape: (2,)\n        dL_db3 = dL_dy       # scalar\n        dL_da2 = dL_dy * self.W3  # shape: (2,)\n\n        # Layer 2 gradients\n        # tanh derivative: d(tanh(x))/dx = 1 - tanh(x)^2\n        dL_dh2 = dL_da2 * (1 - a2**2)  # shape: (2,)\n        dL_dW2 = np.outer(dL_dh2, a1)  # shape: (2, 2)\n        dL_db2 = dL_dh2                # shape: (2,)\n        dL_da1 = self.W2.T @ dL_dh2    # shape: (2,)\n\n        # Layer 1 gradients\n        dL_dh1 = dL_da1 * (1 - a1**2)  # shape: (2,)\n        dL_dW1 = np.outer(dL_dh1, x)   # shape: (2, 2)\n        dL_db1 = dL_dh1                # shape: (2,)\n\n        # Store gradients\n        grads = {\n            'dW3': dL_dW3,\n            'db3': dL_db3,\n            'dW2': dL_dW2,\n            'db2': dL_db2,\n            'dW1': dL_dW1,\n            'db1': dL_db1\n        }\n\n        # Update parameters\n        self.W3 -= learning_rate * dL_dW3\n        self.b3 -= learning_rate * dL_db3\n        self.W2 -= learning_rate * dL_dW2\n        self.b2 -= learning_rate * dL_db2\n        self.W1 -= learning_rate * dL_dW1\n        self.b1 -= learning_rate * dL_db1\n\n        return grads\n\n    def compute_loss(self, y_hat: float, y_target: float) -&gt; float:\n        \"\"\"\n        Compute MSE loss.\n        \"\"\"\n        return 0.5 * (y_hat - y_target)**2\n\n    def get_params(self) -&gt; dict:\n        \"\"\"\n        Get all parameters as a dictionary.\n        \"\"\"\n        return {\n            'W1': self.W1.copy(),\n            'b1': self.b1.copy(),\n            'W2': self.W2.copy(),\n            'b2': self.b2.copy(),\n            'W3': self.W3.copy(),\n            'b3': self.b3\n        }\n\nprint(\"âœ“ Network class defined\")\n\n\nâœ“ Network class defined\n\n\n\n\nTraining Loop\n\n\nShow code\n# Training configuration\nx_input = np.array([0.5, -0.3])\ny_target = 1.0\nlearning_rate = 0.01\nnum_iterations = 1000\n\n# Initialize network\nnetwork = ThreeLayerNetwork(seed=42)\n\n# Storage for logging\nloss_history = []\nparam_history = {\n    'W1': [],\n    'b1': [],\n    'W2': [],\n    'b2': [],\n    'W3': [],\n    'b3': []\n}\n\n# Training loop\nfor i in range(num_iterations):\n    # Forward pass\n    y_hat = network.forward(x_input)\n\n    # Compute loss\n    loss = network.compute_loss(y_hat, y_target)\n    loss_history.append(loss)\n\n    # Backward pass and parameter update\n    grads = network.backward(y_target, learning_rate)\n\n    # Log parameters\n    params = network.get_params()\n    param_history['W1'].append(params['W1'])\n    param_history['b1'].append(params['b1'])\n    param_history['W2'].append(params['W2'])\n    param_history['b2'].append(params['b2'])\n    param_history['W3'].append(params['W3'])\n    param_history['b3'].append(params['b3'])\n\n    # Print loss every 100 iterations\n    if (i + 1) % 100 == 0:\n        print(f\"Iteration {i+1:4d}: Loss = {loss:.6f}, y_hat = {y_hat:.6f}\")\n\nprint(\"\\nTraining completed!\")\n\n\nIteration  100: Loss = 0.019820, y_hat = 0.800902\nIteration  200: Loss = 0.000292, y_hat = 0.975849\nIteration  300: Loss = 0.000004, y_hat = 0.997066\nIteration  400: Loss = 0.000000, y_hat = 0.999644\nIteration  500: Loss = 0.000000, y_hat = 0.999957\nIteration  600: Loss = 0.000000, y_hat = 0.999995\nIteration  700: Loss = 0.000000, y_hat = 0.999999\nIteration  800: Loss = 0.000000, y_hat = 1.000000\nIteration  900: Loss = 0.000000, y_hat = 1.000000\nIteration 1000: Loss = 0.000000, y_hat = 1.000000\n\nTraining completed!\n\n\n\n\nVisualization: Loss Curve\n\n\nShow code\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(loss_history)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Training Loss over Time')\nplt.grid(True)\nplt.yscale('log')\n\nplt.subplot(1, 2, 2)\nplt.plot(loss_history[-500:])\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss (Last 500 iterations)')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nVisualization: Parameter Evolution\n\n\nShow code\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\n\n# W1\naxes[0, 0].plot(np.array(param_history['W1'])[:, 0, 0], label='W1[0,0]')\naxes[0, 0].plot(np.array(param_history['W1'])[:, 0, 1], label='W1[0,1]')\naxes[0, 0].plot(np.array(param_history['W1'])[:, 1, 0], label='W1[1,0]')\naxes[0, 0].plot(np.array(param_history['W1'])[:, 1, 1], label='W1[1,1]')\naxes[0, 0].set_title('Layer 1 Weights (W1)')\naxes[0, 0].legend()\naxes[0, 0].grid(True)\n\n# b1\naxes[0, 1].plot(np.array(param_history['b1'])[:, 0], label='b1[0]')\naxes[0, 1].plot(np.array(param_history['b1'])[:, 1], label='b1[1]')\naxes[0, 1].set_title('Layer 1 Biases (b1)')\naxes[0, 1].legend()\naxes[0, 1].grid(True)\n\n# W2\naxes[0, 2].plot(np.array(param_history['W2'])[:, 0, 0], label='W2[0,0]')\naxes[0, 2].plot(np.array(param_history['W2'])[:, 0, 1], label='W2[0,1]')\naxes[0, 2].plot(np.array(param_history['W2'])[:, 1, 0], label='W2[1,0]')\naxes[0, 2].plot(np.array(param_history['W2'])[:, 1, 1], label='W2[1,1]')\naxes[0, 2].set_title('Layer 2 Weights (W2)')\naxes[0, 2].legend()\naxes[0, 2].grid(True)\n\n# b2\naxes[1, 0].plot(np.array(param_history['b2'])[:, 0], label='b2[0]')\naxes[1, 0].plot(np.array(param_history['b2'])[:, 1], label='b2[1]')\naxes[1, 0].set_title('Layer 2 Biases (b2)')\naxes[1, 0].legend()\naxes[1, 0].grid(True)\n\n# W3\naxes[1, 1].plot(np.array(param_history['W3'])[:, 0], label='W3[0]')\naxes[1, 1].plot(np.array(param_history['W3'])[:, 1], label='W3[1]')\naxes[1, 1].set_title('Layer 3 Weights (W3)')\naxes[1, 1].legend()\naxes[1, 1].grid(True)\n\n# b3\naxes[1, 2].plot(param_history['b3'], label='b3')\naxes[1, 2].set_title('Layer 3 Bias (b3)')\naxes[1, 2].legend()\naxes[1, 2].grid(True)\n\nfor ax in axes.flat:\n    ax.set_xlabel('Iteration')\n    ax.set_ylabel('Parameter Value')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFinal Results\n\n\nShow code\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Initial Loss: {loss_history[0]:.6f}\")\nprint(f\"Final Loss: {loss_history[-1]:.6f}\")\nprint(f\"Loss Reduction: {(1 - loss_history[-1]/loss_history[0])*100:.2f}%\")\nprint(f\"\\nTarget: {y_target}\")\nfinal_prediction = network.forward(x_input)\nprint(f\"Final Prediction: {final_prediction:.6f}\")\nprint(f\"Prediction Error: {abs(final_prediction - y_target):.6f}\")\n\n\n\n============================================================\nTRAINING SUMMARY\n============================================================\nInitial Loss: 1.323064\nFinal Loss: 0.000000\nLoss Reduction: 100.00%\n\nTarget: 1.0\nFinal Prediction: 1.000000\nPrediction Error: 0.000000\n\n\n\nThis implementation demonstrates the power of backpropagation: efficiently computing gradients through the chain rule enables training deep neural networks that would otherwise be computationally intractable."
  },
  {
    "objectID": "ML/index.html",
    "href": "ML/index.html",
    "title": "Machine Learning Topics",
    "section": "",
    "text": "Understanding Axis(Dim) Operations"
  },
  {
    "objectID": "ML/index.html#numpy-fundamentals",
    "href": "ML/index.html#numpy-fundamentals",
    "title": "Machine Learning Topics",
    "section": "",
    "text": "Understanding Axis(Dim) Operations"
  },
  {
    "objectID": "ML/index.html#clustering-algorithms",
    "href": "ML/index.html#clustering-algorithms",
    "title": "Machine Learning Topics",
    "section": "Clustering Algorithms",
    "text": "Clustering Algorithms\n\nK-Means Clustering"
  },
  {
    "objectID": "ML/index.html#deep-learning-fundamentals",
    "href": "ML/index.html#deep-learning-fundamentals",
    "title": "Machine Learning Topics",
    "section": "Deep Learning Fundamentals",
    "text": "Deep Learning Fundamentals\n\nThe XOR Problem: Nonlinearity in Deep Learning\nLikelihood-Based Loss Functions\nHidden Units and Activation Functions\nArchitecture Design: Depth vs Width\nBack-Propagation and Other Differentiation Algorithms"
  },
  {
    "objectID": "ML/index.html#classification-algorithms",
    "href": "ML/index.html#classification-algorithms",
    "title": "Machine Learning Topics",
    "section": "Classification Algorithms",
    "text": "Classification Algorithms\n\nLogistic Regression"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html",
    "href": "ML/likelihood-loss-functions.html",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.2 reveals the fundamental connection between probabilistic assumptions and the loss functions we use to train neural networks.\nğŸ““ For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#the-hidden-connection-why-these-loss-functions",
    "href": "ML/likelihood-loss-functions.html#the-hidden-connection-why-these-loss-functions",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "The Hidden Connection: Why These Loss Functions?",
    "text": "The Hidden Connection: Why These Loss Functions?\nEver wondered why we use mean squared error for regression, cross-entropy for classification, and other specific loss functions? The answer lies in maximum likelihood estimation - each common loss function corresponds to the negative log-likelihood of a specific probabilistic model.\n\n\n\n\n\n\n\n\nProbabilistic Model\nLoss Function\nUse Case\n\n\n\n\nGaussian likelihood\nMean Squared Error\nRegression\n\n\nBernoulli likelihood\nBinary Cross-Entropy\nBinary Classification\n\n\nCategorical likelihood\nSoftmax Cross-Entropy\nMulticlass Classification"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#exploring-the-connection-probabilistic-models-loss-functions",
    "href": "ML/likelihood-loss-functions.html#exploring-the-connection-probabilistic-models-loss-functions",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "ğŸ¯ Exploring the Connection: Probabilistic Models â†’ Loss Functions",
    "text": "ğŸ¯ Exploring the Connection: Probabilistic Models â†’ Loss Functions\n\n\nShow code\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-1-gaussian-likelihood-mean-squared-error",
    "href": "ML/likelihood-loss-functions.html#connection-1-gaussian-likelihood-mean-squared-error",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 1: Gaussian Likelihood â†’ Mean Squared Error",
    "text": "Connection 1: Gaussian Likelihood â†’ Mean Squared Error\nThe Setup: When we assume our targets have Gaussian noise around our predictions:\n\\[p(y|x) = \\mathcal{N}(y; \\hat{y}, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\hat{y})^2}{2\\sigma^2}\\right)\\]\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = \\frac{(y-\\hat{y})^2}{2\\sigma^2} + \\frac{1}{2}\\log(2\\pi\\sigma^2)\\]\nThe Result: Minimizing this is equivalent to minimizing MSE (the constant term doesnâ€™t affect optimization)!\n\n\nShow code\n# Demonstrate Gaussian likelihood = MSE connection\nnp.random.seed(0)\nx = np.linspace(-1, 1, 20)\ny_true = 2 * x + 1\ny = y_true + np.random.normal(0, 0.1, size=x.shape)  # Gaussian noise\n\n# Simple linear model predictions\nw, b = 1.0, 0.0\ny_pred = w * x + b\n\n# Compute MSE\nmse = np.mean((y - y_pred)**2)\n\n# Compute Gaussian negative log-likelihood\nsigma_squared = 0.1**2\nquadratic_term = 0.5 * np.mean((y - y_pred)**2) / sigma_squared\nconst_term = 0.5 * np.log(2 * np.pi * sigma_squared)\nnll_gaussian = quadratic_term + const_term\n\nprint(\"ğŸ“Š Gaussian Likelihood â†” MSE Connection\")\nprint(\"=\" * 45)\nprint(f\"ğŸ“ˆ Mean Squared Error:     {mse:.6f}\")\nprint(f\"ğŸ“Š Gaussian NLL:           {nll_gaussian:.6f}\")\nprint(f\"   â”œâ”€ Quadratic term:      {quadratic_term:.6f}\")\nprint(f\"   â””â”€ Constant term:       {const_term:.6f}\")\n\nscaling_factor = 1 / (2 * sigma_squared)\nprint(f\"\\nğŸ”— Mathematical Connection:\")\nprint(f\"   Quadratic term = {scaling_factor:.1f} Ã— MSE\")\nprint(f\"   {quadratic_term:.6f} = {scaling_factor:.1f} Ã— {mse:.6f}\")\nprint(f\"\\nâœ… Minimizing MSE â‰¡ Maximizing Gaussian likelihood\")\n\n\nğŸ“Š Gaussian Likelihood â†” MSE Connection\n=============================================\nğŸ“ˆ Mean Squared Error:     1.450860\nğŸ“Š Gaussian NLL:           71.159339\n   â”œâ”€ Quadratic term:      72.542985\n   â””â”€ Constant term:       -1.383647\n\nğŸ”— Mathematical Connection:\n   Quadratic term = 50.0 Ã— MSE\n   72.542985 = 50.0 Ã— 1.450860\n\nâœ… Minimizing MSE â‰¡ Maximizing Gaussian likelihood"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-2-bernoulli-likelihood-binary-cross-entropy",
    "href": "ML/likelihood-loss-functions.html#connection-2-bernoulli-likelihood-binary-cross-entropy",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 2: Bernoulli Likelihood â†’ Binary Cross-Entropy",
    "text": "Connection 2: Bernoulli Likelihood â†’ Binary Cross-Entropy\nThe Setup: For binary classification, we assume Bernoulli-distributed targets:\n\\[p(y|x) = \\sigma(z)^y (1-\\sigma(z))^{1-y}\\]\nwhere \\(\\sigma(z) = \\frac{1}{1+e^{-z}}\\) is the sigmoid function.\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = -y\\log\\sigma(z) - (1-y)\\log(1-\\sigma(z))\\]\nThe Result: This is exactly binary cross-entropy loss!\n\n\nShow code\n# Demonstrate Bernoulli likelihood = Binary cross-entropy connection\nz = torch.tensor([-0.5, -0.8, 0.0, 0.8, 0.5])  # Model logits\ny = torch.tensor([0.0, 0.0, 1.0, 1.0, 1.0])     # Binary labels\np = torch.sigmoid(z)  # Convert to probabilities\n\nprint(\"ğŸ² Bernoulli Likelihood â†” Binary Cross-Entropy\")\nprint(\"=\" * 50)\nprint(\"Input Data:\")\nprint(f\"   Logits:        {z.numpy()}\")\nprint(f\"   Labels:        {y.numpy()}\")\nprint(f\"   Probabilities: {p.numpy()}\")\n\n# Manual Bernoulli NLL computation\nbernoulli_nll = torch.mean(-(y * torch.log(p) + (1 - y) * torch.log(1 - p)))\n\n# PyTorch binary cross-entropy\nbce_loss = F.binary_cross_entropy(p, y)\n\nprint(f\"\\nğŸ“Š Loss Function Comparison:\")\nprint(f\"   Manual Bernoulli NLL:  {bernoulli_nll:.6f}\")\nprint(f\"   PyTorch BCE Loss:      {bce_loss:.6f}\")\n\n# Verify they're identical\ndifference = torch.abs(bernoulli_nll - bce_loss)\nprint(f\"\\nğŸ”— Verification:\")\nprint(f\"   Absolute difference:   {difference:.10f}\")\nprint(f\"\\nâœ… Binary cross-entropy IS Bernoulli negative log-likelihood!\")\n\n\nğŸ² Bernoulli Likelihood â†” Binary Cross-Entropy\n==================================================\nInput Data:\n   Logits:        [-0.5 -0.8  0.   0.8  0.5]\n   Labels:        [0. 0. 1. 1. 1.]\n   Probabilities: [0.37754068 0.3100255  0.5        0.6899745  0.62245935]\n\nğŸ“Š Loss Function Comparison:\n   Manual Bernoulli NLL:  0.476700\n   PyTorch BCE Loss:      0.476700\n\nğŸ”— Verification:\n   Absolute difference:   0.0000000000\n\nâœ… Binary cross-entropy IS Bernoulli negative log-likelihood!"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#connection-3-categorical-likelihood-softmax-cross-entropy",
    "href": "ML/likelihood-loss-functions.html#connection-3-categorical-likelihood-softmax-cross-entropy",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Connection 3: Categorical Likelihood â†’ Softmax Cross-Entropy",
    "text": "Connection 3: Categorical Likelihood â†’ Softmax Cross-Entropy\nThe Setup: For multiclass classification, we use the categorical distribution:\n\\[p(y=i|x) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} = \\text{softmax}(z)_i\\]\nThe Derivation: Taking negative log-likelihood:\n\\[-\\log p(y|x) = -\\log \\frac{e^{z_y}}{\\sum_j e^{z_j}} = -z_y + \\log\\sum_j e^{z_j}\\]\nThe Result: This is exactly softmax cross-entropy loss!\n\n\nShow code\n# Demonstrate Categorical likelihood = Softmax cross-entropy connection\nz = torch.tensor([[0.1, 0.2, 0.7],    # Sample 1: class 2 highest\n                  [0.1, 0.7, 0.2],    # Sample 2: class 1 highest  \n                  [0.7, 0.1, 0.2]])   # Sample 3: class 0 highest\n\ny = torch.tensor([2, 1, 0])           # True class indices\n\nprint(\"ğŸ¯ Categorical Likelihood â†” Softmax Cross-Entropy\")\nprint(\"=\" * 55)\nprint(\"Input Data:\")\nprint(f\"   Logits shape:    {z.shape}\")\nprint(f\"   True classes:    {y.numpy()}\")\n\n# Convert to probabilities\nsoftmax_probs = F.softmax(z, dim=1)\nprint(f\"\\nSoftmax Probabilities:\")\nfor i, (logit_row, prob_row, true_class) in enumerate(zip(z, softmax_probs, y)):\n    print(f\"   Sample {i+1}: {prob_row.numpy()} â†’ Class {true_class}\")\n\n# Manual categorical NLL (using log-softmax for numerical stability)\nlog_softmax = F.log_softmax(z, dim=1)\ncategorical_nll = -torch.mean(log_softmax[range(len(y)), y])\n\n# PyTorch cross-entropy\nce_loss = F.cross_entropy(z, y)\n\nprint(f\"\\nğŸ“Š Loss Function Comparison:\")\nprint(f\"   Manual Categorical NLL: {categorical_nll:.6f}\")\nprint(f\"   PyTorch Cross-Entropy:  {ce_loss:.6f}\")\n\n# Verify they're identical\ndifference = torch.abs(categorical_nll - ce_loss)\nprint(f\"\\nğŸ”— Verification:\")\nprint(f\"   Absolute difference:    {difference:.10f}\")\nprint(f\"\\nâœ… Cross-entropy IS categorical negative log-likelihood!\")\n\n\nğŸ¯ Categorical Likelihood â†” Softmax Cross-Entropy\n=======================================================\nInput Data:\n   Logits shape:    torch.Size([3, 3])\n   True classes:    [2 1 0]\n\nSoftmax Probabilities:\n   Sample 1: [0.25462854 0.28140804 0.46396342] â†’ Class 2\n   Sample 2: [0.25462854 0.46396342 0.28140804] â†’ Class 1\n   Sample 3: [0.46396342 0.25462854 0.28140804] â†’ Class 0\n\nğŸ“Š Loss Function Comparison:\n   Manual Categorical NLL: 0.767950\n   PyTorch Cross-Entropy:  0.767950\n\nğŸ”— Verification:\n   Absolute difference:    0.0000000000\n\nâœ… Cross-entropy IS categorical negative log-likelihood!"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#why-this-matters-bce-vs-mse-for-classification",
    "href": "ML/likelihood-loss-functions.html#why-this-matters-bce-vs-mse-for-classification",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Why This Matters: BCE vs MSE for Classification",
    "text": "Why This Matters: BCE vs MSE for Classification\nUnderstanding the probabilistic foundation explains why binary cross-entropy works better than MSE for classification, even though both can theoretically solve binary problems.\nKey Differences: - BCE gradient: \\(\\sigma(z) - y\\) (simple, well-behaved) - MSE gradient: \\(2(\\sigma(z) - y) \\times \\sigma(z) \\times (1 - \\sigma(z))\\) (can vanish!)\nLetâ€™s see this in practice:"
  },
  {
    "objectID": "ML/likelihood-loss-functions.html#key-takeaways",
    "href": "ML/likelihood-loss-functions.html#key-takeaways",
    "title": "Deep Learning Book 6.2: Likelihood-Based Loss Functions",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nUnderstanding the probabilistic foundation of loss functions reveals:\n\nMSE = Gaussian NLL: Mean squared error emerges from assuming Gaussian noise\nBCE = Bernoulli NLL: Binary cross-entropy is exactly Bernoulli negative log-likelihood\n\nCross-entropy = Categorical NLL: Softmax cross-entropy corresponds to categorical distributions\nBetter gradients: Probabilistically-motivated loss functions provide better optimization dynamics\n\nThis connection between probability theory and optimization is fundamental to understanding why certain loss functions work well for specific tasks.\n\nThis mathematical foundation helps explain not just which loss function to use, but why it works so effectively for the given problem type."
  },
  {
    "objectID": "ML/kmeans.html",
    "href": "ML/kmeans.html",
    "title": "K-means Clustering",
    "section": "",
    "text": "Setup points and K\nwe will implement a KNN algorithm to cluster the points\n\n\nX=[[1,1],[2,2.1],[3,2.5],[6,7],[7,7.1],[9,7.5]]\nk=2\n\nmax_iter=3\n\n\n# Visualize the data\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter([x[0] for x in X],[x[1] for x in X])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pure python implementation of K-means clustering\ndef knn_iter(X,centroids):\n    # set up new clusters\n    new_clusters=[[] for _ in range(len(centroids))]\n    # k=len(centroids)\n    # assign each point to the nearest centroid\n    for x in X:\n        k,distance=0,(x[0]-centroids[0][0])**2+(x[1]-centroids[0][1])**2\n        for i,c in enumerate(centroids[1:],1):\n            if (x[0]-c[0])**2+(x[1]-c[1])**2&lt;distance:\n                k=i\n                distance=(x[0]-c[0])**2+(x[1]-c[1])**2\n        new_clusters[k].append(x)\n    \n    # calculate new centroids\n    new_centroids=[[\n        sum([x[0] for x in cluster])/len(cluster),\n        sum([x[1] for x in cluster])/len(cluster)\n    ] if cluster else centroids[i] for i,cluster in enumerate(new_clusters)]\n    return new_centroids\n\n\n\n\n\n\n\n\ndef iter_and_draw(X,k,max_iter):\n    centroids=X[:k]  # Randomly select 2 centroids\n    fig, axes = plt.subplots(max_iter//3+(1 if max_iter%3!=0 else 0),\n        3, figsize=(15, 10))\n    axes=axes.flatten()\n    for i in range(max_iter):\n        \n        # Plot points and centroids\n\n\n        # Assign each point to nearest centroid and plot with corresponding color\n        colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n        for j, x in enumerate(X):\n            # Find nearest centroid\n            min_dist = float('inf')\n            nearest_centroid = 0\n            for k, c in enumerate(centroids):\n                dist = (x[0]-c[0])**2 + (x[1]-c[1])**2\n                if dist &lt; min_dist:\n                    min_dist = dist\n                    nearest_centroid = k\n            # Plot point with color corresponding to its cluster\n            axes[i].scatter(x[0], x[1], c=colors[nearest_centroid % len(colors)], label=f'Cluster {nearest_centroid+1}' if j==0 else \"\")\n        axes[i].scatter([c[0] for c in centroids], [c[1] for c in centroids], c='red', marker='*', s=200, label='Centroids')\n        axes[i].set_title(f'Iteration {i}')\n        centroids = knn_iter(X, centroids)\n\n    plt.tight_layout()\n    plt.show()\n\niter_and_draw(X,k,max_iter)\n# print(centroids)\n\n\n\n\n\n\n\n\n\n# A 3 clusters example\n\nimport numpy as np\n\nX1=np.random.rand(20,2)+5 # Some points in the upper right corner\nX2=np.random.rand(20,2)+3 # Some points in the middle\nX3=np.random.rand(20,2) # Some points in the lower left corner\n\niter_and_draw(np.concatenate((X1,X2,X3)),3,5)\n\n\n\n\n\n\n\n\n\n\nA question?\n\nWhat to do if one cluster has no assigned points during iteration?\n\n\n\nFormula Derivation\nThe goal is to minimize the loss of inertia which is sum of the points to cluster centroids.\n\\[\nLoss= \\sum_{i=1}^n \\sum_{x \\in C_i} ||x-\\mu_i||^2\n\\]\nTo iter \\(\\mu\\) for each cluster, let us find the derivative of the following function. \\[\nf(\\mu)=\\sum_{i=1}^n ||x_i-\\mu||^2 =\n\\sum_{i=1}^n {x_i}^2+\\mu^2-2x_i\\mu\n\\]\nGiven a \\(\\nabla \\mu\\), \\[\nf(\\mu + \\nabla \\mu)=\\sum_{i=1}^n ||x_i+\\nabla \\mu -\\mu||^2 =\n\\sum_{i=1}^n  {x_i}^2+\\mu^2+{\\nabla \\mu}^2-2{x_i \\mu}-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\nf(\\mu + \\nabla \\mu)-f(\\mu)=\n\\sum_{i=1}^n {\\nabla \\mu}^2-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\n\\frac {f(\\mu + \\nabla \\mu)-f(\\mu)}{\\nabla \\mu}=\\sum_{i=1}^n {\\nabla \\mu} -2 \\mu +2{x_i} = 2\\sum_{i=1}^n x_i - 2n\\mu\n\\]\nNow we can see if \\(n\\mu = \\sum_{i=1}^n x_i\\), then the derivative is 0, this is why in each iteration, we need to set the center of the cluster as centroid."
  },
  {
    "objectID": "Math/mit1806-lecture5-permutations.html",
    "href": "Math/mit1806-lecture5-permutations.html",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nPermutation matrices reorder rows and columns using a simple structure of 0s and 1s. This post covers the permutation matrices portion of Lecture 5."
  },
  {
    "objectID": "Math/mit1806-lecture5-permutations.html#context",
    "href": "Math/mit1806-lecture5-permutations.html#context",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nPermutation matrices reorder rows and columns using a simple structure of 0s and 1s. This post covers the permutation matrices portion of Lecture 5."
  },
  {
    "objectID": "Math/mit1806-lecture5-permutations.html#what-is-a-permutation-matrix",
    "href": "Math/mit1806-lecture5-permutations.html#what-is-a-permutation-matrix",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "What is a Permutation Matrix?",
    "text": "What is a Permutation Matrix?\nA matrix is a permutation matrix if: - Each row has exactly one 1, all other entries are 0 - Each column has exactly one 1, all other entries are 0\n\nExample: Row Swapping\n\n\nShow code\nimport numpy as np\nfrom IPython.display import display, Markdown\n\ndef matrix_to_latex(mat, name=\"\"):\n    \"\"\"Convert numpy matrix to LaTeX bmatrix format\"\"\"\n    rows = []\n    for row in mat:\n        rows.append(\" & \".join(map(str, row)))\n    latex = r\"\\begin{bmatrix}\" + \" \\\\\\\\ \".join(rows) + r\"\\end{bmatrix}\"\n    if name:\n        latex = f\"{name} = {latex}\"\n    return f\"$${latex}$$\"\n\n# Create a random 3Ã—4 matrix\nA = np.random.randint(0, 10, (3, 4))\ndisplay(Markdown(\"**Matrix A:**\"))\ndisplay(Markdown(matrix_to_latex(A)))\n\n# Create a permutation matrix that swaps rows 0 and 1\nP = np.zeros((3, 3), dtype=int)\nP[0, 1] = 1\nP[1, 0] = 1\nP[2, 2] = 1\ndisplay(Markdown(\"**Permutation matrix P:**\"))\ndisplay(Markdown(matrix_to_latex(P)))\n\n# Apply permutation: PA swaps rows 0 and 1 of A\ndisplay(Markdown(\"**P @ A (rows 0 and 1 swapped):**\"))\ndisplay(Markdown(matrix_to_latex(P @ A)))\n\n\nMatrix A:\n\n\n\\[\\begin{bmatrix}8 & 1 & 5 & 9 \\\\ 1 & 3 & 7 & 7 \\\\ 1 & 4 & 6 & 4\\end{bmatrix}\\]\n\n\nPermutation matrix P:\n\n\n\\[\\begin{bmatrix}0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\n\n\nP @ A (rows 0 and 1 swapped):\n\n\n\\[\\begin{bmatrix}1 & 3 & 7 & 7 \\\\ 8 & 1 & 5 & 9 \\\\ 1 & 4 & 6 & 4\\end{bmatrix}\\]\n\n\nKey insight: Left multiplication by \\(P\\) reorders the rows of \\(A\\) according to the pattern encoded in \\(P\\)."
  },
  {
    "objectID": "Math/mit1806-lecture5-permutations.html#counting-permutation-matrices",
    "href": "Math/mit1806-lecture5-permutations.html#counting-permutation-matrices",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Counting Permutation Matrices",
    "text": "Counting Permutation Matrices\nAn \\(n \\times n\\) matrix has exactly \\(n!\\) permutation matrices.\nExamples: - \\(n=3\\): \\(3! = 6\\) permutation matrices - \\(n=4\\): \\(4! = 24\\) permutation matrices - \\(n=5\\): \\(5! = 120\\) permutation matrices\n\nGenerating All Permutations\n\n\nShow code\nfrom itertools import permutations\n\n# Generate all permutations for n=3\nn = 3\nperms = list(permutations(range(n)))\ndisplay(Markdown(f\"There are **{len(perms)}** permutation matrices for n={n}\"))\ndisplay(Markdown(\"\"))\n\n# Display all 6 permutation matrices\nfor i, perm in enumerate(perms):\n    display(Markdown(f\"**Permutation {i+1}:** {perm}\"))\n    P = np.zeros((n, n), dtype=int)\n    P[np.arange(n), perm] = 1\n    display(Markdown(matrix_to_latex(P)))\n    display(Markdown(\"\"))\n\n\nThere are 6 permutation matrices for n=3\n\n\n\n\n\nPermutation 1: (0, 1, 2)\n\n\n\\[\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 2: (0, 2, 1)\n\n\n\\[\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 3: (1, 0, 2)\n\n\n\\[\\begin{bmatrix}0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 4: (1, 2, 0)\n\n\n\\[\\begin{bmatrix}0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 5: (2, 0, 1)\n\n\n\\[\\begin{bmatrix}0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0\\end{bmatrix}\\]\n\n\n\n\n\nPermutation 6: (2, 1, 0)\n\n\n\\[\\begin{bmatrix}0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0\\end{bmatrix}\\]\n\n\n\n\n\n\n\nNotable Permutation Matrices for \\(n=3\\)\nIdentity (no permutation): \\[\nI = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\\]\nSingle swap (rows 0 and 1): \\[\nP = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\\]\nCyclic permutation: \\[\nP = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Math/mit1806-lecture5-permutations.html#key-properties-of-permutation-matrices",
    "href": "Math/mit1806-lecture5-permutations.html#key-properties-of-permutation-matrices",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Key Properties of Permutation Matrices",
    "text": "Key Properties of Permutation Matrices\nPermutation matrices satisfy three properties:\n\n\\(P^{-1} = P^T\\) (inverse equals transpose)\n\\(PP^T = P^TP = I\\) (orthogonal matrix)\n\\(\\det(P) = \\pm 1\\) (determinant is \\(\\pm 1\\))"
  },
  {
    "objectID": "Math/mit1806-lecture5-permutations.html#proof-1-p-1-pt",
    "href": "Math/mit1806-lecture5-permutations.html#proof-1-p-1-pt",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Proof 1: \\(P^{-1} = P^T\\)",
    "text": "Proof 1: \\(P^{-1} = P^T\\)\n\nStrategy\nSince \\(PP^{-1} = I\\) by definition of inverse, we only need to prove that \\(PP^T = I\\). Then we can conclude \\(P^{-1} = P^T\\).\n\n\nProof of \\(PP^T = I\\)\n\n\nShow code\n# Example permutation matrix (swaps rows 0 and 1)\nP = np.array([[0, 1, 0],\n              [1, 0, 0],\n              [0, 0, 1]])\n\n# Compute transpose\nP_T = P.T\n\n# Verify PP^T = I\nresult = P @ P_T\ndisplay(Markdown(\"**P @ P^T =**\"))\ndisplay(Markdown(matrix_to_latex(result)))\n\n\nP @ P^T =\n\n\n\\[\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\n\n\nIntuition:\nThe \\(j\\)-th column of \\(P^T\\) is the \\(j\\)-th row of \\(P\\).\nWhen computing \\((PP^T)_{ij}\\) (the \\((i,j)\\) entry): - \\((PP^T)_{ij} = \\text{(row } i \\text{ of } P) \\cdot \\text{(column } j \\text{ of } P^T\\text{)}\\) - \\(= \\text{(row } i \\text{ of } P) \\cdot \\text{(row } j \\text{ of } P\\text{)}\\)\nTwo cases: - When \\(i = j\\): Dot product of a row with itself = 1 (each row has exactly one 1) - When \\(i \\neq j\\): Dot product of different rows = 0 (the 1s are in different positions)\nTherefore, \\(PP^T = I\\).\n\n\nVerification by Row\n\n\nShow code\n# Verify by computing each row of P times P^T\nfor i in range(3):\n    result_row = P[i] @ P_T\n    display(Markdown(f\"**Row {i} of P times P^T:** $[{' \\\\ '.join(map(str, result_row))}]$\"))\ndisplay(Markdown(\"\"))\ndisplay(Markdown(\"Each row gives one row of the identity matrix!\"))\n\n\nRow 0 of P times P^T: \\([1 \\ 0 \\ 0]\\)\n\n\nRow 1 of P times P^T: \\([0 \\ 1 \\ 0]\\)\n\n\nRow 2 of P times P^T: \\([0 \\ 0 \\ 1]\\)\n\n\n\n\n\nEach row gives one row of the identity matrix!\n\n\n\n\nConclusion: \\(P^{-1} = P^T\\)\nProof:\n\\[\n\\begin{align}\nPP^T &= I \\quad \\text{(proved above)} \\\\\nPP^{-1} &= I \\quad \\text{(definition of inverse)} \\\\\n\\therefore P^{-1} &= P^T\n\\end{align}\n\\]\nPractical implications: - Computing \\(P^{-1}\\) is as simple as transposing \\(P\\) - Transposition is \\(O(n^2)\\), much faster than general matrix inversion \\(O(n^3)\\) - If \\(PA\\) permutes rows, then \\(P^T(PA) = A\\) undoes the permutation"
  },
  {
    "objectID": "Math/mit1806-lecture5-permutations.html#proof-2-detp-pm-1",
    "href": "Math/mit1806-lecture5-permutations.html#proof-2-detp-pm-1",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Proof 2: \\(\\det(P) = \\pm 1\\)",
    "text": "Proof 2: \\(\\det(P) = \\pm 1\\)\n\nProperties of Determinants\nProperties used: 1. \\(\\det(AB) = \\det(A)\\det(B)\\) (product property) 2. \\(\\det(A^T) = \\det(A)\\) (transpose property)\n\n\nProof\n\\[\n\\begin{align}\n\\det(PP^T) &= \\det(I) = 1 \\\\\n\\det(PP^T) &= \\det(P)\\det(P^T) \\quad \\text{(product property)} \\\\\n&= \\det(P)\\det(P) \\quad \\text{(transpose property)} \\\\\n&= [\\det(P)]^2 \\\\\n\\therefore [\\det(P)]^2 &= 1 \\\\\n\\det(P) &= \\pm 1\n\\end{align}\n\\]\n\n\nInterpretation: Even vs.Â Odd Permutations\n\n\\(\\det(P) = +1\\): Even permutation (even number of row swaps)\n\\(\\det(P) = -1\\): Odd permutation (odd number of row swaps)\n\nExamples:\n\n\nShow code\n# Identity matrix (0 swaps)\nI = np.eye(3, dtype=int)\ndisplay(Markdown(f\"$\\\\det(I) = {np.linalg.det(I):.0f}$ (even: 0 swaps)\"))\n\n# Single swap (1 swap)\nP_swap = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\ndisplay(Markdown(f\"$\\\\det(P_{{swap}}) = {np.linalg.det(P_swap):.0f}$ (odd: 1 swap)\"))\n\n# Cyclic permutation (2 swaps)\nP_cycle = np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\ndisplay(Markdown(f\"$\\\\det(P_{{cycle}}) = {np.linalg.det(P_cycle):.0f}$ (even: 2 swaps)\"))\n\n\n\\(\\det(I) = 1\\) (even: 0 swaps)\n\n\n\\(\\det(P_{swap}) = -1\\) (odd: 1 swap)\n\n\n\\(\\det(P_{cycle}) = 1\\) (even: 2 swaps)"
  },
  {
    "objectID": "Math/mit1806-lecture5-permutations.html#applications-in-linear-algebra",
    "href": "Math/mit1806-lecture5-permutations.html#applications-in-linear-algebra",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Applications in Linear Algebra",
    "text": "Applications in Linear Algebra\n\nLU Decomposition with Pivoting\nPermutation matrices are essential for numerical stability:\n\nPartial pivoting: \\(PA = LU\\) where \\(P\\) reorders rows to avoid division by small pivots\nNumerical stability: Strategic row exchanges improve computational accuracy\nComplete pivoting: Both row and column permutations for maximum stability\n\n\n\nShow code\nfrom scipy.linalg import lu\n\n# Matrix that needs pivoting\nA = np.array([[0, 1, 2],\n              [3, 4, 5],\n              [6, 7, 8]], dtype=float)\n\n# LU decomposition with pivoting\nP, L, U = lu(A)\n\nprint(\"Original matrix A:\")\nprint(A)\nprint(\"\\nPermutation matrix P:\")\nprint(P.astype(int))\nprint(\"\\nPA = LU:\")\nprint(\"PA:\")\nprint(P @ A)\nprint(\"\\nLU:\")\nprint(L @ U)\n\n\nOriginal matrix A:\n[[0. 1. 2.]\n [3. 4. 5.]\n [6. 7. 8.]]\n\nPermutation matrix P:\n[[0 1 0]\n [0 0 1]\n [1 0 0]]\n\nPA = LU:\nPA:\n[[3. 4. 5.]\n [6. 7. 8.]\n [0. 1. 2.]]\n\nLU:\n[[6. 7. 8.]\n [0. 1. 2.]\n [3. 4. 5.]]\n\n\n\n\nMatrix Operations\n\nRow reordering: Left multiplication \\(PA\\) reorders rows of \\(A\\)\nColumn reordering: Right multiplication \\(AP\\) reorders columns of \\(A\\)\nSimilarity transformations: \\(PAP^T\\) performs symmetric permutation (preserves symmetry)\n\n\n\nComputational Efficiency\n\nComputing inverse: \\(P^{-1} = P^T\\) is instant (just transpose)\nSolving \\(Px = b\\): Multiply by \\(P^T\\) instead of traditional solving: \\(x = P^Tb\\)\nChecking invertibility: All permutation matrices are invertible"
  },
  {
    "objectID": "Math/mit1806-lecture5-permutations.html#summary",
    "href": "Math/mit1806-lecture5-permutations.html#summary",
    "title": "MIT 18.06SC Lecture 5.1: Permutation Matrices",
    "section": "Summary",
    "text": "Summary\nPermutation matrices:\n\nStructure: One 1 per row and column, rest are 0s\nCount: \\(n!\\) permutation matrices for \\(n \\times n\\) matrices\nInverse: \\(P^{-1} = P^T\\) (orthogonal property)\nDeterminant: \\(\\det(P) = \\pm 1\\) (sign indicates even/odd permutation)\nApplications: Critical for LU decomposition, numerical stability, and efficient computation\n\n\nSource: MIT 18.06SC Linear Algebra, Lecture 5"
  }
]