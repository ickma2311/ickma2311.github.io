[
  {
    "objectID": "ML/k_means_clustering.html",
    "href": "ML/k_means_clustering.html",
    "title": "K-means Clustering",
    "section": "",
    "text": "Setup points and K\nwe will implement a KNN algorithm to cluster the points\n\n\nX=[[1,1],[2,2.1],[3,2.5],[6,7],[7,7.1],[9,7.5]]\nk=2\n\nmax_iter=3\n\n\n# Visualize the data\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter([x[0] for x in X],[x[1] for x in X])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pure python implementation of K-means clustering\ndef knn_iter(X,centroids):\n    # set up new clusters\n    new_clusters=[[] for _ in range(len(centroids))]\n    # k=len(centroids)\n    # assign each point to the nearest centroid\n    for x in X:\n        k,distance=0,(x[0]-centroids[0][0])**2+(x[1]-centroids[0][1])**2\n        for i,c in enumerate(centroids[1:],1):\n            if (x[0]-c[0])**2+(x[1]-c[1])**2&lt;distance:\n                k=i\n                distance=(x[0]-c[0])**2+(x[1]-c[1])**2\n        new_clusters[k].append(x)\n    \n    # calculate new centroids\n    new_centroids=[[\n        sum([x[0] for x in cluster])/len(cluster),\n        sum([x[1] for x in cluster])/len(cluster)\n    ] if cluster else centroids[i] for i,cluster in enumerate(new_clusters)]\n    return new_centroids\n\n\n\n\n\n\n\n\ndef iter_and_draw(X,k,max_iter):\n    centroids=X[:k]  # Randomly select 2 centroids\n    fig, axes = plt.subplots(max_iter//3+(1 if max_iter%3!=0 else 0),\n        3, figsize=(15, 10))\n    axes=axes.flatten()\n    for i in range(max_iter):\n        \n        # Plot points and centroids\n\n\n        # Assign each point to nearest centroid and plot with corresponding color\n        colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n        for j, x in enumerate(X):\n            # Find nearest centroid\n            min_dist = float('inf')\n            nearest_centroid = 0\n            for k, c in enumerate(centroids):\n                dist = (x[0]-c[0])**2 + (x[1]-c[1])**2\n                if dist &lt; min_dist:\n                    min_dist = dist\n                    nearest_centroid = k\n            # Plot point with color corresponding to its cluster\n            axes[i].scatter(x[0], x[1], c=colors[nearest_centroid % len(colors)], label=f'Cluster {nearest_centroid+1}' if j==0 else \"\")\n        axes[i].scatter([c[0] for c in centroids], [c[1] for c in centroids], c='red', marker='*', s=200, label='Centroids')\n        axes[i].set_title(f'Iteration {i}')\n        centroids = knn_iter(X, centroids)\n\n    plt.tight_layout()\n    plt.show()\n\niter_and_draw(X,k,max_iter)\n# print(centroids)\n\n\n\n\n\n\n\n\n\n# A 3 clusters example\n\nimport numpy as np\n\nX1=np.random.rand(20,2)+5 # Some points in the upper right corner\nX2=np.random.rand(20,2)+3 # Some points in the middle\nX3=np.random.rand(20,2) # Some points in the lower left corner\n\niter_and_draw(np.concatenate((X1,X2,X3)),3,5)\n\n\n\n\n\n\n\n\n\n\nA question?\n\nWhat to do if one cluster has no assigned points during iteration?\n\n\n\nFormula Derivation\nThe goal is to minimize the loss of inertia which is sum of the points to cluster centroids.\n\\[\nLoss= \\sum_{i=1}^n \\sum_{x \\in C_i} ||x-\\mu_i||^2\n\\]\nTo iter \\(\\mu\\) for each cluster, let us find the derivative of the following function. \\[\nf(\\mu)=\\sum_{i=1}^n ||x_i-\\mu||^2 =\n\\sum_{i=1}^n {x_i}^2+\\mu^2-2x_i\\mu\n\\]\nGiven a \\(\\nabla \\mu\\), \\[\nf(\\mu + \\nabla \\mu)=\\sum_{i=1}^n ||x_i+\\nabla \\mu -\\mu||^2 =\n\\sum_{i=1}^n  {x_i}^2+\\mu^2+{\\nabla \\mu}^2-2{x_i \\mu}-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\nf(\\mu + \\nabla \\mu)-f(\\mu)=\n\\sum_{i=1}^n {\\nabla \\mu}^2-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\n\\frac {f(\\mu + \\nabla \\mu)-f(\\mu)}{\\nabla \\mu}=\\sum_{i=1}^n {\\nabla \\mu} -2 \\mu +2{x_i} = 2\\sum_{i=1}^n x_i - 2n\\mu\n\\]\nNow we can see if \\(n\\mu = \\sum_{i=1}^n x_i\\), then the derivative is 0, this is why in each iteration, we need to set the center of the cluster as centroid."
  },
  {
    "objectID": "ML/activation-functions.html",
    "href": "ML/activation-functions.html",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "",
    "text": "This exploration of Deep Learning Chapter 6.3 reveals how activation functions shape the behavior of hidden units in neural networks - and why choosing the right one matters.\nüìì For the complete implementation with additional exercises, see the notebook on GitHub.\nüìö For theoretical background and summary, see the chapter summary."
  },
  {
    "objectID": "ML/activation-functions.html#why-activation-functions-matter",
    "href": "ML/activation-functions.html#why-activation-functions-matter",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "Why Activation Functions Matter",
    "text": "Why Activation Functions Matter\nLinear transformations alone can only represent linear relationships. No matter how many layers you stack, \\(W_3(W_2(W_1x))\\) is still just a linear function. Activation functions introduce the non-linearity that makes deep learning powerful.\nBut which activation function should you use? The answer depends on understanding their mathematical properties and how they affect gradient flow during training.\n\n\n\n\n\n\n\n\n\nActivation\nOutput Range\nKey Property\nBest For\n\n\n\n\nReLU\n\\([0, \\infty)\\)\nZero for negatives\nHidden layers (default choice)\n\n\nSigmoid\n\\((0, 1)\\)\nSquashing, smooth\nBinary classification output\n\n\nTanh\n\\((-1, 1)\\)\nZero-centered\nHidden layers (when centering helps)"
  },
  {
    "objectID": "ML/activation-functions.html#exploring-activation-functions-shape-and-derivatives",
    "href": "ML/activation-functions.html#exploring-activation-functions-shape-and-derivatives",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "üéØ Exploring Activation Functions: Shape and Derivatives",
    "text": "üéØ Exploring Activation Functions: Shape and Derivatives\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Configure plotting\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['axes.grid'] = True\nplt.rcParams['grid.alpha'] = 0.3\n\n\nThe behavior of an activation function is determined by two things: 1. Its shape - how it transforms inputs 2. Its derivative - how gradients flow backward during training\n\nDefine Activation Functions\n\n\nShow code\ndef relu(x):\n    return np.clip(x, 0, np.inf)\n\ndef relu_derivative(x):\n    return np.where(x &gt; 0, 1, 0)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    return 1 - np.tanh(x)**2\n\n\n\n\nPlot Functions and Derivatives\n\n\nShow code\nx = np.linspace(-5, 5, 1000)\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 8))\nfig.suptitle('Common Activation Functions and Their Derivatives', fontsize=16)\n\n# ReLU\naxes[0, 0].plot(x, relu(x), linewidth=2, color='blue')\naxes[0, 0].set_title('ReLU', fontsize=12)\naxes[0, 0].set_ylabel('f(x)', fontsize=11)\naxes[1, 0].plot(x, relu_derivative(x), linewidth=2, color='blue')\naxes[1, 0].set_title('ReLU Derivative', fontsize=12)\naxes[1, 0].set_ylabel(\"f'(x)\", fontsize=11)\naxes[1, 0].set_xlabel('x', fontsize=11)\n\n# Sigmoid\naxes[0, 1].plot(x, sigmoid(x), linewidth=2, color='red')\naxes[0, 1].set_title('Sigmoid', fontsize=12)\naxes[1, 1].plot(x, sigmoid_derivative(x), linewidth=2, color='red')\naxes[1, 1].set_title('Sigmoid Derivative', fontsize=12)\naxes[1, 1].set_xlabel('x', fontsize=11)\n\n# Tanh\naxes[0, 2].plot(x, tanh(x), linewidth=2, color='green')\naxes[0, 2].set_title('Tanh', fontsize=12)\naxes[1, 2].plot(x, tanh_derivative(x), linewidth=2, color='green')\naxes[1, 2].set_title('Tanh Derivative', fontsize=12)\naxes[1, 2].set_xlabel('x', fontsize=11)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nKey observations:\n\nReLU: \\(f(x) = \\max(0, x)\\) - Zero for negative inputs, identity for positive. Derivative is 0 or 1 (simple!).\nSigmoid: \\(f(x) = \\frac{1}{1+e^{-x}}\\) - Squashes inputs to \\((0, 1)\\). Derivative peaks at 0, vanishes at extremes (gradient vanishing problem).\nTanh: \\(f(x) = \\tanh(x)\\) - Similar to sigmoid but outputs in \\((-1, 1)\\). Zero-centered with stronger gradients than sigmoid."
  },
  {
    "objectID": "ML/activation-functions.html#the-dead-relu-problem-when-neurons-stop-learning",
    "href": "ML/activation-functions.html#the-dead-relu-problem-when-neurons-stop-learning",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "The Dead ReLU Problem: When Neurons Stop Learning",
    "text": "The Dead ReLU Problem: When Neurons Stop Learning\nReLU‚Äôs simplicity is its strength, but also its weakness. A ReLU neuron can ‚Äúdie‚Äù - permanently outputting zero and never learning again.\nWhy does this happen?\nWhen a neuron‚Äôs pre-activation values are consistently negative (due to poor initialization, high learning rate, or bad gradients), ReLU outputs zero. Since the derivative is also zero for negative inputs, no gradient flows backward. The neuron is stuck forever.\n\n\nShow code\n# Generate input data\nx = torch.randn(1000, 10)  # 1000 samples, 10 features\nlinear = nn.Linear(10, 5)   # 5 hidden units\n\n# Set bias to large negative values to \"kill\" neurons\nwith torch.no_grad():\n    linear.bias.fill_(-10.0)\n\n# Forward pass\npre_activation = linear(x)\npost_activation = torch.relu(pre_activation)\n\n# Calculate statistics\ndead_percentage = (post_activation == 0).float().mean() * 100\nprint(f\"Percentage of dead neurons: {dead_percentage:.2f}%\\n\")\n\n# Display table showing ReLU input vs output\nprint(\"ReLU Input vs Output (first 10 samples, neuron 0):\")\nprint(\"-\" * 50)\nprint(f\"{'Sample':&lt;10} {'Pre-Activation':&lt;20} {'Post-Activation':&lt;20}\")\nprint(\"-\" * 50)\n\nfor i in range(10):\n    pre_val = pre_activation[i, 0].item()\n    post_val = post_activation[i, 0].item()\n    print(f\"{i:&lt;10} {pre_val:&lt;20.4f} {post_val:&lt;20.4f}\")\n\nprint(\"\\nObservation: All negative inputs become 0 after ReLU ‚Üí Dead neuron!\")\n\n\nPercentage of dead neurons: 100.00%\n\nReLU Input vs Output (first 10 samples, neuron 0):\n--------------------------------------------------\nSample     Pre-Activation       Post-Activation     \n--------------------------------------------------\n0          -9.7837              0.0000              \n1          -10.0322             0.0000              \n2          -10.4466             0.0000              \n3          -10.3243             0.0000              \n4          -10.5448             0.0000              \n5          -9.7712              0.0000              \n6          -10.8104             0.0000              \n7          -11.3418             0.0000              \n8          -9.8559              0.0000              \n9          -8.6873              0.0000              \n\nObservation: All negative inputs become 0 after ReLU ‚Üí Dead neuron!\n\n\nWith a large negative bias, every input becomes negative after the linear transformation. ReLU zeros them all out. The gradient is zero everywhere. The neuron never updates. It‚Äôs dead."
  },
  {
    "objectID": "ML/activation-functions.html#experiment-do-different-activations-make-a-difference",
    "href": "ML/activation-functions.html#experiment-do-different-activations-make-a-difference",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "Experiment: Do Different Activations Make a Difference?",
    "text": "Experiment: Do Different Activations Make a Difference?\nTheory is nice, but let‚Äôs see activation functions in action. We‚Äôll train three identical networks with different activations on a simple regression task: \\(y = \\sin(x) + x^2 + 1\\).\n\nGenerate Data\n\n\nShow code\n# Training data\nx_train = np.random.rand(200, 1)\ny_train = np.sin(x_train) + np.power(x_train, 2) + 1\n\n# Test data\nx_test = np.random.rand(50, 1)\ny_test = np.sin(x_test) + np.power(x_test, 2) + 1\n\n# Convert to PyTorch tensors\nx_train_tensor = torch.FloatTensor(x_train)\ny_train_tensor = torch.FloatTensor(y_train)\nx_test_tensor = torch.FloatTensor(x_test)\ny_test_tensor = torch.FloatTensor(y_test)\n\n\n\n\nCreate and Train Models\n\n\nShow code\ndef create_regression_model(activation_fn):\n    \"\"\"Create a 2-layer network with specified activation\"\"\"\n    return nn.Sequential(\n        nn.Linear(1, 20),\n        activation_fn,\n        nn.Linear(20, 1)\n    )\n\n# Create 3 models with different activations\nmodels = {\n    'ReLU': create_regression_model(nn.ReLU()),\n    'Sigmoid': create_regression_model(nn.Sigmoid()),\n    'Tanh': create_regression_model(nn.Tanh())\n}\n\n# Training configuration\nn_epochs = 100\nlearning_rate = 0.01\nloss_fn = nn.MSELoss()\n\n# Track metrics\nloss_history = {name: [] for name in models.keys()}\ntest_mse_history = {name: [] for name in models.keys()}\n\n# Train each model\nfor name, model in models.items():\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    for epoch in range(n_epochs):\n        # Training\n        model.train()\n        y_pred = model(x_train_tensor)\n        loss = loss_fn(y_pred, y_train_tensor)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loss_history[name].append(loss.item())\n\n        # Evaluation on test set\n        model.eval()\n        with torch.no_grad():\n            y_test_pred = model(x_test_tensor)\n            test_mse = loss_fn(y_test_pred, y_test_tensor).item()\n            test_mse_history[name].append(test_mse)\n\n\n\n\nCompare Learning Curves\n\n\nShow code\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\ncolors = {'ReLU': 'blue', 'Sigmoid': 'red', 'Tanh': 'green'}\n\n# Plot training loss\nfor name, losses in loss_history.items():\n    axes[0].plot(losses, label=name, linewidth=2, color=colors[name])\n\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Training Loss (MSE)', fontsize=12)\naxes[0].set_title('Training Loss Over Time', fontsize=14)\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\naxes[0].set_yscale('log')\n\n# Plot test MSE\nfor name, test_mse in test_mse_history.items():\n    axes[1].plot(test_mse, label=name, linewidth=2, color=colors[name])\n\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Test Loss (MSE)', fontsize=12)\naxes[1].set_title('Test Loss Over Time', fontsize=14)\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\naxes[1].set_yscale('log')\n\nplt.tight_layout()\nplt.show()\n\n# Print final metrics\nprint(\"\\nFinal Metrics after {} epochs:\".format(n_epochs))\nprint(\"-\" * 60)\nprint(f\"{'Activation':&lt;15} {'Train Loss':&lt;15} {'Test Loss':&lt;15}\")\nprint(\"-\" * 60)\nfor name in models.keys():\n    train_loss = loss_history[name][-1]\n    test_loss = test_mse_history[name][-1]\n    print(f\"{name:&lt;15} {train_loss:&lt;15.6f} {test_loss:&lt;15.6f}\")\n\n\n\n\n\n\n\n\n\n\nFinal Metrics after 100 epochs:\n------------------------------------------------------------\nActivation      Train Loss      Test Loss      \n------------------------------------------------------------\nReLU            0.007420        0.008211       \nSigmoid         0.227441        0.247947       \nTanh            0.035384        0.038743"
  }
]