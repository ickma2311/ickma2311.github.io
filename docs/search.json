[
  {
    "objectID": "ML/k_means_clustering.html",
    "href": "ML/k_means_clustering.html",
    "title": "K-means Clustering",
    "section": "",
    "text": "Setup points and K\nwe will implement a KNN algorithm to cluster the points\n\n\nX=[[1,1],[2,2.1],[3,2.5],[6,7],[7,7.1],[9,7.5]]\nk=2\n\nmax_iter=3\n\n\n# Visualize the data\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter([x[0] for x in X],[x[1] for x in X])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pure python implementation of K-means clustering\ndef knn_iter(X,centroids):\n    # set up new clusters\n    new_clusters=[[] for _ in range(len(centroids))]\n    # k=len(centroids)\n    # assign each point to the nearest centroid\n    for x in X:\n        k,distance=0,(x[0]-centroids[0][0])**2+(x[1]-centroids[0][1])**2\n        for i,c in enumerate(centroids[1:],1):\n            if (x[0]-c[0])**2+(x[1]-c[1])**2&lt;distance:\n                k=i\n                distance=(x[0]-c[0])**2+(x[1]-c[1])**2\n        new_clusters[k].append(x)\n    \n    # calculate new centroids\n    new_centroids=[[\n        sum([x[0] for x in cluster])/len(cluster),\n        sum([x[1] for x in cluster])/len(cluster)\n    ] if cluster else centroids[i] for i,cluster in enumerate(new_clusters)]\n    return new_centroids\n\n\n\n\n\n\n\n\ndef iter_and_draw(X,k,max_iter):\n    centroids=X[:k]  # Randomly select 2 centroids\n    fig, axes = plt.subplots(max_iter//3+(1 if max_iter%3!=0 else 0),\n        3, figsize=(15, 10))\n    axes=axes.flatten()\n    for i in range(max_iter):\n        \n        # Plot points and centroids\n\n\n        # Assign each point to nearest centroid and plot with corresponding color\n        colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n        for j, x in enumerate(X):\n            # Find nearest centroid\n            min_dist = float('inf')\n            nearest_centroid = 0\n            for k, c in enumerate(centroids):\n                dist = (x[0]-c[0])**2 + (x[1]-c[1])**2\n                if dist &lt; min_dist:\n                    min_dist = dist\n                    nearest_centroid = k\n            # Plot point with color corresponding to its cluster\n            axes[i].scatter(x[0], x[1], c=colors[nearest_centroid % len(colors)], label=f'Cluster {nearest_centroid+1}' if j==0 else \"\")\n        axes[i].scatter([c[0] for c in centroids], [c[1] for c in centroids], c='red', marker='*', s=200, label='Centroids')\n        axes[i].set_title(f'Iteration {i}')\n        centroids = knn_iter(X, centroids)\n\n    plt.tight_layout()\n    plt.show()\n\niter_and_draw(X,k,max_iter)\n# print(centroids)\n\n\n\n\n\n\n\n\n\n# A 3 clusters example\n\nimport numpy as np\n\nX1=np.random.rand(20,2)+5 # Some points in the upper right corner\nX2=np.random.rand(20,2)+3 # Some points in the middle\nX3=np.random.rand(20,2) # Some points in the lower left corner\n\niter_and_draw(np.concatenate((X1,X2,X3)),3,5)\n\n\n\n\n\n\n\n\n\n\nA question?\n\nWhat to do if one cluster has no assigned points during iteration?\n\n\n\nFormula Derivation\nThe goal is to minimize the loss of inertia which is sum of the points to cluster centroids.\n\\[\nLoss= \\sum_{i=1}^n \\sum_{x \\in C_i} ||x-\\mu_i||^2\n\\]\nTo iter \\(\\mu\\) for each cluster, let us find the derivative of the following function. \\[\nf(\\mu)=\\sum_{i=1}^n ||x_i-\\mu||^2 =\n\\sum_{i=1}^n {x_i}^2+\\mu^2-2x_i\\mu\n\\]\nGiven a \\(\\nabla \\mu\\), \\[\nf(\\mu + \\nabla \\mu)=\\sum_{i=1}^n ||x_i+\\nabla \\mu -\\mu||^2 =\n\\sum_{i=1}^n  {x_i}^2+\\mu^2+{\\nabla \\mu}^2-2{x_i \\mu}-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\nf(\\mu + \\nabla \\mu)-f(\\mu)=\n\\sum_{i=1}^n {\\nabla \\mu}^2-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\n\\frac {f(\\mu + \\nabla \\mu)-f(\\mu)}{\\nabla \\mu}=\\sum_{i=1}^n {\\nabla \\mu} -2 \\mu +2{x_i} = 2\\sum_{i=1}^n x_i - 2n\\mu\n\\]\nNow we can see if \\(n\\mu = \\sum_{i=1}^n x_i\\), then the derivative is 0, this is why in each iteration, we need to set the center of the cluster as centroid."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ickma.dev",
    "section": "",
    "text": "A growing collection of structured study notes and visual explanations ‚Äî written for clarity, reproducibility, and long-term memory."
  },
  {
    "objectID": "index.html#latest-updates",
    "href": "index.html#latest-updates",
    "title": "ickma.dev",
    "section": "Latest Updates",
    "text": "Latest Updates\n\n‚àá Deep Learning Book 45 chapters\nMy notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\nChapter 10.7: The Challenge of Long-Term Dependencies The fundamental challenge of long-term dependencies in RNNs is training difficulty: gradients propagated across many time steps either vanish exponentially (common) or explode (rare but severe). Eigenvalue analysis shows how powers of the transition matrix govern this instability.\n\n\nChapter 10.6: Recursive Neural Network Recursive neural networks compute over tree structures rather than linear chains, applying shared composition functions at internal nodes to build hierarchical representations bottom-up. This reduces computation depth from O(œÑ) to O(log œÑ), but requires external tree structure specification.\n\n\nChapter 10.5: Deep Recurrent Networks Three architectural patterns for adding depth to RNNs: hierarchical hidden states (vertical stacking), deep transition RNNs (MLPs replace transformations), and deep transition with skip connections (residual paths for gradient flow).\n\n\nChapter 10.4: Encoder-Decoder Sequence-to-Sequence Architecture The seq2seq architecture handles variable-length input and output sequences by compressing the input into a fixed context vector C, then decoding it step-by-step. This enables machine translation, summarization, and dialogue generation where input and output lengths differ.\n\n\n\nSee all Deep Learning chapters ‚Üí\n\n\n\nüìê MIT 18.06SC Linear Algebra 36 lectures\nMy journey through MIT‚Äôs Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\n\nSee all MIT 18.06SC lectures ‚Üí\n\n\n\nüìê MIT 18.065: Linear Algebra Applications 2 lectures\nMy notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning‚Äîexploring how linear algebra powers modern applications.\n\n\nLecture 9: Four Ways to Solve Least Squares Problems Four equivalent methods for solving \\(Ax = b\\) when \\(A\\) has no inverse: pseudo-inverse, normal equations, algebraic minimization, and geometric projection‚Äîall converging to the same optimal solution.\n\n\nLecture 8: Norms of Vectors and Matrices Understanding vector p-norms (\\(\\|v\\|_p\\)) and when they satisfy the triangle inequality (only for \\(p \\geq 1\\)). The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù creates non-convex unit balls for strong sparsity.\n\n\n\nSee all MIT 18.065 lectures ‚Üí\n\n\n\nüìê Stanford EE 364A: Convex Optimization 6 lectures\nMy notes from Stanford EE 364A: Convex Optimization‚Äîtheory and applications of optimization problems.\n\n\nLecture 5.1: Log-Concave and Log-Convex Functions Log-concave functions satisfy \\(f(\\theta x+(1-\\theta)y)\\ge f(x)^\\theta f(y)^{1-\\theta}\\). Powers \\(x^a\\) are log-concave for \\(a \\ge 0\\) and log-convex for \\(a \\le 0\\). Key properties: products preserve log-concavity, but sums do not. Integration of log-concave functions preserves log-concavity.\n\n\nLecture 4 Part 2: Conjugate and Quasiconvex Functions Conjugate functions \\(f^*(y) = \\sup_x (y^\\top x - f(x))\\) are always convex, with examples including negative logarithm and quadratic functions. Quasiconvex functions have convex sublevel sets with modified Jensen inequality \\(f(\\theta x + (1-\\theta)y) \\leq \\max\\{f(x), f(y)\\}\\). Examples include linear-fractional functions and distance ratios.\n\n\nLecture 4 Part 1: Operations preserving Convexity Pointwise maximum and supremum (support function, distance to farthest point, maximum eigenvalue), composition with scalar functions (exponential, reciprocal), vector composition (log-sum-exp), minimization over convex sets (Schur complement, distance to set), and perspective functions with examples.\n\n\nLecture 3: Convex Functions Separating and supporting hyperplane theorems, dual cones, convex function definition and examples, first-order condition (tangent underestimates), second-order condition (Hessian PSD), epigraph and sublevel sets, Jensen‚Äôs inequality, and operations preserving convexity.\n\n\n\nSee all EE 364A lectures ‚Üí"
  },
  {
    "objectID": "index.html#more-topics",
    "href": "index.html#more-topics",
    "title": "ickma.dev",
    "section": "More Topics",
    "text": "More Topics\n\n\nMachine Learning\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\nAlgorithms\n\nDP Regex"
  },
  {
    "objectID": "ML/rnn-unfold-computation-graph.html",
    "href": "ML/rnn-unfold-computation-graph.html",
    "title": "Chapter 10.1: Unfold Computation Graph",
    "section": "",
    "text": "Recurrent Model (RNN) is specified for sequence data processing. While CNN is specified for grid data (images), RNN is designed to process sequences \\((x^{(1)},x^{(2)},...,x^{(t)})\\)."
  },
  {
    "objectID": "ML/rnn-unfold-computation-graph.html#parameter-sharing-through-unfolding",
    "href": "ML/rnn-unfold-computation-graph.html#parameter-sharing-through-unfolding",
    "title": "Chapter 10.1: Unfold Computation Graph",
    "section": "Parameter Sharing Through Unfolding",
    "text": "Parameter Sharing Through Unfolding\nUnfolding the computation graph results in the parameters to be shared.\n\\[\ns^{(t)}=f(s^{(t-1)};\\theta) \\tag{10.1}\n\\]\nwhere \\(s^{(t)}\\) is the system state.\nThe definition of \\(s\\) at time \\(t\\) depends on the moment of \\(t-1\\), so Equation 10.1 is recurrent.\n\\[\ns^{(3)}=f(s^{(2)};\\theta) \\tag{10.2}\n\\]\n\\[\ns^{(3)}=f(f(s^{(1)};\\theta);\\theta) \\tag{10.3}\n\\]\n\n\n\nRNN with output at each timestep"
  },
  {
    "objectID": "ML/rnn-unfold-computation-graph.html#dynamic-systems-with-external-input",
    "href": "ML/rnn-unfold-computation-graph.html#dynamic-systems-with-external-input",
    "title": "Chapter 10.1: Unfold Computation Graph",
    "section": "Dynamic Systems with External Input",
    "text": "Dynamic Systems with External Input\nAnother example is a dynamic system driven by external signal \\(x^{(t)}\\):\n\\[\ns^{(t)}=f(s^{(t-1)}, x^{(t)};\\theta) \\tag{10.4}\n\\]\nA real case: Consider the sentence ‚ÄúI love deep learning‚Äù. When \\(t=3\\):\n\n\\(s^{(t-1)}\\) is the memory of ‚ÄúI love‚Äù\n\\(x^{(t)}\\) is the embedding of ‚Äúdeep‚Äù"
  },
  {
    "objectID": "ML/rnn-unfold-computation-graph.html#hidden-states",
    "href": "ML/rnn-unfold-computation-graph.html#hidden-states",
    "title": "Chapter 10.1: Unfold Computation Graph",
    "section": "Hidden States",
    "text": "Hidden States\nIn deep learning, we usually name \\(s^{(t)}, s^{(t-1)}\\) as hidden units:\n\\[\nh^{(t)}=f(h^{(t-1)}, x^{(t)};\\theta) \\tag{10.5}\n\\]\n\n\n\nRNN unfolded computation graph\n\n\nKey insight: When training an RNN to use past information to predict the future, the network must learn a compressed, lossy summary of the history. It is unnecessary‚Äîand usually impossible‚Äîto store the entire past sequence. Instead, the hidden state learns to retain only the task-relevant information needed for future predictions."
  },
  {
    "objectID": "ML/rnn-unfold-computation-graph.html#the-function-gt",
    "href": "ML/rnn-unfold-computation-graph.html#the-function-gt",
    "title": "Chapter 10.1: Unfold Computation Graph",
    "section": "The Function \\(g^{(t)}\\)",
    "text": "The Function \\(g^{(t)}\\)\n\\(g^{(t)}\\) is the composed transformation of the past \\(t\\) steps, not the loop itself:\n\\[\nh^{(t)}=g^{(t)}(x^{(t)},x^{(t-1)},x^{(t-2)},...,x^{(2)},x^{(1)}) \\tag{10.6}\n\\]\nThis is equivalent to Equation 10.5.\nThe function \\(g^{(t)}\\) takes the entire history of inputs \\((x^{(t)}, x^{(t-1)}, \\ldots, x^{(1)})\\) as its argument.\nThe unrolled recurrent architecture allows us to express \\(g^{(t)}\\) as a repeated composition of the same transition function \\(f\\).\n\nTwo Key Advantages\nThis formulation offers two key advantages:\n\nArbitrary input length: It allows inputs of arbitrary length to be mapped to a fixed-size hidden state\nParameter sharing: It enables parameter sharing, since the same transition function \\(f\\) with the same parameters is reused at every time step\n\nRNN models can also be generalized to unseen input lengths.\n\n\n\nDifferent RNN architectures"
  },
  {
    "objectID": "ML/rnn-unfold-computation-graph.html#key-insight",
    "href": "ML/rnn-unfold-computation-graph.html#key-insight",
    "title": "Chapter 10.1: Unfold Computation Graph",
    "section": "Key Insight",
    "text": "Key Insight\nUnfolding computation graphs in RNNs enables parameter sharing across time steps. The same function \\(f\\) with parameters \\(\\theta\\) is applied repeatedly, allowing the model to process sequences of any length while maintaining a fixed number of parameters. The hidden state \\(h^{(t)}\\) compresses the entire input history into a fixed-size representation, learning to retain only task-relevant information. This architecture generalizes naturally to unseen sequence lengths and enables three fundamental patterns: sequence-to-sequence (many-to-many), sequence-to-vector (many-to-one), and vector-to-sequence (one-to-many) mappings."
  },
  {
    "objectID": "ML/rnn-recursive.html",
    "href": "ML/rnn-recursive.html",
    "title": "Chapter 10.6: Recursive Neural Network",
    "section": "",
    "text": "Recursive neural networks (RvNNs) are an extension of recurrent neural networks in which the computation is carried out over a tree structure rather than a linear chain.\nInstead of applying the same transition function along time, RvNNs apply a shared composition function at every internal node of a tree, combining the representations of its children into a representation of the parent."
  },
  {
    "objectID": "ML/rnn-recursive.html#basic-idea-as-described-in-the-book",
    "href": "ML/rnn-recursive.html#basic-idea-as-described-in-the-book",
    "title": "Chapter 10.6: Recursive Neural Network",
    "section": "1. Basic Idea (as described in the book)",
    "text": "1. Basic Idea (as described in the book)\nAn RvNN performs bottom-up computation on a fixed tree:\n\\[h_{\\text{parent}} = f_{\\theta}(h_{c_1}, h_{c_2}, \\ldots)\\]\nLeaf nodes obtain their representation directly from the input, and internal nodes recursively build higher-level representations. Figure 10.14 in the book illustrates this process with shared parameters (U, W, V) applied at different nodes of the tree."
  },
  {
    "objectID": "ML/rnn-recursive.html#origins-and-applications-based-on-book-citations",
    "href": "ML/rnn-recursive.html#origins-and-applications-based-on-book-citations",
    "title": "Chapter 10.6: Recursive Neural Network",
    "section": "2. Origins and Applications (based on book citations)",
    "text": "2. Origins and Applications (based on book citations)\nThe book notes that:\n\nRecursive networks were introduced by Pollack (1990).\nBottou (2011) discussed their potential for general learning and inference tasks.\nFrasconi et al.¬†(1997, 1998) applied recursive networks to data given as trees.\nSocher et al.¬†(2011a, 2011c) successfully used RvNNs to process syntactic trees in natural language.\nRecursive models have also been applied to computational vision (Socher et al., 2011b).\n\nThese examples highlight that RvNNs are suitable for tasks where the input naturally exhibits a hierarchical tree structure."
  },
  {
    "objectID": "ML/rnn-recursive.html#key-advantage-explicitly-stated-in-the-book",
    "href": "ML/rnn-recursive.html#key-advantage-explicitly-stated-in-the-book",
    "title": "Chapter 10.6: Recursive Neural Network",
    "section": "3. Key Advantage (explicitly stated in the book)",
    "text": "3. Key Advantage (explicitly stated in the book)\nFor sequences of length œÑ, the depth of computation in an RvNN can be drastically reduced:\n\nRNN chain ‚Üí depth = œÑ\nRecursive tree (e.g., balanced binary tree) ‚Üí depth = O(log œÑ)\n\nThus, when a hierarchical structure is available, recursive networks can provide shorter effective paths between distant elements, potentially helping with long-term dependencies."
  },
  {
    "objectID": "ML/rnn-recursive.html#a-major-challenge-as-the-book-highlights",
    "href": "ML/rnn-recursive.html#a-major-challenge-as-the-book-highlights",
    "title": "Chapter 10.6: Recursive Neural Network",
    "section": "4. A Major Challenge (as the book highlights)",
    "text": "4. A Major Challenge (as the book highlights)\nA critical difficulty remains:\n\nIn many applications, the tree structure is not given\n\nThe model must rely on an external source (e.g., a syntactic parser) to provide the structure, or one must design a suitable hierarchy manually.\nThe book notes that an ideal learning system would automatically discover the appropriate tree structure, but this remains an unsolved problem."
  },
  {
    "objectID": "ML/representation-sparsity.html",
    "href": "ML/representation-sparsity.html",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "",
    "text": "Previous chapters focused on parameter regularization ‚Äî constraining the weights \\(W\\) of a model (see Chapter 7.1.2: L1 Regularization and Chapter 7.1.1: L2 Regularization).\nThis chapter introduces representation regularization ‚Äî constraining the activations \\(h\\) (the learned representations).\nKey distinction:\n\nParameter sparsity: Makes the weight matrix sparse (few connections)\nRepresentation sparsity: Makes the activation vector sparse (few active neurons)"
  },
  {
    "objectID": "ML/representation-sparsity.html#overview",
    "href": "ML/representation-sparsity.html#overview",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "",
    "text": "Previous chapters focused on parameter regularization ‚Äî constraining the weights \\(W\\) of a model (see Chapter 7.1.2: L1 Regularization and Chapter 7.1.1: L2 Regularization).\nThis chapter introduces representation regularization ‚Äî constraining the activations \\(h\\) (the learned representations).\nKey distinction:\n\nParameter sparsity: Makes the weight matrix sparse (few connections)\nRepresentation sparsity: Makes the activation vector sparse (few active neurons)"
  },
  {
    "objectID": "ML/representation-sparsity.html#parameter-regularization",
    "href": "ML/representation-sparsity.html#parameter-regularization",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "Parameter Regularization",
    "text": "Parameter Regularization\nEquation 7.46 - Linear system with parameter matrix:\n\\[\n\\begin{array}{c}\n\\begin{bmatrix}\n18\\\\\n5\\\\\n15\\\\\n-9\\\\\n-3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n4 & 0 & 0 & -2 & 0\\\\\n0 & -1 & 0 & 3 & 0\\\\\n1 & 0 & 0 & 0 & 3\\\\\n0 & 5 & 0 & -1 & 0\\\\\n1 & 0 & 0 & -5 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n2\\\\\n3\\\\\n-2\\\\\n-5\\\\\n1\n\\end{bmatrix}\n\\\\[6pt]\ny \\in \\mathbb{R}^m,\\quad\nA \\in \\mathbb{R}^{m\\times n},\\quad\nx \\in \\mathbb{R}^n\n\\end{array}\n\\]\nL1 regularization on parameters:\n\\[\n\\Omega(A) = \\|A\\|_1 = \\sum_{i,j} |A_{ij}|\n\\]\nEffect: Encourages sparsity in the parameter matrix \\(A\\) itself, making many weights zero."
  },
  {
    "objectID": "ML/representation-sparsity.html#representation-sparsity",
    "href": "ML/representation-sparsity.html#representation-sparsity",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "Representation Sparsity",
    "text": "Representation Sparsity\nEquation 7.47 - Linear system with sparse representation:\n\\[\n\\begin{array}{c}\n\\begin{bmatrix}\n-14\\\\\n1\\\\\n3\\\\\n2\\\\\n23\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n3 & -1 & 2 & -5 & 4 & 1\\\\\n4 & -2 & -3 & -1 & 3 & 0\\\\\n1 & 5 & 2 & -4 & 0 & 0\\\\\n3 & 4 & -3 & 0 & 2 & 0\\\\\n-5 & -4 & 2 & 5 & -1 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\n0\\\\\n0\\\\\n1\\\\\n0\\\\\n0\\\\\n0\n\\end{bmatrix}\n\\\\[6pt]\ny \\in \\mathbb{R}^m, \\quad\nB \\in \\mathbb{R}^{m\\times n}, \\quad\nh \\in \\mathbb{R}^n\n\\end{array}\n\\]\nL1 regularization on representation:\n\\[\n\\Omega(h) = \\|h\\|_1 = \\sum_{i} |h_i|\n\\]\nEquation 7.48 - Loss function with representation regularization:\n\\[\n\\tilde{J}(\\theta; x, y) = J(\\theta; x, y) + \\alpha \\Omega(h)\n\\]\nwhere \\(\\alpha \\in [0, \\infty)\\) controls the contribution of the norm penalty to the total loss.\n\nKey Difference: Parameter vs Representation Regularization\n\n\n\n\n\n\n\n\n\nType\nWhat is Regularized\nEffect\nExample\n\n\n\n\nParameter regularization\nWeight matrix \\(A\\) or \\(W\\)\nMakes weights sparse\nL1/L2 weight decay\n\n\nRepresentation regularization\nActivation vector \\(h\\)\nMakes activations sparse\nSparse autoencoders\n\n\n\nInterpretation:\n\nParameter sparsity: Few connections between layers (network pruning)\nRepresentation sparsity: Few neurons active at once (sparse coding)"
  },
  {
    "objectID": "ML/representation-sparsity.html#orthogonal-matching-pursuit",
    "href": "ML/representation-sparsity.html#orthogonal-matching-pursuit",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "Orthogonal Matching Pursuit",
    "text": "Orthogonal Matching Pursuit\nEquation 7.49 - Sparse approximation problem:\n\\[\n\\arg\\min_h \\|x - Wh\\|^2 \\quad \\text{subject to} \\quad \\|h\\|_0 &lt; k\n\\]\nGoal: Find both \\(W\\) and \\(h\\) such that:\n\n\\(Wh\\) closely approximates \\(x\\) (reconstruction accuracy)\n\\(h\\) is sparse ‚Äî the number of nonzero elements in \\(h\\) (denoted \\(\\|h\\|_0\\)) is less than \\(k\\)\n\nInterpretation: Use \\(Wh\\) to represent \\(x\\) with only a few active components.\n\n\n\n\n\n\nNoteOrthogonal Matching Pursuit\n\n\n\nWhen the columns of \\(W\\) are orthogonal, this problem can be efficiently solved using orthogonal matching pursuit (OMP) algorithm.\nKey insight: Orthogonality of basis vectors (columns of \\(W\\)) enables efficient sparse decomposition."
  },
  {
    "objectID": "ML/representation-sparsity.html#real-world-applications",
    "href": "ML/representation-sparsity.html#real-world-applications",
    "title": "Chapter 7.10: Sparse Representations",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nNote: The following table is generated by ChatGPT.\n\n\n\n\n\n\n\n\n\n\nCategory\nTechnique / Model\nRepresentation Type\nHow it Uses Sparsity / Orthogonality\nReal-World Example / Effect\n\n\n\n\nüß† Neuroscience & Vision\nSparse coding (Olshausen & Field, 1996)\nSparse\nThe brain represents visual inputs by activating only a few neurons for each stimulus.\nExplains receptive fields in V1 visual cortex ‚Äî neurons respond only to specific edges or orientations.\n\n\nüßÆ Machine Learning\nLASSO Regression\nSparse\nAdds \\(\\lambda \\|w\\|_1\\) penalty to make weights sparse, selecting only key features.\nFeature selection in predictive models (finance, genomics, etc.).\n\n\nüß© Autoencoders\nSparse Autoencoder\nSparse\nAdds regularizer \\(\\Omega(h) = \\|h\\|_1\\) or KL divergence to force sparse activations.\nUsed in image compression and pretraining deep networks (unsupervised learning).\n\n\nüñºÔ∏è Image Processing / Compression\nDictionary Learning / K-SVD\nSparse\nRepresent an image as a combination of few learned basis patches (atoms).\nJPEG-like compression, denoising, super-resolution.\n\n\nüîä Speech Processing\nNon-negative Matrix Factorization (NMF)\nSparse + Nonnegative\nDecomposes sound spectrograms into few additive components.\nSource separation (e.g., separating voice from music).\n\n\nüß† Transform-based Compression\nDCT (Discrete Cosine Transform)\nOrthogonal\nProjects signals onto orthogonal cosine basis vectors.\nJPEG compression ‚Äî energy concentrated in few coefficients.\n\n\nüì∂ Signal Processing\nFourier Transform\nOrthogonal\nRepresents time-domain signals in orthogonal sinusoidal basis.\nAudio, RF, vibration analysis.\n\n\nüñ•Ô∏è Dimensionality Reduction\nPrincipal Component Analysis (PCA)\nOrthogonal\nFinds orthogonal axes (principal components) that maximize variance.\nDimensionality reduction, data visualization.\n\n\nüéôÔ∏è Source Separation\nIndependent Component Analysis (ICA)\nSparse-like / Orthogonal\nSeeks statistically independent (often sparse) components.\nBlind source separation (‚Äúcocktail party problem‚Äù).\n\n\nü§ñ CNN Filters\nOrthogonal Regularization\nOrthogonal\nForces convolution filters to be orthogonal to prevent redundancy.\nImproves training stability and generalization.\n\n\nüíæ Transformers\nEmbedding Orthogonalization\nOrthogonal or Near-orthogonal\nOrthogonalizes embedding vectors for better separation in latent space.\nImproves language model expressiveness and prevents collapse.\n\n\n\n\nSource: Deep Learning Book, Chapter 7.10"
  },
  {
    "objectID": "ML/tangent-prop-manifold.html",
    "href": "ML/tangent-prop-manifold.html",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "",
    "text": "The assumption is that samples lie on a low-dimensional manifold embedded in a high-dimensional space.\nIf we measure their distance using the Euclidean metric, the points might appear far apart, even though they actually reside on the same manifold."
  },
  {
    "objectID": "ML/tangent-prop-manifold.html#the-manifold-assumption",
    "href": "ML/tangent-prop-manifold.html#the-manifold-assumption",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "",
    "text": "The assumption is that samples lie on a low-dimensional manifold embedded in a high-dimensional space.\nIf we measure their distance using the Euclidean metric, the points might appear far apart, even though they actually reside on the same manifold."
  },
  {
    "objectID": "ML/tangent-prop-manifold.html#tangent-distance-difficulties-and-alternative",
    "href": "ML/tangent-prop-manifold.html#tangent-distance-difficulties-and-alternative",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "Tangent Distance: Difficulties and Alternative",
    "text": "Tangent Distance: Difficulties and Alternative\n\nThe Challenge\nComputing tangent distances directly is computationally expensive. For every pair of samples, it requires solving an optimization problem to find the minimal distance between two tangent planes that approximate their local manifolds. This becomes infeasible with large datasets or high-dimensional input spaces.\n\n\nThe Alternative\nAs an alternative, the method approximates the manifold locally using the tangent plane at a single point. Instead of explicitly finding the closest points between two manifolds, we measure distances using these local linear approximations. This greatly reduces computational cost while still capturing local invariance properties."
  },
  {
    "objectID": "ML/tangent-prop-manifold.html#tangent-prop",
    "href": "ML/tangent-prop-manifold.html#tangent-prop",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "Tangent Prop",
    "text": "Tangent Prop\nThe figure illustrates how Tangent Propagation enforces local invariance of the model output along the tangent directions of the data manifold.\nEach data point lies on a smooth low-dimensional surface embedded in high-dimensional space. The tangent plane represents all directions along which the data can move without changing its semantic meaning (e.g., small translation or rotation).\nThe normal vector points in the direction orthogonal to the manifold ‚Äî changes along this direction correspond to changes in class or semantic meaning.\n\n\n\nTangent Vector and Normal Vector\n\n\nThis approach achieves partial consistency by forcing \\(\\nabla_xf(x) \\perp v^{(i)}\\).\n\nRegularization Term\n\\[\n\\Omega(f)=\\sum_i\\left((\\nabla_xf(x)^\\top v^{(i)})^2\\right)\n\\]\nThis regularization term penalizes the sensitivity of the network‚Äôs output \\(f(x)\\) to small movements along each tangent direction \\(v^{(i)}\\).\nMinimizing this term encourages the model‚Äôs gradient \\(\\nabla_x f(x)\\) to be orthogonal to all tangent vectors, ensuring that \\(f(x)\\) remains approximately constant when the input slides along the manifold.\nIn short: Tangent Propagation achieves local smoothness along the manifold (invariance to small deformations), while still allowing sharp variation in directions orthogonal to the manifold, which separate different classes.\n\n\nTangent Direction Vectors\nIn Tangent Propagation, each \\(v^{(i)}\\) represents a tangent direction of the data manifold at the point \\(x\\).\nIt describes a small, meaningful variation of the input that should not change the output of the network‚Äîfor example, a slight translation, rotation, or scaling of an image.\nMathematically, \\(v^{(i)}\\) can be obtained as the derivative of a transformation \\(T(x, \\alpha_i)\\) with respect to its parameter:\n\\[\nv^{(i)} = \\left.\\frac{\\partial T(x, \\alpha_i)}{\\partial \\alpha_i}\\right|_{\\alpha_i=0}\n\\]\nDuring training, the model is penalized if its output changes along these directions, which enforces invariance and smoothness of \\(f(x)\\) along the manifold."
  },
  {
    "objectID": "ML/tangent-prop-manifold.html#from-tangent-propagation-to-manifold-learning",
    "href": "ML/tangent-prop-manifold.html#from-tangent-propagation-to-manifold-learning",
    "title": "Deep Learning Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier",
    "section": "From Tangent Propagation to Manifold Learning",
    "text": "From Tangent Propagation to Manifold Learning\nTangent Propagation regularizes the model so that its output remains invariant along directions of known transformations, such as translations or rotations.\nIt can be seen as an analytical version of data augmentation‚Äîrather than generating new samples, it directly penalizes the model‚Äôs sensitivity to those transformations.\n\nConnections to Other Methods\nThis idea connects to bidirectional propagation and adversarial training, both of which encourage the model to stay locally smooth in input space.\nAdversarial training extends Tangent Propagation by finding, for each input, the direction that most changes the model‚Äôs prediction and then enforcing robustness along that direction.\nThe Manifold Tangent Classifier (Rifai et al., 2011d) further removes the need to explicitly specify tangent directions.\nIt uses an autoencoder to learn the manifold structure and derive tangent vectors automatically, allowing the network to regularize itself along the data manifold without handcrafted transformations.\n\n\nManual Specification of Tangent Directions\nA key limitation of Tangent Propagation is that these tangent directions \\(v^{(i)}\\) must be manually specified based on prior knowledge about the task.\nFor example, in handwritten digit recognition, we explicitly define directions corresponding to translation or rotation.\nWhile this makes the method interpretable, it also limits its applicability‚Äîmanual definitions are impractical for complex, high-dimensional data.\n\nSource: Deep Learning (Ian Goodfellow, Yoshua Bengio, Aaron Courville) - Chapter 7.14"
  },
  {
    "objectID": "ML/early-stopping.html",
    "href": "ML/early-stopping.html",
    "title": "Chapter 7.8: Early Stopping",
    "section": "",
    "text": "Early stopping is a simple yet effective regularization technique that stops training when validation performance begins to degrade."
  },
  {
    "objectID": "ML/early-stopping.html#overview",
    "href": "ML/early-stopping.html#overview",
    "title": "Chapter 7.8: Early Stopping",
    "section": "",
    "text": "Early stopping is a simple yet effective regularization technique that stops training when validation performance begins to degrade."
  },
  {
    "objectID": "ML/early-stopping.html#the-overfitting-problem",
    "href": "ML/early-stopping.html#the-overfitting-problem",
    "title": "Chapter 7.8: Early Stopping",
    "section": "1. The Overfitting Problem",
    "text": "1. The Overfitting Problem\n\n\n\nOverfitting Visualization\n\n\nObservation: Overfitting almost always occurs during training.\nWhat happens:\n\nTraining error continues to decrease\nValidation error initially decreases, then starts increasing\nThe gap between training and validation error grows\nModel memorizes training data instead of learning generalizable patterns\n\nSolution: Stop training when validation error reaches its minimum."
  },
  {
    "objectID": "ML/early-stopping.html#two-approaches-to-early-stopping",
    "href": "ML/early-stopping.html#two-approaches-to-early-stopping",
    "title": "Chapter 7.8: Early Stopping",
    "section": "2. Two Approaches to Early Stopping",
    "text": "2. Two Approaches to Early Stopping\nSplit data into training \\((x, y)\\) and validation \\((x_{\\text{valid}}, y_{\\text{valid}})\\):\n\nApproach 1: Find Optimal Steps, Then Retrain\n\nTrain on \\((x, y)\\) while monitoring \\(y_{\\text{valid}}\\)\nIdentify the optimal number of steps \\(\\tau^*\\) where validation error is minimized\nRetrain from scratch on full dataset for exactly \\(\\tau^*\\) steps\n\nAdvantage: Uses all data for final training\nDisadvantage: Requires two full training runs\n\n\nApproach 2: Keep Best Model\n\nTrain on \\((x, y)\\) while monitoring \\(y_{\\text{valid}}\\)\nKeep a copy of the model whenever validation performance improves\nStop when validation error stops improving\nUse the saved best model\n\nAdvantage: Only requires one training run\nDisadvantage: Validation data is not used for training\n\n\n\nEarly Stopping Approaches"
  },
  {
    "objectID": "ML/early-stopping.html#costs-of-early-stopping",
    "href": "ML/early-stopping.html#costs-of-early-stopping",
    "title": "Chapter 7.8: Early Stopping",
    "section": "3. Costs of Early Stopping",
    "text": "3. Costs of Early Stopping\nData requirements:\n\nNeed to hold out a portion of data as validation set\nReduces effective training data size\nValidation set typically 10-20% of total data\n\nComputational requirements:\n\nNeed to keep a copy of the best-performing model\nRequires periodic evaluation on validation set\nMay need multiple checkpoints if using approach 1"
  },
  {
    "objectID": "ML/early-stopping.html#benefits-of-early-stopping",
    "href": "ML/early-stopping.html#benefits-of-early-stopping",
    "title": "Chapter 7.8: Early Stopping",
    "section": "4. Benefits of Early Stopping",
    "text": "4. Benefits of Early Stopping\nRegularization:\n\nPrevents overfitting without modifying the loss function\nActs as implicit L2 regularization (proven mathematically below)\nNo hyperparameter tuning needed for regularization strength\n\nEfficiency:\n\nSaves computation by stopping early\nAutomatic hyperparameter selection (number of iterations)\nOften faster than training with explicit regularization to convergence\n\nSimplicity:\n\nEasy to implement\nWidely applicable across different model types\nWorks well in practice without fine-tuning"
  },
  {
    "objectID": "ML/early-stopping.html#mathematical-connection-to-l2-regularization",
    "href": "ML/early-stopping.html#mathematical-connection-to-l2-regularization",
    "title": "Chapter 7.8: Early Stopping",
    "section": "5. Mathematical Connection to L2 Regularization",
    "text": "5. Mathematical Connection to L2 Regularization\n\nGradient Descent Update Rule\nStarting from a quadratic approximation around the optimal weights \\(w^*\\):\nEquation 7.33 - Quadratic approximation of loss:\n\\[\n\\hat{J}(\\theta) = J(w^*) + \\frac{1}{2}(w - w^*)^T H(w - w^*)\n\\]\nwhere \\(H\\) is the Hessian matrix at \\(w^*\\).\nEquation 7.34 - Gradient:\n\\[\n\\nabla_w \\hat{J}(w) = H(w - w^*)\n\\]\nEquation 7.35 - Gradient descent update:\n\\[\nw^{(\\tau)} = w^{(\\tau-1)} - \\epsilon \\nabla_w \\hat{J}(w^{(\\tau-1)})\n\\]\nEquation 7.36 - Substituting the gradient:\n\\[\nw^{(\\tau)} = w^{(\\tau-1)} - \\epsilon H(w^{(\\tau-1)} - w^*)\n\\]\nEquation 7.37 - Rearranging:\n\\[\nw^{(\\tau)} - w^* = (I - \\epsilon H)(w^{(\\tau-1)} - w^*)\n\\]\n\n\nEigendecomposition of Hessian\nEquation 7.38 - Decompose \\(H = Q\\Lambda Q^T\\):\n\\[\nw^{(\\tau)} - w^* = (I - \\epsilon Q\\Lambda Q^T)(w^{(\\tau-1)} - w^*)\n\\]\nEquation 7.39 - Multiply both sides by \\(Q^T\\):\n\\[\nQ^T(w^{(\\tau)} - w^*) = (I - \\epsilon \\Lambda)Q^T(w^{(\\tau-1)} - w^*)\n\\]\n\n\nDeriving the Recursive Formula\nSince \\((I - \\epsilon\\Lambda)\\) is a constant diagonal matrix, we can apply this recursion:\nStep 1 - After 1 iteration:\n\\[\nQ^T(w^{(1)} - w^*) = (I - \\epsilon \\Lambda)Q^T(w^{(0)} - w^*)\n\\]\nStep 2 - After 2 iterations:\n\\[\n\\begin{aligned}\nQ^T(w^{(2)} - w^*) &= (I - \\epsilon \\Lambda)Q^T(w^{(1)} - w^*) \\\\\n&= (I - \\epsilon \\Lambda)Q^T(I - \\epsilon \\Lambda)(w^{(0)} - w^*) \\\\\n&= (I - \\epsilon \\Lambda)^2 Q^T(w^{(0)} - w^*)\n\\end{aligned}\n\\]\nStep 3 - After 3 iterations:\n\\[\n\\begin{aligned}\nQ^T(w^{(3)} - w^*) &= (I - \\epsilon \\Lambda)Q^T(w^{(2)} - w^*) \\\\\n&= (I - \\epsilon \\Lambda)Q^T(I - \\epsilon \\Lambda)^2(w^{(0)} - w^*) \\\\\n&= (I - \\epsilon \\Lambda)^3 Q^T(w^{(0)} - w^*)\n\\end{aligned}\n\\]\nGeneral pattern - After \\(\\tau\\) iterations:\n\\[\nQ^T(w^{(\\tau)} - w^*) = (I - \\epsilon \\Lambda)^\\tau Q^T(w^{(0)} - w^*)\n\\]\n\n\nAssuming Zero Initialization\nEquation 7.40 - If \\(w^{(0)} = 0\\):\n\\[\n\\begin{aligned}\nQ^T(w^{(\\tau)} - w^*) &= (I - \\epsilon \\Lambda)^\\tau Q^T(0 - w^*) \\\\\n&= -(I - \\epsilon \\Lambda)^\\tau Q^T w^*\n\\end{aligned}\n\\]\nTherefore:\n\\[\nQ^T w^{(\\tau)} = Q^T w^* - (I - \\epsilon \\Lambda)^\\tau Q^T w^*\n\\]\n\\[\nQ^T w^{(\\tau)} = [I - (I - \\epsilon \\Lambda)^\\tau] Q^T w^*\n\\]\n\n\nL2 Regularization Solution\nEquation 7.41 - L2 regularized solution:\n\\[\nQ^T \\tilde{w} = (\\Lambda + \\alpha I)^{-1} \\Lambda Q^T w^*\n\\]\nEquation 7.42 - Rewriting:\n\\[\nQ^T \\tilde{w} = [I - (\\Lambda + \\alpha I)^{-1} \\alpha] Q^T w^*\n\\]\n\n\nEquivalence Condition\nEquation 7.43 - Comparing equations 7.40 and 7.42:\nIf we can set:\n\\[\n(I - \\epsilon \\Lambda)^\\tau = (\\Lambda + \\alpha I)^{-1} \\alpha\n\\]\nthen early stopping is equivalent to L2 regularization."
  },
  {
    "objectID": "ML/early-stopping.html#relationship-between-training-steps-and-regularization-strength",
    "href": "ML/early-stopping.html#relationship-between-training-steps-and-regularization-strength",
    "title": "Chapter 7.8: Early Stopping",
    "section": "6. Relationship Between Training Steps and Regularization Strength",
    "text": "6. Relationship Between Training Steps and Regularization Strength\n\nDeriving \\(\\tau\\) from \\(\\alpha\\)\nMathematical tools:\n\nLogarithm properties: \\(\\log(ab) = \\log(a) + \\log(b)\\) and \\(\\log(a^b) = b\\log(a)\\)\nTaylor series approximation: \\(\\log(1 + x) \\approx x\\) and \\(\\log(1 - x) \\approx -x\\)\n\nStarting from equation 7.43:\n\\[\n\\tau \\log(I - \\epsilon \\Lambda) = \\log((\\Lambda + \\alpha I)^{-1} \\alpha)\n\\]\nRight side:\n\\[\n\\log((\\Lambda + \\alpha I)^{-1} \\alpha) = -\\log(\\Lambda + \\alpha I) + \\log(\\alpha)\n\\]\nFactor out \\(\\alpha\\):\n\\[\n\\Lambda + \\alpha I = \\alpha\\left(\\frac{\\Lambda}{\\alpha} + I\\right)\n\\]\nTherefore:\n\\[\n-\\log(\\Lambda + \\alpha I) + \\log(\\alpha) = -\\log(\\alpha) - \\log\\left(\\frac{\\Lambda}{\\alpha} + I\\right) + \\log(\\alpha) = -\\log\\left(\\frac{\\Lambda}{\\alpha} + I\\right)\n\\]\nLeft side using Taylor approximation:\n\\[\n\\log(I - \\epsilon \\Lambda) \\approx -\\epsilon \\Lambda\n\\]\nRight side using Taylor approximation:\n\\[\n\\log\\left(I + \\frac{\\Lambda}{\\alpha}\\right) \\approx \\frac{\\Lambda}{\\alpha}\n\\]\nCombining:\n\\[\n\\tau(-\\epsilon \\Lambda) \\approx -\\frac{\\Lambda}{\\alpha}\n\\]\nEquation 7.44 - Solving for \\(\\tau\\):\n\\[\n\\tau \\approx \\frac{1}{\\epsilon \\alpha}\n\\]\nEquation 7.45 - Solving for \\(\\alpha\\):\n\\[\n\\alpha \\approx \\frac{1}{\\epsilon \\tau}\n\\]\n\n\nKey Insight\nThe number of training steps \\(\\tau\\) is inversely proportional to the L2 regularization strength \\(\\alpha\\).\nInterpretation:\n\nMore training steps (\\(\\tau\\) large) ‚ÜîÔ∏é Weaker regularization (\\(\\alpha\\) small)\nFewer training steps (\\(\\tau\\) small) ‚ÜîÔ∏é Stronger regularization (\\(\\alpha\\) large)\nEarly stopping implicitly applies L2 regularization with strength \\(\\alpha \\approx \\frac{1}{\\epsilon \\tau}\\)\n\nPractical implication: Choosing when to stop training is equivalent to choosing the regularization strength.\n\nKey concepts:\n\nEarly stopping: Stop training when validation error stops improving\nTwo approaches: Find optimal steps and retrain, or keep best model\nCosts: Requires validation data and model checkpointing\nBenefits: Simple, effective, computationally efficient regularization\nMathematical equivalence: Early stopping ‚âà implicit L2 regularization\nInverse relationship: \\(\\tau \\approx \\frac{1}{\\epsilon \\alpha}\\) (more steps = less regularization)\n\n\nSource: Deep Learning Book, Chapter 7.8"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html#proof",
    "href": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html#proof",
    "title": "EE 364A (Convex Optimization): Lecture 5.1 - Log-Concave and Log-Convex Functions",
    "section": "Proof",
    "text": "Proof\n\nTo prove: \\[\n\\log(f(\\theta x+(1-\\theta)y))\\ge \\theta \\log f(x)+(1-\\theta)\\log f(y)\n\\]\nTake exponential on both sides: \\[\n\\begin{aligned}\n\\exp(\\log(f(\\theta x+(1-\\theta)y))) &\\ge \\exp(\\theta \\log f(x)+(1-\\theta)\\log f(y))\\\\\nf(\\theta x+(1-\\theta)y) &\\ge \\exp(\\theta \\log f(x)) \\cdot \\exp((1-\\theta)\\log f(y))\\\\\n&\\ge f(x)^\\theta \\cdot f(y)^{1-\\theta}\n\\end{aligned}\n\\]\nFinal inequality: \\[\nf(\\theta x+(1-\\theta)y)\\ge  f(x)^\\theta\\cdot  f(y)^{1-\\theta}\n\\]\n\n\n\n\nGeometric interpretation of log-concavity"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html#examples",
    "href": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html#examples",
    "title": "EE 364A (Convex Optimization): Lecture 5.1 - Log-Concave and Log-Convex Functions",
    "section": "Examples",
    "text": "Examples\n\nPowers: \\(x^a\\) on \\(\\mathbb{R}_{++}\\) is log-convex for \\(a \\le 0\\), log-concave for \\(a \\ge 0\\)\n\n\\(f(x)=\\log(x^a)=a\\log(x)\\)\n\\(f'(x)=a\\cdot \\frac{1}{x}\\)\n\\(f''(x)=a\\cdot -\\frac{1}{x^2}\\)\n\nThus, the sign of \\(a\\) determines if \\(\\log(x^a)\\) is convex or concave.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction \\(f(x)\\)\nDomain\nSecond Derivative / Curvature\nConvex / Concave\n\\(\\log f(x)\\)\nLog-Convex / Log-Concave\nKey Notes\n\n\n\n\n\\(x^a\\)\n\\(\\mathbb{R}_{++}\\)\n\\(a(a-1)x^{a-2}\\)\ndepends on \\(a\\)\n\\(a\\log x\\)\n\\(a\\ge 0\\): log-concave\\(a\\le 0\\): log-convex\nClassic Boyd example\n\n\n\\(e^{ax}\\)\n\\(\\mathbb{R}\\)\n\\(a^2 e^{ax}\\)\nconvex (all \\(a\\))\n\\(ax\\)\nboth\nExponential is always convex\n\n\n\\(\\log x\\)\n\\(\\mathbb{R}_{++}\\)\n\\(-1/x^2\\)\nconcave\n\\(\\log\\log x\\)\n‚Äî\nFundamental concave function\n\n\nGaussian pdf \\(\\phi(x)\\)\n\\(\\mathbb{R}\\)\n‚Äî\n‚ùå\n\\(-x^2/2 + c\\)\nlog-concave\nCore likelihood model\n\n\nGaussian CDF \\(\\Phi(x)\\)\n\\(\\mathbb{R}\\)\n\\(-x\\phi(x)\\)\n‚ùå (S-shaped)\n\\(\\log\\Phi(x)\\)\nlog-concave\nProbit models\n\n\nLogistic \\(\\sigma(x)\\)\n\\(\\mathbb{R}\\)\nchanges sign\n‚ùå\nconcave\nlog-concave\nLogistic regression\n\n\n\\(\\\\|x\\\\|_2\\)\n\\(\\mathbb{R}^n\\)\n‚Äî\nconvex\n‚Äî\n‚Äî\nAll norms are convex\n\n\n\\(\\log\\sum_i e^{x_i}\\)\n\\(\\mathbb{R}^n\\)\nHessian \\(\\succeq 0\\)\nconvex\n‚Äî\n‚Äî\nSoftmax / log-partition\n\n\n\\(\\prod_i x_i\\)\n\\(\\mathbb{R}_{++}^n\\)\n‚Äî\n‚ùå\n\\(\\sum_i \\log x_i\\)\nlog-concave\nGeometric programming"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html#properties-of-log-concave-functions",
    "href": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html#properties-of-log-concave-functions",
    "title": "EE 364A (Convex Optimization): Lecture 5.1 - Log-Concave and Log-Convex Functions",
    "section": "Properties of log-concave functions",
    "text": "Properties of log-concave functions\n\nTwice differentiable function \\(f\\) with convex domain is log-concave iff \\(f(x)\\nabla^2f(x) \\preceq \\nabla f(x)\\nabla f(x)^\\top\\) for all \\(x \\in \\mathrm{dom}\\, f\\)\nProduct of log-concave functions is log-concave\nSum of log-concave functions is not always log-concave\nIf \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is log-concave, then \\(g(x)=\\int f(x,y) dy\\) is log-concave\n\n\n\n\nProperties of log-concave functions"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html",
    "href": "Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html",
    "title": "EE 364A (Convex Optimization): Lecture 5.1 - Log-Concave and Log-Convex Functions",
    "section": "",
    "text": "Convex Optimization Textbook - Chapter 3.5 (page 118)"
  },
  {
    "objectID": "ML/activation-functions.html",
    "href": "ML/activation-functions.html",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "",
    "text": "This exploration of Deep Learning Chapter 6.3 reveals how activation functions shape the behavior of hidden units in neural networks - and why choosing the right one matters.\nüìì For the complete implementation with additional exercises, see the notebook on GitHub.\nüìö For theoretical background and summary, see the chapter summary."
  },
  {
    "objectID": "ML/activation-functions.html#why-activation-functions-matter",
    "href": "ML/activation-functions.html#why-activation-functions-matter",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "Why Activation Functions Matter",
    "text": "Why Activation Functions Matter\nLinear transformations alone can only represent linear relationships. No matter how many layers you stack, \\(W_3(W_2(W_1x))\\) is still just a linear function. Activation functions introduce the non-linearity that makes deep learning powerful.\nBut which activation function should you use? The answer depends on understanding their mathematical properties and how they affect gradient flow during training.\n\n\n\n\n\n\n\n\n\nActivation\nOutput Range\nKey Property\nBest For\n\n\n\n\nReLU\n\\([0, \\infty)\\)\nZero for negatives\nHidden layers (default choice)\n\n\nSigmoid\n\\((0, 1)\\)\nSquashing, smooth\nBinary classification output\n\n\nTanh\n\\((-1, 1)\\)\nZero-centered\nHidden layers (when centering helps)"
  },
  {
    "objectID": "ML/activation-functions.html#exploring-activation-functions-shape-and-derivatives",
    "href": "ML/activation-functions.html#exploring-activation-functions-shape-and-derivatives",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "üéØ Exploring Activation Functions: Shape and Derivatives",
    "text": "üéØ Exploring Activation Functions: Shape and Derivatives\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Configure plotting\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['axes.grid'] = True\nplt.rcParams['grid.alpha'] = 0.3\n\n\nThe behavior of an activation function is determined by two things: 1. Its shape - how it transforms inputs 2. Its derivative - how gradients flow backward during training\n\nDefine Activation Functions\n\n\nShow code\ndef relu(x):\n    return np.clip(x, 0, np.inf)\n\ndef relu_derivative(x):\n    return np.where(x &gt; 0, 1, 0)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    return 1 - np.tanh(x)**2\n\n\n\n\nPlot Functions and Derivatives\n\n\nShow code\nx = np.linspace(-5, 5, 1000)\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 8))\nfig.suptitle('Common Activation Functions and Their Derivatives', fontsize=16)\n\n# ReLU\naxes[0, 0].plot(x, relu(x), linewidth=2, color='blue')\naxes[0, 0].set_title('ReLU', fontsize=12)\naxes[0, 0].set_ylabel('f(x)', fontsize=11)\naxes[1, 0].plot(x, relu_derivative(x), linewidth=2, color='blue')\naxes[1, 0].set_title('ReLU Derivative', fontsize=12)\naxes[1, 0].set_ylabel(\"f'(x)\", fontsize=11)\naxes[1, 0].set_xlabel('x', fontsize=11)\n\n# Sigmoid\naxes[0, 1].plot(x, sigmoid(x), linewidth=2, color='red')\naxes[0, 1].set_title('Sigmoid', fontsize=12)\naxes[1, 1].plot(x, sigmoid_derivative(x), linewidth=2, color='red')\naxes[1, 1].set_title('Sigmoid Derivative', fontsize=12)\naxes[1, 1].set_xlabel('x', fontsize=11)\n\n# Tanh\naxes[0, 2].plot(x, tanh(x), linewidth=2, color='green')\naxes[0, 2].set_title('Tanh', fontsize=12)\naxes[1, 2].plot(x, tanh_derivative(x), linewidth=2, color='green')\naxes[1, 2].set_title('Tanh Derivative', fontsize=12)\naxes[1, 2].set_xlabel('x', fontsize=11)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nKey observations:\n\nReLU: \\(f(x) = \\max(0, x)\\) - Zero for negative inputs, identity for positive. Derivative is 0 or 1 (simple!).\nSigmoid: \\(f(x) = \\frac{1}{1+e^{-x}}\\) - Squashes inputs to \\((0, 1)\\). Derivative peaks at 0, vanishes at extremes (gradient vanishing problem).\nTanh: \\(f(x) = \\tanh(x)\\) - Similar to sigmoid but outputs in \\((-1, 1)\\). Zero-centered with stronger gradients than sigmoid."
  },
  {
    "objectID": "ML/activation-functions.html#the-dead-relu-problem-when-neurons-stop-learning",
    "href": "ML/activation-functions.html#the-dead-relu-problem-when-neurons-stop-learning",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "The Dead ReLU Problem: When Neurons Stop Learning",
    "text": "The Dead ReLU Problem: When Neurons Stop Learning\nReLU‚Äôs simplicity is its strength, but also its weakness. A ReLU neuron can ‚Äúdie‚Äù - permanently outputting zero and never learning again.\nWhy does this happen?\nWhen a neuron‚Äôs pre-activation values are consistently negative (due to poor initialization, high learning rate, or bad gradients), ReLU outputs zero. Since the derivative is also zero for negative inputs, no gradient flows backward. The neuron is stuck forever.\n\n\nShow code\n# Generate input data\nx = torch.randn(1000, 10)  # 1000 samples, 10 features\nlinear = nn.Linear(10, 5)   # 5 hidden units\n\n# Set bias to large negative values to \"kill\" neurons\nwith torch.no_grad():\n    linear.bias.fill_(-10.0)\n\n# Forward pass\npre_activation = linear(x)\npost_activation = torch.relu(pre_activation)\n\n# Calculate statistics\ndead_percentage = (post_activation == 0).float().mean() * 100\nprint(f\"Percentage of dead neurons: {dead_percentage:.2f}%\\n\")\n\n# Display table showing ReLU input vs output\nprint(\"ReLU Input vs Output (first 10 samples, neuron 0):\")\nprint(\"-\" * 50)\nprint(f\"{'Sample':&lt;10} {'Pre-Activation':&lt;20} {'Post-Activation':&lt;20}\")\nprint(\"-\" * 50)\n\nfor i in range(10):\n    pre_val = pre_activation[i, 0].item()\n    post_val = post_activation[i, 0].item()\n    print(f\"{i:&lt;10} {pre_val:&lt;20.4f} {post_val:&lt;20.4f}\")\n\nprint(\"\\nObservation: All negative inputs become 0 after ReLU ‚Üí Dead neuron!\")\n\n\nPercentage of dead neurons: 100.00%\n\nReLU Input vs Output (first 10 samples, neuron 0):\n--------------------------------------------------\nSample     Pre-Activation       Post-Activation     \n--------------------------------------------------\n0          -9.7837              0.0000              \n1          -10.0322             0.0000              \n2          -10.4466             0.0000              \n3          -10.3243             0.0000              \n4          -10.5448             0.0000              \n5          -9.7712              0.0000              \n6          -10.8104             0.0000              \n7          -11.3418             0.0000              \n8          -9.8559              0.0000              \n9          -8.6873              0.0000              \n\nObservation: All negative inputs become 0 after ReLU ‚Üí Dead neuron!\n\n\nWith a large negative bias, every input becomes negative after the linear transformation. ReLU zeros them all out. The gradient is zero everywhere. The neuron never updates. It‚Äôs dead."
  },
  {
    "objectID": "ML/activation-functions.html#experiment-do-different-activations-make-a-difference",
    "href": "ML/activation-functions.html#experiment-do-different-activations-make-a-difference",
    "title": "Deep Learning Book 6.3: Hidden Units and Activation Functions",
    "section": "Experiment: Do Different Activations Make a Difference?",
    "text": "Experiment: Do Different Activations Make a Difference?\nTheory is nice, but let‚Äôs see activation functions in action. We‚Äôll train three identical networks with different activations on a simple regression task: \\(y = \\sin(x) + x^2 + 1\\).\n\nGenerate Data\n\n\nShow code\n# Training data\nx_train = np.random.rand(200, 1)\ny_train = np.sin(x_train) + np.power(x_train, 2) + 1\n\n# Test data\nx_test = np.random.rand(50, 1)\ny_test = np.sin(x_test) + np.power(x_test, 2) + 1\n\n# Convert to PyTorch tensors\nx_train_tensor = torch.FloatTensor(x_train)\ny_train_tensor = torch.FloatTensor(y_train)\nx_test_tensor = torch.FloatTensor(x_test)\ny_test_tensor = torch.FloatTensor(y_test)\n\n\n\n\nCreate and Train Models\n\n\nShow code\ndef create_regression_model(activation_fn):\n    \"\"\"Create a 2-layer network with specified activation\"\"\"\n    return nn.Sequential(\n        nn.Linear(1, 20),\n        activation_fn,\n        nn.Linear(20, 1)\n    )\n\n# Create 3 models with different activations\nmodels = {\n    'ReLU': create_regression_model(nn.ReLU()),\n    'Sigmoid': create_regression_model(nn.Sigmoid()),\n    'Tanh': create_regression_model(nn.Tanh())\n}\n\n# Training configuration\nn_epochs = 100\nlearning_rate = 0.01\nloss_fn = nn.MSELoss()\n\n# Track metrics\nloss_history = {name: [] for name in models.keys()}\ntest_mse_history = {name: [] for name in models.keys()}\n\n# Train each model\nfor name, model in models.items():\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    for epoch in range(n_epochs):\n        # Training\n        model.train()\n        y_pred = model(x_train_tensor)\n        loss = loss_fn(y_pred, y_train_tensor)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loss_history[name].append(loss.item())\n\n        # Evaluation on test set\n        model.eval()\n        with torch.no_grad():\n            y_test_pred = model(x_test_tensor)\n            test_mse = loss_fn(y_test_pred, y_test_tensor).item()\n            test_mse_history[name].append(test_mse)\n\n\n\n\nCompare Learning Curves\n\n\nShow code\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\ncolors = {'ReLU': 'blue', 'Sigmoid': 'red', 'Tanh': 'green'}\n\n# Plot training loss\nfor name, losses in loss_history.items():\n    axes[0].plot(losses, label=name, linewidth=2, color=colors[name])\n\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Training Loss (MSE)', fontsize=12)\naxes[0].set_title('Training Loss Over Time', fontsize=14)\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\naxes[0].set_yscale('log')\n\n# Plot test MSE\nfor name, test_mse in test_mse_history.items():\n    axes[1].plot(test_mse, label=name, linewidth=2, color=colors[name])\n\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Test Loss (MSE)', fontsize=12)\naxes[1].set_title('Test Loss Over Time', fontsize=14)\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\naxes[1].set_yscale('log')\n\nplt.tight_layout()\nplt.show()\n\n# Print final metrics\nprint(\"\\nFinal Metrics after {} epochs:\".format(n_epochs))\nprint(\"-\" * 60)\nprint(f\"{'Activation':&lt;15} {'Train Loss':&lt;15} {'Test Loss':&lt;15}\")\nprint(\"-\" * 60)\nfor name in models.keys():\n    train_loss = loss_history[name][-1]\n    test_loss = test_mse_history[name][-1]\n    print(f\"{name:&lt;15} {train_loss:&lt;15.6f} {test_loss:&lt;15.6f}\")\n\n\n\n\n\n\n\n\n\n\nFinal Metrics after 100 epochs:\n------------------------------------------------------------\nActivation      Train Loss      Test Loss      \n------------------------------------------------------------\nReLU            0.007420        0.008211       \nSigmoid         0.227441        0.247947       \nTanh            0.035384        0.038743"
  },
  {
    "objectID": "Math/MIT18.06/lectures.html",
    "href": "Math/MIT18.06/lectures.html",
    "title": "MIT 18.06SC Linear Algebra",
    "section": "",
    "text": "My journey through MIT‚Äôs Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.\n\n\n\n\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation.\n\n\nFrom Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series The beautiful mathematical journey from Taylor expansions to Euler‚Äôs formula and ultimately to Fourier series, revealing deep connections between polynomials, exponentials, and trigonometric functions.\n\n\n\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations Two powerful perspectives: row picture vs column picture.\n\n\nLecture 2: Elimination with Matrices The systematic algorithm that transforms linear systems into upper triangular form.\n\n\nLecture 3: Matrix Multiplication and Inverse Five different perspectives on matrix multiplication.\n\n\nLecture 4: LU Decomposition Factoring matrices into Lower √ó Upper triangular form.\n\n\nLecture 5.1: Permutation Matrices Permutation matrices reorder rows and columns.\n\n\nLecture 5.2: Transpose The transpose operation switches rows to columns.\n\n\nLecture 5.3: Vector Spaces Vector spaces and subspaces: closed under addition and scalar multiplication.\n\n\nLecture 6: Column Space and Null Space Column space determines which \\(b\\) make \\(Ax = b\\) solvable.\n\n\nLecture 7: Solving Ax=0 Systematic algorithm to find null space using pivot/free variables and RREF.\n\n\nLecture 8: Solving Ax=b Complete solution is particular solution plus null space.\n\n\nLecture 9: Independence, Basis, and Dimension Linear independence, basis, and dimension. Rank-nullity theorem.\n\n\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix.\n\n\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.\n\n\nLecture 12: Graphs, Networks, and Incidence Matrices Applying linear algebra to graph theory: incidence matrices, Kirchhoff‚Äôs laws, and Euler‚Äôs formula.\n\n\nLecture 13: Quiz 1 Review Practice problems reviewing key concepts: subspaces, rank, null spaces, and the four fundamental subspaces.\n\n\n\n\n\n\n\n\nLecture 14: Orthogonal Vectors and Subspaces Orthogonal vectors, orthogonal subspaces, and the fundamental relationships between the four subspaces.\n\n\nLecture 15: Projection onto Subspaces Projecting onto lines and subspaces, the geometry of least squares, and deriving normal equations.\n\n\nLecture 16: Projection Matrices and Least Squares Projection matrices, least squares fitting, and the connection between linear algebra and calculus optimization.\n\n\nLecture 17: Orthogonal Matrices and Gram-Schmidt Orthonormal matrices, the Gram-Schmidt process for orthogonalization, and QR factorization.\n\n\nLecture 18: Properties of Determinants Ten fundamental properties that completely define the determinant and reveal when matrices are invertible.\n\n\nLecture 19: Determinant Formulas and Cofactors Three computational methods for determinants: pivots, the big formula, and cofactor expansion.\n\n\nLecture 20: Inverse & Volume The inverse matrix formula using cofactors, Cramer‚Äôs rule for solving linear systems, and the geometric interpretation of determinants as volume.\n\n\nLecture 21: Eigenvalues and Eigenvectors The directions that matrices can only scale, not rotate: \\(Ax = \\lambda x\\).\n\n\nLecture 22: Diagonalization and Powers of A Computing matrix powers efficiently and solving Fibonacci with eigenvalues.\n\n\nLecture 23: Differential Equations and exp(At) Connecting linear algebra to differential equations: how eigenvalues determine stability and matrix exponentials solve systems.\n\n\nLecture 24: Markov Matrices and Fourier Series Two powerful applications of eigenvalues: Markov matrices for modeling state transitions and Fourier series for decomposing functions into orthogonal components.\n\n\n\n\n\n\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 29: Singular Value Decomposition The most important matrix factorization: SVD provides orthonormal bases for all four fundamental subspaces, revealing how any matrix maps its row space to its column space through scaling by singular values.\n\n\nLecture 30: Linear Transformations and Their Matrices The fundamental connection between abstract linear transformations and concrete matrices: every linear transformation can be represented as matrix multiplication, and the choice of basis determines the matrix representation.\n\n\nLecture 31: Change of Basis and Image Compression How choosing the right basis enables compression: JPEG transforms 512√ó512 images (262,144 pixels) to Fourier basis and discards small coefficients. Three properties of good bases‚Äîfast inverse (FFT in O(n log n)), sparsity (few coefficients enough), and orthogonality (no redundancy).\n\n\nLecture 33: Left and Right Inverse; Pseudo-inverse When matrices aren‚Äôt square: full column rank matrices (\\(r = n &lt; m\\)) have left inverses \\((A^T A)^{-1} A^T\\), full row rank matrices (\\(r = m &lt; n\\)) have right inverses \\(A^T (AA^T)^{-1}\\), and the pseudo-inverse \\(A^+ = V \\Sigma^+ U^T\\) generalizes both using SVD‚Äîinverting non-zero singular values and transposing the shape."
  },
  {
    "objectID": "Math/MIT18.06/lectures.html#all-lectures",
    "href": "Math/MIT18.06/lectures.html#all-lectures",
    "title": "MIT 18.06SC Linear Algebra",
    "section": "",
    "text": "My journey through MIT‚Äôs Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.\n\n\n\n\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation.\n\n\nFrom Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series The beautiful mathematical journey from Taylor expansions to Euler‚Äôs formula and ultimately to Fourier series, revealing deep connections between polynomials, exponentials, and trigonometric functions.\n\n\n\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations Two powerful perspectives: row picture vs column picture.\n\n\nLecture 2: Elimination with Matrices The systematic algorithm that transforms linear systems into upper triangular form.\n\n\nLecture 3: Matrix Multiplication and Inverse Five different perspectives on matrix multiplication.\n\n\nLecture 4: LU Decomposition Factoring matrices into Lower √ó Upper triangular form.\n\n\nLecture 5.1: Permutation Matrices Permutation matrices reorder rows and columns.\n\n\nLecture 5.2: Transpose The transpose operation switches rows to columns.\n\n\nLecture 5.3: Vector Spaces Vector spaces and subspaces: closed under addition and scalar multiplication.\n\n\nLecture 6: Column Space and Null Space Column space determines which \\(b\\) make \\(Ax = b\\) solvable.\n\n\nLecture 7: Solving Ax=0 Systematic algorithm to find null space using pivot/free variables and RREF.\n\n\nLecture 8: Solving Ax=b Complete solution is particular solution plus null space.\n\n\nLecture 9: Independence, Basis, and Dimension Linear independence, basis, and dimension. Rank-nullity theorem.\n\n\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix.\n\n\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.\n\n\nLecture 12: Graphs, Networks, and Incidence Matrices Applying linear algebra to graph theory: incidence matrices, Kirchhoff‚Äôs laws, and Euler‚Äôs formula.\n\n\nLecture 13: Quiz 1 Review Practice problems reviewing key concepts: subspaces, rank, null spaces, and the four fundamental subspaces.\n\n\n\n\n\n\n\n\nLecture 14: Orthogonal Vectors and Subspaces Orthogonal vectors, orthogonal subspaces, and the fundamental relationships between the four subspaces.\n\n\nLecture 15: Projection onto Subspaces Projecting onto lines and subspaces, the geometry of least squares, and deriving normal equations.\n\n\nLecture 16: Projection Matrices and Least Squares Projection matrices, least squares fitting, and the connection between linear algebra and calculus optimization.\n\n\nLecture 17: Orthogonal Matrices and Gram-Schmidt Orthonormal matrices, the Gram-Schmidt process for orthogonalization, and QR factorization.\n\n\nLecture 18: Properties of Determinants Ten fundamental properties that completely define the determinant and reveal when matrices are invertible.\n\n\nLecture 19: Determinant Formulas and Cofactors Three computational methods for determinants: pivots, the big formula, and cofactor expansion.\n\n\nLecture 20: Inverse & Volume The inverse matrix formula using cofactors, Cramer‚Äôs rule for solving linear systems, and the geometric interpretation of determinants as volume.\n\n\nLecture 21: Eigenvalues and Eigenvectors The directions that matrices can only scale, not rotate: \\(Ax = \\lambda x\\).\n\n\nLecture 22: Diagonalization and Powers of A Computing matrix powers efficiently and solving Fibonacci with eigenvalues.\n\n\nLecture 23: Differential Equations and exp(At) Connecting linear algebra to differential equations: how eigenvalues determine stability and matrix exponentials solve systems.\n\n\nLecture 24: Markov Matrices and Fourier Series Two powerful applications of eigenvalues: Markov matrices for modeling state transitions and Fourier series for decomposing functions into orthogonal components.\n\n\n\n\n\n\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 29: Singular Value Decomposition The most important matrix factorization: SVD provides orthonormal bases for all four fundamental subspaces, revealing how any matrix maps its row space to its column space through scaling by singular values.\n\n\nLecture 30: Linear Transformations and Their Matrices The fundamental connection between abstract linear transformations and concrete matrices: every linear transformation can be represented as matrix multiplication, and the choice of basis determines the matrix representation.\n\n\nLecture 31: Change of Basis and Image Compression How choosing the right basis enables compression: JPEG transforms 512√ó512 images (262,144 pixels) to Fourier basis and discards small coefficients. Three properties of good bases‚Äîfast inverse (FFT in O(n log n)), sparsity (few coefficients enough), and orthogonality (no redundancy).\n\n\nLecture 33: Left and Right Inverse; Pseudo-inverse When matrices aren‚Äôt square: full column rank matrices (\\(r = n &lt; m\\)) have left inverses \\((A^T A)^{-1} A^T\\), full row rank matrices (\\(r = m &lt; n\\)) have right inverses \\(A^T (AA^T)^{-1}\\), and the pseudo-inverse \\(A^+ = V \\Sigma^+ U^T\\) generalizes both using SVD‚Äîinverting non-zero singular values and transposing the shape."
  }
]