[
  {
    "objectID": "ML/kmeans.html",
    "href": "ML/kmeans.html",
    "title": "K-means Clustering",
    "section": "",
    "text": "Setup points and K\nwe will implement a KNN algorithm to cluster the points\n\n\nX=[[1,1],[2,2.1],[3,2.5],[6,7],[7,7.1],[9,7.5]]\nk=2\n\nmax_iter=3\n\n\n# Visualize the data\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter([x[0] for x in X],[x[1] for x in X])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pure python implementation of K-means clustering\ndef knn_iter(X,centroids):\n    # set up new clusters\n    new_clusters=[[] for _ in range(len(centroids))]\n    # k=len(centroids)\n    # assign each point to the nearest centroid\n    for x in X:\n        k,distance=0,(x[0]-centroids[0][0])**2+(x[1]-centroids[0][1])**2\n        for i,c in enumerate(centroids[1:],1):\n            if (x[0]-c[0])**2+(x[1]-c[1])**2&lt;distance:\n                k=i\n                distance=(x[0]-c[0])**2+(x[1]-c[1])**2\n        new_clusters[k].append(x)\n    \n    # calculate new centroids\n    new_centroids=[[\n        sum([x[0] for x in cluster])/len(cluster),\n        sum([x[1] for x in cluster])/len(cluster)\n    ] if cluster else centroids[i] for i,cluster in enumerate(new_clusters)]\n    return new_centroids\n\n\n\n\n\n\n\n\ndef iter_and_draw(X,k,max_iter):\n    centroids=X[:k]  # Randomly select 2 centroids\n    fig, axes = plt.subplots(max_iter//3+(1 if max_iter%3!=0 else 0),\n        3, figsize=(15, 10))\n    axes=axes.flatten()\n    for i in range(max_iter):\n        \n        # Plot points and centroids\n\n\n        # Assign each point to nearest centroid and plot with corresponding color\n        colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n        for j, x in enumerate(X):\n            # Find nearest centroid\n            min_dist = float('inf')\n            nearest_centroid = 0\n            for k, c in enumerate(centroids):\n                dist = (x[0]-c[0])**2 + (x[1]-c[1])**2\n                if dist &lt; min_dist:\n                    min_dist = dist\n                    nearest_centroid = k\n            # Plot point with color corresponding to its cluster\n            axes[i].scatter(x[0], x[1], c=colors[nearest_centroid % len(colors)], label=f'Cluster {nearest_centroid+1}' if j==0 else \"\")\n        axes[i].scatter([c[0] for c in centroids], [c[1] for c in centroids], c='red', marker='*', s=200, label='Centroids')\n        axes[i].set_title(f'Iteration {i}')\n        centroids = knn_iter(X, centroids)\n\n    plt.tight_layout()\n    plt.show()\n\niter_and_draw(X,k,max_iter)\n# print(centroids)\n\n\n\n\n\n\n\n\n\n# A 3 clusters example\n\nimport numpy as np\n\nX1=np.random.rand(20,2)+5 # Some points in the upper right corner\nX2=np.random.rand(20,2)+3 # Some points in the middle\nX3=np.random.rand(20,2) # Some points in the lower left corner\n\niter_and_draw(np.concatenate((X1,X2,X3)),3,5)\n\n\n\n\n\n\n\n\n\n\nA question?\n\nWhat to do if one cluster has no assigned points during iteration?\n\n\n\nFormula Derivation\nThe goal is to minimize the loss of inertia which is sum of the points to cluster centroids.\n\\[\nLoss= \\sum_{i=1}^n \\sum_{x \\in C_i} ||x-\\mu_i||^2\n\\]\nTo iter \\(\\mu\\) for each cluster, let us find the derivative of the following function. \\[\nf(\\mu)=\\sum_{i=1}^n ||x_i-\\mu||^2 =\n\\sum_{i=1}^n {x_i}^2+\\mu^2-2x_i\\mu\n\\]\nGiven a \\(\\nabla \\mu\\), \\[\nf(\\mu + \\nabla \\mu)=\\sum_{i=1}^n ||x_i+\\nabla \\mu -\\mu||^2 =\n\\sum_{i=1}^n  {x_i}^2+\\mu^2+{\\nabla \\mu}^2-2{x_i \\mu}-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\nf(\\mu + \\nabla \\mu)-f(\\mu)=\n\\sum_{i=1}^n {\\nabla \\mu}^2-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\n\\frac {f(\\mu + \\nabla \\mu)-f(\\mu)}{\\nabla \\mu}=\\sum_{i=1}^n {\\nabla \\mu} -2 \\mu +2{x_i} = 2\\sum_{i=1}^n x_i - 2n\\mu\n\\]\nNow we can see if \\(n\\mu = \\sum_{i=1}^n x_i\\), then the derivative is 0, this is why in each iteration, we need to set the center of the cluster as centroid."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ickma.dev",
    "section": "",
    "text": "A growing collection of structured study notes and visual explanations ‚Äî written for clarity, reproducibility, and long-term memory."
  },
  {
    "objectID": "index.html#latest-updates",
    "href": "index.html#latest-updates",
    "title": "ickma.dev",
    "section": "Latest Updates",
    "text": "Latest Updates\n\n‚àá Deep Learning Book 37 chapters\nMy notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\nChapter 9.9: Unsupervised or Semi-Supervised Feature Learning Before CNNs, computer vision relied on hand-crafted kernels (Sobel, Laplacian) and unsupervised methods (sparse coding, autoencoders, k-means). Modern CNNs surpass both by learning hierarchical features end-to-end‚Äîfrom edges to semantic concepts‚Äîoptimized for the task, making hand-crafted filters largely obsolete.\n\n\nChapter 9.8: Efficient Convolution Algorithms Separable convolution decomposes a 2D kernel into two 1D filters, reducing computational cost from \\(O(HWk^2)\\) to \\(O(HWk)\\) and parameter storage from \\(k^2\\) to \\(2k\\). This factorization enables faster, more memory-efficient models without sacrificing accuracy‚Äîfoundational for architectures like MobileNet.\n\n\nChapter 9.7: Data Types CNNs can operate on different data types: 1D (audio, time series), 2D (images), and 3D (videos, CT scans) with varying channel counts. Unlike fully connected networks, convolutional kernels handle variable-sized inputs‚Äîa unique flexibility for diverse domains.\n\n\nChapter 9.6: Structured Outputs CNNs can generate high-dimensional structured objects through pixel-level predictions. Recurrent convolution refines predictions iteratively, producing dense outputs for segmentation, depth estimation, and flow prediction.\n\n\n\nSee all Deep Learning chapters ‚Üí\n\n\n\nüìê MIT 18.06SC Linear Algebra 36 lectures\nMy journey through MIT‚Äôs Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\n\nSee all MIT 18.06SC lectures ‚Üí\n\n\n\nüìê MIT 18.065: Linear Algebra Applications 2 lectures\nMy notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning‚Äîexploring how linear algebra powers modern applications.\n\n\nLecture 9: Four Ways to Solve Least Squares Problems Four equivalent methods for solving \\(Ax = b\\) when \\(A\\) has no inverse: pseudo-inverse, normal equations, algebraic minimization, and geometric projection‚Äîall converging to the same optimal solution.\n\n\nLecture 8: Norms of Vectors and Matrices Understanding vector p-norms (\\(\\|v\\|_p\\)) and when they satisfy the triangle inequality (only for \\(p \\geq 1\\)). The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù creates non-convex unit balls for strong sparsity.\n\n\n\nSee all MIT 18.065 lectures ‚Üí\n\n\n\nüìê Stanford EE 364A: Convex Optimization 1 lectures\nMy notes from Stanford EE 364A: Convex Optimization‚Äîtheory and applications of optimization problems.\n\n\nLecture 1: Introduction to Convex Optimization Introduction to constraint optimization: least squares (\\(\\|Ax-b\\|_2^2\\)), linear programming, and convex optimization. Convex problems generalize both while maintaining polynomial-time solvability through interior-point methods‚Äîbridging tractable special cases and general nonlinear programming.\n\n\n\nSee all EE 364A lectures ‚Üí"
  },
  {
    "objectID": "index.html#more-topics",
    "href": "index.html#more-topics",
    "title": "ickma.dev",
    "section": "More Topics",
    "text": "More Topics\n\n\nMachine Learning\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\nAlgorithms\n\nDP Regex"
  },
  {
    "objectID": "ML/cnn-data-types.html",
    "href": "ML/cnn-data-types.html",
    "title": "Chapter 9.7: Data Types",
    "section": "",
    "text": "Convolutional networks can operate on many different kinds of data, depending on the number of channels and the dimensionality of the input."
  },
  {
    "objectID": "ML/cnn-data-types.html#overview",
    "href": "ML/cnn-data-types.html#overview",
    "title": "Chapter 9.7: Data Types",
    "section": "",
    "text": "Convolutional networks can operate on many different kinds of data, depending on the number of channels and the dimensionality of the input."
  },
  {
    "objectID": "ML/cnn-data-types.html#input-formats",
    "href": "ML/cnn-data-types.html#input-formats",
    "title": "Chapter 9.7: Data Types",
    "section": "Input Formats",
    "text": "Input Formats\nTable 9.1 illustrates how 1D, 2D, and 3D inputs can each appear in both single-channel and multi-channel forms.\n Figure: Examples of different data types CNNs can process. 1D inputs include audio waveforms and time series. 2D inputs include grayscale images (single-channel) and RGB images (multi-channel). 3D inputs correspond to volumes such as CT scans (single-channel) or videos and skeleton animations (multi-channel spatiotemporal data).\n\n1D inputs include audio waveforms, time series, or multi-sensor signals.\n2D inputs include grayscale images (single-channel) and RGB or multispectral images (multi-channel).\n3D inputs correspond to volumes such as CT scans (single-channel) or videos and skeleton animations (multi-channel spatiotemporal data)."
  },
  {
    "objectID": "ML/cnn-data-types.html#flexibility-of-convolution",
    "href": "ML/cnn-data-types.html#flexibility-of-convolution",
    "title": "Chapter 9.7: Data Types",
    "section": "Flexibility of Convolution",
    "text": "Flexibility of Convolution\nOne advantage of convolution is that it naturally supports these different input structures without requiring a fixed dimensionality or a fixed number of channels.\nThe same convolutional kernel can be applied across various data types because convolution is fundamentally a translation-equivariant, local operation."
  },
  {
    "objectID": "ML/cnn-data-types.html#variable-sized-inputs",
    "href": "ML/cnn-data-types.html#variable-sized-inputs",
    "title": "Chapter 9.7: Data Types",
    "section": "Variable-Sized Inputs",
    "text": "Variable-Sized Inputs\nAdditionally, convolution can also process variable-sized inputs.\nAs long as the kernel size is fixed, the convolution operation slides across the input regardless of its spatial extent, producing outputs whose dimensions scale accordingly.\nThis flexibility enables CNNs to handle inputs with different widths or heights‚Äîa property that fully connected networks do not have.\n Figure: CNNs can process inputs of different sizes. The same convolutional kernel slides across inputs regardless of spatial extent, producing outputs that scale with the input dimensions. This flexibility is unique to convolutional architectures‚Äîfully connected networks require fixed-size inputs."
  },
  {
    "objectID": "ML/cnn-data-types.html#key-insight",
    "href": "ML/cnn-data-types.html#key-insight",
    "title": "Chapter 9.7: Data Types",
    "section": "Key Insight",
    "text": "Key Insight\nThe power of CNNs lies in their flexibility. Unlike fully connected networks that require fixed input dimensions, convolutional networks can handle varying channel counts, different spatial dimensionalities (1D, 2D, 3D), and variable-sized inputs‚Äîall with the same kernel. This versatility makes CNNs applicable to a wide range of domains beyond computer vision, from audio processing to volumetric medical imaging."
  },
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "",
    "section": "Project Overview",
    "text": "Project Overview\nThis is a Quarto-based technical blog hosted on GitHub Pages (ickma2311.github.io). The site covers machine learning, algorithms, and technical tutorials with a focus on mathematical foundations and practical implementations."
  },
  {
    "objectID": "CLAUDE.html#common-commands",
    "href": "CLAUDE.html#common-commands",
    "title": "",
    "section": "Common Commands",
    "text": "Common Commands\n\nDevelopment Workflow\n\n./render-site.sh - Recommended: Clean build of entire website (handles cleanup automatically)\nquarto render - Build the entire website (outputs to docs/ directory)\n\n‚ö†Ô∏è May encounter file movement errors; use render-site.sh instead for reliable builds\n\nquarto preview - Start local development server with live reload\nquarto render &lt;file.qmd&gt; - Render a specific document\nquarto check - Verify Quarto installation and project setup\n\n\n\nContent Management\n\nCreate new ML content in ML/ directory\nCreate new algorithm content in Algorithm/ directory\nUpdate navigation by editing _quarto.yml navbar section\nAdd new content to respective index.qmd files for discoverability"
  },
  {
    "objectID": "CLAUDE.html#project-structure",
    "href": "CLAUDE.html#project-structure",
    "title": "",
    "section": "Project Structure",
    "text": "Project Structure\n‚îú‚îÄ‚îÄ _quarto.yml          # Main configuration file\n‚îú‚îÄ‚îÄ docs/                # Generated output (GitHub Pages source)\n‚îú‚îÄ‚îÄ index.qmd            # Homepage\n‚îú‚îÄ‚îÄ about.qmd            # About page\n‚îú‚îÄ‚îÄ ML/                  # Machine Learning content\n‚îÇ   ‚îú‚îÄ‚îÄ index.qmd        # ML topics overview\n‚îÇ   ‚îú‚îÄ‚îÄ *.qmd            # ML articles\n‚îÇ   ‚îî‚îÄ‚îÄ *.ipynb          # Jupyter notebooks\n‚îú‚îÄ‚îÄ Algorithm/           # Algorithm content\n‚îÇ   ‚îú‚îÄ‚îÄ index.qmd        # Algorithm topics overview\n‚îÇ   ‚îî‚îÄ‚îÄ *.qmd            # Algorithm articles\n‚îú‚îÄ‚îÄ imgs/                # Image assets\n‚îú‚îÄ‚îÄ media/               # Media files\n‚îî‚îÄ‚îÄ styles.css           # Custom CSS styles"
  },
  {
    "objectID": "CLAUDE.html#content-organization",
    "href": "CLAUDE.html#content-organization",
    "title": "",
    "section": "Content Organization",
    "text": "Content Organization\nThe site uses a hierarchical navigation structure defined in _quarto.yml: - Two main sections: ‚ÄúML‚Äù and ‚ÄúAlgorithm‚Äù - Each section has an index page that serves as a directory - Content is categorized by topic (e.g., ‚ÄúNumPy Fundamentals‚Äù, ‚ÄúClustering Algorithms‚Äù)\n\nAdding New Content\n\nCreate the content file in the appropriate directory (ML/ or Math/ or Algorithm/)\nAdd to list page: Update the corresponding list page (e.g., ML/deep-learning-book.qmd, Math/MIT18.06/lectures.qmd)\nUpdate homepage automatically: Run ./update-counts.sh to update section counts on homepage\nAdd navigation entry to _quarto.yml if it should appear in the navbar dropdown\nUse consistent frontmatter with title field\nSet publication date: Always use the current date from the system for the date field in frontmatter\n\nGet current date with: date +\"%Y-%m-%d\" (format: YYYY-MM-DD)\nExample: date: \"2025-10-26\"\n\nImportant: After adding new navbar items, run quarto render (full site render) to update the navbar on ALL existing pages. Individual file renders only update that specific page.\n\n\n\nMaintaining the Homepage\nThe homepage (index.qmd) shows the latest 4 items from each section with a count badge.\nCRITICAL: When adding new content, you MUST:\n\nAdd the new item to the appropriate list page:\n\nDeep Learning: ML/deep-learning-book.qmd\nMIT 18.06SC: Math/MIT18.06/lectures.qmd\nMIT 18.065: Math/MIT18.065/lectures.qmd\n\nUpdate the homepage manually by editing index.qmd:\n\nReplace the oldest item in the ‚ÄúLatest 4‚Äù with the new item\nKeep the 4 most recent items visible\nOrder: newest first (top), oldest last (bottom)\n\nUpdate section counts automatically:\n./update-counts.sh\nThis script automatically counts items in list pages and updates the count badges in index.qmd.\nRender and verify:\nquarto render index.qmd\n\nExample workflow when adding Chapter 9.8:\n# 1. Create the new content file\nvim ML/chapter-9-8.qmd\n\n# 2. Add to list page\nvim ML/deep-learning-book.qmd  # Add Chapter 9.8 entry\n\n# 3. Update homepage\nvim index.qmd  # Replace Chapter 9.4 with 9.8, keep 9.7, 9.6, and 9.5\n\n# 4. Update counts automatically\n./update-counts.sh\n\n# 5. Render\nquarto render index.qmd\nWarning: The homepage will NOT automatically update when you add new content. You must manually update index.qmd to show the latest 4 items."
  },
  {
    "objectID": "CLAUDE.html#configuration-notes",
    "href": "CLAUDE.html#configuration-notes",
    "title": "",
    "section": "Configuration Notes",
    "text": "Configuration Notes\n\nOutput directory is set to docs/ for GitHub Pages compatibility\nTheme: Cosmo with custom branding\nAll pages include table of contents (toc: true)\nSite uses custom CSS from styles.css\nJupyter notebooks are supported alongside Quarto markdown"
  },
  {
    "objectID": "CLAUDE.html#github-pages-deployment",
    "href": "CLAUDE.html#github-pages-deployment",
    "title": "",
    "section": "GitHub Pages Deployment",
    "text": "GitHub Pages Deployment\nThe site is automatically deployed from the docs/ directory. After rendering, commit and push the docs/ folder to trigger GitHub Pages rebuild. - Author is Chao Ma - GitHub Pages URL: https://ickma2311.github.io/\n\nPre-Push Checklist (CRITICAL)\nALWAYS verify locally before pushing to prevent broken images on production:\n\nRun full render: Use ./render-site.sh for automatic cleanup, or quarto render (not individual file renders)\nCRITICAL: Restore ALL deleted images after full render\nFull site renders delete images from multiple locations. You MUST restore them:\n# 1. Restore *_files/ directories (code-generated matplotlib/plotly figures)\ncp -r ML/*_files docs/ML/ 2&gt;/dev/null\ncp -r Math/*_files docs/Math/ 2&gt;/dev/null\ncp -r Algorithm/*_files docs/Algorithm/ 2&gt;/dev/null\n\n# 2. Restore static images in ML/ and Math/\ncp ML/*.png docs/ML/ 2&gt;/dev/null\ncp Math/MIT18.06/*.png docs/Math/MIT18.06/ 2&gt;/dev/null\n\n# 3. Restore media/ directory images (referenced with ../media/image.png)\nmkdir -p docs/media\ncp media/*.png docs/media/ 2&gt;/dev/null\n\n# 4. Restore imgs/ directory images (referenced with ../imgs/image.png)\nmkdir -p docs/imgs\ncp imgs/*.png docs/imgs/ 2&gt;/dev/null\n\n# 5. Move any HTML files that failed to move\nfind ML -maxdepth 1 -name \"*.html\" -exec mv {} docs/ML/ \\; 2&gt;/dev/null\nfind Math/MIT18.06 -maxdepth 1 -name \"*.html\" -exec mv {} docs/Math/MIT18.06/ \\; 2&gt;/dev/null\nfind Algorithm -maxdepth 1 -name \"*.html\" -exec mv {} docs/Algorithm/ \\; 2&gt;/dev/null\n\n# 6. Move index files\nmv Algorithm/index.html docs/Algorithm/ 2&gt;/dev/null\nmv Math/index.html docs/Math/ 2&gt;/dev/null\nmv index-backup.html docs/ 2&gt;/dev/null\nCheck git status: git status - verify all image files are staged\n\nLook for docs/**/*_files/figure-html/*.png files (code-generated)\nLook for docs/media/*.png files (media directory)\nLook for docs/imgs/*.png files (imgs directory)\nLook for docs/ML/*.png and docs/Math/MIT18.06/*.png files (static images)\n\nLocal preview: Open docs/ HTML files in browser to verify images load\n\nCheck pages with Jupyter notebooks (e.g., mit1806-lecture1-geometry.html)\nCheck pages with media references (e.g., dropout.html)\nVerify matplotlib/plotly figures appear correctly\n\nCommit ALL generated files: Don‚Äôt commit .html without their images\nOnly then push: git push\n\nWhy this matters: GitHub Pages serves from the docs/ directory. Quarto‚Äôs full render deletes images from docs/ but keeps them in source directories. If images aren‚Äôt copied back to docs/, the HTML will reference missing files, causing broken images on production even though they work locally.\nImage locations that get deleted: - docs/**/*_files/ - Code-generated figures from Python/matplotlib - docs/media/ - Shared media referenced with ../media/ - docs/imgs/ - Shared images referenced with ../imgs/ - docs/ML/*.png - Static chapter images - docs/Math/MIT18.06/*.png - Static lecture images"
  },
  {
    "objectID": "CLAUDE.html#linkedin-post-guidelines",
    "href": "CLAUDE.html#linkedin-post-guidelines",
    "title": "",
    "section": "LinkedIn Post Guidelines",
    "text": "LinkedIn Post Guidelines\n\nEmoji Usage\nWhen drafting LinkedIn posts for blog content, use these emojis: - Deep Learning topics: ‚àá (delta/nabla symbol) - represents gradients and optimization - Linear Algebra topics: üìê (triangle/ruler) - represents geometric and matrix concepts\n\n\nWriting Process\n\nIdentify the key insight: Focus on the main conceptual connection or ‚Äúaha moment‚Äù from the blog post\nUse ‚Äúconnecting the dots‚Äù tone: Emphasize how concepts link together (e.g., ‚Äúhow linear algebra connects to machine learning‚Äù)\nStructure (Knowledge Card Format):\n\nStart with emoji and chapter reference (e.g., ‚Äú‚àá Deep Learning Book (Chapter 8.1)‚Äù or ‚Äúüìê MIT 18.06SC Linear Algebra (Lecture 19)‚Äù)\nClear statement or equation (e.g., ‚ÄúLearning ‚â† Optimization‚Äù)\n2-4 bullet points (üîπ) with key insights\nPhilosophical closing line (üí°)\nLink to full blog post (üìñ)\nSource attribution at the end (e.g., ‚ÄúMy notes on Deep Learning (Ian Goodfellow) Chapter X.X‚Äù or ‚ÄúMy notes on MIT 18.06SC Linear Algebra - Lecture XX‚Äù)\n\nKeep it concise: Aim for clarity over comprehensiveness - use knowledge card format for quick, digestible insights\nCourse naming:\n\nAlways use ‚ÄúMIT 18.06SC‚Äù (not just ‚ÄúMIT 18.06‚Äù) for linear algebra posts\nUse full course titles to maintain consistency\n\nInclude relevant hashtags: #MachineLearning #LinearAlgebra #DeepLearning"
  },
  {
    "objectID": "ML/cnn-efficient-convolution.html",
    "href": "ML/cnn-efficient-convolution.html",
    "title": "Chapter 9.8: Efficient Convolution Algorithms",
    "section": "",
    "text": "A convolution kernel is separable when its multi-dimensional weights can be written as the outer product of several one-dimensional filters. In this case, a 2D convolution with a \\(k \\times k\\) kernel can be executed as two sequential 1D convolutions: first applying a vertical filter, then a horizontal one. This decomposition does not change the output‚Äîeach pixel still aggregates information from the full \\(k \\times k\\) neighborhood‚Äîbut it significantly reduces computational cost.\nStandard 2D convolution requires \\(O(HWk^2)\\) operations, while separable convolution reduces this to \\(O(HWk)\\). Parameter storage also shrinks from \\(k^2\\) to \\(2k\\). When such a factorization exists, it enables faster and more memory-efficient models without sacrificing accuracy, and is therefore an important strategy for designing efficient convolutional networks.\n\n Figure: Separable convolution decomposes a 2D kernel into two 1D filters (vertical and horizontal), reducing computational complexity from O(HWk¬≤) to O(HWk) while maintaining the same receptive field."
  },
  {
    "objectID": "ML/cnn-efficient-convolution.html#separable-convolution",
    "href": "ML/cnn-efficient-convolution.html#separable-convolution",
    "title": "Chapter 9.8: Efficient Convolution Algorithms",
    "section": "",
    "text": "A convolution kernel is separable when its multi-dimensional weights can be written as the outer product of several one-dimensional filters. In this case, a 2D convolution with a \\(k \\times k\\) kernel can be executed as two sequential 1D convolutions: first applying a vertical filter, then a horizontal one. This decomposition does not change the output‚Äîeach pixel still aggregates information from the full \\(k \\times k\\) neighborhood‚Äîbut it significantly reduces computational cost.\nStandard 2D convolution requires \\(O(HWk^2)\\) operations, while separable convolution reduces this to \\(O(HWk)\\). Parameter storage also shrinks from \\(k^2\\) to \\(2k\\). When such a factorization exists, it enables faster and more memory-efficient models without sacrificing accuracy, and is therefore an important strategy for designing efficient convolutional networks.\n\n Figure: Separable convolution decomposes a 2D kernel into two 1D filters (vertical and horizontal), reducing computational complexity from O(HWk¬≤) to O(HWk) while maintaining the same receptive field."
  },
  {
    "objectID": "ML/cnn-efficient-convolution.html#key-insight",
    "href": "ML/cnn-efficient-convolution.html#key-insight",
    "title": "Chapter 9.8: Efficient Convolution Algorithms",
    "section": "Key Insight",
    "text": "Key Insight\nSeparable convolution achieves the same output as standard 2D convolution but with dramatically reduced computational cost. By factorizing a \\(k \\times k\\) kernel into two \\(k \\times 1\\) filters, we transform an \\(O(k^2)\\) operation into \\(O(k)\\)‚Äîa massive speedup for large kernels. This decomposition is particularly valuable in mobile and edge deployments where computational resources are limited, and forms the foundation of efficient architectures like MobileNet."
  },
  {
    "objectID": "ML/cnn-unsupervised-learning.html",
    "href": "ML/cnn-unsupervised-learning.html",
    "title": "Chapter 9.9: Unsupervised or Semi-Supervised Feature Learning",
    "section": "",
    "text": "Before modern deep learning, visual features were extracted using manually designed filters such as Sobel, Laplacian, Gaussian blur, sharpening, and emboss kernels. These filters capture simple patterns‚Äîedges, gradients, corners, and blobs‚Äîbased on human intuition about what matters in images. They work reasonably well for low-level vision tasks but cannot automatically adapt to new data distributions or complex visual concepts."
  },
  {
    "objectID": "ML/cnn-unsupervised-learning.html#hand-crafted-convolution-kernels",
    "href": "ML/cnn-unsupervised-learning.html#hand-crafted-convolution-kernels",
    "title": "Chapter 9.9: Unsupervised or Semi-Supervised Feature Learning",
    "section": "",
    "text": "Before modern deep learning, visual features were extracted using manually designed filters such as Sobel, Laplacian, Gaussian blur, sharpening, and emboss kernels. These filters capture simple patterns‚Äîedges, gradients, corners, and blobs‚Äîbased on human intuition about what matters in images. They work reasonably well for low-level vision tasks but cannot automatically adapt to new data distributions or complex visual concepts."
  },
  {
    "objectID": "ML/cnn-unsupervised-learning.html#unsupervised-feature-learning",
    "href": "ML/cnn-unsupervised-learning.html#unsupervised-feature-learning",
    "title": "Chapter 9.9: Unsupervised or Semi-Supervised Feature Learning",
    "section": "2) Unsupervised Feature Learning",
    "text": "2) Unsupervised Feature Learning\nTo avoid manually designing all filters, researchers explored unsupervised or semi-supervised methods that automatically learn useful features from unlabeled data. Approaches such as sparse coding, autoencoders, k-means feature extraction, and clustering-based templates attempted to replace hand-crafted kernels by discovering edge-like or texture-like patterns directly from raw images without labels. These methods were effective for simple patterns and could reduce the need for labeled data, but they were limited in scale, depth, and representational power."
  },
  {
    "objectID": "ML/cnn-unsupervised-learning.html#why-modern-systems-still-rely-on-cnns-to-learn-features",
    "href": "ML/cnn-unsupervised-learning.html#why-modern-systems-still-rely-on-cnns-to-learn-features",
    "title": "Chapter 9.9: Unsupervised or Semi-Supervised Feature Learning",
    "section": "3) Why Modern Systems Still Rely on CNNs to Learn Features",
    "text": "3) Why Modern Systems Still Rely on CNNs to Learn Features\nAlthough hand-crafted kernels and unsupervised feature learning provided meaningful progress, they cannot match the expressiveness, depth, and adaptability of convolutional neural networks. CNNs learn hierarchical features end-to-end‚Äîfrom simple edges to complex textures, shapes, and object parts‚Äîoptimized directly for the final task. CNNs scale to large datasets, capture richer invariances, and consistently outperform manually designed or shallow unsupervised methods. As a result, feature extraction today is dominated by CNNs (or even more powerful architectures like Vision Transformers), making handcrafted filters and early unsupervised methods largely obsolete except for instructional or specialized uses.\n\n Figure: Classic hand-crafted convolution kernels including Sobel (edge detection), Laplacian (corners and edges), Gaussian blur (smoothing), sharpening, and emboss filters. These filters were the foundation of computer vision before deep learning, capturing simple patterns based on human intuition about visual features."
  },
  {
    "objectID": "ML/cnn-unsupervised-learning.html#key-insight",
    "href": "ML/cnn-unsupervised-learning.html#key-insight",
    "title": "Chapter 9.9: Unsupervised or Semi-Supervised Feature Learning",
    "section": "Key Insight",
    "text": "Key Insight\nThe evolution from hand-crafted to learned features represents a fundamental shift in computer vision. Hand-crafted kernels like Sobel and Laplacian were limited to simple, predefined patterns. Unsupervised learning methods (sparse coding, autoencoders, k-means) attempted to discover features automatically but lacked depth and scalability. Modern CNNs surpass both by learning hierarchical features end-to-end‚Äîfrom low-level edges to high-level semantic concepts‚Äîoptimized directly for the task at hand. This adaptability and representational power explain why CNNs became the dominant paradigm in visual feature extraction."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html",
    "href": "Math/EE364A/ee364a-lecture1-intro.html",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "",
    "text": "Constraint Optimization Problems\nMinimize \\(f_0(x)\\), subject to \\(f_i(x) \\leq b_i\\), \\(i=1,2,...,m\\)\n\n\\(x=(x_1,...,x_n)\\): optimization variable\n\\(f_0: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\): objective function\n\\(f_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), \\(i=1,2,...,m\\): constraint functions\n\nSolution or optimal point \\(x^*\\) has the smallest value of \\(f_0\\) among all vectors that satisfy the constraints."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#mathematical-definition",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#mathematical-definition",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Mathematical Definition",
    "text": "Mathematical Definition\nConstraint Optimization Problems\nMinimize \\(f_0(x)\\), subject to \\(f_i(x) \\leq b_i\\), \\(i=1,2,...,m\\)\n\n\\(x=(x_1,...,x_n)\\): optimization variable\n\\(f_0: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\): objective function\n\\(f_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), \\(i=1,2,...,m\\): constraint functions\n\nSolution or optimal point \\(x^*\\) has the smallest value of \\(f_0\\) among all vectors that satisfy the constraints."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#examples",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#examples",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Examples",
    "text": "Examples\n\nProfit Optimization\n\nVariables: amount invested in different assets\nConstraints: budget, max/min per asset, minimum return\nObjective: overall risk or return variance\n\n\n\nDevice Sizing in Electronic Circuits\n\nVariables: device width and lengths\nConstraints: manufacturing limits, timing requirements, maximum area\nObjective: power consumption\n\n\n\nData Fitting\n\nVariables: model parameters\nConstraints: prior information, parameter limits\nObjective: measure of misfit or prediction error, plus regularization term"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#solving-optimization-problems",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#solving-optimization-problems",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Solving Optimization Problems",
    "text": "Solving Optimization Problems\n\nGeneral Optimization Problems\n\nDifficult to solve\nSome compromise: very long computation time, or not always finding the solution\n\n\n\nExceptions\n\nLeast square problems\nLinear programming\nConvex optimization"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#least-squares",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#least-squares",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Least Squares",
    "text": "Least Squares\nMinimize \\(\\|Ax-b\\|_2^2\\)\n\nAnalytical solution: \\(x^* = (A^\\top A)^{-1}A^\\top b\\)\nComputation time: proportional to \\(n^2k\\) (where \\(A \\in \\mathbb{R}^{k \\times n}\\))"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#linear-programming",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#linear-programming",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Linear Programming",
    "text": "Linear Programming\nMinimize \\(c^\\top x\\)\nSubject to \\(a_i^\\top x \\leq b_i\\), \\(i=1,2,...,m\\)\n\nNo analytical solution\nReliable and efficient algorithms, software available\nComputation time: proportional to \\(n^2m\\) (if \\(m \\geq n\\))"
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#convex-optimization",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#convex-optimization",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Convex Optimization",
    "text": "Convex Optimization\nMinimize \\(f_0(x)\\)\nSubject to \\(f_i(x) \\leq b_i\\)\nConvexity requirement: objective and constraint functions are convex:\n\\[f(\\alpha x + \\beta y) \\leq \\alpha f(x) + \\beta f(y)\\]\nwhere \\(\\alpha &gt; 0\\), \\(\\beta &gt; 0\\), \\(\\alpha + \\beta = 1\\)\n\nLeast squares and linear programming are special cases\nNo analytical solution\nComputation time: proportional to \\(\\max(n^3, n^2m, F)\\), where \\(F\\) is the cost of evaluating \\(f_i\\)‚Äôs and their first and second derivatives\n\n Figure: Comparing least squares, linear programming, and convex optimization in terms of complexity and solution methods. Least squares has analytical solutions, linear programming has polynomial-time algorithms, and convex optimization generalizes both while maintaining tractability."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#example-lamp-illumination-optimization",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#example-lamp-illumination-optimization",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Example: Lamp Illumination Optimization",
    "text": "Example: Lamp Illumination Optimization\nProblem: There are \\(m\\) lamps illuminating \\(n\\) patches. The goal is to choose lamp powers so that all patches have illumination \\(I_k\\) close to a desired value \\(I_{\\text{des}}\\), by minimizing the maximum log-illumination error, subject to power constraints.\n Figure: Lamp illumination optimization problem. Given m lamps and n patches, find lamp powers that achieve uniform illumination across all patches while respecting power constraints. This is a convex optimization problem that can be solved efficiently."
  },
  {
    "objectID": "Math/EE364A/ee364a-lecture1-intro.html#key-insight",
    "href": "Math/EE364A/ee364a-lecture1-intro.html#key-insight",
    "title": "Lecture 1: Introduction to Convex Optimization",
    "section": "Key Insight",
    "text": "Key Insight\nConvex optimization bridges the gap between tractable special cases and general nonlinear programming. Least squares problems have closed-form solutions but are limited in expressiveness. General optimization is flexible but computationally intractable. Convex optimization occupies a sweet spot: it generalizes least squares and linear programming while maintaining polynomial-time solvability through interior-point methods. This makes convex optimization the foundation for practical applications in machine learning, control systems, signal processing, and engineering design."
  }
]