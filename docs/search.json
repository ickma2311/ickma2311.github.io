[
  {
    "objectID": "ML/xor-deep-learning.html",
    "href": "ML/xor-deep-learning.html",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "",
    "text": "This recap of Deep Learning Chapter 6.1 shows how ReLU activations let neural networks solve the XOR problem that defeats any linear model.\n📓 For a deeper dive with additional exercises and analysis, see the complete notebook on GitHub."
  },
  {
    "objectID": "ML/xor-deep-learning.html#the-xor-problem-a-challenge-for-linear-models",
    "href": "ML/xor-deep-learning.html#the-xor-problem-a-challenge-for-linear-models",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "The XOR Problem: A Challenge for Linear Models",
    "text": "The XOR Problem: A Challenge for Linear Models\nXOR (Exclusive OR) returns 1 precisely when the two binary inputs differ:\n\\[\\text{XOR}(x_1, x_2) = \\begin{pmatrix}0 & 1\\\\1 & 0\\end{pmatrix}\\]\nThe XOR truth table shows why this is challenging for linear models - the positive class (1) appears at diagonally opposite corners, making it impossible to separate with any single straight line.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Define XOR dataset\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([0, 1, 1, 0])\n\nprint(\"XOR Truth Table:\")\nprint(\"================\")\nprint()\nprint(\"┌─────────┬────────┐\")\nprint(\"│ Input   │ Output │\")\nprint(\"│ (x₁,x₂) │  XOR   │\")\nprint(\"├─────────┼────────┤\")\nfor i in range(4):\n    input_str = f\"({X[i,0]}, {X[i,1]})\"\n    output_str = f\"{y[i]}\"\n    print(f\"│ {input_str:7} │   {output_str:2}   │\")\nprint(\"└─────────┴────────┘\")\nprint()\nprint(\"Notice: XOR = 1 when inputs differ, XOR = 0 when inputs match\")\n\n\nXOR Truth Table:\n================\n\n┌─────────┬────────┐\n│ Input   │ Output │\n│ (x₁,x₂) │  XOR   │\n├─────────┼────────┤\n│ (0, 0)  │   0    │\n│ (0, 1)  │   1    │\n│ (1, 0)  │   1    │\n│ (1, 1)  │   0    │\n└─────────┴────────┘\n\nNotice: XOR = 1 when inputs differ, XOR = 0 when inputs match"
  },
  {
    "objectID": "ML/xor-deep-learning.html#limitation-1-single-layer-linear-model",
    "href": "ML/xor-deep-learning.html#limitation-1-single-layer-linear-model",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Limitation 1: Single Layer Linear Model",
    "text": "Limitation 1: Single Layer Linear Model\nA single layer perceptron can only create linear decision boundaries. Let’s see what happens when we try to solve XOR with logistic regression:\n\n\nShow code\n# Demonstrate single layer linear model failure\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\ncolors = ['red', 'blue']\n\n# Plot XOR data\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'Class {i}', edgecolors='black', linewidth=2)\n\n# Overlay representative linear separators to illustrate the impossibility\nx_line = np.linspace(-0.2, 1.2, 100)\nax.plot(x_line, 0.5 * np.ones_like(x_line), '--', color='gray', alpha=0.7, label='candidate lines')\nax.plot(0.5 * np.ones_like(x_line), x_line, '--', color='orange', alpha=0.7)\nax.plot(x_line, x_line, '--', color='green', alpha=0.7)\nax.plot(x_line, 1 - x_line, '--', color='purple', alpha=0.7)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x₁', fontsize=12)\nax.set_ylabel('x₂', fontsize=12)\nax.set_title('XOR Problem: No Linear Solution', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Fit logistic regression just to report its performance\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\naccuracy = log_reg.score(X, y)\nprint(f'Single layer model accuracy: {accuracy:.1%} - still misclassifies XOR.')\n\n\n\n\n\n\n\n\n\nSingle layer model accuracy: 50.0% - still misclassifies XOR."
  },
  {
    "objectID": "ML/xor-deep-learning.html#limitation-2-multiple-layer-linear-model-without-activation",
    "href": "ML/xor-deep-learning.html#limitation-2-multiple-layer-linear-model-without-activation",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Limitation 2: Multiple Layer Linear Model (Without Activation)",
    "text": "Limitation 2: Multiple Layer Linear Model (Without Activation)\nEven stacking multiple linear layers doesn’t help! Multiple linear transformations are mathematically equivalent to a single linear transformation.\nMathematical proof:\n\\[\\text{Layer 1: } h_1 = W_1 x + b_1\\] \\[\\text{Layer 2: } h_2 = W_2 h_1 + b_2 = W_2(W_1 x + b_1) + b_2 = (W_2W_1)x + (W_2b_1 + b_2)\\]\nResult: Still just \\(Wx + b\\) (a single linear transformation)\nConclusion: Stacking linear layers without activation functions doesn’t increase the model’s expressive power!"
  },
  {
    "objectID": "ML/xor-deep-learning.html#the-solution-relu-activation-function",
    "href": "ML/xor-deep-learning.html#the-solution-relu-activation-function",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "The Solution: ReLU Activation Function",
    "text": "The Solution: ReLU Activation Function\nReLU (Rectified Linear Unit) provides the nonlinearity needed to solve XOR: - ReLU(z) = max(0, z) - Clips negative values to zero, keeping positive values unchanged\nUsing the hand-crafted network from the next code cell, the forward pass can be written compactly in matrix form:\n\\[\nX = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix},\n\\quad\nW_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix},\n\\quad\nb_1 = \\begin{bmatrix} 0 & 0 \\end{bmatrix}\n\\]\n\\[\nZ = X W_1^{\\top} + b_1 = \\begin{bmatrix} 0 & 0 \\\\ -1 & 1 \\\\ 1 & -1 \\\\ 0 & 0 \\end{bmatrix},\n\\qquad\nH = \\text{ReLU}(Z) = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \\end{bmatrix}\n\\]\nWith output parameters \\[\nw_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix},\n\\quad\nb_2 = -0.5\n\\] the final linear scores are \\[\na = H w_2^{\\top} + b_2 = \\begin{bmatrix} -0.5 \\\\ 0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}\n\\Rightarrow\n\\text{sign}_+(a) = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\n\\]\nHere \\(\\text{sign}_+(a)\\) maps non-negative entries to 1 and negative entries to 0. Let’s see how ReLU transforms the XOR problem to make it solvable.\n\n\nShow code\n# Hand-crafted network weights and biases that solve XOR\nfrom IPython.display import display, Math\n\ndef relu(z):\n    return np.maximum(0, z)\n\nW1 = np.array([[1, -1],\n               [-1, 1]])\nb1 = np.array([0, 0])\nw2 = np.array([1, 1])\nb2 = -0.5\n\ndisplay(Math(r\"\\text{Hidden layer: } W_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}\"))\ndisplay(Math(r\"b_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\"))\ndisplay(Math(r\"\\text{Output layer: } w_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix}\"))\ndisplay(Math(r\"b_2 = -0.5\"))\n\ndef forward_pass(X, W1, b1, w2, b2):\n    z1 = X @ W1.T + b1\n    h1 = relu(z1)\n    logits = h1 @ w2 + b2\n    return logits, h1, z1\n\nlogits, hidden_activations, pre_activations = forward_pass(X, W1, b1, w2, b2)\npredictions = (logits &gt;= 0).astype(int)\n\nprint(\"Step-by-step Forward Pass Results:\")\nprint(\"=\" * 80)\nprint()\nprint(\"┌─────────┬──────────────────┬──────────────────┬─────────┬──────────┐\")\nprint(\"│ Input   │  Before ReLU     │  After ReLU      │  Logit  │   Pred   │\")\nprint(\"│ (x₁,x₂) │    (z₁, z₂)      │    (h₁, h₂)      │  score  │  class   │\")\nprint(\"├─────────┼──────────────────┼──────────────────┼─────────┼──────────┤\")\nfor i in range(len(X)):\n    x1, x2 = X[i]\n    z1_vals = pre_activations[i]\n    h1_vals = hidden_activations[i]\n    logit = logits[i]\n    pred = predictions[i]\n    \n    input_str = f\"({x1:.0f}, {x2:.0f})\"\n    pre_relu_str = f\"({z1_vals[0]:4.1f}, {z1_vals[1]:4.1f})\"\n    post_relu_str = f\"({h1_vals[0]:4.1f}, {h1_vals[1]:4.1f})\"\n    logit_str = f\"{logit:6.2f}\"\n    pred_str = f\"{pred:4d}\"\n    \n    print(f\"│ {input_str:7} │ {pre_relu_str:16} │ {post_relu_str:16} │ {logit_str:7} │ {pred_str:8} │\")\nprint(\"└─────────┴──────────────────┴──────────────────┴─────────┴──────────┘\")\n\naccuracy = (predictions == y).mean()\nprint(f\"\\nNetwork Accuracy: {accuracy:.0%} ✅\")\nprint(\"\\nKey transformations:\")\nprint(\"• (-1, 1) → (0, 1) makes XOR(0,1) = 1 separable\")\nprint(\"• ( 1,-1) → (1, 0) makes XOR(1,0) = 1 separable\")\n\n\n\\(\\displaystyle \\text{Hidden layer: } W_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle b_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle \\text{Output layer: } w_2 = \\begin{bmatrix} 1 & 1 \\end{bmatrix}\\)\n\n\n\\(\\displaystyle b_2 = -0.5\\)\n\n\nStep-by-step Forward Pass Results:\n================================================================================\n\n┌─────────┬──────────────────┬──────────────────┬─────────┬──────────┐\n│ Input   │  Before ReLU     │  After ReLU      │  Logit  │   Pred   │\n│ (x₁,x₂) │    (z₁, z₂)      │    (h₁, h₂)      │  score  │  class   │\n├─────────┼──────────────────┼──────────────────┼─────────┼──────────┤\n│ (0, 0)  │ ( 0.0,  0.0)     │ ( 0.0,  0.0)     │  -0.50  │    0     │\n│ (0, 1)  │ (-1.0,  1.0)     │ ( 0.0,  1.0)     │   0.50  │    1     │\n│ (1, 0)  │ ( 1.0, -1.0)     │ ( 1.0,  0.0)     │   0.50  │    1     │\n│ (1, 1)  │ ( 0.0,  0.0)     │ ( 0.0,  0.0)     │  -0.50  │    0     │\n└─────────┴──────────────────┴──────────────────┴─────────┴──────────┘\n\nNetwork Accuracy: 100% ✅\n\nKey transformations:\n• (-1, 1) → (0, 1) makes XOR(0,1) = 1 separable\n• ( 1,-1) → (1, 0) makes XOR(1,0) = 1 separable\n\n\n\nTransformation Table: How ReLU Solves XOR\nLet’s trace through exactly what happens to each input:\n\n\nShow code\n\n# Create detailed transformation table\nprint(\"Complete Transformation Table:\")\nprint(\"=============================\")\nprint()\nprint(\"Input   | Pre-ReLU  | Post-ReLU | Logit | Prediction | Target | Correct?\")\nprint(\"(x₁,x₂) | (z₁, z₂)  | (h₁, h₂)  | score | class      | y      |\")\nprint(\"--------|-----------|-----------|-------|------------|--------|----------\")\n\nfor i in range(4):\n    input_str = f\"({X[i,0]},{X[i,1]})\"\n    pre_relu_str = f\"({pre_activations[i,0]:2.0f},{pre_activations[i,1]:2.0f})\"\n    post_relu_str = f\"({hidden_activations[i,0]:.0f},{hidden_activations[i,1]:.0f})\"\n    logit_str = f\"{logits[i]:.2f}\"\n    pred_str = f\"{predictions[i]}\"\n    target_str = f\"{y[i]}\"\n    correct_str = \"✓\" if predictions[i] == y[i] else \"✗\"\n\n    print(f\"{input_str:7} | {pre_relu_str:9} | {post_relu_str:9} | {logit_str:5} | {pred_str:10} | {target_str:6} | {correct_str}\")\n\nprint()\nprint(\"Key Insight: ReLU transforms (-1,1) → (0,1) and (1,-1) → (1,0)\")\nprint(\"This makes the XOR classes linearly separable in the hidden space!\")\n\n\nComplete Transformation Table:\n=============================\n\nInput   | Pre-ReLU  | Post-ReLU | Logit | Prediction | Target | Correct?\n(x₁,x₂) | (z₁, z₂)  | (h₁, h₂)  | score | class      | y      |\n--------|-----------|-----------|-------|------------|--------|----------\n(0,0)   | ( 0, 0)   | (0,0)     | -0.50 | 0          | 0      | ✓\n(0,1)   | (-1, 1)   | (0,1)     | 0.50  | 1          | 1      | ✓\n(1,0)   | ( 1,-1)   | (1,0)     | 0.50  | 1          | 1      | ✓\n(1,1)   | ( 0, 0)   | (0,0)     | -0.50 | 0          | 0      | ✓\n\nKey Insight: ReLU transforms (-1,1) → (0,1) and (1,-1) → (1,0)\nThis makes the XOR classes linearly separable in the hidden space!\n\n\n\n\nStep 1: Original Input Space\nThe XOR problem in its raw form - notice how no single line can separate the classes:\n\n\nShow code\n# Step 1 visualization: Original Input Space\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'XOR = {i}', edgecolors='black', linewidth=2)\n\n# Annotate each point\nfor i in range(4):\n    ax.annotate(f'({X[i,0]},{X[i,1]})', X[i], xytext=(10, 10), \n                textcoords='offset points', fontsize=10)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x₁', fontsize=12)\nax.set_ylabel('x₂', fontsize=12)\nax.set_title('Step 1: Original Input Space', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Linear Transformation (Before ReLU)\nThe network applies weights W₁ and biases b₁ to transform the input space:\n\n\nShow code\n# Step 2 visualization: Pre-activation space\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\nfor i in range(4):\n    ax.scatter(pre_activations[i, 0], pre_activations[i, 1], \n               c=colors[y[i]], s=200, edgecolors='black', linewidth=2)\n\n# Draw ReLU boundaries\nax.axhline(0, color='gray', linestyle='-', alpha=0.8, linewidth=2)\nax.axvline(0, color='gray', linestyle='-', alpha=0.8, linewidth=2)\n\n# Shade all regions where coordinates turn negative (and thus get clipped by ReLU)\nax.axvspan(-1.2, 0, alpha=0.15, color='red')\nax.axhspan(-1.2, 0, alpha=0.15, color='red')\nax.text(-0.75, 0.85, 'Negative z₁ → ReLU sets to 0', ha='left', fontsize=10,\n        bbox=dict(boxstyle='round', facecolor='red', alpha=0.25))\nax.text(0.95, -0.75, 'Negative z₂ → ReLU sets to 0', ha='right', fontsize=10,\n        bbox=dict(boxstyle='round', facecolor='red', alpha=0.25))\n\n# Annotate points with input labels\nlabels = ['(0,0)', '(0,1)', '(1,0)', '(1,1)']\nfor i, label in enumerate(labels):\n    pre_coord = f'({pre_activations[i,0]:.0f},{pre_activations[i,1]:.0f})'\n    ax.annotate(f'{label}→{pre_coord}', pre_activations[i], xytext=(10, 10), \n                textcoords='offset points', fontsize=9)\n\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1.2, 1.2)\nax.set_xlabel('z₁ (Pre-activation)', fontsize=12)\nax.set_ylabel('z₂ (Pre-activation)', fontsize=12)\nax.set_title('Step 2: Before ReLU (Linear Transform)', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 3: ReLU Transformation\nReLU clips negative values to zero, transforming the space to make it linearly separable:\n\n\nShow code\n# Step 3 visualization: ReLU transformation with arrows\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\n\nfor i in range(4):\n    # Pre-ReLU positions (X marks)\n    ax.scatter(pre_activations[i, 0], pre_activations[i, 1], \n               marker='x', s=150, c=colors[y[i]], alpha=0.5, linewidth=3)\n    # Post-ReLU positions (circles) \n    ax.scatter(hidden_activations[i, 0], hidden_activations[i, 1], \n               marker='o', s=200, c=colors[y[i]], edgecolors='black', linewidth=2)\n    \n    # Draw transformation arrows\n    start = pre_activations[i]\n    end = hidden_activations[i]\n    if not np.array_equal(start, end):\n        ax.annotate('', xy=end, xytext=start,\n                    arrowprops=dict(arrowstyle='-&gt;', lw=2, color=colors[y[i]], alpha=0.8))\n\n\n# Add text box explaining the key transformation\nax.text(0.5, 0.8, 'ReLU clips negative coordinates to zero\\n(-1,1) → (0,1) and (1,-1) → (1,0)', \n        ha='center', va='center', fontsize=11, \n        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n\nax.set_xlim(-1.2, 1.1)\nax.set_ylim(-1.2, 1.1)\nax.set_xlabel('Hidden dimension 1', fontsize=12)\nax.set_ylabel('Hidden dimension 2', fontsize=12)\nax.set_title('Step 3: ReLU Mapping (Before → After)', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Final Classification\nWith the transformed hidden representation, the network can now perfectly classify XOR:\n\n\nShow code\n\n\n# Step 4 visualization: Final classification results\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\n\n# Create decision boundary\nxx, yy = np.meshgrid(np.linspace(-0.2, 1.2, 100), np.linspace(-0.2, 1.2, 100))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\ngrid_logits, _, _ = forward_pass(grid_points, W1, b1, w2, b2)\ngrid_preds = (grid_logits &gt;= 0).astype(int).reshape(xx.shape)\n\nax.contourf(xx, yy, grid_preds, levels=[-0.5, 0.5, 1.5], \n            colors=['#ffcccc', '#ccccff'], alpha=0.6)\nax.contour(xx, yy, grid_logits.reshape(xx.shape), levels=[0], \n           colors='black', linewidths=2, linestyles='--')\n\nfor i in range(2):\n    mask = y == i\n    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=200, \n               label=f'XOR = {i}', edgecolors='black', linewidth=2)\n\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.set_xlabel('x₁', fontsize=12)\nax.set_ylabel('x₂', fontsize=12)\nax.set_title('Step 4: Final Classification (100% Accuracy)', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nsample_logits, _, _ = forward_pass(X, W1, b1, w2, b2)\nsample_preds = (sample_logits &gt;= 0).astype(int)\nfor i in range(4):\n    pred_text = f'Pred: {sample_preds[i]}'\n    ax.annotate(pred_text, X[i], xytext=(10, -15), \n                textcoords='offset points', fontsize=9,\n                bbox=dict(boxstyle='round,pad=0.2', facecolor='lightgreen', alpha=0.7))\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ML/xor-deep-learning.html#conclusion",
    "href": "ML/xor-deep-learning.html#conclusion",
    "title": "Deep Learning Book 6.1: XOR Problem & ReLU Networks",
    "section": "Conclusion",
    "text": "Conclusion\nThe XOR problem demonstrates several fundamental principles in deep learning:\n\nNecessity of Nonlinearity: Linear models cannot solve XOR, establishing the critical role of nonlinear activation functions.\nUniversal Approximation: Even simple architectures with sufficient nonlinearity can solve complex classification problems."
  }
]