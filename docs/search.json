[
  {
    "objectID": "ML/kmeans.html",
    "href": "ML/kmeans.html",
    "title": "K-means Clustering",
    "section": "",
    "text": "Setup points and K\nwe will implement a KNN algorithm to cluster the points\n\n\nX=[[1,1],[2,2.1],[3,2.5],[6,7],[7,7.1],[9,7.5]]\nk=2\n\nmax_iter=3\n\n\n# Visualize the data\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter([x[0] for x in X],[x[1] for x in X])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pure python implementation of K-means clustering\ndef knn_iter(X,centroids):\n    # set up new clusters\n    new_clusters=[[] for _ in range(len(centroids))]\n    # k=len(centroids)\n    # assign each point to the nearest centroid\n    for x in X:\n        k,distance=0,(x[0]-centroids[0][0])**2+(x[1]-centroids[0][1])**2\n        for i,c in enumerate(centroids[1:],1):\n            if (x[0]-c[0])**2+(x[1]-c[1])**2&lt;distance:\n                k=i\n                distance=(x[0]-c[0])**2+(x[1]-c[1])**2\n        new_clusters[k].append(x)\n    \n    # calculate new centroids\n    new_centroids=[[\n        sum([x[0] for x in cluster])/len(cluster),\n        sum([x[1] for x in cluster])/len(cluster)\n    ] if cluster else centroids[i] for i,cluster in enumerate(new_clusters)]\n    return new_centroids\n\n\n\n\n\n\n\n\ndef iter_and_draw(X,k,max_iter):\n    centroids=X[:k]  # Randomly select 2 centroids\n    fig, axes = plt.subplots(max_iter//3+(1 if max_iter%3!=0 else 0),\n        3, figsize=(15, 10))\n    axes=axes.flatten()\n    for i in range(max_iter):\n        \n        # Plot points and centroids\n\n\n        # Assign each point to nearest centroid and plot with corresponding color\n        colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n        for j, x in enumerate(X):\n            # Find nearest centroid\n            min_dist = float('inf')\n            nearest_centroid = 0\n            for k, c in enumerate(centroids):\n                dist = (x[0]-c[0])**2 + (x[1]-c[1])**2\n                if dist &lt; min_dist:\n                    min_dist = dist\n                    nearest_centroid = k\n            # Plot point with color corresponding to its cluster\n            axes[i].scatter(x[0], x[1], c=colors[nearest_centroid % len(colors)], label=f'Cluster {nearest_centroid+1}' if j==0 else \"\")\n        axes[i].scatter([c[0] for c in centroids], [c[1] for c in centroids], c='red', marker='*', s=200, label='Centroids')\n        axes[i].set_title(f'Iteration {i}')\n        centroids = knn_iter(X, centroids)\n\n    plt.tight_layout()\n    plt.show()\n\niter_and_draw(X,k,max_iter)\n# print(centroids)\n\n\n\n\n\n\n\n\n\n# A 3 clusters example\n\nimport numpy as np\n\nX1=np.random.rand(20,2)+5 # Some points in the upper right corner\nX2=np.random.rand(20,2)+3 # Some points in the middle\nX3=np.random.rand(20,2) # Some points in the lower left corner\n\niter_and_draw(np.concatenate((X1,X2,X3)),3,5)\n\n\n\n\n\n\n\n\n\n\nA question?\n\nWhat to do if one cluster has no assigned points during iteration?\n\n\n\nFormula Derivation\nThe goal is to minimize the loss of inertia which is sum of the points to cluster centroids.\n\\[\nLoss= \\sum_{i=1}^n \\sum_{x \\in C_i} ||x-\\mu_i||^2\n\\]\nTo iter \\(\\mu\\) for each cluster, let us find the derivative of the following function. \\[\nf(\\mu)=\\sum_{i=1}^n ||x_i-\\mu||^2 =\n\\sum_{i=1}^n {x_i}^2+\\mu^2-2x_i\\mu\n\\]\nGiven a \\(\\nabla \\mu\\), \\[\nf(\\mu + \\nabla \\mu)=\\sum_{i=1}^n ||x_i+\\nabla \\mu -\\mu||^2 =\n\\sum_{i=1}^n  {x_i}^2+\\mu^2+{\\nabla \\mu}^2-2{x_i \\mu}-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\nf(\\mu + \\nabla \\mu)-f(\\mu)=\n\\sum_{i=1}^n {\\nabla \\mu}^2-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\n\\frac {f(\\mu + \\nabla \\mu)-f(\\mu)}{\\nabla \\mu}=\\sum_{i=1}^n {\\nabla \\mu} -2 \\mu +2{x_i} = 2\\sum_{i=1}^n x_i - 2n\\mu\n\\]\nNow we can see if \\(n\\mu = \\sum_{i=1}^n x_i\\), then the derivative is 0, this is why in each iteration, we need to set the center of the cluster as centroid."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ickma.dev",
    "section": "",
    "text": "A growing collection of structured study notes and visual explanations ‚Äî written for clarity, reproducibility, and long-term memory."
  },
  {
    "objectID": "index.html#deep-learning-book",
    "href": "index.html#deep-learning-book",
    "title": "ickma.dev",
    "section": "Deep Learning Book",
    "text": "Deep Learning Book\n\n\nClick to expand/collapse Deep Learning chapters\n\n\n\nChapter 6.1: XOR Problem & ReLU Networks How ReLU solves problems that linear models cannot handle.\n\n\nChapter 6.2: Likelihood-Based Loss Functions The mathematical connection between probabilistic models and loss functions.\n\n\nChapter 6.3: Hidden Units and Activation Functions Exploring activation functions and their impact on neural network learning.\n\n\nChapter 6.4: Architecture Design - Depth vs Width How depth enables hierarchical feature reuse and exponential expressiveness.\n\n\nChapter 6.5: Back-Propagation and Other Differentiation Algorithms The algorithm that makes training deep networks computationally feasible.\n\n\nChapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature Essential second-order calculus concepts needed before Chapter 7.\n\n\nChapter 7.1.1: L2 Regularization How L2 regularization shrinks weights based on Hessian eigenvalues.\n\n\nChapter 7.1.2: L1 Regularization L1 regularization uses soft thresholding to create sparse solutions.\n\n\nChapter 7.2: Constrained Optimization View of Regularization Regularization as constrained optimization with KKT conditions.\n\n\nChapter 7.3: Regularization and Under-Constrained Problems Why regularization is mathematically necessary and ensures invertibility.\n\n\nChapter 7.4: Dataset Augmentation How transforming existing data improves generalization.\n\n\nChapter 7.5: Noise Robustness How adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\nChapter 7.6: Semi-Supervised Learning Leveraging unlabeled data to improve model performance when labeled data is scarce.\n\n\nChapter 7.7: Multi-Task Learning Training a single model on multiple related tasks to improve generalization.\n\n\nChapter 7.8: Early Stopping Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.\n\n\nChapter 7.9: Parameter Tying and Parameter Sharing Two strategies for reducing parameters: encouraging similarity vs.¬†enforcing identity.\n\n\nChapter 7.10: Sparse Representations Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.\n\n\nChapter 7.11: Bagging and Other Ensemble Methods How training multiple models on bootstrap samples and averaging their predictions reduces variance.\n\n\nChapter 7.12: Dropout Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.\n\n\nChapter 7.13: Adversarial Training How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.\n\n\nChapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.\n\n\nChapter 8.1: How Learning Differs from Pure Optimization Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.\n\n\nChapter 8.2: Challenges in Deep Learning Optimization Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.\n\n\nChapter 8.3: Basic Algorithms SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.\n\n\nChapter 8.4: Parameter Initialization Strategies Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.\n\n\nChapter 8.5: Algorithms with Adaptive Learning Rates From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.\n\n\nChapter 8.6: Second-Order Optimization Methods Newton‚Äôs method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.\n\n\nChapter 8.7: Optimization Strategies and Meta-Algorithms Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.\n\n\nChapter 9.1: Convolution Computation The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.\n\n\nChapter 9.2: Motivation for Convolutional Networks Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m¬∑n) to O(k¬∑n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere‚Äîachieving 30,000√ó speedup over dense layers.\n\n\nChapter 9.3: Pooling Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28√ó28 feature maps to 14√ó14 (4√ó fewer activations). Comparing three architectures‚Äîstrided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.\n\n\nChapter 9.4: Convolution and Pooling as an Infinitely Strong Prior Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off‚Äîstrong priors reduce sample complexity but only when assumptions match the data structure.\n\n\nChapter 9.5: Convolutional Functions Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula \\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\), stride for downsampling, three padding strategies (valid/same/full), and gradient computation‚Äîkernel gradients via correlation with input, input gradients via convolution with flipped kernel."
  },
  {
    "objectID": "index.html#mathematics",
    "href": "index.html#mathematics",
    "title": "ickma.dev",
    "section": "Mathematics",
    "text": "Mathematics\n\nMy journey through MIT‚Äôs Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.\n\n\n\nClick to expand/collapse MIT 18.06SC Linear Algebra\n\n\nReflections & Synthesis\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation.\n\n\nFrom Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series The beautiful mathematical journey from Taylor expansions to Euler‚Äôs formula and ultimately to Fourier series, revealing deep connections between polynomials, exponentials, and trigonometric functions.\n\n\n\nMIT 18.06SC Linear Algebra\n\n\nLecture 1: The Geometry of Linear Equations Two powerful perspectives: row picture vs column picture.\n\n\nLecture 2: Elimination with Matrices The systematic algorithm that transforms linear systems into upper triangular form.\n\n\nLecture 3: Matrix Multiplication and Inverse Five different perspectives on matrix multiplication.\n\n\nLecture 4: LU Decomposition Factoring matrices into Lower √ó Upper triangular form.\n\n\nLecture 5.1: Permutation Matrices Permutation matrices reorder rows and columns.\n\n\nLecture 5.2: Transpose The transpose operation switches rows to columns.\n\n\nLecture 5.3: Vector Spaces Vector spaces and subspaces: closed under addition and scalar multiplication.\n\n\nLecture 6: Column Space and Null Space Column space determines which \\(b\\) make \\(Ax = b\\) solvable.\n\n\nLecture 7: Solving Ax=0 Systematic algorithm to find null space using pivot/free variables and RREF.\n\n\nLecture 8: Solving Ax=b Complete solution is particular solution plus null space.\n\n\nLecture 9: Independence, Basis, and Dimension Linear independence, basis, and dimension. Rank-nullity theorem.\n\n\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix.\n\n\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.\n\n\nLecture 12: Graphs, Networks, and Incidence Matrices Applying linear algebra to graph theory: incidence matrices, Kirchhoff‚Äôs laws, and Euler‚Äôs formula.\n\n\nLecture 13: Quiz 1 Review Practice problems reviewing key concepts: subspaces, rank, null spaces, and the four fundamental subspaces.\n\n\nLecture 14: Orthogonal Vectors and Subspaces Orthogonal vectors, orthogonal subspaces, and the fundamental relationships between the four subspaces.\n\n\nLecture 15: Projection onto Subspaces Projecting onto lines and subspaces, the geometry of least squares, and deriving normal equations.\n\n\nLecture 16: Projection Matrices and Least Squares Projection matrices, least squares fitting, and the connection between linear algebra and calculus optimization.\n\n\nLecture 17: Orthogonal Matrices and Gram-Schmidt Orthonormal matrices, the Gram-Schmidt process for orthogonalization, and QR factorization.\n\n\nLecture 18: Properties of Determinants Ten fundamental properties that completely define the determinant and reveal when matrices are invertible.\n\n\nLecture 19: Determinant Formulas and Cofactors Three computational methods for determinants: pivots, the big formula, and cofactor expansion.\n\n\nLecture 20: Inverse & Volume The inverse matrix formula using cofactors, Cramer‚Äôs rule for solving linear systems, and the geometric interpretation of determinants as volume.\n\n\nLecture 21: Eigenvalues and Eigenvectors The directions that matrices can only scale, not rotate: \\(Ax = \\lambda x\\).\n\n\nLecture 22: Diagonalization and Powers of A Computing matrix powers efficiently and solving Fibonacci with eigenvalues.\n\n\nLecture 23: Differential Equations and exp(At) Connecting linear algebra to differential equations: how eigenvalues determine stability and matrix exponentials solve systems.\n\n\nLecture 24: Markov Matrices and Fourier Series Two powerful applications of eigenvalues: Markov matrices for modeling state transitions and Fourier series for decomposing functions into orthogonal components.\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 29: Singular Value Decomposition The most important matrix factorization: SVD provides orthonormal bases for all four fundamental subspaces, revealing how any matrix maps its row space to its column space through scaling by singular values.\n\n\nLecture 30: Linear Transformations and Their Matrices The fundamental connection between abstract linear transformations and concrete matrices: every linear transformation can be represented as matrix multiplication, and the choice of basis determines the matrix representation.\n\n\nLecture 31: Change of Basis and Image Compression How choosing the right basis enables compression: JPEG transforms 512√ó512 images (262,144 pixels) to Fourier basis and discards small coefficients. Three properties of good bases‚Äîfast inverse (FFT in O(n log n)), sparsity (few coefficients enough), and orthogonality (no redundancy).\n\n\nLecture 33: Left and Right Inverse; Pseudo-inverse When matrices aren‚Äôt square: full column rank matrices (\\(r = n &lt; m\\)) have left inverses \\((A^T A)^{-1} A^T\\), full row rank matrices (\\(r = m &lt; n\\)) have right inverses \\(A^T (AA^T)^{-1}\\), and the pseudo-inverse \\(A^+ = V \\Sigma^+ U^T\\) generalizes both using SVD‚Äîinverting non-zero singular values and transposing the shape.\n\n\n\n\n\nClick to expand/collapse MIT 18.065 Matrix Methods"
  },
  {
    "objectID": "index.html#more-topics",
    "href": "index.html#more-topics",
    "title": "ickma.dev",
    "section": "More Topics",
    "text": "More Topics\n\n\nMachine Learning\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\nAlgorithms\n\nDP Regex"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "",
    "text": "This lecture covers vector and matrix norms with applications to regularization and sparsity:\n\nReview vector p-norms and the geometry of unit balls\nExplain when norms are valid (triangle inequality) and the S-norm defined by \\(v^T S v\\)\nCompare \\(\\ell_1\\), \\(\\ell_2\\), and the ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù as regularizers when solving \\(Ax = b\\)\nIntroduce matrix norms (spectral, Frobenius, nuclear) and relate them to singular values"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#overview",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#overview",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "",
    "text": "This lecture covers vector and matrix norms with applications to regularization and sparsity:\n\nReview vector p-norms and the geometry of unit balls\nExplain when norms are valid (triangle inequality) and the S-norm defined by \\(v^T S v\\)\nCompare \\(\\ell_1\\), \\(\\ell_2\\), and the ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù as regularizers when solving \\(Ax = b\\)\nIntroduce matrix norms (spectral, Frobenius, nuclear) and relate them to singular values"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#vector-norms",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#vector-norms",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Vector Norms",
    "text": "Vector Norms\n\nDefinition\nThe p-norm of a vector \\(v \\in \\mathbb{R}^n\\) is defined as:\n\\[\n\\|v\\|_p = \\sqrt[p]{|v_1|^p + |v_2|^p + \\cdots + |v_n|^p}\n\\]\nCommon values: \\(p = 0, 1, 2, \\infty\\)\n\n\\(\\|v\\|_0\\): Number of nonzero components (not a true norm)\n\\(\\|v\\|_1\\): Sum of absolute values (Manhattan norm)\n\\(\\|v\\|_2\\): Euclidean length\n\\(\\|v\\|_\\infty\\): Maximum absolute value"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#geometry-of-unit-balls",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#geometry-of-unit-balls",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Geometry of Unit Balls",
    "text": "Geometry of Unit Balls\nThe unit ball \\(\\|v\\|_p = 1\\) in \\(\\mathbb{R}^2\\) has different shapes for different \\(p\\):\n Figure: Unit balls \\(\\|v\\|_p = 1\\) in \\(\\mathbb{R}^2\\) for different values of \\(p\\). As \\(p\\) increases, the unit ball transitions from diamond (\\(p=1\\)) to circle (\\(p=2\\)) to square (\\(p=\\infty\\))."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#when-is-it-a-true-norm",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#when-is-it-a-true-norm",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "When Is It a True Norm?",
    "text": "When Is It a True Norm?\nTriangle inequality requirement: \\(\\|x + y\\| \\leq \\|x\\| + \\|y\\|\\)\n\nWhen \\(p \\geq 1\\): \\(\\|\\cdot\\|_p\\) is a valid norm\nWhen \\(p &lt; 1\\): Not a true norm (triangle inequality fails)\n\n\nThe ‚Äú\\(\\frac{1}{2}\\)-Norm‚Äù\nFor \\(\\|x\\|_{1/2}\\), this is not a true norm since \\(p &lt; 1\\), but it provides a very strong sparsity penalty that pushes many components of \\(x\\) to zero when minimized.\nIntuition: A norm must satisfy the triangle inequality (going straight is never longer than going in two steps). For \\(p &lt; 1\\), the unit ball is not convex, so the triangle inequality fails.\n Figure: The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù unit ball is non-convex. The triangle inequality fails because the straight path between two points can be longer than the sum of their norms."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#s-norm",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#s-norm",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "S-Norm",
    "text": "S-Norm\nLet \\(S\\) be a symmetric positive definite matrix. The S-norm is defined as:\n\\[\n\\|v\\|_S = \\sqrt{v^T S v}\n\\]\nSpecial case: When \\(S = I\\) (identity matrix), we get \\(\\|v\\|_2\\) (Euclidean norm).\n\nExample\n\\[\nS = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3\n\\end{bmatrix}\n\\]\nThe unit ball \\(v^T S v = 1\\) forms an ellipse whose axes are determined by the eigenvectors of \\(S\\).\n Figure: The unit ball for the S-norm \\(\\sqrt{v^T S v} = 1\\) forms an ellipse. The shape and orientation depend on the eigenvalues and eigenvectors of the symmetric positive definite matrix \\(S\\)."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#minimizing-norms-regularization",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#minimizing-norms-regularization",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Minimizing Norms: Regularization",
    "text": "Minimizing Norms: Regularization\nWhen solving \\(Ax = b\\), we often want to minimize \\(\\|x\\|_p\\) to prefer certain solutions:\n\n\\(\\ell_1\\) Regularization\n\nProperty: Sparse solutions\nWinner: Solution has many zero components (e.g., \\([0, b]\\) or \\([a, 0]\\))\nUse case: Feature selection, compressed sensing\n\n\n\n\\(\\ell_2\\) Regularization\n\nProperty: Smooth, distributed solutions\nGeometric interpretation: Find the point on the constraint line \\(c_1 x_1 + c_2 x_2 = 0\\) that intersects the smallest \\(\\|v\\|_2 = c\\) level set (circle)\nUse case: Ridge regression, preventing overfitting\n\n Figure: Comparison of minimizing \\(\\ell_1\\) norm (diamond-shaped level sets, leading to sparse solutions at axes) versus \\(\\ell_2\\) norm (circular level sets, leading to distributed solutions). The constraint line intersects different norms at different points, illustrating why \\(\\ell_1\\) regularization produces sparsity."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#matrix-norms",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#matrix-norms",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Matrix Norms",
    "text": "Matrix Norms\n\nSpectral Norm: \\(\\|A\\|_2 = \\sigma_1\\)\nThe spectral norm measures the maximum ‚Äúblow-up‚Äù of a vector:\n\\[\n\\|A\\|_2 = \\max_x \\frac{\\|Ax\\|_2}{\\|x\\|_2} = \\sigma_1\n\\]\nwhere \\(\\sigma_1\\) is the largest singular value of \\(A\\).\nWinner: \\(x = v_1\\) (first right singular vector)\n\n\n\nFrobenius Norm: \\(\\|A\\|_F\\)\nThe Frobenius norm is the square root of the sum of all squared entries:\n\\[\n\\|A\\|_F = \\sqrt{a_{11}^2 + a_{12}^2 + \\cdots + a_{mn}^2}\n\\]\nConnection to SVD:\n\\[\n\\|A\\|_F = \\sqrt{\\sigma_1^2 + \\sigma_2^2 + \\cdots + \\sigma_r^2}\n\\]\nWhy? Because \\(A = U \\Sigma V^T\\), and both \\(U\\) and \\(V\\) are orthonormal matrices (they preserve \\(\\ell_2\\) norm).\n\n\n\nNuclear Norm: \\(\\|A\\|_N\\)\nThe nuclear norm (also called trace norm) is the sum of singular values:\n\\[\n\\|A\\|_N = \\sigma_1 + \\sigma_2 + \\cdots + \\sigma_r\n\\]\nUse case: Low-rank matrix completion (convex relaxation of rank minimization)"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#summary-of-matrix-norms",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#summary-of-matrix-norms",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Summary of Matrix Norms",
    "text": "Summary of Matrix Norms\n Figure: Comparison of matrix norms - spectral norm \\(\\|A\\|_2\\) (largest singular value), Frobenius norm \\(\\|A\\|_F\\) (root sum of squared singular values), and nuclear norm \\(\\|A\\|_N\\) (sum of singular values).\n\n\n\n\n\n\n\n\nNorm\nFormula\nGeometric Meaning\n\n\n\n\nSpectral\n\\(\\|A\\|_2 = \\sigma_1\\)\nMaximum amplification\n\n\nFrobenius\n\\(\\|A\\|_F = \\sqrt{\\sigma_1^2 + \\cdots + \\sigma_r^2}\\)\nRoot mean square of entries\n\n\nNuclear\n\\(\\|A\\|_N = \\sigma_1 + \\cdots + \\sigma_r\\)\nConvex surrogate for rank\n\n\n\nKey insight: All three matrix norms can be expressed in terms of singular values, connecting them to the fundamental SVD decomposition."
  },
  {
    "objectID": "index.html#latest-updates",
    "href": "index.html#latest-updates",
    "title": "ickma.dev",
    "section": "Latest Updates",
    "text": "Latest Updates\n\n‚àá Deep Learning Book 34 chapters\nMy notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\nChapter 9.6: Structured Outputs CNNs can generate high-dimensional structured objects through pixel-level predictions. Recurrent convolution refines predictions iteratively, producing dense outputs for segmentation, depth estimation, and flow prediction.\n\n\nChapter 9.5: Convolutional Functions Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula \\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\), stride for downsampling, three padding strategies (valid/same/full), and gradient computation.\n\n\nChapter 9.4: Convolution and Pooling as an Infinitely Strong Prior Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others.\n\n\n\nSee all Deep Learning chapters ‚Üí\n\n\n\nüìê MIT 18.06SC Linear Algebra 36 lectures\nMy journey through MIT‚Äôs Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\n\nSee all MIT 18.06SC lectures ‚Üí\n\n\n\nüìê MIT 18.065 Matrix Methods 1 lecture\nMy notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning.\n\n\nLecture 8: Norms of Vectors and Matrices Understanding vector p-norms (\\(\\|v\\|_p\\)) and when they satisfy the triangle inequality (only for \\(p \\geq 1\\)). The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù creates non-convex unit balls for strong sparsity.\n\n\n\nSee all MIT 18.065 lectures ‚Üí"
  },
  {
    "objectID": "ML/deep-learning-book.html",
    "href": "ML/deep-learning-book.html",
    "title": "Deep Learning Book",
    "section": "",
    "text": "My notes and implementations while studying the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\n\n\n\nChapter 6.1: XOR Problem & ReLU Networks How ReLU solves problems that linear models cannot handle.\n\n\nChapter 6.2: Likelihood-Based Loss Functions The mathematical connection between probabilistic models and loss functions.\n\n\nChapter 6.3: Hidden Units and Activation Functions Exploring activation functions and their impact on neural network learning.\n\n\nChapter 6.4: Architecture Design - Depth vs Width How depth enables hierarchical feature reuse and exponential expressiveness.\n\n\nChapter 6.5: Back-Propagation and Other Differentiation Algorithms The algorithm that makes training deep networks computationally feasible.\n\n\n\n\n\n\n\n\nChapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature Essential second-order calculus concepts needed before Chapter 7.\n\n\nChapter 7.1.1: L2 Regularization How L2 regularization shrinks weights based on Hessian eigenvalues.\n\n\nChapter 7.1.2: L1 Regularization L1 regularization uses soft thresholding to create sparse solutions.\n\n\nChapter 7.2: Constrained Optimization View of Regularization Regularization as constrained optimization with KKT conditions.\n\n\nChapter 7.3: Regularization and Under-Constrained Problems Why regularization is mathematically necessary and ensures invertibility.\n\n\nChapter 7.4: Dataset Augmentation How transforming existing data improves generalization.\n\n\nChapter 7.5: Noise Robustness How adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\nChapter 7.6: Semi-Supervised Learning Leveraging unlabeled data to improve model performance when labeled data is scarce.\n\n\nChapter 7.7: Multi-Task Learning Training a single model on multiple related tasks to improve generalization.\n\n\nChapter 7.8: Early Stopping Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.\n\n\nChapter 7.9: Parameter Tying and Parameter Sharing Two strategies for reducing parameters: encouraging similarity vs.¬†enforcing identity.\n\n\nChapter 7.10: Sparse Representations Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.\n\n\nChapter 7.11: Bagging and Other Ensemble Methods How training multiple models on bootstrap samples and averaging their predictions reduces variance.\n\n\nChapter 7.12: Dropout Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.\n\n\nChapter 7.13: Adversarial Training How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.\n\n\nChapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.\n\n\n\n\n\n\n\n\nChapter 8.1: How Learning Differs from Pure Optimization Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.\n\n\nChapter 8.2: Challenges in Deep Learning Optimization Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.\n\n\nChapter 8.3: Basic Algorithms SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.\n\n\nChapter 8.4: Parameter Initialization Strategies Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.\n\n\nChapter 8.5: Algorithms with Adaptive Learning Rates From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.\n\n\nChapter 8.6: Second-Order Optimization Methods Newton‚Äôs method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.\n\n\nChapter 8.7: Optimization Strategies and Meta-Algorithms Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.\n\n\n\n\n\n\n\n\nChapter 9.1: Convolution Computation The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.\n\n\nChapter 9.2: Motivation for Convolutional Networks Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m¬∑n) to O(k¬∑n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere‚Äîachieving 30,000√ó speedup over dense layers.\n\n\nChapter 9.3: Pooling Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28√ó28 feature maps to 14√ó14 (4√ó fewer activations). Comparing three architectures‚Äîstrided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.\n\n\nChapter 9.4: Convolution and Pooling as an Infinitely Strong Prior Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off‚Äîstrong priors reduce sample complexity but only when assumptions match the data structure.\n\n\nChapter 9.5: Convolutional Functions Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula \\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\), stride for downsampling, three padding strategies (valid/same/full), and gradient computation‚Äîkernel gradients via correlation with input, input gradients via convolution with flipped kernel.\n\n\nChapter 9.6: Structured Outputs CNNs can generate high-dimensional structured objects through pixel-level predictions. Preserving spatial dimensions (no pooling, no stride &gt; 1, SAME padding) enables full-resolution outputs. Recurrent convolution refines predictions iteratively: \\(U*X + H(t-1)*W = H(t)\\), producing dense predictions for segmentation, depth estimation, and flow prediction."
  },
  {
    "objectID": "ML/deep-learning-book.html#all-chapters",
    "href": "ML/deep-learning-book.html#all-chapters",
    "title": "Deep Learning Book",
    "section": "",
    "text": "My notes and implementations while studying the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\n\n\n\nChapter 6.1: XOR Problem & ReLU Networks How ReLU solves problems that linear models cannot handle.\n\n\nChapter 6.2: Likelihood-Based Loss Functions The mathematical connection between probabilistic models and loss functions.\n\n\nChapter 6.3: Hidden Units and Activation Functions Exploring activation functions and their impact on neural network learning.\n\n\nChapter 6.4: Architecture Design - Depth vs Width How depth enables hierarchical feature reuse and exponential expressiveness.\n\n\nChapter 6.5: Back-Propagation and Other Differentiation Algorithms The algorithm that makes training deep networks computationally feasible.\n\n\n\n\n\n\n\n\nChapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature Essential second-order calculus concepts needed before Chapter 7.\n\n\nChapter 7.1.1: L2 Regularization How L2 regularization shrinks weights based on Hessian eigenvalues.\n\n\nChapter 7.1.2: L1 Regularization L1 regularization uses soft thresholding to create sparse solutions.\n\n\nChapter 7.2: Constrained Optimization View of Regularization Regularization as constrained optimization with KKT conditions.\n\n\nChapter 7.3: Regularization and Under-Constrained Problems Why regularization is mathematically necessary and ensures invertibility.\n\n\nChapter 7.4: Dataset Augmentation How transforming existing data improves generalization.\n\n\nChapter 7.5: Noise Robustness How adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\nChapter 7.6: Semi-Supervised Learning Leveraging unlabeled data to improve model performance when labeled data is scarce.\n\n\nChapter 7.7: Multi-Task Learning Training a single model on multiple related tasks to improve generalization.\n\n\nChapter 7.8: Early Stopping Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.\n\n\nChapter 7.9: Parameter Tying and Parameter Sharing Two strategies for reducing parameters: encouraging similarity vs.¬†enforcing identity.\n\n\nChapter 7.10: Sparse Representations Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.\n\n\nChapter 7.11: Bagging and Other Ensemble Methods How training multiple models on bootstrap samples and averaging their predictions reduces variance.\n\n\nChapter 7.12: Dropout Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.\n\n\nChapter 7.13: Adversarial Training How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.\n\n\nChapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.\n\n\n\n\n\n\n\n\nChapter 8.1: How Learning Differs from Pure Optimization Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.\n\n\nChapter 8.2: Challenges in Deep Learning Optimization Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.\n\n\nChapter 8.3: Basic Algorithms SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.\n\n\nChapter 8.4: Parameter Initialization Strategies Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.\n\n\nChapter 8.5: Algorithms with Adaptive Learning Rates From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.\n\n\nChapter 8.6: Second-Order Optimization Methods Newton‚Äôs method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.\n\n\nChapter 8.7: Optimization Strategies and Meta-Algorithms Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.\n\n\n\n\n\n\n\n\nChapter 9.1: Convolution Computation The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.\n\n\nChapter 9.2: Motivation for Convolutional Networks Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m¬∑n) to O(k¬∑n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere‚Äîachieving 30,000√ó speedup over dense layers.\n\n\nChapter 9.3: Pooling Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28√ó28 feature maps to 14√ó14 (4√ó fewer activations). Comparing three architectures‚Äîstrided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.\n\n\nChapter 9.4: Convolution and Pooling as an Infinitely Strong Prior Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off‚Äîstrong priors reduce sample complexity but only when assumptions match the data structure.\n\n\nChapter 9.5: Convolutional Functions Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula \\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\), stride for downsampling, three padding strategies (valid/same/full), and gradient computation‚Äîkernel gradients via correlation with input, input gradients via convolution with flipped kernel.\n\n\nChapter 9.6: Structured Outputs CNNs can generate high-dimensional structured objects through pixel-level predictions. Preserving spatial dimensions (no pooling, no stride &gt; 1, SAME padding) enables full-resolution outputs. Recurrent convolution refines predictions iteratively: \\(U*X + H(t-1)*W = H(t)\\), producing dense predictions for segmentation, depth estimation, and flow prediction."
  },
  {
    "objectID": "Math/MIT18.06/lectures.html",
    "href": "Math/MIT18.06/lectures.html",
    "title": "MIT 18.06SC Linear Algebra",
    "section": "",
    "text": "My journey through MIT‚Äôs Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.\n\n\n\n\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation.\n\n\nFrom Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series The beautiful mathematical journey from Taylor expansions to Euler‚Äôs formula and ultimately to Fourier series, revealing deep connections between polynomials, exponentials, and trigonometric functions.\n\n\n\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations Two powerful perspectives: row picture vs column picture.\n\n\nLecture 2: Elimination with Matrices The systematic algorithm that transforms linear systems into upper triangular form.\n\n\nLecture 3: Matrix Multiplication and Inverse Five different perspectives on matrix multiplication.\n\n\nLecture 4: LU Decomposition Factoring matrices into Lower √ó Upper triangular form.\n\n\nLecture 5.1: Permutation Matrices Permutation matrices reorder rows and columns.\n\n\nLecture 5.2: Transpose The transpose operation switches rows to columns.\n\n\nLecture 5.3: Vector Spaces Vector spaces and subspaces: closed under addition and scalar multiplication.\n\n\nLecture 6: Column Space and Null Space Column space determines which \\(b\\) make \\(Ax = b\\) solvable.\n\n\nLecture 7: Solving Ax=0 Systematic algorithm to find null space using pivot/free variables and RREF.\n\n\nLecture 8: Solving Ax=b Complete solution is particular solution plus null space.\n\n\nLecture 9: Independence, Basis, and Dimension Linear independence, basis, and dimension. Rank-nullity theorem.\n\n\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix.\n\n\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.\n\n\nLecture 12: Graphs, Networks, and Incidence Matrices Applying linear algebra to graph theory: incidence matrices, Kirchhoff‚Äôs laws, and Euler‚Äôs formula.\n\n\nLecture 13: Quiz 1 Review Practice problems reviewing key concepts: subspaces, rank, null spaces, and the four fundamental subspaces.\n\n\n\n\n\n\n\n\nLecture 14: Orthogonal Vectors and Subspaces Orthogonal vectors, orthogonal subspaces, and the fundamental relationships between the four subspaces.\n\n\nLecture 15: Projection onto Subspaces Projecting onto lines and subspaces, the geometry of least squares, and deriving normal equations.\n\n\nLecture 16: Projection Matrices and Least Squares Projection matrices, least squares fitting, and the connection between linear algebra and calculus optimization.\n\n\nLecture 17: Orthogonal Matrices and Gram-Schmidt Orthonormal matrices, the Gram-Schmidt process for orthogonalization, and QR factorization.\n\n\nLecture 18: Properties of Determinants Ten fundamental properties that completely define the determinant and reveal when matrices are invertible.\n\n\nLecture 19: Determinant Formulas and Cofactors Three computational methods for determinants: pivots, the big formula, and cofactor expansion.\n\n\nLecture 20: Inverse & Volume The inverse matrix formula using cofactors, Cramer‚Äôs rule for solving linear systems, and the geometric interpretation of determinants as volume.\n\n\nLecture 21: Eigenvalues and Eigenvectors The directions that matrices can only scale, not rotate: \\(Ax = \\lambda x\\).\n\n\nLecture 22: Diagonalization and Powers of A Computing matrix powers efficiently and solving Fibonacci with eigenvalues.\n\n\nLecture 23: Differential Equations and exp(At) Connecting linear algebra to differential equations: how eigenvalues determine stability and matrix exponentials solve systems.\n\n\nLecture 24: Markov Matrices and Fourier Series Two powerful applications of eigenvalues: Markov matrices for modeling state transitions and Fourier series for decomposing functions into orthogonal components.\n\n\n\n\n\n\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 29: Singular Value Decomposition The most important matrix factorization: SVD provides orthonormal bases for all four fundamental subspaces, revealing how any matrix maps its row space to its column space through scaling by singular values.\n\n\nLecture 30: Linear Transformations and Their Matrices The fundamental connection between abstract linear transformations and concrete matrices: every linear transformation can be represented as matrix multiplication, and the choice of basis determines the matrix representation.\n\n\nLecture 31: Change of Basis and Image Compression How choosing the right basis enables compression: JPEG transforms 512√ó512 images (262,144 pixels) to Fourier basis and discards small coefficients. Three properties of good bases‚Äîfast inverse (FFT in O(n log n)), sparsity (few coefficients enough), and orthogonality (no redundancy).\n\n\nLecture 33: Left and Right Inverse; Pseudo-inverse When matrices aren‚Äôt square: full column rank matrices (\\(r = n &lt; m\\)) have left inverses \\((A^T A)^{-1} A^T\\), full row rank matrices (\\(r = m &lt; n\\)) have right inverses \\(A^T (AA^T)^{-1}\\), and the pseudo-inverse \\(A^+ = V \\Sigma^+ U^T\\) generalizes both using SVD‚Äîinverting non-zero singular values and transposing the shape."
  },
  {
    "objectID": "Math/MIT18.06/lectures.html#all-lectures",
    "href": "Math/MIT18.06/lectures.html#all-lectures",
    "title": "MIT 18.06SC Linear Algebra",
    "section": "",
    "text": "My journey through MIT‚Äôs Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.\n\n\n\n\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation.\n\n\nFrom Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series The beautiful mathematical journey from Taylor expansions to Euler‚Äôs formula and ultimately to Fourier series, revealing deep connections between polynomials, exponentials, and trigonometric functions.\n\n\n\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations Two powerful perspectives: row picture vs column picture.\n\n\nLecture 2: Elimination with Matrices The systematic algorithm that transforms linear systems into upper triangular form.\n\n\nLecture 3: Matrix Multiplication and Inverse Five different perspectives on matrix multiplication.\n\n\nLecture 4: LU Decomposition Factoring matrices into Lower √ó Upper triangular form.\n\n\nLecture 5.1: Permutation Matrices Permutation matrices reorder rows and columns.\n\n\nLecture 5.2: Transpose The transpose operation switches rows to columns.\n\n\nLecture 5.3: Vector Spaces Vector spaces and subspaces: closed under addition and scalar multiplication.\n\n\nLecture 6: Column Space and Null Space Column space determines which \\(b\\) make \\(Ax = b\\) solvable.\n\n\nLecture 7: Solving Ax=0 Systematic algorithm to find null space using pivot/free variables and RREF.\n\n\nLecture 8: Solving Ax=b Complete solution is particular solution plus null space.\n\n\nLecture 9: Independence, Basis, and Dimension Linear independence, basis, and dimension. Rank-nullity theorem.\n\n\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix.\n\n\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.\n\n\nLecture 12: Graphs, Networks, and Incidence Matrices Applying linear algebra to graph theory: incidence matrices, Kirchhoff‚Äôs laws, and Euler‚Äôs formula.\n\n\nLecture 13: Quiz 1 Review Practice problems reviewing key concepts: subspaces, rank, null spaces, and the four fundamental subspaces.\n\n\n\n\n\n\n\n\nLecture 14: Orthogonal Vectors and Subspaces Orthogonal vectors, orthogonal subspaces, and the fundamental relationships between the four subspaces.\n\n\nLecture 15: Projection onto Subspaces Projecting onto lines and subspaces, the geometry of least squares, and deriving normal equations.\n\n\nLecture 16: Projection Matrices and Least Squares Projection matrices, least squares fitting, and the connection between linear algebra and calculus optimization.\n\n\nLecture 17: Orthogonal Matrices and Gram-Schmidt Orthonormal matrices, the Gram-Schmidt process for orthogonalization, and QR factorization.\n\n\nLecture 18: Properties of Determinants Ten fundamental properties that completely define the determinant and reveal when matrices are invertible.\n\n\nLecture 19: Determinant Formulas and Cofactors Three computational methods for determinants: pivots, the big formula, and cofactor expansion.\n\n\nLecture 20: Inverse & Volume The inverse matrix formula using cofactors, Cramer‚Äôs rule for solving linear systems, and the geometric interpretation of determinants as volume.\n\n\nLecture 21: Eigenvalues and Eigenvectors The directions that matrices can only scale, not rotate: \\(Ax = \\lambda x\\).\n\n\nLecture 22: Diagonalization and Powers of A Computing matrix powers efficiently and solving Fibonacci with eigenvalues.\n\n\nLecture 23: Differential Equations and exp(At) Connecting linear algebra to differential equations: how eigenvalues determine stability and matrix exponentials solve systems.\n\n\nLecture 24: Markov Matrices and Fourier Series Two powerful applications of eigenvalues: Markov matrices for modeling state transitions and Fourier series for decomposing functions into orthogonal components.\n\n\n\n\n\n\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 29: Singular Value Decomposition The most important matrix factorization: SVD provides orthonormal bases for all four fundamental subspaces, revealing how any matrix maps its row space to its column space through scaling by singular values.\n\n\nLecture 30: Linear Transformations and Their Matrices The fundamental connection between abstract linear transformations and concrete matrices: every linear transformation can be represented as matrix multiplication, and the choice of basis determines the matrix representation.\n\n\nLecture 31: Change of Basis and Image Compression How choosing the right basis enables compression: JPEG transforms 512√ó512 images (262,144 pixels) to Fourier basis and discards small coefficients. Three properties of good bases‚Äîfast inverse (FFT in O(n log n)), sparsity (few coefficients enough), and orthogonality (no redundancy).\n\n\nLecture 33: Left and Right Inverse; Pseudo-inverse When matrices aren‚Äôt square: full column rank matrices (\\(r = n &lt; m\\)) have left inverses \\((A^T A)^{-1} A^T\\), full row rank matrices (\\(r = m &lt; n\\)) have right inverses \\(A^T (AA^T)^{-1}\\), and the pseudo-inverse \\(A^+ = V \\Sigma^+ U^T\\) generalizes both using SVD‚Äîinverting non-zero singular values and transposing the shape."
  },
  {
    "objectID": "Math/MIT18.065/lectures.html",
    "href": "Math/MIT18.065/lectures.html",
    "title": "MIT 18.065 Matrix Methods in Data Analysis",
    "section": "",
    "text": "My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. This course applies linear algebra to modern data analysis and machine learning.\n\n\n\nLecture 8: Norms of Vectors and Matrices Understanding vector p-norms (\\(\\|v\\|_p\\)) and when they satisfy the triangle inequality (only for \\(p \\geq 1\\)). The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù creates non-convex unit balls for strong sparsity. S-norms \\(\\sqrt{v^T S v}\\) generalize to ellipses. Matrix norms: spectral \\(\\|A\\|_2 = \\sigma_1\\), Frobenius \\(\\|A\\|_F = \\sqrt{\\sum \\sigma_i^2}\\), nuclear \\(\\|A\\|_N = \\sum \\sigma_i\\)‚Äîall expressed via singular values."
  },
  {
    "objectID": "Math/MIT18.065/lectures.html#all-lectures",
    "href": "Math/MIT18.065/lectures.html#all-lectures",
    "title": "MIT 18.065 Matrix Methods in Data Analysis",
    "section": "",
    "text": "My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. This course applies linear algebra to modern data analysis and machine learning.\n\n\n\nLecture 8: Norms of Vectors and Matrices Understanding vector p-norms (\\(\\|v\\|_p\\)) and when they satisfy the triangle inequality (only for \\(p \\geq 1\\)). The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù creates non-convex unit balls for strong sparsity. S-norms \\(\\sqrt{v^T S v}\\) generalize to ellipses. Matrix norms: spectral \\(\\|A\\|_2 = \\sigma_1\\), Frobenius \\(\\|A\\|_F = \\sqrt{\\sum \\sigma_i^2}\\), nuclear \\(\\|A\\|_N = \\sum \\sigma_i\\)‚Äîall expressed via singular values."
  },
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "",
    "section": "Project Overview",
    "text": "Project Overview\nThis is a Quarto-based technical blog hosted on GitHub Pages (ickma2311.github.io). The site covers machine learning, algorithms, and technical tutorials with a focus on mathematical foundations and practical implementations."
  },
  {
    "objectID": "CLAUDE.html#common-commands",
    "href": "CLAUDE.html#common-commands",
    "title": "",
    "section": "Common Commands",
    "text": "Common Commands\n\nDevelopment Workflow\n\nquarto render - Build the entire website (outputs to docs/ directory)\nquarto preview - Start local development server with live reload\nquarto render &lt;file.qmd&gt; - Render a specific document\nquarto check - Verify Quarto installation and project setup\n\n\n\nContent Management\n\nCreate new ML content in ML/ directory\nCreate new algorithm content in Algorithm/ directory\nUpdate navigation by editing _quarto.yml navbar section\nAdd new content to respective index.qmd files for discoverability"
  },
  {
    "objectID": "CLAUDE.html#project-structure",
    "href": "CLAUDE.html#project-structure",
    "title": "",
    "section": "Project Structure",
    "text": "Project Structure\n‚îú‚îÄ‚îÄ _quarto.yml          # Main configuration file\n‚îú‚îÄ‚îÄ docs/                # Generated output (GitHub Pages source)\n‚îú‚îÄ‚îÄ index.qmd            # Homepage\n‚îú‚îÄ‚îÄ about.qmd            # About page\n‚îú‚îÄ‚îÄ ML/                  # Machine Learning content\n‚îÇ   ‚îú‚îÄ‚îÄ index.qmd        # ML topics overview\n‚îÇ   ‚îú‚îÄ‚îÄ *.qmd            # ML articles\n‚îÇ   ‚îî‚îÄ‚îÄ *.ipynb          # Jupyter notebooks\n‚îú‚îÄ‚îÄ Algorithm/           # Algorithm content\n‚îÇ   ‚îú‚îÄ‚îÄ index.qmd        # Algorithm topics overview\n‚îÇ   ‚îî‚îÄ‚îÄ *.qmd            # Algorithm articles\n‚îú‚îÄ‚îÄ imgs/                # Image assets\n‚îú‚îÄ‚îÄ media/               # Media files\n‚îî‚îÄ‚îÄ styles.css           # Custom CSS styles"
  },
  {
    "objectID": "CLAUDE.html#content-organization",
    "href": "CLAUDE.html#content-organization",
    "title": "",
    "section": "Content Organization",
    "text": "Content Organization\nThe site uses a hierarchical navigation structure defined in _quarto.yml: - Two main sections: ‚ÄúML‚Äù and ‚ÄúAlgorithm‚Äù - Each section has an index page that serves as a directory - Content is categorized by topic (e.g., ‚ÄúNumPy Fundamentals‚Äù, ‚ÄúClustering Algorithms‚Äù)\n\nAdding New Content\n\nCreate the content file in the appropriate directory (ML/ or Math/ or Algorithm/)\nAdd to list page: Update the corresponding list page (e.g., ML/deep-learning-book.qmd, Math/MIT18.06/lectures.qmd)\nUpdate homepage automatically: Run ./update-counts.sh to update section counts on homepage\nAdd navigation entry to _quarto.yml if it should appear in the navbar dropdown\nUse consistent frontmatter with title field\nSet publication date: Always use the current date from the system for the date field in frontmatter\n\nGet current date with: date +\"%Y-%m-%d\" (format: YYYY-MM-DD)\nExample: date: \"2025-10-26\"\n\nImportant: After adding new navbar items, run quarto render (full site render) to update the navbar on ALL existing pages. Individual file renders only update that specific page.\n\n\n\nMaintaining the Homepage\nThe homepage (index.qmd) shows the latest 3 items from each section with a count badge.\nCRITICAL: When adding new content, you MUST:\n\nAdd the new item to the appropriate list page:\n\nDeep Learning: ML/deep-learning-book.qmd\nMIT 18.06SC: Math/MIT18.06/lectures.qmd\nMIT 18.065: Math/MIT18.065/lectures.qmd\n\nUpdate the homepage manually by editing index.qmd:\n\nReplace the oldest item in the ‚ÄúLatest 3‚Äù with the new item\nKeep the 3 most recent items visible\nOrder: newest first (top), oldest last (bottom)\n\nUpdate section counts automatically:\n./update-counts.sh\nThis script automatically counts items in list pages and updates the count badges in index.qmd.\nRender and verify:\nquarto render index.qmd\n\nExample workflow when adding Chapter 9.6:\n# 1. Create the new content file\nvim ML/chapter-9-6.qmd\n\n# 2. Add to list page\nvim ML/deep-learning-book.qmd  # Add Chapter 9.6 entry\n\n# 3. Update homepage\nvim index.qmd  # Replace Chapter 9.3 with 9.6, keep 9.4 and 9.5\n\n# 4. Update counts automatically\n./update-counts.sh\n\n# 5. Render\nquarto render index.qmd\nWarning: The homepage will NOT automatically update when you add new content. You must manually update index.qmd to show the latest 3 items."
  },
  {
    "objectID": "CLAUDE.html#configuration-notes",
    "href": "CLAUDE.html#configuration-notes",
    "title": "",
    "section": "Configuration Notes",
    "text": "Configuration Notes\n\nOutput directory is set to docs/ for GitHub Pages compatibility\nTheme: Cosmo with custom branding\nAll pages include table of contents (toc: true)\nSite uses custom CSS from styles.css\nJupyter notebooks are supported alongside Quarto markdown"
  },
  {
    "objectID": "CLAUDE.html#github-pages-deployment",
    "href": "CLAUDE.html#github-pages-deployment",
    "title": "",
    "section": "GitHub Pages Deployment",
    "text": "GitHub Pages Deployment\nThe site is automatically deployed from the docs/ directory. After rendering, commit and push the docs/ folder to trigger GitHub Pages rebuild. - Author is Chao Ma - GitHub Pages URL: https://ickma2311.github.io/\n\nPre-Push Checklist (CRITICAL)\nALWAYS verify locally before pushing to prevent broken images on production:\n\nRun full render: quarto render (not individual file renders)\nCRITICAL: Restore ALL deleted images after full render\nFull site renders delete images from multiple locations. You MUST restore them:\n# 1. Restore *_files/ directories (code-generated matplotlib/plotly figures)\ncp -r ML/*_files docs/ML/ 2&gt;/dev/null\ncp -r Math/*_files docs/Math/ 2&gt;/dev/null\ncp -r Algorithm/*_files docs/Algorithm/ 2&gt;/dev/null\n\n# 2. Restore static images in ML/ and Math/\ncp ML/*.png docs/ML/ 2&gt;/dev/null\ncp Math/MIT18.06/*.png docs/Math/MIT18.06/ 2&gt;/dev/null\n\n# 3. Restore media/ directory images (referenced with ../media/image.png)\nmkdir -p docs/media\ncp media/*.png docs/media/ 2&gt;/dev/null\n\n# 4. Restore imgs/ directory images (referenced with ../imgs/image.png)\nmkdir -p docs/imgs\ncp imgs/*.png docs/imgs/ 2&gt;/dev/null\n\n# 5. Move any HTML files that failed to move\nfind ML -maxdepth 1 -name \"*.html\" -exec mv {} docs/ML/ \\; 2&gt;/dev/null\nfind Math/MIT18.06 -maxdepth 1 -name \"*.html\" -exec mv {} docs/Math/MIT18.06/ \\; 2&gt;/dev/null\nfind Algorithm -maxdepth 1 -name \"*.html\" -exec mv {} docs/Algorithm/ \\; 2&gt;/dev/null\n\n# 6. Move index files\nmv Algorithm/index.html docs/Algorithm/ 2&gt;/dev/null\nmv Math/index.html docs/Math/ 2&gt;/dev/null\nmv index-backup.html docs/ 2&gt;/dev/null\nCheck git status: git status - verify all image files are staged\n\nLook for docs/**/*_files/figure-html/*.png files (code-generated)\nLook for docs/media/*.png files (media directory)\nLook for docs/imgs/*.png files (imgs directory)\nLook for docs/ML/*.png and docs/Math/MIT18.06/*.png files (static images)\n\nLocal preview: Open docs/ HTML files in browser to verify images load\n\nCheck pages with Jupyter notebooks (e.g., mit1806-lecture1-geometry.html)\nCheck pages with media references (e.g., dropout.html)\nVerify matplotlib/plotly figures appear correctly\n\nCommit ALL generated files: Don‚Äôt commit .html without their images\nOnly then push: git push\n\nWhy this matters: GitHub Pages serves from the docs/ directory. Quarto‚Äôs full render deletes images from docs/ but keeps them in source directories. If images aren‚Äôt copied back to docs/, the HTML will reference missing files, causing broken images on production even though they work locally.\nImage locations that get deleted: - docs/**/*_files/ - Code-generated figures from Python/matplotlib - docs/media/ - Shared media referenced with ../media/ - docs/imgs/ - Shared images referenced with ../imgs/ - docs/ML/*.png - Static chapter images - docs/Math/MIT18.06/*.png - Static lecture images"
  },
  {
    "objectID": "CLAUDE.html#linkedin-post-guidelines",
    "href": "CLAUDE.html#linkedin-post-guidelines",
    "title": "",
    "section": "LinkedIn Post Guidelines",
    "text": "LinkedIn Post Guidelines\n\nEmoji Usage\nWhen drafting LinkedIn posts for blog content, use these emojis: - Deep Learning topics: ‚àá (delta/nabla symbol) - represents gradients and optimization - Linear Algebra topics: üìê (triangle/ruler) - represents geometric and matrix concepts\n\n\nWriting Process\n\nIdentify the key insight: Focus on the main conceptual connection or ‚Äúaha moment‚Äù from the blog post\nUse ‚Äúconnecting the dots‚Äù tone: Emphasize how concepts link together (e.g., ‚Äúhow linear algebra connects to machine learning‚Äù)\nStructure (Knowledge Card Format):\n\nStart with emoji and chapter reference (e.g., ‚Äú‚àá Deep Learning Book (Chapter 8.1)‚Äù or ‚Äúüìê MIT 18.06SC Linear Algebra (Lecture 19)‚Äù)\nClear statement or equation (e.g., ‚ÄúLearning ‚â† Optimization‚Äù)\n2-4 bullet points (üîπ) with key insights\nPhilosophical closing line (üí°)\nLink to full blog post (üìñ)\nSource attribution at the end (e.g., ‚ÄúMy notes on Deep Learning (Ian Goodfellow) Chapter X.X‚Äù or ‚ÄúMy notes on MIT 18.06SC Linear Algebra - Lecture XX‚Äù)\n\nKeep it concise: Aim for clarity over comprehensiveness - use knowledge card format for quick, digestible insights\nCourse naming:\n\nAlways use ‚ÄúMIT 18.06SC‚Äù (not just ‚ÄúMIT 18.06‚Äù) for linear algebra posts\nUse full course titles to maintain consistency\n\nInclude relevant hashtags: #MachineLearning #LinearAlgebra #DeepLearning"
  },
  {
    "objectID": "ML/cnn-structured-outputs.html",
    "href": "ML/cnn-structured-outputs.html",
    "title": "Chapter 9.6: Structured Outputs",
    "section": "",
    "text": "CNNs can generate high-dimensional structured objects, enabling pixel-level predictions for tasks like segmentation, depth estimation, and flow prediction."
  },
  {
    "objectID": "ML/cnn-structured-outputs.html#overview",
    "href": "ML/cnn-structured-outputs.html#overview",
    "title": "Chapter 9.6: Structured Outputs",
    "section": "",
    "text": "CNNs can generate high-dimensional structured objects, enabling pixel-level predictions for tasks like segmentation, depth estimation, and flow prediction."
  },
  {
    "objectID": "ML/cnn-structured-outputs.html#preserving-spatial-dimensions",
    "href": "ML/cnn-structured-outputs.html#preserving-spatial-dimensions",
    "title": "Chapter 9.6: Structured Outputs",
    "section": "Preserving Spatial Dimensions",
    "text": "Preserving Spatial Dimensions\nTo generate pixel-level (full-resolution) outputs, the convolutions must preserve spatial dimensions. This means:\n\nNo pooling layers\nNo stride &gt; 1\nConvolution uses SAME padding (e.g., padding=1 for 3√ó3 kernels)\n\nBy maintaining spatial resolution throughout the network, we can produce outputs that match the input dimensions pixel-for-pixel."
  },
  {
    "objectID": "ML/cnn-structured-outputs.html#recurrent-convolution",
    "href": "ML/cnn-structured-outputs.html#recurrent-convolution",
    "title": "Chapter 9.6: Structured Outputs",
    "section": "Recurrent Convolution",
    "text": "Recurrent Convolution\nRecurrent convolution repeatedly refines pixel-level predictions by applying the same convolutional transform across time, combining high-resolution input with the previous hidden state to produce increasingly accurate structured outputs.\n\nThe Process\nThe recurrent convolution follows this pattern:\nStep 1: \\[U * X = H(1), \\quad H(1) * V = \\hat{Y}(1)\\]\nStep 2: \\[U * X + H(1) * W = H(2), \\quad H(2) * V = \\hat{Y}(2)\\]\nStep 3: \\[U * X + H(2) * W = H(3), \\quad H(3) * V = \\hat{Y}(3)\\]\nWhere: - \\(U\\): Input convolution kernel - \\(X\\): Input image - \\(H(t)\\): Hidden state at time \\(t\\) - \\(W\\): Recurrent kernel (processes previous hidden state) - \\(V\\): Output convolution kernel - \\(\\hat{Y}(t)\\): Predicted output at time \\(t\\)\n Figure: Recurrent convolution repeatedly refines predictions by combining the input with previous hidden states through convolutional operations."
  },
  {
    "objectID": "ML/cnn-structured-outputs.html#applications-dense-pixel-level-predictions",
    "href": "ML/cnn-structured-outputs.html#applications-dense-pixel-level-predictions",
    "title": "Chapter 9.6: Structured Outputs",
    "section": "Applications: Dense Pixel-Level Predictions",
    "text": "Applications: Dense Pixel-Level Predictions\nOnce a model can produce structured outputs, we can design networks that generate full spatial maps‚Äîpredicting an entire image-like object rather than a single scalar.\nThis enables tasks such as:\n\nSemantic Segmentation: Classifying every pixel into object categories\nDepth Estimation: Predicting distance from camera for each pixel\nOptical Flow Prediction: Estimating motion vectors between frames\nDense Correspondence: Finding pixel-level matches across images\n\n Figure: Examples of structured output tasks where CNNs generate full spatial maps with pixel-level predictions, moving beyond single scalar outputs to dense, coherent predictions."
  },
  {
    "objectID": "ML/cnn-structured-outputs.html#key-insight",
    "href": "ML/cnn-structured-outputs.html#key-insight",
    "title": "Chapter 9.6: Structured Outputs",
    "section": "Key Insight",
    "text": "Key Insight\nThe power of structured outputs lies in generating spatially coherent predictions. Rather than treating each output pixel independently, recurrent convolution allows information to flow across the spatial map, ensuring that neighboring predictions are consistent and that the network can refine its outputs iteratively based on context from the entire image."
  }
]