[
  {
    "objectID": "ML/kmeans.html",
    "href": "ML/kmeans.html",
    "title": "K-means Clustering",
    "section": "",
    "text": "Setup points and K\nwe will implement a KNN algorithm to cluster the points\n\n\nX=[[1,1],[2,2.1],[3,2.5],[6,7],[7,7.1],[9,7.5]]\nk=2\n\nmax_iter=3\n\n\n# Visualize the data\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter([x[0] for x in X],[x[1] for x in X])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pure python implementation of K-means clustering\ndef knn_iter(X,centroids):\n    # set up new clusters\n    new_clusters=[[] for _ in range(len(centroids))]\n    # k=len(centroids)\n    # assign each point to the nearest centroid\n    for x in X:\n        k,distance=0,(x[0]-centroids[0][0])**2+(x[1]-centroids[0][1])**2\n        for i,c in enumerate(centroids[1:],1):\n            if (x[0]-c[0])**2+(x[1]-c[1])**2&lt;distance:\n                k=i\n                distance=(x[0]-c[0])**2+(x[1]-c[1])**2\n        new_clusters[k].append(x)\n    \n    # calculate new centroids\n    new_centroids=[[\n        sum([x[0] for x in cluster])/len(cluster),\n        sum([x[1] for x in cluster])/len(cluster)\n    ] if cluster else centroids[i] for i,cluster in enumerate(new_clusters)]\n    return new_centroids\n\n\n\n\n\n\n\n\ndef iter_and_draw(X,k,max_iter):\n    centroids=X[:k]  # Randomly select 2 centroids\n    fig, axes = plt.subplots(max_iter//3+(1 if max_iter%3!=0 else 0),\n        3, figsize=(15, 10))\n    axes=axes.flatten()\n    for i in range(max_iter):\n        \n        # Plot points and centroids\n\n\n        # Assign each point to nearest centroid and plot with corresponding color\n        colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n        for j, x in enumerate(X):\n            # Find nearest centroid\n            min_dist = float('inf')\n            nearest_centroid = 0\n            for k, c in enumerate(centroids):\n                dist = (x[0]-c[0])**2 + (x[1]-c[1])**2\n                if dist &lt; min_dist:\n                    min_dist = dist\n                    nearest_centroid = k\n            # Plot point with color corresponding to its cluster\n            axes[i].scatter(x[0], x[1], c=colors[nearest_centroid % len(colors)], label=f'Cluster {nearest_centroid+1}' if j==0 else \"\")\n        axes[i].scatter([c[0] for c in centroids], [c[1] for c in centroids], c='red', marker='*', s=200, label='Centroids')\n        axes[i].set_title(f'Iteration {i}')\n        centroids = knn_iter(X, centroids)\n\n    plt.tight_layout()\n    plt.show()\n\niter_and_draw(X,k,max_iter)\n# print(centroids)\n\n\n\n\n\n\n\n\n\n# A 3 clusters example\n\nimport numpy as np\n\nX1=np.random.rand(20,2)+5 # Some points in the upper right corner\nX2=np.random.rand(20,2)+3 # Some points in the middle\nX3=np.random.rand(20,2) # Some points in the lower left corner\n\niter_and_draw(np.concatenate((X1,X2,X3)),3,5)\n\n\n\n\n\n\n\n\n\n\nA question?\n\nWhat to do if one cluster has no assigned points during iteration?\n\n\n\nFormula Derivation\nThe goal is to minimize the loss of inertia which is sum of the points to cluster centroids.\n\\[\nLoss= \\sum_{i=1}^n \\sum_{x \\in C_i} ||x-\\mu_i||^2\n\\]\nTo iter \\(\\mu\\) for each cluster, let us find the derivative of the following function. \\[\nf(\\mu)=\\sum_{i=1}^n ||x_i-\\mu||^2 =\n\\sum_{i=1}^n {x_i}^2+\\mu^2-2x_i\\mu\n\\]\nGiven a \\(\\nabla \\mu\\), \\[\nf(\\mu + \\nabla \\mu)=\\sum_{i=1}^n ||x_i+\\nabla \\mu -\\mu||^2 =\n\\sum_{i=1}^n  {x_i}^2+\\mu^2+{\\nabla \\mu}^2-2{x_i \\mu}-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\nf(\\mu + \\nabla \\mu)-f(\\mu)=\n\\sum_{i=1}^n {\\nabla \\mu}^2-2{\\mu \\nabla \\mu}+2{x_i \\nabla \\mu}\n\\]\n\\[\n\\frac {f(\\mu + \\nabla \\mu)-f(\\mu)}{\\nabla \\mu}=\\sum_{i=1}^n {\\nabla \\mu} -2 \\mu +2{x_i} = 2\\sum_{i=1}^n x_i - 2n\\mu\n\\]\nNow we can see if \\(n\\mu = \\sum_{i=1}^n x_i\\), then the derivative is 0, this is why in each iteration, we need to set the center of the cluster as centroid."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ickma.dev",
    "section": "",
    "text": "A growing collection of structured study notes and visual explanations ‚Äî written for clarity, reproducibility, and long-term memory."
  },
  {
    "objectID": "index.html#deep-learning-book",
    "href": "index.html#deep-learning-book",
    "title": "ickma.dev",
    "section": "Deep Learning Book",
    "text": "Deep Learning Book\n\n\nClick to expand/collapse Deep Learning chapters\n\n\n\nChapter 6.1: XOR Problem & ReLU Networks How ReLU solves problems that linear models cannot handle.\n\n\nChapter 6.2: Likelihood-Based Loss Functions The mathematical connection between probabilistic models and loss functions.\n\n\nChapter 6.3: Hidden Units and Activation Functions Exploring activation functions and their impact on neural network learning.\n\n\nChapter 6.4: Architecture Design - Depth vs Width How depth enables hierarchical feature reuse and exponential expressiveness.\n\n\nChapter 6.5: Back-Propagation and Other Differentiation Algorithms The algorithm that makes training deep networks computationally feasible.\n\n\nChapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature Essential second-order calculus concepts needed before Chapter 7.\n\n\nChapter 7.1.1: L2 Regularization How L2 regularization shrinks weights based on Hessian eigenvalues.\n\n\nChapter 7.1.2: L1 Regularization L1 regularization uses soft thresholding to create sparse solutions.\n\n\nChapter 7.2: Constrained Optimization View of Regularization Regularization as constrained optimization with KKT conditions.\n\n\nChapter 7.3: Regularization and Under-Constrained Problems Why regularization is mathematically necessary and ensures invertibility.\n\n\nChapter 7.4: Dataset Augmentation How transforming existing data improves generalization.\n\n\nChapter 7.5: Noise Robustness How adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\nChapter 7.6: Semi-Supervised Learning Leveraging unlabeled data to improve model performance when labeled data is scarce.\n\n\nChapter 7.7: Multi-Task Learning Training a single model on multiple related tasks to improve generalization.\n\n\nChapter 7.8: Early Stopping Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.\n\n\nChapter 7.9: Parameter Tying and Parameter Sharing Two strategies for reducing parameters: encouraging similarity vs.¬†enforcing identity.\n\n\nChapter 7.10: Sparse Representations Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.\n\n\nChapter 7.11: Bagging and Other Ensemble Methods How training multiple models on bootstrap samples and averaging their predictions reduces variance.\n\n\nChapter 7.12: Dropout Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.\n\n\nChapter 7.13: Adversarial Training How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.\n\n\nChapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.\n\n\nChapter 8.1: How Learning Differs from Pure Optimization Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.\n\n\nChapter 8.2: Challenges in Deep Learning Optimization Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.\n\n\nChapter 8.3: Basic Algorithms SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.\n\n\nChapter 8.4: Parameter Initialization Strategies Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.\n\n\nChapter 8.5: Algorithms with Adaptive Learning Rates From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.\n\n\nChapter 8.6: Second-Order Optimization Methods Newton‚Äôs method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.\n\n\nChapter 8.7: Optimization Strategies and Meta-Algorithms Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.\n\n\nChapter 9.1: Convolution Computation The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.\n\n\nChapter 9.2: Motivation for Convolutional Networks Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m¬∑n) to O(k¬∑n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere‚Äîachieving 30,000√ó speedup over dense layers.\n\n\nChapter 9.3: Pooling Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28√ó28 feature maps to 14√ó14 (4√ó fewer activations). Comparing three architectures‚Äîstrided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.\n\n\nChapter 9.4: Convolution and Pooling as an Infinitely Strong Prior Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off‚Äîstrong priors reduce sample complexity but only when assumptions match the data structure.\n\n\nChapter 9.5: Convolutional Functions Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula \\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\), stride for downsampling, three padding strategies (valid/same/full), and gradient computation‚Äîkernel gradients via correlation with input, input gradients via convolution with flipped kernel."
  },
  {
    "objectID": "index.html#mathematics",
    "href": "index.html#mathematics",
    "title": "ickma.dev",
    "section": "Mathematics",
    "text": "Mathematics\n\nMy journey through MIT‚Äôs Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.\n\n\n\nClick to expand/collapse MIT 18.06SC Linear Algebra\n\n\nReflections & Synthesis\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation.\n\n\nFrom Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series The beautiful mathematical journey from Taylor expansions to Euler‚Äôs formula and ultimately to Fourier series, revealing deep connections between polynomials, exponentials, and trigonometric functions.\n\n\n\nMIT 18.06SC Linear Algebra\n\n\nLecture 1: The Geometry of Linear Equations Two powerful perspectives: row picture vs column picture.\n\n\nLecture 2: Elimination with Matrices The systematic algorithm that transforms linear systems into upper triangular form.\n\n\nLecture 3: Matrix Multiplication and Inverse Five different perspectives on matrix multiplication.\n\n\nLecture 4: LU Decomposition Factoring matrices into Lower √ó Upper triangular form.\n\n\nLecture 5.1: Permutation Matrices Permutation matrices reorder rows and columns.\n\n\nLecture 5.2: Transpose The transpose operation switches rows to columns.\n\n\nLecture 5.3: Vector Spaces Vector spaces and subspaces: closed under addition and scalar multiplication.\n\n\nLecture 6: Column Space and Null Space Column space determines which \\(b\\) make \\(Ax = b\\) solvable.\n\n\nLecture 7: Solving Ax=0 Systematic algorithm to find null space using pivot/free variables and RREF.\n\n\nLecture 8: Solving Ax=b Complete solution is particular solution plus null space.\n\n\nLecture 9: Independence, Basis, and Dimension Linear independence, basis, and dimension. Rank-nullity theorem.\n\n\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix.\n\n\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.\n\n\nLecture 12: Graphs, Networks, and Incidence Matrices Applying linear algebra to graph theory: incidence matrices, Kirchhoff‚Äôs laws, and Euler‚Äôs formula.\n\n\nLecture 13: Quiz 1 Review Practice problems reviewing key concepts: subspaces, rank, null spaces, and the four fundamental subspaces.\n\n\nLecture 14: Orthogonal Vectors and Subspaces Orthogonal vectors, orthogonal subspaces, and the fundamental relationships between the four subspaces.\n\n\nLecture 15: Projection onto Subspaces Projecting onto lines and subspaces, the geometry of least squares, and deriving normal equations.\n\n\nLecture 16: Projection Matrices and Least Squares Projection matrices, least squares fitting, and the connection between linear algebra and calculus optimization.\n\n\nLecture 17: Orthogonal Matrices and Gram-Schmidt Orthonormal matrices, the Gram-Schmidt process for orthogonalization, and QR factorization.\n\n\nLecture 18: Properties of Determinants Ten fundamental properties that completely define the determinant and reveal when matrices are invertible.\n\n\nLecture 19: Determinant Formulas and Cofactors Three computational methods for determinants: pivots, the big formula, and cofactor expansion.\n\n\nLecture 20: Inverse & Volume The inverse matrix formula using cofactors, Cramer‚Äôs rule for solving linear systems, and the geometric interpretation of determinants as volume.\n\n\nLecture 21: Eigenvalues and Eigenvectors The directions that matrices can only scale, not rotate: \\(Ax = \\lambda x\\).\n\n\nLecture 22: Diagonalization and Powers of A Computing matrix powers efficiently and solving Fibonacci with eigenvalues.\n\n\nLecture 23: Differential Equations and exp(At) Connecting linear algebra to differential equations: how eigenvalues determine stability and matrix exponentials solve systems.\n\n\nLecture 24: Markov Matrices and Fourier Series Two powerful applications of eigenvalues: Markov matrices for modeling state transitions and Fourier series for decomposing functions into orthogonal components.\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 29: Singular Value Decomposition The most important matrix factorization: SVD provides orthonormal bases for all four fundamental subspaces, revealing how any matrix maps its row space to its column space through scaling by singular values.\n\n\nLecture 30: Linear Transformations and Their Matrices The fundamental connection between abstract linear transformations and concrete matrices: every linear transformation can be represented as matrix multiplication, and the choice of basis determines the matrix representation.\n\n\nLecture 31: Change of Basis and Image Compression How choosing the right basis enables compression: JPEG transforms 512√ó512 images (262,144 pixels) to Fourier basis and discards small coefficients. Three properties of good bases‚Äîfast inverse (FFT in O(n log n)), sparsity (few coefficients enough), and orthogonality (no redundancy).\n\n\nLecture 33: Left and Right Inverse; Pseudo-inverse When matrices aren‚Äôt square: full column rank matrices (\\(r = n &lt; m\\)) have left inverses \\((A^T A)^{-1} A^T\\), full row rank matrices (\\(r = m &lt; n\\)) have right inverses \\(A^T (AA^T)^{-1}\\), and the pseudo-inverse \\(A^+ = V \\Sigma^+ U^T\\) generalizes both using SVD‚Äîinverting non-zero singular values and transposing the shape.\n\n\n\n\n\nClick to expand/collapse MIT 18.065 Matrix Methods"
  },
  {
    "objectID": "index.html#more-topics",
    "href": "index.html#more-topics",
    "title": "ickma.dev",
    "section": "More Topics",
    "text": "More Topics\n\n\nMachine Learning\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\nAlgorithms\n\nDP Regex"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "",
    "text": "This lecture covers vector and matrix norms with applications to regularization and sparsity:\n\nReview vector p-norms and the geometry of unit balls\nExplain when norms are valid (triangle inequality) and the S-norm defined by \\(v^T S v\\)\nCompare \\(\\ell_1\\), \\(\\ell_2\\), and the ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù as regularizers when solving \\(Ax = b\\)\nIntroduce matrix norms (spectral, Frobenius, nuclear) and relate them to singular values"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#overview",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#overview",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "",
    "text": "This lecture covers vector and matrix norms with applications to regularization and sparsity:\n\nReview vector p-norms and the geometry of unit balls\nExplain when norms are valid (triangle inequality) and the S-norm defined by \\(v^T S v\\)\nCompare \\(\\ell_1\\), \\(\\ell_2\\), and the ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù as regularizers when solving \\(Ax = b\\)\nIntroduce matrix norms (spectral, Frobenius, nuclear) and relate them to singular values"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#vector-norms",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#vector-norms",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Vector Norms",
    "text": "Vector Norms\n\nDefinition\nThe p-norm of a vector \\(v \\in \\mathbb{R}^n\\) is defined as:\n\\[\n\\|v\\|_p = \\sqrt[p]{|v_1|^p + |v_2|^p + \\cdots + |v_n|^p}\n\\]\nCommon values: \\(p = 0, 1, 2, \\infty\\)\n\n\\(\\|v\\|_0\\): Number of nonzero components (not a true norm)\n\\(\\|v\\|_1\\): Sum of absolute values (Manhattan norm)\n\\(\\|v\\|_2\\): Euclidean length\n\\(\\|v\\|_\\infty\\): Maximum absolute value"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#geometry-of-unit-balls",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#geometry-of-unit-balls",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Geometry of Unit Balls",
    "text": "Geometry of Unit Balls\nThe unit ball \\(\\|v\\|_p = 1\\) in \\(\\mathbb{R}^2\\) has different shapes for different \\(p\\):\n Figure: Unit balls \\(\\|v\\|_p = 1\\) in \\(\\mathbb{R}^2\\) for different values of \\(p\\). As \\(p\\) increases, the unit ball transitions from diamond (\\(p=1\\)) to circle (\\(p=2\\)) to square (\\(p=\\infty\\))."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#when-is-it-a-true-norm",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#when-is-it-a-true-norm",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "When Is It a True Norm?",
    "text": "When Is It a True Norm?\nTriangle inequality requirement: \\(\\|x + y\\| \\leq \\|x\\| + \\|y\\|\\)\n\nWhen \\(p \\geq 1\\): \\(\\|\\cdot\\|_p\\) is a valid norm\nWhen \\(p &lt; 1\\): Not a true norm (triangle inequality fails)\n\n\nThe ‚Äú\\(\\frac{1}{2}\\)-Norm‚Äù\nFor \\(\\|x\\|_{1/2}\\), this is not a true norm since \\(p &lt; 1\\), but it provides a very strong sparsity penalty that pushes many components of \\(x\\) to zero when minimized.\nIntuition: A norm must satisfy the triangle inequality (going straight is never longer than going in two steps). For \\(p &lt; 1\\), the unit ball is not convex, so the triangle inequality fails.\n Figure: The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù unit ball is non-convex. The triangle inequality fails because the straight path between two points can be longer than the sum of their norms."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#s-norm",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#s-norm",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "S-Norm",
    "text": "S-Norm\nLet \\(S\\) be a symmetric positive definite matrix. The S-norm is defined as:\n\\[\n\\|v\\|_S = \\sqrt{v^T S v}\n\\]\nSpecial case: When \\(S = I\\) (identity matrix), we get \\(\\|v\\|_2\\) (Euclidean norm).\n\nExample\n\\[\nS = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3\n\\end{bmatrix}\n\\]\nThe unit ball \\(v^T S v = 1\\) forms an ellipse whose axes are determined by the eigenvectors of \\(S\\).\n Figure: The unit ball for the S-norm \\(\\sqrt{v^T S v} = 1\\) forms an ellipse. The shape and orientation depend on the eigenvalues and eigenvectors of the symmetric positive definite matrix \\(S\\)."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#minimizing-norms-regularization",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#minimizing-norms-regularization",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Minimizing Norms: Regularization",
    "text": "Minimizing Norms: Regularization\nWhen solving \\(Ax = b\\), we often want to minimize \\(\\|x\\|_p\\) to prefer certain solutions:\n\n\\(\\ell_1\\) Regularization\n\nProperty: Sparse solutions\nWinner: Solution has many zero components (e.g., \\([0, b]\\) or \\([a, 0]\\))\nUse case: Feature selection, compressed sensing\n\n\n\n\\(\\ell_2\\) Regularization\n\nProperty: Smooth, distributed solutions\nGeometric interpretation: Find the point on the constraint line \\(c_1 x_1 + c_2 x_2 = 0\\) that intersects the smallest \\(\\|v\\|_2 = c\\) level set (circle)\nUse case: Ridge regression, preventing overfitting"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#matrix-norms",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#matrix-norms",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Matrix Norms",
    "text": "Matrix Norms\n\nSpectral Norm: \\(\\|A\\|_2 = \\sigma_1\\)\nThe spectral norm measures the maximum ‚Äúblow-up‚Äù of a vector:\n\\[\n\\|A\\|_2 = \\max_x \\frac{\\|Ax\\|_2}{\\|x\\|_2} = \\sigma_1\n\\]\nwhere \\(\\sigma_1\\) is the largest singular value of \\(A\\).\nWinner: \\(x = v_1\\) (first right singular vector)\n\n\n\nFrobenius Norm: \\(\\|A\\|_F\\)\nThe Frobenius norm is the square root of the sum of all squared entries:\n\\[\n\\|A\\|_F = \\sqrt{a_{11}^2 + a_{12}^2 + \\cdots + a_{mn}^2}\n\\]\nConnection to SVD:\n\\[\n\\|A\\|_F = \\sqrt{\\sigma_1^2 + \\sigma_2^2 + \\cdots + \\sigma_r^2}\n\\]\nWhy? Because \\(A = U \\Sigma V^T\\), and both \\(U\\) and \\(V\\) are orthonormal matrices (they preserve \\(\\ell_2\\) norm).\n\n\n\nNuclear Norm: \\(\\|A\\|_N\\)\nThe nuclear norm (also called trace norm) is the sum of singular values:\n\\[\n\\|A\\|_N = \\sigma_1 + \\sigma_2 + \\cdots + \\sigma_r\n\\]\nUse case: Low-rank matrix completion (convex relaxation of rank minimization)"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture8-norms.html#summary-of-matrix-norms",
    "href": "Math/MIT18.065/mit18065-lecture8-norms.html#summary-of-matrix-norms",
    "title": "Lecture 8: Norms of Vectors and Matrices",
    "section": "Summary of Matrix Norms",
    "text": "Summary of Matrix Norms\n Figure: Comparison of matrix norms - spectral norm \\(\\|A\\|_2\\) (largest singular value), Frobenius norm \\(\\|A\\|_F\\) (root sum of squared singular values), and nuclear norm \\(\\|A\\|_N\\) (sum of singular values).\n\n\n\n\n\n\n\n\nNorm\nFormula\nGeometric Meaning\n\n\n\n\nSpectral\n\\(\\|A\\|_2 = \\sigma_1\\)\nMaximum amplification\n\n\nFrobenius\n\\(\\|A\\|_F = \\sqrt{\\sigma_1^2 + \\cdots + \\sigma_r^2}\\)\nRoot mean square of entries\n\n\nNuclear\n\\(\\|A\\|_N = \\sigma_1 + \\cdots + \\sigma_r\\)\nConvex surrogate for rank\n\n\n\nKey insight: All three matrix norms can be expressed in terms of singular values, connecting them to the fundamental SVD decomposition."
  },
  {
    "objectID": "index.html#latest-updates",
    "href": "index.html#latest-updates",
    "title": "ickma.dev",
    "section": "Latest Updates",
    "text": "Latest Updates\n\n‚àá Deep Learning Book 31 chapters\nMy notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\nChapter 9.5: Convolutional Functions Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula \\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\), stride for downsampling, three padding strategies (valid/same/full), and gradient computation.\n\n\nChapter 9.4: Convolution and Pooling as an Infinitely Strong Prior Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others.\n\n\nChapter 9.3: Pooling Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28√ó28 feature maps to 14√ó14 (4√ó fewer activations).\n\n\n\nSee all Deep Learning chapters ‚Üí\n\n\n\nüìê MIT 18.06SC Linear Algebra 35 lectures\nMy journey through MIT‚Äôs Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\n\nSee all MIT 18.06SC lectures ‚Üí\n\n\n\nüìê MIT 18.065 Matrix Methods 1 lecture\nMy notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning.\n\n\nLecture 8: Norms of Vectors and Matrices Understanding vector p-norms (\\(\\|v\\|_p\\)) and when they satisfy the triangle inequality (only for \\(p \\geq 1\\)). The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù creates non-convex unit balls for strong sparsity.\n\n\n\nSee all MIT 18.065 lectures ‚Üí"
  },
  {
    "objectID": "ML/deep-learning-book.html",
    "href": "ML/deep-learning-book.html",
    "title": "Deep Learning Book",
    "section": "",
    "text": "My notes and implementations while studying the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\n\n\n\nChapter 6.1: XOR Problem & ReLU Networks How ReLU solves problems that linear models cannot handle.\n\n\nChapter 6.2: Likelihood-Based Loss Functions The mathematical connection between probabilistic models and loss functions.\n\n\nChapter 6.3: Hidden Units and Activation Functions Exploring activation functions and their impact on neural network learning.\n\n\nChapter 6.4: Architecture Design - Depth vs Width How depth enables hierarchical feature reuse and exponential expressiveness.\n\n\nChapter 6.5: Back-Propagation and Other Differentiation Algorithms The algorithm that makes training deep networks computationally feasible.\n\n\n\n\n\n\n\n\nChapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature Essential second-order calculus concepts needed before Chapter 7.\n\n\nChapter 7.1.1: L2 Regularization How L2 regularization shrinks weights based on Hessian eigenvalues.\n\n\nChapter 7.1.2: L1 Regularization L1 regularization uses soft thresholding to create sparse solutions.\n\n\nChapter 7.2: Constrained Optimization View of Regularization Regularization as constrained optimization with KKT conditions.\n\n\nChapter 7.3: Regularization and Under-Constrained Problems Why regularization is mathematically necessary and ensures invertibility.\n\n\nChapter 7.4: Dataset Augmentation How transforming existing data improves generalization.\n\n\nChapter 7.5: Noise Robustness How adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\nChapter 7.6: Semi-Supervised Learning Leveraging unlabeled data to improve model performance when labeled data is scarce.\n\n\nChapter 7.7: Multi-Task Learning Training a single model on multiple related tasks to improve generalization.\n\n\nChapter 7.8: Early Stopping Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.\n\n\nChapter 7.9: Parameter Tying and Parameter Sharing Two strategies for reducing parameters: encouraging similarity vs.¬†enforcing identity.\n\n\nChapter 7.10: Sparse Representations Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.\n\n\nChapter 7.11: Bagging and Other Ensemble Methods How training multiple models on bootstrap samples and averaging their predictions reduces variance.\n\n\nChapter 7.12: Dropout Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.\n\n\nChapter 7.13: Adversarial Training How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.\n\n\nChapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.\n\n\n\n\n\n\n\n\nChapter 8.1: How Learning Differs from Pure Optimization Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.\n\n\nChapter 8.2: Challenges in Deep Learning Optimization Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.\n\n\nChapter 8.3: Basic Algorithms SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.\n\n\nChapter 8.4: Parameter Initialization Strategies Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.\n\n\nChapter 8.5: Algorithms with Adaptive Learning Rates From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.\n\n\nChapter 8.6: Second-Order Optimization Methods Newton‚Äôs method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.\n\n\nChapter 8.7: Optimization Strategies and Meta-Algorithms Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.\n\n\n\n\n\n\n\n\nChapter 9.1: Convolution Computation The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.\n\n\nChapter 9.2: Motivation for Convolutional Networks Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m¬∑n) to O(k¬∑n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere‚Äîachieving 30,000√ó speedup over dense layers.\n\n\nChapter 9.3: Pooling Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28√ó28 feature maps to 14√ó14 (4√ó fewer activations). Comparing three architectures‚Äîstrided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.\n\n\nChapter 9.4: Convolution and Pooling as an Infinitely Strong Prior Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off‚Äîstrong priors reduce sample complexity but only when assumptions match the data structure.\n\n\nChapter 9.5: Convolutional Functions Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula \\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\), stride for downsampling, three padding strategies (valid/same/full), and gradient computation‚Äîkernel gradients via correlation with input, input gradients via convolution with flipped kernel."
  },
  {
    "objectID": "ML/deep-learning-book.html#all-chapters",
    "href": "ML/deep-learning-book.html#all-chapters",
    "title": "Deep Learning Book",
    "section": "",
    "text": "My notes and implementations while studying the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\n\n\n\nChapter 6.1: XOR Problem & ReLU Networks How ReLU solves problems that linear models cannot handle.\n\n\nChapter 6.2: Likelihood-Based Loss Functions The mathematical connection between probabilistic models and loss functions.\n\n\nChapter 6.3: Hidden Units and Activation Functions Exploring activation functions and their impact on neural network learning.\n\n\nChapter 6.4: Architecture Design - Depth vs Width How depth enables hierarchical feature reuse and exponential expressiveness.\n\n\nChapter 6.5: Back-Propagation and Other Differentiation Algorithms The algorithm that makes training deep networks computationally feasible.\n\n\n\n\n\n\n\n\nChapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature Essential second-order calculus concepts needed before Chapter 7.\n\n\nChapter 7.1.1: L2 Regularization How L2 regularization shrinks weights based on Hessian eigenvalues.\n\n\nChapter 7.1.2: L1 Regularization L1 regularization uses soft thresholding to create sparse solutions.\n\n\nChapter 7.2: Constrained Optimization View of Regularization Regularization as constrained optimization with KKT conditions.\n\n\nChapter 7.3: Regularization and Under-Constrained Problems Why regularization is mathematically necessary and ensures invertibility.\n\n\nChapter 7.4: Dataset Augmentation How transforming existing data improves generalization.\n\n\nChapter 7.5: Noise Robustness How adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\nChapter 7.6: Semi-Supervised Learning Leveraging unlabeled data to improve model performance when labeled data is scarce.\n\n\nChapter 7.7: Multi-Task Learning Training a single model on multiple related tasks to improve generalization.\n\n\nChapter 7.8: Early Stopping Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.\n\n\nChapter 7.9: Parameter Tying and Parameter Sharing Two strategies for reducing parameters: encouraging similarity vs.¬†enforcing identity.\n\n\nChapter 7.10: Sparse Representations Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.\n\n\nChapter 7.11: Bagging and Other Ensemble Methods How training multiple models on bootstrap samples and averaging their predictions reduces variance.\n\n\nChapter 7.12: Dropout Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.\n\n\nChapter 7.13: Adversarial Training How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.\n\n\nChapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.\n\n\n\n\n\n\n\n\nChapter 8.1: How Learning Differs from Pure Optimization Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.\n\n\nChapter 8.2: Challenges in Deep Learning Optimization Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.\n\n\nChapter 8.3: Basic Algorithms SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.\n\n\nChapter 8.4: Parameter Initialization Strategies Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.\n\n\nChapter 8.5: Algorithms with Adaptive Learning Rates From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.\n\n\nChapter 8.6: Second-Order Optimization Methods Newton‚Äôs method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.\n\n\nChapter 8.7: Optimization Strategies and Meta-Algorithms Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.\n\n\n\n\n\n\n\n\nChapter 9.1: Convolution Computation The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.\n\n\nChapter 9.2: Motivation for Convolutional Networks Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m¬∑n) to O(k¬∑n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere‚Äîachieving 30,000√ó speedup over dense layers.\n\n\nChapter 9.3: Pooling Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28√ó28 feature maps to 14√ó14 (4√ó fewer activations). Comparing three architectures‚Äîstrided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.\n\n\nChapter 9.4: Convolution and Pooling as an Infinitely Strong Prior Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off‚Äîstrong priors reduce sample complexity but only when assumptions match the data structure.\n\n\nChapter 9.5: Convolutional Functions Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula \\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\), stride for downsampling, three padding strategies (valid/same/full), and gradient computation‚Äîkernel gradients via correlation with input, input gradients via convolution with flipped kernel."
  },
  {
    "objectID": "Math/MIT18.06/lectures.html",
    "href": "Math/MIT18.06/lectures.html",
    "title": "MIT 18.06SC Linear Algebra",
    "section": "",
    "text": "My journey through MIT‚Äôs Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.\n\n\n\n\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation.\n\n\nFrom Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series The beautiful mathematical journey from Taylor expansions to Euler‚Äôs formula and ultimately to Fourier series, revealing deep connections between polynomials, exponentials, and trigonometric functions.\n\n\n\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations Two powerful perspectives: row picture vs column picture.\n\n\nLecture 2: Elimination with Matrices The systematic algorithm that transforms linear systems into upper triangular form.\n\n\nLecture 3: Matrix Multiplication and Inverse Five different perspectives on matrix multiplication.\n\n\nLecture 4: LU Decomposition Factoring matrices into Lower √ó Upper triangular form.\n\n\nLecture 5.1: Permutation Matrices Permutation matrices reorder rows and columns.\n\n\nLecture 5.2: Transpose The transpose operation switches rows to columns.\n\n\nLecture 5.3: Vector Spaces Vector spaces and subspaces: closed under addition and scalar multiplication.\n\n\nLecture 6: Column Space and Null Space Column space determines which \\(b\\) make \\(Ax = b\\) solvable.\n\n\nLecture 7: Solving Ax=0 Systematic algorithm to find null space using pivot/free variables and RREF.\n\n\nLecture 8: Solving Ax=b Complete solution is particular solution plus null space.\n\n\nLecture 9: Independence, Basis, and Dimension Linear independence, basis, and dimension. Rank-nullity theorem.\n\n\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix.\n\n\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.\n\n\nLecture 12: Graphs, Networks, and Incidence Matrices Applying linear algebra to graph theory: incidence matrices, Kirchhoff‚Äôs laws, and Euler‚Äôs formula.\n\n\nLecture 13: Quiz 1 Review Practice problems reviewing key concepts: subspaces, rank, null spaces, and the four fundamental subspaces.\n\n\n\n\n\n\n\n\nLecture 14: Orthogonal Vectors and Subspaces Orthogonal vectors, orthogonal subspaces, and the fundamental relationships between the four subspaces.\n\n\nLecture 15: Projection onto Subspaces Projecting onto lines and subspaces, the geometry of least squares, and deriving normal equations.\n\n\nLecture 16: Projection Matrices and Least Squares Projection matrices, least squares fitting, and the connection between linear algebra and calculus optimization.\n\n\nLecture 17: Orthogonal Matrices and Gram-Schmidt Orthonormal matrices, the Gram-Schmidt process for orthogonalization, and QR factorization.\n\n\nLecture 18: Properties of Determinants Ten fundamental properties that completely define the determinant and reveal when matrices are invertible.\n\n\nLecture 19: Determinant Formulas and Cofactors Three computational methods for determinants: pivots, the big formula, and cofactor expansion.\n\n\nLecture 20: Inverse & Volume The inverse matrix formula using cofactors, Cramer‚Äôs rule for solving linear systems, and the geometric interpretation of determinants as volume.\n\n\nLecture 21: Eigenvalues and Eigenvectors The directions that matrices can only scale, not rotate: \\(Ax = \\lambda x\\).\n\n\nLecture 22: Diagonalization and Powers of A Computing matrix powers efficiently and solving Fibonacci with eigenvalues.\n\n\nLecture 23: Differential Equations and exp(At) Connecting linear algebra to differential equations: how eigenvalues determine stability and matrix exponentials solve systems.\n\n\nLecture 24: Markov Matrices and Fourier Series Two powerful applications of eigenvalues: Markov matrices for modeling state transitions and Fourier series for decomposing functions into orthogonal components.\n\n\n\n\n\n\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 29: Singular Value Decomposition The most important matrix factorization: SVD provides orthonormal bases for all four fundamental subspaces, revealing how any matrix maps its row space to its column space through scaling by singular values.\n\n\nLecture 30: Linear Transformations and Their Matrices The fundamental connection between abstract linear transformations and concrete matrices: every linear transformation can be represented as matrix multiplication, and the choice of basis determines the matrix representation.\n\n\nLecture 31: Change of Basis and Image Compression How choosing the right basis enables compression: JPEG transforms 512√ó512 images (262,144 pixels) to Fourier basis and discards small coefficients. Three properties of good bases‚Äîfast inverse (FFT in O(n log n)), sparsity (few coefficients enough), and orthogonality (no redundancy).\n\n\nLecture 33: Left and Right Inverse; Pseudo-inverse When matrices aren‚Äôt square: full column rank matrices (\\(r = n &lt; m\\)) have left inverses \\((A^T A)^{-1} A^T\\), full row rank matrices (\\(r = m &lt; n\\)) have right inverses \\(A^T (AA^T)^{-1}\\), and the pseudo-inverse \\(A^+ = V \\Sigma^+ U^T\\) generalizes both using SVD‚Äîinverting non-zero singular values and transposing the shape."
  },
  {
    "objectID": "Math/MIT18.06/lectures.html#all-lectures",
    "href": "Math/MIT18.06/lectures.html#all-lectures",
    "title": "MIT 18.06SC Linear Algebra",
    "section": "",
    "text": "My journey through MIT‚Äôs Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.\n\n\n\n\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation.\n\n\nFrom Taylor Expansion to Euler‚Äôs Formula: The Mathematical Foundation of Fourier Series The beautiful mathematical journey from Taylor expansions to Euler‚Äôs formula and ultimately to Fourier series, revealing deep connections between polynomials, exponentials, and trigonometric functions.\n\n\n\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations Two powerful perspectives: row picture vs column picture.\n\n\nLecture 2: Elimination with Matrices The systematic algorithm that transforms linear systems into upper triangular form.\n\n\nLecture 3: Matrix Multiplication and Inverse Five different perspectives on matrix multiplication.\n\n\nLecture 4: LU Decomposition Factoring matrices into Lower √ó Upper triangular form.\n\n\nLecture 5.1: Permutation Matrices Permutation matrices reorder rows and columns.\n\n\nLecture 5.2: Transpose The transpose operation switches rows to columns.\n\n\nLecture 5.3: Vector Spaces Vector spaces and subspaces: closed under addition and scalar multiplication.\n\n\nLecture 6: Column Space and Null Space Column space determines which \\(b\\) make \\(Ax = b\\) solvable.\n\n\nLecture 7: Solving Ax=0 Systematic algorithm to find null space using pivot/free variables and RREF.\n\n\nLecture 8: Solving Ax=b Complete solution is particular solution plus null space.\n\n\nLecture 9: Independence, Basis, and Dimension Linear independence, basis, and dimension. Rank-nullity theorem.\n\n\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix.\n\n\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.\n\n\nLecture 12: Graphs, Networks, and Incidence Matrices Applying linear algebra to graph theory: incidence matrices, Kirchhoff‚Äôs laws, and Euler‚Äôs formula.\n\n\nLecture 13: Quiz 1 Review Practice problems reviewing key concepts: subspaces, rank, null spaces, and the four fundamental subspaces.\n\n\n\n\n\n\n\n\nLecture 14: Orthogonal Vectors and Subspaces Orthogonal vectors, orthogonal subspaces, and the fundamental relationships between the four subspaces.\n\n\nLecture 15: Projection onto Subspaces Projecting onto lines and subspaces, the geometry of least squares, and deriving normal equations.\n\n\nLecture 16: Projection Matrices and Least Squares Projection matrices, least squares fitting, and the connection between linear algebra and calculus optimization.\n\n\nLecture 17: Orthogonal Matrices and Gram-Schmidt Orthonormal matrices, the Gram-Schmidt process for orthogonalization, and QR factorization.\n\n\nLecture 18: Properties of Determinants Ten fundamental properties that completely define the determinant and reveal when matrices are invertible.\n\n\nLecture 19: Determinant Formulas and Cofactors Three computational methods for determinants: pivots, the big formula, and cofactor expansion.\n\n\nLecture 20: Inverse & Volume The inverse matrix formula using cofactors, Cramer‚Äôs rule for solving linear systems, and the geometric interpretation of determinants as volume.\n\n\nLecture 21: Eigenvalues and Eigenvectors The directions that matrices can only scale, not rotate: \\(Ax = \\lambda x\\).\n\n\nLecture 22: Diagonalization and Powers of A Computing matrix powers efficiently and solving Fibonacci with eigenvalues.\n\n\nLecture 23: Differential Equations and exp(At) Connecting linear algebra to differential equations: how eigenvalues determine stability and matrix exponentials solve systems.\n\n\nLecture 24: Markov Matrices and Fourier Series Two powerful applications of eigenvalues: Markov matrices for modeling state transitions and Fourier series for decomposing functions into orthogonal components.\n\n\n\n\n\n\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 29: Singular Value Decomposition The most important matrix factorization: SVD provides orthonormal bases for all four fundamental subspaces, revealing how any matrix maps its row space to its column space through scaling by singular values.\n\n\nLecture 30: Linear Transformations and Their Matrices The fundamental connection between abstract linear transformations and concrete matrices: every linear transformation can be represented as matrix multiplication, and the choice of basis determines the matrix representation.\n\n\nLecture 31: Change of Basis and Image Compression How choosing the right basis enables compression: JPEG transforms 512√ó512 images (262,144 pixels) to Fourier basis and discards small coefficients. Three properties of good bases‚Äîfast inverse (FFT in O(n log n)), sparsity (few coefficients enough), and orthogonality (no redundancy).\n\n\nLecture 33: Left and Right Inverse; Pseudo-inverse When matrices aren‚Äôt square: full column rank matrices (\\(r = n &lt; m\\)) have left inverses \\((A^T A)^{-1} A^T\\), full row rank matrices (\\(r = m &lt; n\\)) have right inverses \\(A^T (AA^T)^{-1}\\), and the pseudo-inverse \\(A^+ = V \\Sigma^+ U^T\\) generalizes both using SVD‚Äîinverting non-zero singular values and transposing the shape."
  },
  {
    "objectID": "Math/MIT18.065/lectures.html",
    "href": "Math/MIT18.065/lectures.html",
    "title": "MIT 18.065 Matrix Methods in Data Analysis",
    "section": "",
    "text": "My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. This course applies linear algebra to modern data analysis and machine learning.\n\n\n\nLecture 8: Norms of Vectors and Matrices Understanding vector p-norms (\\(\\|v\\|_p\\)) and when they satisfy the triangle inequality (only for \\(p \\geq 1\\)). The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù creates non-convex unit balls for strong sparsity. S-norms \\(\\sqrt{v^T S v}\\) generalize to ellipses. Matrix norms: spectral \\(\\|A\\|_2 = \\sigma_1\\), Frobenius \\(\\|A\\|_F = \\sqrt{\\sum \\sigma_i^2}\\), nuclear \\(\\|A\\|_N = \\sum \\sigma_i\\)‚Äîall expressed via singular values."
  },
  {
    "objectID": "Math/MIT18.065/lectures.html#all-lectures",
    "href": "Math/MIT18.065/lectures.html#all-lectures",
    "title": "MIT 18.065 Matrix Methods in Data Analysis",
    "section": "",
    "text": "My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. This course applies linear algebra to modern data analysis and machine learning.\n\n\n\nLecture 8: Norms of Vectors and Matrices Understanding vector p-norms (\\(\\|v\\|_p\\)) and when they satisfy the triangle inequality (only for \\(p \\geq 1\\)). The ‚Äú\\(\\frac{1}{2}\\)-norm‚Äù creates non-convex unit balls for strong sparsity. S-norms \\(\\sqrt{v^T S v}\\) generalize to ellipses. Matrix norms: spectral \\(\\|A\\|_2 = \\sigma_1\\), Frobenius \\(\\|A\\|_F = \\sqrt{\\sum \\sigma_i^2}\\), nuclear \\(\\|A\\|_N = \\sum \\sigma_i\\)‚Äîall expressed via singular values."
  }
]