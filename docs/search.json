[
  {
    "objectID": "ML/hessian-prerequisites.html",
    "href": "ML/hessian-prerequisites.html",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "",
    "text": "My notebook\nBefore diving into optimization algorithms for deep learning (Chapter 7), we need to understand second-order derivatives in multiple dimensions. The Hessian matrix is the key tool that generalizes the concept of curvature to high-dimensional spaces."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#context",
    "href": "ML/hessian-prerequisites.html#context",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "",
    "text": "My notebook\nBefore diving into optimization algorithms for deep learning (Chapter 7), we need to understand second-order derivatives in multiple dimensions. The Hessian matrix is the key tool that generalizes the concept of curvature to high-dimensional spaces."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#why-second-derivatives-matter",
    "href": "ML/hessian-prerequisites.html#why-second-derivatives-matter",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Why Second Derivatives Matter",
    "text": "Why Second Derivatives Matter\nIn one dimension, optimizing \\(f(x)\\) involves:\n\nFirst derivative \\(f'(x) = 0\\) ‚Üí Find critical points\n\nWhy set \\(f'(x) = 0\\)? At a minimum or maximum, the slope is flat (zero)\nThink of a hill: at the very top, you stop going up ‚Üí slope = 0\nAt the bottom of a valley, you stop going down ‚Üí slope = 0\nExample: For \\(f(x) = x^2\\), we have \\(f'(x) = 2x\\). Setting \\(f'(x) = 0\\) gives \\(x = 0\\) (the minimum)\n\nSecond derivative \\(f''(x)\\) ‚Üí Classify the critical point:\n\n\\(f''(x) &gt; 0\\) ‚Üí Local minimum (curves upward like a bowl)\n\\(f''(x) &lt; 0\\) ‚Üí Local maximum (curves downward like a dome)\n\\(f''(x) = 0\\) ‚Üí Inconclusive (could be an inflection point)\nWhy needed? Not all points where \\(f'(x) = 0\\) are minima! For \\(f(x) = x^3\\), we have \\(f'(0) = 0\\) but it‚Äôs neither a min nor max.\n\n\nThe challenge: How do we extend this to functions of many variables \\(f(x_1, x_2, \\ldots, x_n)\\)?\nThe answer: The Hessian matrix captures all second-order information.\n\nVisualizing Second Derivatives\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = np.linspace(-3, 3, 200)\n\n# Three functions with different second derivatives\nf1 = x**2           # f''(x) = 2 (positive, curves up)\nf2 = -x**2          # f''(x) = -2 (negative, curves down)\nf3 = x**3           # f''(x) = 6x (changes sign at x=0)\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\n\n# Function 1: f(x) = x¬≤\naxes[0, 0].plot(x, f1, 'b-', linewidth=2)\naxes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 0].set_title(\"f(x) = x¬≤\\nf''(x) = 2 &gt; 0\\n(Curves UP)\", fontsize=10)\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel('f(x)')\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].annotate('Minimum', xy=(0, 0), xytext=(0.5, 2),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Function 2: f(x) = -x¬≤\naxes[0, 1].plot(x, f2, 'r-', linewidth=2)\naxes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0, 1].set_title(\"f(x) = -x¬≤\\nf''(x) = -2 &lt; 0\\n(Curves DOWN)\", fontsize=10)\naxes[0, 1].set_xlabel('x')\naxes[0, 1].set_ylabel('f(x)')\naxes[0, 1].grid(True, alpha=0.3)\naxes[0, 1].annotate('Maximum', xy=(0, 0), xytext=(0.5, -2),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Function 3: f(x) = x¬≥\naxes[1, 0].plot(x, f3, 'g-', linewidth=2)\naxes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[1, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[1, 0].set_title(\"f(x) = x¬≥\\nf''(x) = 6x\\n(Changes sign)\", fontsize=10)\naxes[1, 0].set_xlabel('x')\naxes[1, 0].set_ylabel('f(x)')\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].annotate('Inflection point', xy=(0, 0), xytext=(1, -10),\n                arrowprops=dict(arrowstyle='-&gt;', color='red'),\n                fontsize=9, color='red')\n\n# Hide the unused subplot\naxes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nKey observations:\n\n\n\n\n\n\n\n\n\nSecond Derivative\nCurvature\nShape\nPoint Type\n\n\n\n\n\\(f''(x) &gt; 0\\)\nCurves upward\nBowl shape\nPotential minimum\n\n\n\\(f''(x) &lt; 0\\)\nCurves downward\nDome shape\nPotential maximum\n\n\n\\(f''(x) = 0\\) (at critical point)\nChanges sign\nFlat at that point\nInflection point\n\n\n\nNote on the third example: For \\(f(x) = x^3\\), we have \\(f''(x) = 6x\\). At the critical point \\(x = 0\\), \\(f''(0) = 0\\), which is inconclusive. The curvature changes sign: negative for \\(x &lt; 0\\) and positive for \\(x &gt; 0\\)."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#the-hessian-matrix",
    "href": "ML/hessian-prerequisites.html#the-hessian-matrix",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "The Hessian Matrix",
    "text": "The Hessian Matrix\n\nDefinition\nFor a scalar function \\(f(\\mathbf{x}) = f(x_1, x_2, \\ldots, x_n)\\), the Hessian matrix is the square matrix of all second-order partial derivatives:\n\\[\nH(f) =\n\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n\\]\n\n\nKey Properties\n\nSymmetric: If mixed partial derivatives are continuous, then \\(\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}\\), so \\(H = H^T\\).\nShape: Always \\(n \\times n\\) (determined by number of variables, not terms in the function)\nDescribes curvature in all directions simultaneously\nEigenvalue decomposition: Since the Hessian is symmetric, it can be decomposed as \\(H = Q\\Lambda Q^T\\) where \\(Q\\) contains orthonormal eigenvectors and \\(\\Lambda\\) is a diagonal matrix of eigenvalues\n\n\n\nSimple Example\nFor \\(f(x, y) = x^2 + 3y^2\\):\nStep 1: Compute first derivatives \\[\n\\frac{\\partial f}{\\partial x} = 2x, \\quad \\frac{\\partial f}{\\partial y} = 6y\n\\]\nStep 2: Compute second derivatives \\[\n\\frac{\\partial^2 f}{\\partial x^2} = 2, \\quad \\frac{\\partial^2 f}{\\partial y^2} = 6, \\quad \\frac{\\partial^2 f}{\\partial x \\partial y} = 0\n\\]\nStep 3: Build Hessian \\[\nH = \\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}\n\\]\nInterpretation: - Curvature along \\(x\\)-axis: 2 - Curvature along \\(y\\)-axis: 6 - No cross-dependency (off-diagonal = 0)\n\n\nExample with Cross Terms\nFor \\(f(x, y) = x^2 + xy + y^2\\):\n\\[\nH = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\n\\]\nThe off-diagonal term (1) indicates that \\(x\\) and \\(y\\) are coupled‚Äîchanging one affects the rate of change of the other."
  },
  {
    "objectID": "ML/hessian-prerequisites.html#matrix-definiteness",
    "href": "ML/hessian-prerequisites.html#matrix-definiteness",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Matrix Definiteness",
    "text": "Matrix Definiteness\nFor a symmetric matrix \\(A\\), its definiteness is determined by the signs of its eigenvalues.\n\n\n\n\n\n\n\n\n\nType\nEigenvalues\nQuadratic Form \\(x^T A x\\)\nGeometric Shape\n\n\n\n\nPositive definite (PD)\nall \\(&gt; 0\\)\n\\(&gt; 0\\) for all \\(x \\neq 0\\)\nBowl (curves upward)\n\n\nNegative definite (ND)\nall \\(&lt; 0\\)\n\\(&lt; 0\\) for all \\(x \\neq 0\\)\nDome (curves downward)\n\n\nIndefinite\nsome \\(+\\), some \\(-\\)\ndepends on direction\nSaddle\n\n\nPositive semi-definite (PSD)\nall \\(\\geq 0\\)\n\\(\\geq 0\\) for all \\(x\\)\nFlat-bottom bowl\n\n\nNegative semi-definite (NSD)\nall \\(\\leq 0\\)\n\\(\\leq 0\\) for all \\(x\\)\nFlat-top dome\n\n\n\n\nQuick Test (2√ó2 case)\nFor \\(A = \\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}\\):\n\nPositive definite if \\(a &gt; 0\\) and \\(ac - b^2 &gt; 0\\)\nNegative definite if \\(a &lt; 0\\) and \\(ac - b^2 &gt; 0\\)\nIndefinite if \\(ac - b^2 &lt; 0\\)\n\n\n\nExamples\n\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}\\): eigenvalues = [2, 6] ‚Üí Positive definite\n\\(\\begin{bmatrix} -2 & 0 \\\\ 0 & -3 \\end{bmatrix}\\): eigenvalues = [-2, -3] ‚Üí Negative definite\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\\): eigenvalues = [2, -2] ‚Üí Indefinite\n\\(\\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix}\\): eigenvalues = [4, 0] ‚Üí Positive semi-definite"
  },
  {
    "objectID": "ML/hessian-prerequisites.html#interpreting-hessian-at-critical-points",
    "href": "ML/hessian-prerequisites.html#interpreting-hessian-at-critical-points",
    "title": "Deep Learning Book Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature",
    "section": "Interpreting Hessian at Critical Points",
    "text": "Interpreting Hessian at Critical Points\nAt a critical point where \\(\\nabla f = 0\\), the Hessian determines the nature of the point:\n\n\n\n\n\n\n\n\n\nHessian Type\nEigenvalues\nSurface Shape\nPoint Type\n\n\n\n\nPositive definite\nall positive\nBowl (convex)\nLocal minimum\n\n\nNegative definite\nall negative\nDome (concave)\nLocal maximum\n\n\nIndefinite\nmixed signs\nSaddle\nNeither min nor max\n\n\nSemi-definite\nsome zero\nFlat in some directions\nInconclusive\n\n\n\n\nVisualization: Different Surface Types\n\n\nShow code\n# Create grid for plotting\nx_grid = np.linspace(-2, 2, 100)\ny_grid = np.linspace(-2, 2, 100)\nX, Y = np.meshgrid(x_grid, y_grid)\n\n# Define different functions with different Hessian types\ndef positive_definite(x, y):\n    \"\"\"Minimum: f = x¬≤ + y¬≤\"\"\"\n    return x**2 + y**2\n\ndef negative_definite(x, y):\n    \"\"\"Maximum: f = -x¬≤ - y¬≤\"\"\"\n    return -x**2 - y**2\n\ndef indefinite(x, y):\n    \"\"\"Saddle: f = x¬≤ - y¬≤\"\"\"\n    return x**2 - y**2\n\ndef semi_definite(x, y):\n    \"\"\"Flat direction: f = x¬≤\"\"\"\n    return x**2\n\n# Create 3D surface plots in 2x2 grid\nfig = plt.figure(figsize=(12, 10))\n\nfunctions = [\n    (positive_definite, \"Positive Definite\\n(Bowl - Minimum)\", \"Greens\"),\n    (negative_definite, \"Negative Definite\\n(Dome - Maximum)\", \"Reds\"),\n    (indefinite, \"Indefinite\\n(Saddle Point)\", \"RdBu\"),\n    (semi_definite, \"Semi-Definite\\n(Flat Direction)\", \"YlOrRd\")\n]\n\nfor idx, (func, title, cmap) in enumerate(functions, 1):\n    ax = fig.add_subplot(2, 2, idx, projection='3d')\n    Z = func(X, Y)\n\n    surf = ax.plot_surface(X, Y, Z, cmap=cmap, alpha=0.8,\n                           linewidth=0, antialiased=True)\n\n    ax.set_xlabel('x', fontsize=9)\n    ax.set_ylabel('y', fontsize=9)\n    ax.set_zlabel('f(x,y)', fontsize=9)\n    ax.set_title(title, fontsize=10)\n    ax.view_init(elev=25, azim=45)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nContour plots for better understanding:\n\n\nShow code\n# Contour plots in 2x2 grid\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\naxes = axes.flatten()\n\nfor idx, (func, title, cmap) in enumerate(functions):\n    ax = axes[idx]\n    Z = func(X, Y)\n\n    contour = ax.contour(X, Y, Z, levels=15, cmap=cmap)\n    ax.clabel(contour, inline=True, fontsize=7)\n\n    # Mark the critical point at origin\n    ax.plot(0, 0, 'r*', markersize=12, label='Critical point')\n\n    ax.set_xlabel('x', fontsize=9)\n    ax.set_ylabel('y', fontsize=9)\n    ax.set_title(title, fontsize=10)\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=8)\n    ax.set_aspect('equal')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "This is for MIT 18.06SC Lecture 1, covering how to understand linear systems from two perspectives: geometry (row picture) and algebra (column picture)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#the-example-system",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#the-example-system",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "The Example System",
    "text": "The Example System\nLet‚Äôs work with this concrete example:\n\\[\\begin{align}\nx + 2y &= 5 \\\\\n3x + 4y &= 6\n\\end{align}\\]\nIn matrix form: \\[\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nWe can interpret this system in two completely different ways."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#row-picture-geometry",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#row-picture-geometry",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Row Picture (Geometry)",
    "text": "Row Picture (Geometry)\nIn the row picture, each equation represents a geometric object: - In 2D: each equation is a line - In 3D: each equation is a plane\n- In higher dimensions: each equation is a hyperplane\nThe solution is where all these objects intersect.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the equations in the form y = mx + c\n# Line 1: x + 2y = 5  =&gt;  y = -1/2*x + 5/2\n# Line 2: 3x + 4y = 6  =&gt;  y = -3/4*x + 3/2\nx = np.linspace(-10, 10, 100)\ny1 = -1/2 * x + 5/2\ny2 = -3/4 * x + 3/2\n\n# Solve for intersection point\nA = np.array([[1, 2], [3, 4]])\nb = np.array([5, 6])\nsolution = np.linalg.solve(A, b)\n\n# Plot both lines and intersection\nplt.figure(figsize=(8, 6))\nplt.plot(x, y1, 'b-', label='Line 1: x + 2y = 5', linewidth=2)\nplt.plot(x, y2, 'r-', label='Line 2: 3x + 4y = 6', linewidth=2)\nplt.scatter(solution[0], solution[1], color='green', s=100, zorder=5, \n           label=f'Solution: ({solution[0]:.1f}, {solution[1]:.1f})', edgecolor='white', linewidth=2)\n\nplt.xlim(-8, 8)\nplt.ylim(-1, 8)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Row Picture: Where Lines Meet')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Solution: x = {solution[0]:.3f}, y = {solution[1]:.3f}\")\nprint(f\"Verification: {A @ solution} equals {b}\")\n\n\n\n\n\n\n\n\n\nSolution: x = -4.000, y = 4.500\nVerification: [5. 6.] equals [5 6]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#column-picture-algebra",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#column-picture-algebra",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Column Picture (Algebra)",
    "text": "Column Picture (Algebra)\nThe column picture reframes the same system as a question about vector combinations:\n\\[x \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} + y \\begin{bmatrix}2 \\\\ 4\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nInstead of asking ‚Äúwhere do lines intersect?‚Äù, we ask: ‚ÄúCan we combine these vectors to reach our target?‚Äù\n\n\nCode\n# Define column vectors and target vector\na1 = np.array([1, 3])\na2 = np.array([2, 4])\nb = np.array([5, 6])\n\n# Solve for coefficients\nA = np.column_stack([a1, a2])\nsolution = np.linalg.solve(A, b)\nx, y = solution[0], solution[1]\n\nprint(f\"Question: Can we write b as a linear combination of a‚ÇÅ and a‚ÇÇ?\")\nprint(f\"Answer: {x:.3f} √ó a‚ÇÅ + {y:.3f} √ó a‚ÇÇ = b\")\nprint(f\"Verification: {x*a1} + {y*a2} = {x*a1 + y*a2}\")\n\n# Visualize the vector construction\nplt.figure(figsize=(8, 6))\n\n# Step 1: Draw x*a1 (scaled version)\nplt.arrow(0, 0, x*a1[0], x*a1[1], head_width=0.2, head_length=0.2, \n         fc='blue', ec='blue', linewidth=3,\n         label=f'{x:.2f} √ó a‚ÇÅ')\n\n# Step 2: Draw y*a2 starting from the tip of x*a1\nplt.arrow(x*a1[0], x*a1[1], y*a2[0], y*a2[1], head_width=0.2, head_length=0.2, \n         fc='green', ec='green', linewidth=3,\n         label=f'{y:.2f} √ó a‚ÇÇ')\n\n# Show final result vector b\nplt.arrow(0, 0, b[0], b[1], head_width=0.25, head_length=0.25, \n         fc='red', ec='red', linewidth=4, alpha=0.8,\n         label=f'b = [{b[0]}, {b[1]}]')\n\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.xlim(-1, 6)\nplt.ylim(-12, 7)\nplt.xlabel('x-component')\nplt.ylabel('y-component')\nplt.title('Column Picture: Vector Combination')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\n\n\nQuestion: Can we write b as a linear combination of a‚ÇÅ and a‚ÇÇ?\nAnswer: -4.000 √ó a‚ÇÅ + 4.500 √ó a‚ÇÇ = b\nVerification: [ -4. -12.] + [ 9. 18.] = [5. 6.]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Three Types of Linear Systems",
    "text": "Three Types of Linear Systems\nLinear systems can have three possible outcomes:\n\nUnique solution - Lines intersect at one point\nNo solution - Lines are parallel (don‚Äôt intersect)\nInfinitely many solutions - Lines are the same (overlap completely)\n\n\n\nCode\n# Case (a): Unique solution - non-parallel vectors\nprint(\"üéØ Case (a) - Unique Solution:\")\nA_a = np.array([[1, 2], [3, 4]])\nb_a = np.array([5, 6])\nsolution_a = np.linalg.solve(A_a, b_a)\ndet_a = np.linalg.det(A_a)\nprint(f\"   Solution: {solution_a}\")\nprint(f\"   Matrix determinant: {det_a:.3f} ‚â† 0 ‚Üí linearly independent columns\")\nprint(f\"   Column space: ENTIRE 2D plane (any point reachable)\")\n\n# Case (b): No solution - parallel vectors, b not in span\nprint(f\"\\n‚ùå Case (b) - No Solution:\")\nA_b = np.array([[1, 2], [2, 4]])  # Columns are parallel\nb_b = np.array([5, 6])            # b not in span\ndet_b = np.linalg.det(A_b)\nprint(f\"   Matrix determinant: {det_b:.3f} = 0 ‚Üí linearly dependent columns\")\nprint(f\"   Column space: 1D line only (most points unreachable)\")\nprint(f\"   Target b = {b_b} is NOT on the line ‚Üí No solution exists\")\n\n# Case (c): Infinitely many solutions - parallel vectors, b in span\nprint(f\"\\n‚ôæÔ∏è  Case (c) - Infinitely Many Solutions:\")\nA_c = np.array([[1, 2], [2, 4]])  # Same parallel columns\nb_c = np.array([3, 6])            # b = 3 * [1, 2], so b is in span\ndet_c = np.linalg.det(A_c)\nprint(f\"   Matrix determinant: {det_c:.3f} = 0 ‚Üí linearly dependent columns\")\nprint(f\"   Column space: 1D line only\")\nprint(f\"   Target b = {b_c} IS on the line ‚Üí Infinite solutions exist\")\n\n# Find one particular solution using pseudoinverse\nsolution_c = np.linalg.pinv(A_c) @ b_c\nprint(f\"   One particular solution: {solution_c}\")\nprint(f\"   Other solutions: {solution_c} + t√ó[2, -1] for any real number t\")\n\n\nüéØ Case (a) - Unique Solution:\n   Solution: [-4.   4.5]\n   Matrix determinant: -2.000 ‚â† 0 ‚Üí linearly independent columns\n   Column space: ENTIRE 2D plane (any point reachable)\n\n‚ùå Case (b) - No Solution:\n   Matrix determinant: 0.000 = 0 ‚Üí linearly dependent columns\n   Column space: 1D line only (most points unreachable)\n   Target b = [5 6] is NOT on the line ‚Üí No solution exists\n\n‚ôæÔ∏è  Case (c) - Infinitely Many Solutions:\n   Matrix determinant: 0.000 = 0 ‚Üí linearly dependent columns\n   Column space: 1D line only\n   Target b = [3 6] IS on the line ‚Üí Infinite solutions exist\n   One particular solution: [0.6 1.2]\n   Other solutions: [0.6 1.2] + t√ó[2, -1] for any real number t\n\n\n\n\nCode\n# Visualize all three cases\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Case (a): Unique solution\nax = axes[0]\nax.fill_between([-1, 6], [-1, -1], [7, 7], color='lightblue', alpha=0.2, \n                label='Column space = ENTIRE plane')\n\n# Draw vectors\nax.arrow(0, 0, A_a[0,0], A_a[1,0], head_width=0.15, head_length=0.15,\n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,3]')\nax.arrow(0, 0, A_a[0,1], A_a[1,1], head_width=0.15, head_length=0.15,\n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4]')\nax.arrow(0, 0, b_a[0], b_a[1], head_width=0.2, head_length=0.2,\n         fc='red', ec='red', linewidth=3, label='b = [5,6]')\n\nax.set_title('Unique Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (b): No solution\nax = axes[1]\nt = np.linspace(-2, 5, 100)\nspan_x, span_y = t * A_b[0,0], t * A_b[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6, \n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_b[0,0], A_b[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,2]')\nax.arrow(0, 0, A_b[0,1], A_b[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4] = 2√óa‚ÇÅ')\nax.arrow(0, 0, b_b[0], b_b[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [5,6] (off line)')\n\nax.set_title('No Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (c): Infinitely many solutions\nax = axes[2]\nt = np.linspace(-1, 4, 100)\nspan_x, span_y = t * A_c[0,0], t * A_c[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6,\n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_c[0,0], A_c[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,2]')\nax.arrow(0, 0, A_c[0,1], A_c[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4] = 2√óa‚ÇÅ')\nax.arrow(0, 0, b_c[0], b_c[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [3,6] (on line)')\n\nax.set_title('Infinite Solutions')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key insight: Solution depends on whether target vector b lies in the column space\")\n\n\n\n\n\n\n\n\n\nKey insight: Solution depends on whether target vector b lies in the column space\n\n\n\nThis covers the core geometric foundations from MIT 18.06SC Lecture 1: understanding linear systems through both row and column perspectives."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strang‚Äôs fourth lecture introduces one of the most important matrix factorizations: LU decomposition, which factors any invertible matrix \\(A\\) into the product of a Lower triangular matrix and an Upper triangular matrix. This factorization is the foundation of efficient numerical linear algebra."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#context",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#context",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strang‚Äôs fourth lecture introduces one of the most important matrix factorizations: LU decomposition, which factors any invertible matrix \\(A\\) into the product of a Lower triangular matrix and an Upper triangular matrix. This factorization is the foundation of efficient numerical linear algebra."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#what-is-lu-decomposition",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#what-is-lu-decomposition",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "What is LU Decomposition?",
    "text": "What is LU Decomposition?\nGoal: Factor any invertible matrix \\(A\\) as the product of: - \\(L\\) = Lower triangular matrix (with 1‚Äôs on diagonal) - \\(U\\) = Upper triangular matrix (the result of elimination)\n\\[\nA = LU\n\\]\n\nWhy is this useful?\n\nEfficient solving: \\(Ax = b\\) becomes two simpler triangular solves:\nStep 1 - Forward substitution: Solve \\(Lc = b\\) for \\(c\\)\nStep 2 - Back substitution: Solve \\(Ux = c\\) for \\(x\\)\nHow this works:\nSince \\(A = LU\\), we have \\(Ax = LUx = b\\). Let \\(Ux = c\\), then: \\[\nLUx = Lc = b\n\\]\nForward substitution (solving \\(Lc = b\\)):\nSince \\(L\\) is lower triangular with 1‚Äôs on the diagonal, we can solve for \\(c\\) step by step: \\[\n\\begin{aligned}\nc_1 &= b_1 \\\\\nc_2 &= b_2 - m_{21}c_1 \\\\\nc_3 &= b_3 - m_{31}c_1 - m_{32}c_2 \\\\\n&\\vdots\n\\end{aligned}\n\\]\nEach \\(c_i\\) depends only on previously computed values, so we solve forward from \\(c_1\\) to \\(c_n\\).\nBack substitution (solving \\(Ux = c\\)):\nSince \\(U\\) is upper triangular, we solve backward from \\(x_n\\) to \\(x_1\\): \\[\n\\begin{aligned}\nx_n &= \\frac{c_n}{u_{nn}} \\\\\nx_{n-1} &= \\frac{c_{n-1} - u_{n-1,n}x_n}{u_{n-1,n-1}} \\\\\n&\\vdots\n\\end{aligned}\n\\]\nResult: We‚Äôve solved \\(Ax = b\\) without ever explicitly computing \\(A^{-1}\\)!\nReusable factorization: When \\(A\\) is fixed but \\(b\\) changes, we can reuse \\(L\\) and \\(U\\)\n\nFactorization: \\(O(n^3)\\) operations (done once)\nEach solve: \\(O(n^2)\\) operations\nHuge savings for multiple right-hand sides!\n\nFoundation of numerical computing: Used in MATLAB, NumPy, and all scientific computing libraries"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#how-elimination-creates-u",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#how-elimination-creates-u",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "How Elimination Creates U",
    "text": "How Elimination Creates U\n\nThe Elimination Process\nStarting with \\(A\\), we apply elimination matrices \\(E_{21}, E_{31}, E_{32}, \\ldots\\) to get upper triangular \\(U\\):\n\\[\nE_{32} E_{31} E_{21} A = U\n\\]\nExample (3√ó3 case):\n\\[\nA = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\n\\]\nStep 1: Eliminate below first pivot (rows 2 and 3)\n\\[\nE_{21} = \\begin{bmatrix} 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad\nE_{31} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix}\n\\]\nStep 2: Eliminate below second pivot (row 3)\n\\[\nE_{32} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -1 & 1 \\end{bmatrix}\n\\]\n\n\nStructure of Elimination Matrices\nAn elimination matrix \\(E_{ij}\\) eliminates the entry at position \\((i,j)\\) by subtracting a multiple of row \\(j\\) from row \\(i\\).\nGeneral form: \\[\nE_{ij} = I - m_{ij} \\mathbf{e}_i \\mathbf{e}_j^T\n\\]\nwhere: - \\(m_{ij}\\) = multiplier = \\(\\frac{A_{ij}}{\\text{pivot at } (j,j)}\\) - \\(\\mathbf{e}_i\\) = \\(i\\)-th standard basis vector - The \\((i,j)\\) entry of \\(E_{ij}\\) is \\(-m_{ij}\\)\nKey properties: 1. Lower triangular (operates below diagonal) 2. Determinant = 1 (doesn‚Äôt change volume) 3. Easy to invert: \\(E_{ij}^{-1} = I + m_{ij} \\mathbf{e}_i \\mathbf{e}_j^T\\) (just flip the sign!)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#inverting-to-get-l-the-key-insight",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#inverting-to-get-l-the-key-insight",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Inverting to Get L: The Key Insight",
    "text": "Inverting to Get L: The Key Insight\nFrom elimination, we have:\n\\[\nE_{32} E_{31} E_{21} A = U\n\\]\nMultiply both sides by the inverses (in reverse order):\n\\[\nA = E_{21}^{-1} E_{31}^{-1} E_{32}^{-1} U = LU\n\\]\nwhere: \\[\nL = E_{21}^{-1} E_{31}^{-1} E_{32}^{-1}\n\\]\n\nThe Beautiful Result\nWhen elimination matrices are multiplied in the right order, their inverses combine beautifully:\n\\[\nL = \\begin{bmatrix}\n1 & 0 & 0 & \\cdots \\\\\nm_{21} & 1 & 0 & \\cdots \\\\\nm_{31} & m_{32} & 1 & \\cdots \\\\\n\\vdots & \\vdots & \\ddots & \\ddots\n\\end{bmatrix}\n\\]\nThe multipliers \\(m_{ij}\\) (used during elimination) directly fill in the entries of \\(L\\) below the diagonal!\nNo extra computation needed ‚Äî just save the multipliers as you eliminate."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#computational-complexity",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#computational-complexity",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Computational Complexity",
    "text": "Computational Complexity\n\nOperation Counts\nFor an \\(n \\times n\\) matrix:\n\n\n\nStep\nOperations\nOrder\n\n\n\n\nElimination (find U)\n\\(\\frac{n^3}{3} + O(n^2)\\)\n\\(O(n^3)\\)\n\n\nForward substitution \\((Lc = b)\\)\n\\(\\frac{n^2}{2}\\)\n\\(O(n^2)\\)\n\n\nBack substitution \\((Ux = c)\\)\n\\(\\frac{n^2}{2}\\)\n\\(O(n^2)\\)\n\n\n\n\n\nWhy \\(\\frac{n^3}{3}\\)?\nAt step \\(k\\), we update an \\((n-k) \\times (n-k)\\) submatrix:\n\\[\n\\text{Total operations} = \\sum_{k=1}^{n-1} (n-k)^2 \\approx \\int_0^n x^2 \\, dx = \\frac{n^3}{3}\n\\]\n\n\nWhen is LU Worth It?\nSingle solve: \\(Ax = b\\) costs \\(O(n^3)\\) either way\nMultiple solves: If solving \\(Ax = b_1, Ax = b_2, \\ldots, Ax = b_m\\): - Without LU: \\(m \\times O(n^3)\\) - With LU: \\(O(n^3)\\) (once) + \\(m \\times O(n^2)\\) ‚úÖ\nHuge savings when \\(A\\) is fixed but \\(b\\) changes!"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#hands-on-exercises",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#hands-on-exercises",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Hands-On Exercises",
    "text": "Hands-On Exercises\nLet‚Äôs practice LU decomposition with concrete examples.\n\n\nShow code\nimport numpy as np\n\nprint(\"‚úì Libraries imported successfully\")\n\n\n‚úì Libraries imported successfully\n\n\n\nExercise 1: Manual LU Decomposition (2√ó2)\nCompute the LU decomposition of \\(A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}\\) by hand.\nSteps: 1. Perform elimination to get \\(U\\) 2. Record the multiplier \\(m_{21}\\) to build \\(L\\) 3. Verify \\(A = LU\\)\n\n\nShow code\nfrom IPython.display import display, Markdown, Latex\n\n# Original matrix\nA = np.array([[2, 3],\n              [4, 7]])\n\ndisplay(Markdown(\"**Original matrix A:**\"))\ndisplay(Latex(r\"$$A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}$$\"))\n\n# Compute multiplier m21\nm21 = 4/2  # row2[0] / row1[0]\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 1: Compute multiplier**\"))\ndisplay(Latex(f\"$$m_{{21}} = \\\\frac{{4}}{{2}} = {m21}$$\"))\n\n# Build L matrix\nL = np.array([[1, 0],\n              [m21, 1]])\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 2: Build L matrix**\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 \\\\ m_{21} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}$$\"))\n\n# Build U matrix (result after elimination)\n# After: row2 = row2 - m21*row1\n# [2, 3]        [2, 3]\n# [4, 7]  --&gt;   [0, 1]  (because 7 - 2*3 = 1)\nU = np.array([[2, 3],\n              [0, 1]])\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 3: Build U matrix (after elimination)**\"))\ndisplay(Markdown(\"Row 2 ‚Üí Row 2 - 2 √ó Row 1\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Verification: $A = LU$**\"))\ndisplay(Latex(r\"$$LU = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix} = A \\quad \\checkmark$$\"))\n\n\nOriginal matrix A:\n\n\n\\[A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}\\]\n\n\n\n\n\nStep 1: Compute multiplier\n\n\n\\[m_{21} = \\frac{4}{2} = 2.0\\]\n\n\n\n\n\nStep 2: Build L matrix\n\n\n\\[L = \\begin{bmatrix} 1 & 0 \\\\ m_{21} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nStep 3: Build U matrix (after elimination)\n\n\nRow 2 ‚Üí Row 2 - 2 √ó Row 1\n\n\n\\[U = \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(A = LU\\)\n\n\n\\[LU = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix} = A \\quad \\checkmark\\]\n\n\nKey observation: The multiplier \\(m_{21} = 2\\) goes directly into position \\((2,1)\\) of \\(L\\)!\n\n\nExercise 2: LU Decomposition (3√ó3)\nPerform LU decomposition on:\n\\[\nA = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\n\\]\nGoal: Find \\(L\\) and \\(U\\) such that \\(A = LU\\)\n\n\nShow code\nfrom IPython.display import display, Markdown, Latex\n\nA = np.array([[2, 1, 1],\n              [4, -6, 0],\n              [-2, 7, 2]])\n\ndisplay(Markdown(\"**Original matrix A:**\"))\ndisplay(Latex(r\"$$A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Step 1: Eliminate column 1\"))\n\n# Calculate multipliers for column 1\nm21 = A[1, 0] / A[0, 0]  # 4/2 = 2\nm31 = A[2, 0] / A[0, 0]  # -2/2 = -1\n\ndisplay(Markdown(\"**Multipliers:**\"))\ndisplay(Latex(f\"$$m_{{21}} = \\\\frac{{4}}{{2}} = {m21}, \\\\quad m_{{31}} = \\\\frac{{-2}}{{2}} = {m31}$$\"))\n\n# Create A1 after first elimination\nA1 = A.copy().astype(float)\nA1[1] = A1[1] - m21 * A1[0]  # row2 - 2*row1\nA1[2] = A1[2] - m31 * A1[0]  # row3 - (-1)*row1\n\ndisplay(Markdown(\"**After eliminating column 1:**\"))\ndisplay(Markdown(\"- Row 2 ‚Üí Row 2 - 2 √ó Row 1\"))\ndisplay(Markdown(\"- Row 3 ‚Üí Row 3 - (-1) √ó Row 1\"))\ndisplay(Latex(r\"$$A^{(1)} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 8 & 3 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Step 2: Eliminate column 2\"))\n\n# Calculate multiplier for column 2\nm32 = A1[2, 1] / A1[1, 1]  # 8/(-8) = -1\n\ndisplay(Markdown(\"**Multiplier:**\"))\ndisplay(Latex(f\"$$m_{{32}} = \\\\frac{{8}}{{-8}} = {m32}$$\"))\n\n# Create U (final upper triangular)\nU = A1.copy()\nU[2] = U[2] - m32 * U[1]  # row3 - (-1)*row2\n\ndisplay(Markdown(\"**After eliminating column 2:**\"))\ndisplay(Markdown(\"- Row 3 ‚Üí Row 3 - (-1) √ó Row 2\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Build L from multipliers\"))\n\n# Build L from multipliers\nL = np.array([[1, 0, 0],\n              [m21, 1, 0],\n              [m31, m32, 1]])\n\ndisplay(Markdown(\"The multipliers directly fill in $L$:\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 & 0 \\\\ m_{21} & 1 & 0 \\\\ m_{31} & m_{32} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Verification: $A = LU$\"))\n\ndisplay(Latex(r\"$$LU = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix} = A \\quad \\checkmark$$\"))\n\n\nOriginal matrix A:\n\n\n\\[A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\\]\n\n\n\n\n\nStep 1: Eliminate column 1\n\n\nMultipliers:\n\n\n\\[m_{21} = \\frac{4}{2} = 2.0, \\quad m_{31} = \\frac{-2}{2} = -1.0\\]\n\n\nAfter eliminating column 1:\n\n\n\nRow 2 ‚Üí Row 2 - 2 √ó Row 1\n\n\n\n\nRow 3 ‚Üí Row 3 - (-1) √ó Row 1\n\n\n\n\\[A^{(1)} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 8 & 3 \\end{bmatrix}\\]\n\n\n\n\n\nStep 2: Eliminate column 2\n\n\nMultiplier:\n\n\n\\[m_{32} = \\frac{8}{-8} = -1.0\\]\n\n\nAfter eliminating column 2:\n\n\n\nRow 3 ‚Üí Row 3 - (-1) √ó Row 2\n\n\n\n\\[U = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nBuild L from multipliers\n\n\nThe multipliers directly fill in \\(L\\):\n\n\n\\[L = \\begin{bmatrix} 1 & 0 & 0 \\\\ m_{21} & 1 & 0 \\\\ m_{31} & m_{32} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(A = LU\\)\n\n\n\\[LU = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix} = A \\quad \\checkmark\\]\n\n\nKey observation: All three multipliers \\((m_{21}, m_{31}, m_{32})\\) go directly into their corresponding positions in \\(L\\):\n\\[\nL = \\begin{bmatrix}\n1 & 0 & 0 \\\\\nm_{21} & 1 & 0 \\\\\nm_{31} & m_{32} & 1\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n-1 & -1 & 1\n\\end{bmatrix}\n\\]\n\nNote: In practice, numerical libraries like SciPy provide scipy.linalg.lu() which computes LU decomposition efficiently and includes automatic row permutation (pivoting) for numerical stability.\n\n\nShow code\nfrom scipy.linalg import lu\nfrom IPython.display import display, Markdown, Latex\n\nA = np.array([[2, 1, 1],\n              [4, -6, 0],\n              [-2, 7, 2]], dtype=float)\n\n# SciPy returns P, L, U where PA = LU (P is permutation matrix)\nP, L_scipy, U_scipy = lu(A)\n\ndisplay(Markdown(\"**SciPy's LU decomposition:**\"))\ndisplay(Markdown(\"SciPy returns $P$, $L$, $U$ where $PA = LU$ ($P$ is a permutation matrix)\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Permutation matrix P:**\"))\ndisplay(Latex(r\"$$P = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\ndisplay(Markdown(\"(This swaps rows 1 and 2 for numerical stability)\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Lower triangular L:**\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0.5 & 1 & 0 \\\\ -0.5 & 1 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Upper triangular U:**\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 4 & -6 & 0 \\\\ 0 & 4 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Verification: $PA = LU$**\"))\n\n# Note: If P = I (identity), then our manual L and U should match\nif np.allclose(P, np.eye(3)):\n    display(Markdown(\"‚úì No row swaps needed! Our manual $L$ and $U$ match SciPy.\"))\nelse:\n    display(Markdown(\"‚ö† **Row swaps were performed** (pivot strategy for numerical stability).\"))\n    display(Markdown(\"SciPy chose the largest pivot to minimize rounding errors.\"))\n    display(Markdown(\"Our manual decomposition is valid but uses a different pivot order.\"))\n\n\nSciPy‚Äôs LU decomposition:\n\n\nSciPy returns \\(P\\), \\(L\\), \\(U\\) where \\(PA = LU\\) (\\(P\\) is a permutation matrix)\n\n\n\n\n\nPermutation matrix P:\n\n\n\\[P = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n(This swaps rows 1 and 2 for numerical stability)\n\n\n\n\n\nLower triangular L:\n\n\n\\[L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0.5 & 1 & 0 \\\\ -0.5 & 1 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nUpper triangular U:\n\n\n\\[U = \\begin{bmatrix} 4 & -6 & 0 \\\\ 0 & 4 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(PA = LU\\)\n\n\n‚ö† Row swaps were performed (pivot strategy for numerical stability).\n\n\nSciPy chose the largest pivot to minimize rounding errors.\n\n\nOur manual decomposition is valid but uses a different pivot order."
  },
  {
    "objectID": "index-backup.html",
    "href": "index-backup.html",
    "title": "ickma.dev",
    "section": "",
    "text": "My learning notes and thoughts on math and machine learning.\nCurrently reading the Deep Learning book.\n\n\n\n\nHow ReLU solves problems that linear models cannot handle.\n\n\n\nThe mathematical connection between probabilistic models and loss functions.\n\n\n\nExploring activation functions and their impact on neural network learning.\n\n\n\nHow depth enables hierarchical feature reuse and exponential expressiveness with fewer parameters.\n\n\n\nThe algorithm that makes training deep networks computationally feasible through efficient gradient computation.\n\n\n\nEssential second-order calculus concepts needed before Chapter 7 on optimization algorithms.\n\n\n\nHow L2 regularization shrinks weights based on Hessian eigenvalues, preserving important directions while penalizing less sensitive ones.\n\n\n\nL1 regularization uses soft thresholding to push small weights to exactly zero, creating sparse solutions that perform feature selection.\n\n\n\nRegularization as constrained optimization: penalty form vs Lagrangian with KKT conditions and min-max dual training.\n\n\n\nWhy regularization is mathematically necessary when solving under-constrained linear systems, and how it ensures invertibility.\n\n\n\nHow transforming existing data can improve generalization and combat overfitting when training data is limited.\n\n\n\nMathematical derivation showing how adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\n\n\n\n\n\nDeep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation in linear transformations.\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations\nTwo powerful perspectives that reveal the hidden beauty of linear systems: row picture vs column picture.\nLecture 2: Elimination with Matrices\nThe systematic algorithm that transforms linear systems into upper triangular form for easy solution.\nLecture 3: Matrix Multiplication and Inverse\nFive different perspectives on matrix multiplication, from element-wise computation to rank-1 decomposition, plus understanding when matrices can‚Äôt be inverted.\nLecture 4: LU Decomposition\nFactoring matrices into Lower √ó Upper triangular form: the foundation of efficient numerical linear algebra and solving multiple systems with the same matrix.\nLecture 5.1: Permutation Matrices\nPermutation matrices reorder rows and columns using a simple structure of 0s and 1s.\nLecture 5.2: Transpose\nThe transpose operation switches rows to columns, creating symmetric matrices.\nLecture 5.3: Vector Spaces\nVector spaces and subspaces: closed under addition and scalar multiplication.\nLecture 6: Column Space and Null Space\nColumn space determines which \\(b\\) make \\(Ax = b\\) solvable. Null space contains all solutions to \\(Ax = 0\\).\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nSystematic algorithm to find null space using pivot/free variables and RREF. Dimension of null space is n-r.\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nComplete solution is particular solution plus null space. Four cases based on rank: exactly determined (unique), overdetermined (0 or 1), underdetermined (infinite), and rank deficient (0 or infinite).\nLecture 9: Independence, Basis, and Dimension Linear independence prevents redundancy, basis is minimal spanning set, dimension measures space size. Rank-nullity theorem: dim(C(A)) + dim(N(A)) = n.\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix: column space, null space, row space, and left null space.\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas for subspace intersections and sums, and differential equations as vector spaces.\n\n\n\n\n\n\n\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\n\n\nDP Regex"
  },
  {
    "objectID": "index-backup.html#deep-learning-book",
    "href": "index-backup.html#deep-learning-book",
    "title": "ickma.dev",
    "section": "",
    "text": "How ReLU solves problems that linear models cannot handle.\n\n\n\nThe mathematical connection between probabilistic models and loss functions.\n\n\n\nExploring activation functions and their impact on neural network learning.\n\n\n\nHow depth enables hierarchical feature reuse and exponential expressiveness with fewer parameters.\n\n\n\nThe algorithm that makes training deep networks computationally feasible through efficient gradient computation.\n\n\n\nEssential second-order calculus concepts needed before Chapter 7 on optimization algorithms.\n\n\n\nHow L2 regularization shrinks weights based on Hessian eigenvalues, preserving important directions while penalizing less sensitive ones.\n\n\n\nL1 regularization uses soft thresholding to push small weights to exactly zero, creating sparse solutions that perform feature selection.\n\n\n\nRegularization as constrained optimization: penalty form vs Lagrangian with KKT conditions and min-max dual training.\n\n\n\nWhy regularization is mathematically necessary when solving under-constrained linear systems, and how it ensures invertibility.\n\n\n\nHow transforming existing data can improve generalization and combat overfitting when training data is limited.\n\n\n\nMathematical derivation showing how adding Gaussian noise to weights is equivalent to penalizing large gradients."
  },
  {
    "objectID": "index-backup.html#mathematics",
    "href": "index-backup.html#mathematics",
    "title": "ickma.dev",
    "section": "",
    "text": "Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots Exploring how these fundamental concepts are different perspectives on information preservation in linear transformations.\n\n\n\n\n\nLecture 1: The Geometry of Linear Equations\nTwo powerful perspectives that reveal the hidden beauty of linear systems: row picture vs column picture.\nLecture 2: Elimination with Matrices\nThe systematic algorithm that transforms linear systems into upper triangular form for easy solution.\nLecture 3: Matrix Multiplication and Inverse\nFive different perspectives on matrix multiplication, from element-wise computation to rank-1 decomposition, plus understanding when matrices can‚Äôt be inverted.\nLecture 4: LU Decomposition\nFactoring matrices into Lower √ó Upper triangular form: the foundation of efficient numerical linear algebra and solving multiple systems with the same matrix.\nLecture 5.1: Permutation Matrices\nPermutation matrices reorder rows and columns using a simple structure of 0s and 1s.\nLecture 5.2: Transpose\nThe transpose operation switches rows to columns, creating symmetric matrices.\nLecture 5.3: Vector Spaces\nVector spaces and subspaces: closed under addition and scalar multiplication.\nLecture 6: Column Space and Null Space\nColumn space determines which \\(b\\) make \\(Ax = b\\) solvable. Null space contains all solutions to \\(Ax = 0\\).\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nSystematic algorithm to find null space using pivot/free variables and RREF. Dimension of null space is n-r.\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nComplete solution is particular solution plus null space. Four cases based on rank: exactly determined (unique), overdetermined (0 or 1), underdetermined (infinite), and rank deficient (0 or infinite).\nLecture 9: Independence, Basis, and Dimension Linear independence prevents redundancy, basis is minimal spanning set, dimension measures space size. Rank-nullity theorem: dim(C(A)) + dim(N(A)) = n.\nLecture 10: Four Fundamental Subspaces The four fundamental subspaces completely characterize any matrix: column space, null space, row space, and left null space.\nLecture 11: Matrix Spaces, Rank-1, and Graphs Matrix spaces as vector spaces, rank-1 matrices, dimension formulas for subspace intersections and sums, and differential equations as vector spaces."
  },
  {
    "objectID": "index-backup.html#more",
    "href": "index-backup.html#more",
    "title": "ickma.dev",
    "section": "",
    "text": "K-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\n\n\nDP Regex"
  },
  {
    "objectID": "Math/index.html",
    "href": "Math/index.html",
    "title": "Math",
    "section": "",
    "text": "Mathematical foundations and explorations.\n\n\n\nThe Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots\n\n\n\n\n\nLecture 1: Geometry of Linear Equations\nLecture 2: Elimination with Matrices\nLecture 3: Matrix Multiplication and Inverse\nLecture 4: LU Decomposition\nLecture 5.1: Permutation Matrices\nLecture 5.2: Transpose\nLecture 5.3: Vector Spaces\nLecture 6: Column Space and Null Space\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nLecture 9: Independence, Basis, and Dimension\nLecture 10: Four Fundamental Subspaces\nLecture 11: Matrix Spaces, Rank-1, and Graphs\nLecture 12: Graphs, Networks, and Incidence Matrices\nLecture 13: Quiz 1 Review\nLecture 14: Orthogonal Vectors and Subspaces\nLecture 15: Projection onto Subspaces\nLecture 16: Projection Matrices and Least Squares\nLecture 17: Orthogonal Matrices and Gram-Schmidt\nLecture 18: Properties of Determinants\nLecture 19: Determinant Formulas and Cofactors\nLecture 20: Cramer‚Äôs Rule, Inverse Matrix, and Volume\nLecture 21: Eigenvalues and Eigenvectors\nLecture 22: Diagonalization and Powers of A\nLecture 23: Differential Equations and exp(At)\n\n\n\n\n\nLecture 1: The Column Space of A Contains All Vectors Ax\nLecture 2: Multiplying and Factoring Matrices\nLecture 8: Norms of Vectors and Matrices\nLecture 9: Four Ways to Solve Least Squares Problems"
  },
  {
    "objectID": "Math/index.html#reflections-synthesis",
    "href": "Math/index.html#reflections-synthesis",
    "title": "Math",
    "section": "",
    "text": "The Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots"
  },
  {
    "objectID": "Math/index.html#mit-18.06sc-linear-algebra",
    "href": "Math/index.html#mit-18.06sc-linear-algebra",
    "title": "Math",
    "section": "",
    "text": "Lecture 1: Geometry of Linear Equations\nLecture 2: Elimination with Matrices\nLecture 3: Matrix Multiplication and Inverse\nLecture 4: LU Decomposition\nLecture 5.1: Permutation Matrices\nLecture 5.2: Transpose\nLecture 5.3: Vector Spaces\nLecture 6: Column Space and Null Space\nLecture 7: Solving Ax=0 - Pivot Variables and Special Solutions\nLecture 8: Solving Ax=b - Complete Solution to Linear Systems\nLecture 9: Independence, Basis, and Dimension\nLecture 10: Four Fundamental Subspaces\nLecture 11: Matrix Spaces, Rank-1, and Graphs\nLecture 12: Graphs, Networks, and Incidence Matrices\nLecture 13: Quiz 1 Review\nLecture 14: Orthogonal Vectors and Subspaces\nLecture 15: Projection onto Subspaces\nLecture 16: Projection Matrices and Least Squares\nLecture 17: Orthogonal Matrices and Gram-Schmidt\nLecture 18: Properties of Determinants\nLecture 19: Determinant Formulas and Cofactors\nLecture 20: Cramer‚Äôs Rule, Inverse Matrix, and Volume\nLecture 21: Eigenvalues and Eigenvectors\nLecture 22: Diagonalization and Powers of A\nLecture 23: Differential Equations and exp(At)"
  },
  {
    "objectID": "Math/index.html#mit-18.065-linear-algebra-applications",
    "href": "Math/index.html#mit-18.065-linear-algebra-applications",
    "title": "Math",
    "section": "",
    "text": "Lecture 1: The Column Space of A Contains All Vectors Ax\nLecture 2: Multiplying and Factoring Matrices\nLecture 8: Norms of Vectors and Matrices\nLecture 9: Four Ways to Solve Least Squares Problems"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ickma.dev",
    "section": "",
    "text": "A growing collection of structured study notes and visual explanations ‚Äî written for clarity, reproducibility, and long-term memory."
  },
  {
    "objectID": "index.html#latest-updates",
    "href": "index.html#latest-updates",
    "title": "ickma.dev",
    "section": "Latest Updates",
    "text": "Latest Updates\n\n‚àá Deep Learning Book 58 chapters\nMy notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\nChapter 14: Autoencoders Autoencoders learn compressed representations by training encoder \\(h=f(x)\\) and decoder \\(r=g(h)\\) to reconstruct inputs through a bottleneck. Undercomplete autoencoders constrain \\(\\dim(h)&lt;\\dim(x)\\) to learn meaningful features. Regularized variants include sparse autoencoders (L1 penalty as Laplace prior), denoising autoencoders (learn manifold structure from corrupted inputs), and contractive autoencoders (penalize Jacobian for local invariance). Applications: dimensionality reduction, semantic hashing, and manifold learning.\n\n\nChapter 13: Linear Factor Models Linear factor models decompose observed data into latent factors: \\(x = Wh + b + \\text{noise}\\). PCA uses Gaussian priors for dimensionality reduction. ICA recovers statistically independent non-Gaussian sources for signal separation. SFA learns slowly-varying features via temporal coherence. Sparse coding enforces L1 sparsity for interpretable representations. These models form the foundation for modern unsupervised learning and generative models.\n\n\nChapter 12.5: Other Applications Collaborative filtering uses matrix factorization to learn latent user and item embeddings, decomposing ratings into user bias, item bias, and personalized interaction. Cold-start problems require side information. Recommendation systems face exploration-exploitation tradeoffs modeled as contextual bandits. Knowledge graphs represent facts as (subject, relation, object) triples; deep learning maps entities and relations to continuous embeddings for link prediction and reasoning. Evaluation challenges arise from open-world assumptions where unseen facts may be missing rather than false.\n\n\nChapter 12.4: NLP Applications N-gram models compute conditional probabilities over fixed contexts but suffer from sparsity and exponential growth. Neural language models use word embeddings to map discrete tokens into continuous space, enabling generalization across semantically similar words. High-dimensional vocabulary outputs require optimization: short lists partition frequent/rare words, hierarchical softmax reduces complexity to O(log|V|), and importance sampling approximates gradients. Attention mechanisms dynamically focus on relevant input positions, forming weighted context vectors that alleviate fixed-size representation bottlenecks in seq2seq tasks.\n\n\n\nSee all Deep Learning chapters ‚Üí\n\n\n\nüìê MIT 18.06SC Linear Algebra 36 lectures\nMy journey through MIT‚Äôs Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\n\nSee all MIT 18.06SC lectures ‚Üí\n\n\n\nüìê MIT 18.065: Linear Algebra Applications 9 lectures\nMy notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning‚Äîexploring how linear algebra powers modern applications.\n\n\nLecture 7: Eckart-Young Theorem - The Closest Rank k Matrix The Eckart-Young theorem proves that truncated SVD \\(A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^\\top\\) gives the best rank-\\(k\\) approximation. Applications: Netflix collaborative filtering predicts movie ratings using low-rank matrices, MRI reconstruction exploits natural image structure, and PCA finds optimal low-dimensional subspaces by computing dominant singular vectors of centered data.\n\n\nLecture 6: Singular Value Decomposition (SVD) SVD factorizes any \\(m \\times n\\) matrix as \\(A = U\\Sigma V^\\top\\): rotate-scale-rotate. Compute U from eigenvectors of \\(AA^\\top\\), V from eigenvectors of \\(A^\\top A\\), and \\(\\Sigma\\) from square roots of shared eigenvalues. Singular values relate to eigenvalues: \\(\\det A = \\prod \\sigma_i = \\prod \\lambda_i\\) for square matrices. Polar decomposition \\(A = (U\\Sigma U^\\top)(UV^\\top)\\) splits any matrix into symmetric positive semidefinite √ó orthogonal.\n\n\nLecture 5: Positive Definite and Semidefinite Matrices Positive definite matrices have all \\(\\lambda_i &gt; 0\\) and energy \\(x^\\top Sx &gt; 0\\) for all \\(x \\ne 0\\). Factorize as \\(S = AA^\\top\\) with full rank A. The quadratic form creates an energy bowl‚Äîconvex and minimizable via gradient descent. This connects linear algebra to optimization: deep learning minimizes loss landscapes just as we minimize \\(x^\\top Sx\\). Positive semidefinite relaxes to \\(\\lambda_i \\ge 0\\).\n\n\nLecture 4: Eigenvalues and Eigenvectors Eigenvectors \\(Ax = \\lambda x\\) preserve direction under transformation. Key properties: \\(A^k x = \\lambda^k x\\), solving difference equations \\(v_{k+1} = Av_k\\) and differential equations. Similar matrices \\(B = M^{-1}AM\\) share eigenvalues. Symmetric matrices have real eigenvalues and orthogonal eigenvectors.\n\n\n\nSee all MIT 18.065 lectures ‚Üí\n\n\n\nüìê Stanford EE 364A: Convex Optimization 9 lectures\nMy notes from Stanford EE 364A: Convex Optimization‚Äîtheory and applications of optimization problems.\n\n\nChapter 4.2: Convex Optimization Convex problems in standard form (convex objective/inequalities, affine equalities), local vs global optimality, first-order conditions, equivalent reformulations (change of variables, slack variables), and quasiconvex problems via bisection.\n\n\nChapter 4.1: Optimization Problems Basic terminology (decision variables, objective, constraints, domain, feasibility, optimal values, local optimality), standard form conversion, equivalent problems (change of variables, slack variables, constraint elimination, epigraph form), and parameter vs oracle problem descriptions.\n\n\nLecture 5.2: Monotonicity with Generalized Inequalities K-nondecreasing functions satisfy \\(x \\preceq_K y \\Rightarrow f(x)\\le f(y)\\). Gradient condition: \\(\\nabla f(x) \\succeq_{K^*} 0\\) (dual inequality). Matrix monotone examples: \\(\\mathrm{tr}(WX)\\), \\(\\mathrm{det}X\\). K-convexity extends convexity to generalized inequalities with dual characterization and composition theorems.\n\n\nLecture 5.1: Log-Concave and Log-Convex Functions Log-concave functions satisfy \\(f(\\theta x+(1-\\theta)y)\\ge f(x)^\\theta f(y)^{1-\\theta}\\). Powers \\(x^a\\) are log-concave for \\(a \\ge 0\\) and log-convex for \\(a \\le 0\\). Key properties: products preserve log-concavity, but sums do not. Integration of log-concave functions preserves log-concavity.\n\n\nLecture 4 Part 2: Conjugate and Quasiconvex Functions Conjugate functions \\(f^*(y) = \\sup_x (y^\\top x - f(x))\\) are always convex, with examples including negative logarithm and quadratic functions. Quasiconvex functions have convex sublevel sets with modified Jensen inequality \\(f(\\theta x + (1-\\theta)y) \\leq \\max\\{f(x), f(y)\\}\\). Examples include linear-fractional functions and distance ratios.\n\n\n\nSee all EE 364A lectures ‚Üí"
  },
  {
    "objectID": "index.html#more-topics",
    "href": "index.html#more-topics",
    "title": "ickma.dev",
    "section": "More Topics",
    "text": "More Topics\n\n\nMachine Learning\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\nAlgorithms\n\nDP Regex"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture7-eckart-young.html",
    "href": "Math/MIT18.065/mit18065-lecture7-eckart-young.html",
    "title": "MIT 18.065 Lecture 7: Eckart-Young Theorem - The Closest Rank k Matrix",
    "section": "",
    "text": "MIT 18.065 Course Page"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#the-low-rank-approximation-problem",
    "href": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#the-low-rank-approximation-problem",
    "title": "MIT 18.065 Lecture 7: Eckart-Young Theorem - The Closest Rank k Matrix",
    "section": "The Low-Rank Approximation Problem",
    "text": "The Low-Rank Approximation Problem\nGiven a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), how do we find the best rank-\\(k\\) approximation? That is, among all matrices \\(B\\) with \\(\\text{rank}(B) \\le k\\), which one minimizes \\(\\|A - B\\|\\)?\nThe answer comes from the Eckart-Young theorem: simply truncate the SVD at the first \\(k\\) terms."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#svd-and-rank-k-truncation",
    "href": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#svd-and-rank-k-truncation",
    "title": "MIT 18.065 Lecture 7: Eckart-Young Theorem - The Closest Rank k Matrix",
    "section": "SVD and Rank-k Truncation",
    "text": "SVD and Rank-k Truncation\nRecall the SVD decomposition:\n\\[\nA = U\\Sigma V^\\top = \\sigma_1 u_1 v_1^\\top + \\sigma_2 u_2 v_2^\\top + \\cdots + \\sigma_r u_r v_r^\\top\n\\]\nwhere \\(\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_r &gt; 0\\) are the singular values, and \\(r = \\text{rank}(A)\\).\nThe rank-k truncation keeps only the first \\(k\\) terms:\n\\[\nA_k = \\sigma_1 u_1 v_1^\\top + \\sigma_2 u_2 v_2^\\top + \\cdots + \\sigma_k u_k v_k^\\top\n\\]\nEquivalently, \\(A_k = (U\\Sigma V^\\top)_k\\) where we zero out singular values \\(\\sigma_{k+1}, \\ldots, \\sigma_r\\).\n\n\n\n\n\n\nImportantEckart-Young Theorem\n\n\n\nFor any matrix \\(B\\) with \\(\\text{rank}(B) \\le k\\):\n\\[\n\\|A - B\\| \\ge \\|A - A_k\\|\n\\]\nThis holds for the 2-norm (\\(\\|A\\|_2 = \\sigma_1\\)) and the Frobenius norm (\\(\\|A\\|_F = \\sqrt{\\sigma_1^2 + \\cdots + \\sigma_r^2}\\)).\nIn other words: The best rank-\\(k\\) approximation to \\(A\\) is the truncated SVD \\(A_k\\)."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#matrix-norms",
    "href": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#matrix-norms",
    "title": "MIT 18.065 Lecture 7: Eckart-Young Theorem - The Closest Rank k Matrix",
    "section": "Matrix Norms",
    "text": "Matrix Norms\nBefore diving into applications, let‚Äôs review the key matrix norms:\n\nVector Norms\nFor a vector \\(v \\in \\mathbb{R}^n\\):\n\nL1 norm: \\(\\|v\\|_1 = |v_1| + |v_2| + \\cdots + |v_n|\\)\nL2 norm (Euclidean): \\(\\|v\\|_2 = \\sqrt{|v_1|^2 + |v_2|^2 + \\cdots + |v_n|^2}\\)\nL‚àû norm (max norm): \\(\\|v\\|_\\infty = \\max_i |v_i|\\)\n\n\n\nMatrix Norms\nFor a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) with SVD \\(A = U\\Sigma V^\\top\\):\n\nSpectral norm (2-norm): \\(\\|A\\|_2 = \\sigma_1\\) (largest singular value)\nFrobenius norm: \\(\\|A\\|_F = \\sqrt{a_{11}^2 + a_{12}^2 + \\cdots + a_{mn}^2} = \\sqrt{\\sigma_1^2 + \\cdots + \\sigma_r^2}\\)\nNuclear norm: \\(\\|A\\|_* = \\sigma_1 + \\sigma_2 + \\cdots + \\sigma_r\\) (sum of singular values)\n\n\n\nNorm Properties\nAll norms satisfy:\n\nHomogeneity: \\(\\|cv\\| = |c| \\cdot \\|v\\|\\)\nTriangle inequality: \\(\\|v + w\\| \\le \\|v\\| + \\|w\\|\\)\nPositive definiteness: \\(\\|v\\| = 0 \\iff v = 0\\)\n\n\n\n\nVisualization of different matrix norms and their geometric interpretations"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#why-eckart-young-works-orthogonal-invariance",
    "href": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#why-eckart-young-works-orthogonal-invariance",
    "title": "MIT 18.065 Lecture 7: Eckart-Young Theorem - The Closest Rank k Matrix",
    "section": "Why Eckart-Young Works: Orthogonal Invariance",
    "text": "Why Eckart-Young Works: Orthogonal Invariance\nThe key insight behind the Eckart-Young theorem is orthogonal invariance:\n\nVector Case\nFor an orthonormal matrix \\(Q\\) and vector \\(v\\):\n\\[\n\\|Qv\\|_2^2 = (Qv)^\\top (Qv) = v^\\top Q^\\top Q v = v^\\top v = \\|v\\|_2^2\n\\]\nMultiplying by an orthogonal matrix preserves the norm!\n\n\nMatrix Case\nFor a matrix \\(A = U\\Sigma V^\\top\\) and orthogonal \\(Q\\):\n\\[\nQA = (QU)\\Sigma V^\\top\n\\]\nSince \\(QU\\) is also orthogonal, the norms remain unchanged. This property allows us to work in a simpler coordinate system where the proof becomes transparent."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#real-world-applications",
    "href": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#real-world-applications",
    "title": "MIT 18.065 Lecture 7: Eckart-Young Theorem - The Closest Rank k Matrix",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\nNetflix Prize Competition\nThe Netflix Prize famously used low-rank matrix approximation for collaborative filtering:\n\n\\(A \\in \\mathbb{R}^{m \\times n}\\): \\(m\\) users √ó \\(n\\) movies (sparse, mostly unrated)\nSVD factorization: \\(A \\approx U\\Sigma V^\\top\\)\n\n\\(U\\): user preferences/features\n\\(\\Sigma\\): strength of latent patterns\n\\(V\\): movie characteristics/genres\n\n\nStrategy: Use low-rank approximation \\(\\hat{A} = A_k\\) to predict missing ratings.\nFor each user-movie pair \\((i,j)\\) where rating is unknown:\n\\[\n\\hat{A}_{ij} = \\sum_{\\ell=1}^k \\sigma_\\ell u_{i\\ell} v_{j\\ell}\n\\]\nThis score predicts how much user \\(i\\) would enjoy movie \\(j\\).\n\n\n\nApplications of low-rank approximation in Netflix recommendation and MRI reconstruction\n\n\n\n\nMedical Imaging (MRI)\nIn MRI, we face incomplete measurements but can exploit natural structure:\n\n\\(A \\in \\mathbb{R}^{m \\times n}\\): image matrix (many pixels)\nAssumption: Real images are low rank (smooth, structured)\nGoal: Recover full image from partial measurements\n\nSVD gives the best rank-\\(k\\) approximation:\n\\[\nA = U\\Sigma V^\\top, \\quad A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^\\top\n\\]\nBy Eckart-Young, \\(A_k\\) minimizes reconstruction error among all rank-\\(k\\) matrices."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#connection-to-principal-component-analysis-pca",
    "href": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#connection-to-principal-component-analysis-pca",
    "title": "MIT 18.065 Lecture 7: Eckart-Young Theorem - The Closest Rank k Matrix",
    "section": "Connection to Principal Component Analysis (PCA)",
    "text": "Connection to Principal Component Analysis (PCA)\nPCA is fundamentally about finding the best low-rank approximation to centered data.\n\nSetting Up the Data Matrix\nSuppose we have \\(n\\) data points in \\(\\mathbb{R}^2\\) (e.g., height and age):\n\\[\nA_0 = \\begin{bmatrix} | & | & & | \\\\ v_1 & v_2 & \\cdots & v_n \\\\ | & | & & | \\end{bmatrix}\n\\]\nStep 1: Center the data by subtracting the mean of each feature:\n\\[\nA = A_0 - \\begin{bmatrix} a & a & \\cdots & a \\\\ h & h & \\cdots & h \\end{bmatrix}\n\\]\nwhere \\(a\\) = mean age, \\(h\\) = mean height.\n\n\nCovariance Matrix\nThe sample covariance matrix is:\n\\[\nC = \\frac{AA^\\top}{n-1} = \\begin{bmatrix} \\text{var}_{\\text{age}} & \\text{cov}_{\\text{age,height}} \\\\ \\text{cov}_{\\text{height,age}} & \\text{var}_{\\text{height}} \\end{bmatrix}\n\\]\nKey connection: The SVD of \\(A\\) diagonalizes the covariance matrix!\n\nThe left singular vectors \\(u_i\\) are the principal components (directions of maximum variance)\nThe singular values \\(\\sigma_i\\) relate to variance explained: \\(\\text{var}_i = \\sigma_i^2 / (n-1)\\)\n\nBy Eckart-Young, projecting onto the first \\(k\\) principal components gives the best rank-\\(k\\) approximation to the centered data."
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#exercises",
    "href": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#exercises",
    "title": "MIT 18.065 Lecture 7: Eckart-Young Theorem - The Closest Rank k Matrix",
    "section": "Exercises",
    "text": "Exercises\n\n1. Find the Closest Rank-1 Approximation\nFor each matrix, find \\(A_1\\) (the best rank-1 approximation):\n(a) \\(A = \\begin{bmatrix} 3 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)\nSolution: Already diagonal with \\(\\sigma_1 = 3\\), so\n\\[\nA_1 = \\begin{bmatrix} 3 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\n\\]\n(b) \\(A = \\begin{bmatrix} 0 & 3 \\\\ 2 & 0 \\end{bmatrix}\\)\nSolution: The largest singular value is \\(\\sigma_1 = 3\\) (since \\(A^\\top A = \\operatorname{diag}(4, 9)\\)), but we can verify the rank-1 approximation by inspection. Computing the SVD or using the dominant singular vector gives:\n\\[\nA_1 = \\begin{bmatrix} 0 & 3 \\\\ 0 & 0 \\end{bmatrix}\n\\]\n(c) \\(A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\\)\nSolution: Symmetric with eigenvalues \\(\\lambda_1 = 3\\), \\(\\lambda_2 = 1\\). The dominant eigenvector is \\(\\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), giving:\n\\[\nA_1 = 3 \\cdot \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\cdot \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 & 1 \\end{bmatrix} = \\frac{3}{2} \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1.5 & 1.5 \\\\ 1.5 & 1.5 \\end{bmatrix}\n\\]\n\n\n2. Norms of the Inverse\nIf \\(A\\) is \\(2 \\times 2\\) with singular values \\(\\sigma_1 \\ge \\sigma_2 &gt; 0\\), find:\n\n\\(\\|A^{-1}\\|_2\\)\n\\(\\|A^{-1}\\|_F^2\\)\n\nSolution:\nSince \\(A = U\\Sigma V^\\top\\), we have \\(A^{-1} = V\\Sigma^{-1} U^\\top\\) where\n\\[\n\\Sigma^{-1} = \\begin{bmatrix} 1/\\sigma_1 & 0 \\\\ 0 & 1/\\sigma_2 \\end{bmatrix}\n\\]\nThus:\n\\[\n\\|A^{-1}\\|_2 = \\frac{1}{\\sigma_2} \\quad \\text{(smallest singular value of } A \\text{ becomes largest of } A^{-1}\\text{)}\n\\]\n\\[\n\\|A^{-1}\\|_F^2 = \\left(\\frac{1}{\\sigma_1}\\right)^2 + \\left(\\frac{1}{\\sigma_2}\\right)^2 = \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}\n\\]"
  },
  {
    "objectID": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#key-takeaways",
    "href": "Math/MIT18.065/mit18065-lecture7-eckart-young.html#key-takeaways",
    "title": "MIT 18.065 Lecture 7: Eckart-Young Theorem - The Closest Rank k Matrix",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nEckart-Young theorem says the truncated SVD \\(A_k\\) is the optimal rank-\\(k\\) approximation\nThis holds for both 2-norm and Frobenius norm\nApplications span data compression (Netflix), medical imaging (MRI), and dimensionality reduction (PCA)\nThe proof relies on orthogonal invariance of norms\nUnderstanding different matrix norms helps us choose the right approximation criterion for each application"
  }
]