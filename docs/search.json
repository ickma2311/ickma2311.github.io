[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ickma.dev",
    "section": "",
    "text": "A growing collection of structured study notes and visual explanations ‚Äî written for clarity, reproducibility, and long-term memory."
  },
  {
    "objectID": "index.html#latest-updates",
    "href": "index.html#latest-updates",
    "title": "ickma.dev",
    "section": "Latest Updates",
    "text": "Latest Updates\n\n‚àá Goodfellow Deep Learning Book 64 chapters\nMy notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\nChapter 20: Deep Generative Models Deep generative modeling spans energy-based models, directed latent-variable models, and implicit generators. This chapter surveys RBMs, DBNs/DBMs, VAEs, GANs, autoregressive models, and evaluation pitfalls.\n\n\nChapter 19: Approximate Inference Exact posterior inference is intractable in deep latent models, so we optimize the ELBO instead. EM, MAP inference, and mean-field variational updates provide scalable approximations.\n\n\nChapter 18: Confronting the Partition Function Energy-based models require a partition function for normalization. This chapter follows how \\(\\nabla_\\theta \\log Z(\\theta)\\) enters the log-likelihood gradient and surveys training strategies like contrastive divergence, pseudolikelihood, score matching, NCE, and AIS that avoid or estimate \\(Z\\).\n\n\nChapter 17: Monte Carlo Methods Monte Carlo estimation replaces intractable expectations with sample averages. Importance sampling reweights proposal draws to reduce variance, while Markov chain methods like Gibbs sampling generate dependent samples when direct sampling is infeasible. Tempering improves mixing across multimodal landscapes.\n\n\n\nSee all Deep Learning chapters ‚Üí\n\n\n\nüìÑ Papers in Deep Learning 1 note\nPaper reading notes that focus on key ideas, math intuition, and practical takeaways.\n\n\nLoRA: Low-Rank Adaptation of Large Language Models Freeze the base model and learn a low‚Äërank update \\(\\\\Delta W=BA\\) for selected layers, enabling efficient fine‚Äëtuning with minimal storage.\n\n\n\nSee all Deep Learning papers ‚Üí\n\n\n\nüß™ Theory-to-Repro 1 note\nLow-level ML understanding and paper reproduction through derivations and code.\n\n\nLinear Regression via Three Solvers Solve the same least-squares objective with pseudo-inverse, convex optimization, and SGD, then compare assumptions and scalability.\n\n\n\nSee all Theory-to-Repro notes ‚Üí\n\n\n\n‚à´ Calculus 1 note\nFoundational notes on calculus, centered on rates of change, accumulation, and geometric intuition.\n\n\nGilbert Strang‚Äôs Calculus: Highlights A concise tour of derivatives, slopes, second derivatives, exponential growth, extrema, and the integral as accumulation‚Äîguided by graphs and intuition.\n\n\n\nSee all Calculus notes ‚Üí\n\n\n\nüìê MIT 18.06SC Linear Algebra 36 lectures\nMy journey through MIT‚Äôs Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.\n\n\nLecture 27: Positive Definite Matrices and Minima Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.\n\n\nLecture 26: Complex Matrices and Fast Fourier Transform Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).\n\n\nLecture 28: Similar Matrices and Jordan Form When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.\n\n\nLecture 25: Symmetric Matrices and Positive Definiteness The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.\n\n\n\nSee all MIT 18.06SC lectures ‚Üí\n\n\n\nüìê MIT 18.065: Linear Algebra Applications 17 lectures\nMy notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning‚Äîexploring how linear algebra powers modern applications.\n\n\nLecture 17: Rapidly Decreasing Singular Values Rapid singular-value decay makes matrices effectively low rank, enabling compression. Numerical rank captures this decay, and Sylvester equations explain why spectral separation drives it.\n\n\nLecture 16: Derivative of Inverse and Singular Values Order-sensitive derivatives give \\(\\frac{d}{dt}A^2 = \\dot A A + A \\dot A\\). Singular value motion follows \\(\\dot\\sigma=u^\\top \\dot A v\\), Weyl‚Äôs inequality bounds eigenvalue shifts, and nuclear norm minimization enables matrix completion.\n\n\nLecture 15: Matrix Derivatives and Eigenvalue Changes Differentiate the inverse with \\(\\dot A^{-1}=-A^{-1}\\dot A A^{-1}\\) and track eigenvalue motion via \\(\\dot\\lambda=y^\\top \\dot A x\\). Low-rank PSD updates interlace eigenvalues and bound spectral shifts.\n\n\nLecture 14: Low Rank Changes and Their Inverse Low-rank updates reuse an existing inverse via the Sherman‚ÄìMorrison‚ÄìWoodbury identity. Rank-1 updates power recursive least squares; the full SMW formula updates \\((A-UV^\\top)^{-1}\\) in terms of \\(A^{-1}\\), avoiding a full recomputation.\n\n\n\nSee all MIT 18.065 lectures ‚Üí\n\n\n\nüìê Stanford EE 364A: Convex Optimization 15 lectures\nMy notes from Stanford EE 364A: Convex Optimization‚Äîtheory and applications of optimization problems.\n\n\nChapter 5.1: The Lagrange Dual Function The dual function takes the infimum of the Lagrangian over \\(x\\), producing a concave lower bound on the primal optimum and revealing a direct connection to conjugate functions.\n\n\nChapter 4.7: Vector Optimization Vector objectives are only partially ordered, so a global optimum may not exist. Pareto optimality captures efficient trade-offs, and scalarization recovers Pareto points via weighted sums.\n\n\nChapter 4.6: Generalized Inequality Constraints Generalized inequalities replace componentwise order with cone-induced order. Cone programs unify LP and SOCP, while SDPs use positive semidefinite cones to express linear matrix inequalities.\n\n\nChapter 4.5: Geometric Programming Geometric programming uses monomials and posynomials over positive variables; a log change of variables turns constraints into convex log-sum-exp and affine forms, enabling global optimization with applications like cantilever beam design.\n\n\n\nSee all EE 364A lectures ‚Üí"
  },
  {
    "objectID": "index.html#more-topics",
    "href": "index.html#more-topics",
    "title": "ickma.dev",
    "section": "More Topics",
    "text": "More Topics\n\n\nMachine Learning\n\nK-Means Clustering\nLogistic Regression\nAxis Operations\n\n\n\nAlgorithms\n\nDP Regex"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "",
    "text": "This is for MIT 18.06SC Lecture 1, covering how to understand linear systems from two perspectives: geometry (row picture) and algebra (column picture)."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#the-example-system",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#the-example-system",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "The Example System",
    "text": "The Example System\nLet‚Äôs work with this concrete example:\n\\[\\begin{align}\nx + 2y &= 5 \\\\\n3x + 4y &= 6\n\\end{align}\\]\nIn matrix form: \\[\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nWe can interpret this system in two completely different ways."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#row-picture-geometry",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#row-picture-geometry",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Row Picture (Geometry)",
    "text": "Row Picture (Geometry)\nIn the row picture, each equation represents a geometric object: - In 2D: each equation is a line - In 3D: each equation is a plane\n- In higher dimensions: each equation is a hyperplane\nThe solution is where all these objects intersect.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the equations in the form y = mx + c\n# Line 1: x + 2y = 5  =&gt;  y = -1/2*x + 5/2\n# Line 2: 3x + 4y = 6  =&gt;  y = -3/4*x + 3/2\nx = np.linspace(-10, 10, 100)\ny1 = -1/2 * x + 5/2\ny2 = -3/4 * x + 3/2\n\n# Solve for intersection point\nA = np.array([[1, 2], [3, 4]])\nb = np.array([5, 6])\nsolution = np.linalg.solve(A, b)\n\n# Plot both lines and intersection\nplt.figure(figsize=(8, 6))\nplt.plot(x, y1, 'b-', label='Line 1: x + 2y = 5', linewidth=2)\nplt.plot(x, y2, 'r-', label='Line 2: 3x + 4y = 6', linewidth=2)\nplt.scatter(solution[0], solution[1], color='green', s=100, zorder=5, \n           label=f'Solution: ({solution[0]:.1f}, {solution[1]:.1f})', edgecolor='white', linewidth=2)\n\nplt.xlim(-8, 8)\nplt.ylim(-1, 8)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Row Picture: Where Lines Meet')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Solution: x = {solution[0]:.3f}, y = {solution[1]:.3f}\")\nprint(f\"Verification: {A @ solution} equals {b}\")\n\n\n\n\n\n\n\n\n\nSolution: x = -4.000, y = 4.500\nVerification: [5. 6.] equals [5 6]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#column-picture-algebra",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#column-picture-algebra",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Column Picture (Algebra)",
    "text": "Column Picture (Algebra)\nThe column picture reframes the same system as a question about vector combinations:\n\\[x \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} + y \\begin{bmatrix}2 \\\\ 4\\end{bmatrix} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\\]\nInstead of asking ‚Äúwhere do lines intersect?‚Äù, we ask: ‚ÄúCan we combine these vectors to reach our target?‚Äù\n\n\nCode\n# Define column vectors and target vector\na1 = np.array([1, 3])\na2 = np.array([2, 4])\nb = np.array([5, 6])\n\n# Solve for coefficients\nA = np.column_stack([a1, a2])\nsolution = np.linalg.solve(A, b)\nx, y = solution[0], solution[1]\n\nprint(f\"Question: Can we write b as a linear combination of a‚ÇÅ and a‚ÇÇ?\")\nprint(f\"Answer: {x:.3f} √ó a‚ÇÅ + {y:.3f} √ó a‚ÇÇ = b\")\nprint(f\"Verification: {x*a1} + {y*a2} = {x*a1 + y*a2}\")\n\n# Visualize the vector construction\nplt.figure(figsize=(8, 6))\n\n# Step 1: Draw x*a1 (scaled version)\nplt.arrow(0, 0, x*a1[0], x*a1[1], head_width=0.2, head_length=0.2, \n         fc='blue', ec='blue', linewidth=3,\n         label=f'{x:.2f} √ó a‚ÇÅ')\n\n# Step 2: Draw y*a2 starting from the tip of x*a1\nplt.arrow(x*a1[0], x*a1[1], y*a2[0], y*a2[1], head_width=0.2, head_length=0.2, \n         fc='green', ec='green', linewidth=3,\n         label=f'{y:.2f} √ó a‚ÇÇ')\n\n# Show final result vector b\nplt.arrow(0, 0, b[0], b[1], head_width=0.25, head_length=0.25, \n         fc='red', ec='red', linewidth=4, alpha=0.8,\n         label=f'b = [{b[0]}, {b[1]}]')\n\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.xlim(-1, 6)\nplt.ylim(-12, 7)\nplt.xlabel('x-component')\nplt.ylabel('y-component')\nplt.title('Column Picture: Vector Combination')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\nIgnoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\n\n\nQuestion: Can we write b as a linear combination of a‚ÇÅ and a‚ÇÇ?\nAnswer: -4.000 √ó a‚ÇÅ + 4.500 √ó a‚ÇÇ = b\nVerification: [ -4. -12.] + [ 9. 18.] = [5. 6.]"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "href": "Math/MIT18.06/mit1806-lecture1-geometry.html#three-types-of-linear-systems",
    "title": "MIT 18.06SC Lecture 1: The Geometry of Linear Equations",
    "section": "Three Types of Linear Systems",
    "text": "Three Types of Linear Systems\nLinear systems can have three possible outcomes:\n\nUnique solution - Lines intersect at one point\nNo solution - Lines are parallel (don‚Äôt intersect)\nInfinitely many solutions - Lines are the same (overlap completely)\n\n\n\nCode\n# Case (a): Unique solution - non-parallel vectors\nprint(\"üéØ Case (a) - Unique Solution:\")\nA_a = np.array([[1, 2], [3, 4]])\nb_a = np.array([5, 6])\nsolution_a = np.linalg.solve(A_a, b_a)\ndet_a = np.linalg.det(A_a)\nprint(f\"   Solution: {solution_a}\")\nprint(f\"   Matrix determinant: {det_a:.3f} ‚â† 0 ‚Üí linearly independent columns\")\nprint(f\"   Column space: ENTIRE 2D plane (any point reachable)\")\n\n# Case (b): No solution - parallel vectors, b not in span\nprint(f\"\\n‚ùå Case (b) - No Solution:\")\nA_b = np.array([[1, 2], [2, 4]])  # Columns are parallel\nb_b = np.array([5, 6])            # b not in span\ndet_b = np.linalg.det(A_b)\nprint(f\"   Matrix determinant: {det_b:.3f} = 0 ‚Üí linearly dependent columns\")\nprint(f\"   Column space: 1D line only (most points unreachable)\")\nprint(f\"   Target b = {b_b} is NOT on the line ‚Üí No solution exists\")\n\n# Case (c): Infinitely many solutions - parallel vectors, b in span\nprint(f\"\\n‚ôæÔ∏è  Case (c) - Infinitely Many Solutions:\")\nA_c = np.array([[1, 2], [2, 4]])  # Same parallel columns\nb_c = np.array([3, 6])            # b = 3 * [1, 2], so b is in span\ndet_c = np.linalg.det(A_c)\nprint(f\"   Matrix determinant: {det_c:.3f} = 0 ‚Üí linearly dependent columns\")\nprint(f\"   Column space: 1D line only\")\nprint(f\"   Target b = {b_c} IS on the line ‚Üí Infinite solutions exist\")\n\n# Find one particular solution using pseudoinverse\nsolution_c = np.linalg.pinv(A_c) @ b_c\nprint(f\"   One particular solution: {solution_c}\")\nprint(f\"   Other solutions: {solution_c} + t√ó[2, -1] for any real number t\")\n\n\nüéØ Case (a) - Unique Solution:\n   Solution: [-4.   4.5]\n   Matrix determinant: -2.000 ‚â† 0 ‚Üí linearly independent columns\n   Column space: ENTIRE 2D plane (any point reachable)\n\n‚ùå Case (b) - No Solution:\n   Matrix determinant: 0.000 = 0 ‚Üí linearly dependent columns\n   Column space: 1D line only (most points unreachable)\n   Target b = [5 6] is NOT on the line ‚Üí No solution exists\n\n‚ôæÔ∏è  Case (c) - Infinitely Many Solutions:\n   Matrix determinant: 0.000 = 0 ‚Üí linearly dependent columns\n   Column space: 1D line only\n   Target b = [3 6] IS on the line ‚Üí Infinite solutions exist\n   One particular solution: [0.6 1.2]\n   Other solutions: [0.6 1.2] + t√ó[2, -1] for any real number t\n\n\n\n\nCode\n# Visualize all three cases\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Case (a): Unique solution\nax = axes[0]\nax.fill_between([-1, 6], [-1, -1], [7, 7], color='lightblue', alpha=0.2, \n                label='Column space = ENTIRE plane')\n\n# Draw vectors\nax.arrow(0, 0, A_a[0,0], A_a[1,0], head_width=0.15, head_length=0.15,\n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,3]')\nax.arrow(0, 0, A_a[0,1], A_a[1,1], head_width=0.15, head_length=0.15,\n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4]')\nax.arrow(0, 0, b_a[0], b_a[1], head_width=0.2, head_length=0.2,\n         fc='red', ec='red', linewidth=3, label='b = [5,6]')\n\nax.set_title('Unique Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (b): No solution\nax = axes[1]\nt = np.linspace(-2, 5, 100)\nspan_x, span_y = t * A_b[0,0], t * A_b[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6, \n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_b[0,0], A_b[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,2]')\nax.arrow(0, 0, A_b[0,1], A_b[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4] = 2√óa‚ÇÅ')\nax.arrow(0, 0, b_b[0], b_b[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [5,6] (off line)')\n\nax.set_title('No Solution')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\n# Case (c): Infinitely many solutions\nax = axes[2]\nt = np.linspace(-1, 4, 100)\nspan_x, span_y = t * A_c[0,0], t * A_c[1,0]\nax.plot(span_x, span_y, 'lightblue', linewidth=6, alpha=0.6,\n        label='Column space (1D line)')\n\nax.arrow(0, 0, A_c[0,0], A_c[1,0], head_width=0.15, head_length=0.15, \n         fc='blue', ec='blue', linewidth=2, label='a‚ÇÅ = [1,2]')\nax.arrow(0, 0, A_c[0,1], A_c[1,1], head_width=0.15, head_length=0.15, \n         fc='green', ec='green', linewidth=2, label='a‚ÇÇ = [2,4] = 2√óa‚ÇÅ')\nax.arrow(0, 0, b_c[0], b_c[1], head_width=0.2, head_length=0.2, \n         fc='red', ec='red', linewidth=3, label='b = [3,6] (on line)')\n\nax.set_title('Infinite Solutions')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 7)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key insight: Solution depends on whether target vector b lies in the column space\")\n\n\n\n\n\n\n\n\n\nKey insight: Solution depends on whether target vector b lies in the column space\n\n\n\nThis covers the core geometric foundations from MIT 18.06SC Lecture 1: understanding linear systems through both row and column perspectives."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strang‚Äôs fourth lecture introduces one of the most important matrix factorizations: LU decomposition, which factors any invertible matrix \\(A\\) into the product of a Lower triangular matrix and an Upper triangular matrix. This factorization is the foundation of efficient numerical linear algebra."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#context",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#context",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "",
    "text": "My lecture notes | Exercises notebook\nGilbert Strang‚Äôs fourth lecture introduces one of the most important matrix factorizations: LU decomposition, which factors any invertible matrix \\(A\\) into the product of a Lower triangular matrix and an Upper triangular matrix. This factorization is the foundation of efficient numerical linear algebra."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#what-is-lu-decomposition",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#what-is-lu-decomposition",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "What is LU Decomposition?",
    "text": "What is LU Decomposition?\nGoal: Factor any invertible matrix \\(A\\) as the product of: - \\(L\\) = Lower triangular matrix (with 1‚Äôs on diagonal) - \\(U\\) = Upper triangular matrix (the result of elimination)\n\\[\nA = LU\n\\]\n\nWhy is this useful?\n\nEfficient solving: \\(Ax = b\\) becomes two simpler triangular solves:\nStep 1 - Forward substitution: Solve \\(Lc = b\\) for \\(c\\)\nStep 2 - Back substitution: Solve \\(Ux = c\\) for \\(x\\)\nHow this works:\nSince \\(A = LU\\), we have \\(Ax = LUx = b\\). Let \\(Ux = c\\), then: \\[\nLUx = Lc = b\n\\]\nForward substitution (solving \\(Lc = b\\)):\nSince \\(L\\) is lower triangular with 1‚Äôs on the diagonal, we can solve for \\(c\\) step by step: \\[\n\\begin{aligned}\nc_1 &= b_1 \\\\\nc_2 &= b_2 - m_{21}c_1 \\\\\nc_3 &= b_3 - m_{31}c_1 - m_{32}c_2 \\\\\n&\\vdots\n\\end{aligned}\n\\]\nEach \\(c_i\\) depends only on previously computed values, so we solve forward from \\(c_1\\) to \\(c_n\\).\nBack substitution (solving \\(Ux = c\\)):\nSince \\(U\\) is upper triangular, we solve backward from \\(x_n\\) to \\(x_1\\): \\[\n\\begin{aligned}\nx_n &= \\frac{c_n}{u_{nn}} \\\\\nx_{n-1} &= \\frac{c_{n-1} - u_{n-1,n}x_n}{u_{n-1,n-1}} \\\\\n&\\vdots\n\\end{aligned}\n\\]\nResult: We‚Äôve solved \\(Ax = b\\) without ever explicitly computing \\(A^{-1}\\)!\nReusable factorization: When \\(A\\) is fixed but \\(b\\) changes, we can reuse \\(L\\) and \\(U\\)\n\nFactorization: \\(O(n^3)\\) operations (done once)\nEach solve: \\(O(n^2)\\) operations\nHuge savings for multiple right-hand sides!\n\nFoundation of numerical computing: Used in MATLAB, NumPy, and all scientific computing libraries"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#how-elimination-creates-u",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#how-elimination-creates-u",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "How Elimination Creates U",
    "text": "How Elimination Creates U\n\nThe Elimination Process\nStarting with \\(A\\), we apply elimination matrices \\(E_{21}, E_{31}, E_{32}, \\ldots\\) to get upper triangular \\(U\\):\n\\[\nE_{32} E_{31} E_{21} A = U\n\\]\nExample (3√ó3 case):\n\\[\nA = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\n\\]\nStep 1: Eliminate below first pivot (rows 2 and 3)\n\\[\nE_{21} = \\begin{bmatrix} 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad\nE_{31} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix}\n\\]\nStep 2: Eliminate below second pivot (row 3)\n\\[\nE_{32} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -1 & 1 \\end{bmatrix}\n\\]\n\n\nStructure of Elimination Matrices\nAn elimination matrix \\(E_{ij}\\) eliminates the entry at position \\((i,j)\\) by subtracting a multiple of row \\(j\\) from row \\(i\\).\nGeneral form: \\[\nE_{ij} = I - m_{ij} \\mathbf{e}_i \\mathbf{e}_j^T\n\\]\nwhere: - \\(m_{ij}\\) = multiplier = \\(\\frac{A_{ij}}{\\text{pivot at } (j,j)}\\) - \\(\\mathbf{e}_i\\) = \\(i\\)-th standard basis vector - The \\((i,j)\\) entry of \\(E_{ij}\\) is \\(-m_{ij}\\)\nKey properties: 1. Lower triangular (operates below diagonal) 2. Determinant = 1 (doesn‚Äôt change volume) 3. Easy to invert: \\(E_{ij}^{-1} = I + m_{ij} \\mathbf{e}_i \\mathbf{e}_j^T\\) (just flip the sign!)"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#inverting-to-get-l-the-key-insight",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#inverting-to-get-l-the-key-insight",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Inverting to Get L: The Key Insight",
    "text": "Inverting to Get L: The Key Insight\nFrom elimination, we have:\n\\[\nE_{32} E_{31} E_{21} A = U\n\\]\nMultiply both sides by the inverses (in reverse order):\n\\[\nA = E_{21}^{-1} E_{31}^{-1} E_{32}^{-1} U = LU\n\\]\nwhere: \\[\nL = E_{21}^{-1} E_{31}^{-1} E_{32}^{-1}\n\\]\n\nThe Beautiful Result\nWhen elimination matrices are multiplied in the right order, their inverses combine beautifully:\n\\[\nL = \\begin{bmatrix}\n1 & 0 & 0 & \\cdots \\\\\nm_{21} & 1 & 0 & \\cdots \\\\\nm_{31} & m_{32} & 1 & \\cdots \\\\\n\\vdots & \\vdots & \\ddots & \\ddots\n\\end{bmatrix}\n\\]\nThe multipliers \\(m_{ij}\\) (used during elimination) directly fill in the entries of \\(L\\) below the diagonal!\nNo extra computation needed ‚Äî just save the multipliers as you eliminate."
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#computational-complexity",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#computational-complexity",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Computational Complexity",
    "text": "Computational Complexity\n\nOperation Counts\nFor an \\(n \\times n\\) matrix:\n\n\n\nStep\nOperations\nOrder\n\n\n\n\nElimination (find U)\n\\(\\frac{n^3}{3} + O(n^2)\\)\n\\(O(n^3)\\)\n\n\nForward substitution \\((Lc = b)\\)\n\\(\\frac{n^2}{2}\\)\n\\(O(n^2)\\)\n\n\nBack substitution \\((Ux = c)\\)\n\\(\\frac{n^2}{2}\\)\n\\(O(n^2)\\)\n\n\n\n\n\nWhy \\(\\frac{n^3}{3}\\)?\nAt step \\(k\\), we update an \\((n-k) \\times (n-k)\\) submatrix:\n\\[\n\\text{Total operations} = \\sum_{k=1}^{n-1} (n-k)^2 \\approx \\int_0^n x^2 \\, dx = \\frac{n^3}{3}\n\\]\n\n\nWhen is LU Worth It?\nSingle solve: \\(Ax = b\\) costs \\(O(n^3)\\) either way\nMultiple solves: If solving \\(Ax = b_1, Ax = b_2, \\ldots, Ax = b_m\\): - Without LU: \\(m \\times O(n^3)\\) - With LU: \\(O(n^3)\\) (once) + \\(m \\times O(n^2)\\) ‚úÖ\nHuge savings when \\(A\\) is fixed but \\(b\\) changes!"
  },
  {
    "objectID": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#hands-on-exercises",
    "href": "Math/MIT18.06/mit1806-lecture4-lu-decomposition.html#hands-on-exercises",
    "title": "MIT 18.06SC Lecture 4: LU Decomposition",
    "section": "Hands-On Exercises",
    "text": "Hands-On Exercises\nLet‚Äôs practice LU decomposition with concrete examples.\n\n\nShow code\nimport numpy as np\n\nprint(\"‚úì Libraries imported successfully\")\n\n\n‚úì Libraries imported successfully\n\n\n\nExercise 1: Manual LU Decomposition (2√ó2)\nCompute the LU decomposition of \\(A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}\\) by hand.\nSteps: 1. Perform elimination to get \\(U\\) 2. Record the multiplier \\(m_{21}\\) to build \\(L\\) 3. Verify \\(A = LU\\)\n\n\nShow code\nfrom IPython.display import display, Markdown, Latex\n\n# Original matrix\nA = np.array([[2, 3],\n              [4, 7]])\n\ndisplay(Markdown(\"**Original matrix A:**\"))\ndisplay(Latex(r\"$$A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}$$\"))\n\n# Compute multiplier m21\nm21 = 4/2  # row2[0] / row1[0]\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 1: Compute multiplier**\"))\ndisplay(Latex(f\"$$m_{{21}} = \\\\frac{{4}}{{2}} = {m21}$$\"))\n\n# Build L matrix\nL = np.array([[1, 0],\n              [m21, 1]])\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 2: Build L matrix**\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 \\\\ m_{21} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}$$\"))\n\n# Build U matrix (result after elimination)\n# After: row2 = row2 - m21*row1\n# [2, 3]        [2, 3]\n# [4, 7]  --&gt;   [0, 1]  (because 7 - 2*3 = 1)\nU = np.array([[2, 3],\n              [0, 1]])\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Step 3: Build U matrix (after elimination)**\"))\ndisplay(Markdown(\"Row 2 ‚Üí Row 2 - 2 √ó Row 1\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Verification: $A = LU$**\"))\ndisplay(Latex(r\"$$LU = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix} = A \\quad \\checkmark$$\"))\n\n\nOriginal matrix A:\n\n\n\\[A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}\\]\n\n\n\n\n\nStep 1: Compute multiplier\n\n\n\\[m_{21} = \\frac{4}{2} = 2.0\\]\n\n\n\n\n\nStep 2: Build L matrix\n\n\n\\[L = \\begin{bmatrix} 1 & 0 \\\\ m_{21} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nStep 3: Build U matrix (after elimination)\n\n\nRow 2 ‚Üí Row 2 - 2 √ó Row 1\n\n\n\\[U = \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(A = LU\\)\n\n\n\\[LU = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix} = A \\quad \\checkmark\\]\n\n\nKey observation: The multiplier \\(m_{21} = 2\\) goes directly into position \\((2,1)\\) of \\(L\\)!\n\n\nExercise 2: LU Decomposition (3√ó3)\nPerform LU decomposition on:\n\\[\nA = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\n\\]\nGoal: Find \\(L\\) and \\(U\\) such that \\(A = LU\\)\n\n\nShow code\nfrom IPython.display import display, Markdown, Latex\n\nA = np.array([[2, 1, 1],\n              [4, -6, 0],\n              [-2, 7, 2]])\n\ndisplay(Markdown(\"**Original matrix A:**\"))\ndisplay(Latex(r\"$$A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Step 1: Eliminate column 1\"))\n\n# Calculate multipliers for column 1\nm21 = A[1, 0] / A[0, 0]  # 4/2 = 2\nm31 = A[2, 0] / A[0, 0]  # -2/2 = -1\n\ndisplay(Markdown(\"**Multipliers:**\"))\ndisplay(Latex(f\"$$m_{{21}} = \\\\frac{{4}}{{2}} = {m21}, \\\\quad m_{{31}} = \\\\frac{{-2}}{{2}} = {m31}$$\"))\n\n# Create A1 after first elimination\nA1 = A.copy().astype(float)\nA1[1] = A1[1] - m21 * A1[0]  # row2 - 2*row1\nA1[2] = A1[2] - m31 * A1[0]  # row3 - (-1)*row1\n\ndisplay(Markdown(\"**After eliminating column 1:**\"))\ndisplay(Markdown(\"- Row 2 ‚Üí Row 2 - 2 √ó Row 1\"))\ndisplay(Markdown(\"- Row 3 ‚Üí Row 3 - (-1) √ó Row 1\"))\ndisplay(Latex(r\"$$A^{(1)} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 8 & 3 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Step 2: Eliminate column 2\"))\n\n# Calculate multiplier for column 2\nm32 = A1[2, 1] / A1[1, 1]  # 8/(-8) = -1\n\ndisplay(Markdown(\"**Multiplier:**\"))\ndisplay(Latex(f\"$$m_{{32}} = \\\\frac{{8}}{{-8}} = {m32}$$\"))\n\n# Create U (final upper triangular)\nU = A1.copy()\nU[2] = U[2] - m32 * U[1]  # row3 - (-1)*row2\n\ndisplay(Markdown(\"**After eliminating column 2:**\"))\ndisplay(Markdown(\"- Row 3 ‚Üí Row 3 - (-1) √ó Row 2\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Build L from multipliers\"))\n\n# Build L from multipliers\nL = np.array([[1, 0, 0],\n              [m21, 1, 0],\n              [m31, m32, 1]])\n\ndisplay(Markdown(\"The multipliers directly fill in $L$:\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 & 0 \\\\ m_{21} & 1 & 0 \\\\ m_{31} & m_{32} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"### Verification: $A = LU$\"))\n\ndisplay(Latex(r\"$$LU = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix} = A \\quad \\checkmark$$\"))\n\n\nOriginal matrix A:\n\n\n\\[A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix}\\]\n\n\n\n\n\nStep 1: Eliminate column 1\n\n\nMultipliers:\n\n\n\\[m_{21} = \\frac{4}{2} = 2.0, \\quad m_{31} = \\frac{-2}{2} = -1.0\\]\n\n\nAfter eliminating column 1:\n\n\n\nRow 2 ‚Üí Row 2 - 2 √ó Row 1\n\n\n\n\nRow 3 ‚Üí Row 3 - (-1) √ó Row 1\n\n\n\n\\[A^{(1)} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 8 & 3 \\end{bmatrix}\\]\n\n\n\n\n\nStep 2: Eliminate column 2\n\n\nMultiplier:\n\n\n\\[m_{32} = \\frac{8}{-8} = -1.0\\]\n\n\nAfter eliminating column 2:\n\n\n\nRow 3 ‚Üí Row 3 - (-1) √ó Row 2\n\n\n\n\\[U = \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nBuild L from multipliers\n\n\nThe multipliers directly fill in \\(L\\):\n\n\n\\[L = \\begin{bmatrix} 1 & 0 & 0 \\\\ m_{21} & 1 & 0 \\\\ m_{31} & m_{32} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(A = LU\\)\n\n\n\\[LU = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & -8 & -2 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{bmatrix} = A \\quad \\checkmark\\]\n\n\nKey observation: All three multipliers \\((m_{21}, m_{31}, m_{32})\\) go directly into their corresponding positions in \\(L\\):\n\\[\nL = \\begin{bmatrix}\n1 & 0 & 0 \\\\\nm_{21} & 1 & 0 \\\\\nm_{31} & m_{32} & 1\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n-1 & -1 & 1\n\\end{bmatrix}\n\\]\n\nNote: In practice, numerical libraries like SciPy provide scipy.linalg.lu() which computes LU decomposition efficiently and includes automatic row permutation (pivoting) for numerical stability.\n\n\nShow code\nfrom scipy.linalg import lu\nfrom IPython.display import display, Markdown, Latex\n\nA = np.array([[2, 1, 1],\n              [4, -6, 0],\n              [-2, 7, 2]], dtype=float)\n\n# SciPy returns P, L, U where PA = LU (P is permutation matrix)\nP, L_scipy, U_scipy = lu(A)\n\ndisplay(Markdown(\"**SciPy's LU decomposition:**\"))\ndisplay(Markdown(\"SciPy returns $P$, $L$, $U$ where $PA = LU$ ($P$ is a permutation matrix)\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Permutation matrix P:**\"))\ndisplay(Latex(r\"$$P = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\ndisplay(Markdown(\"(This swaps rows 1 and 2 for numerical stability)\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Lower triangular L:**\"))\ndisplay(Latex(r\"$$L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0.5 & 1 & 0 \\\\ -0.5 & 1 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Upper triangular U:**\"))\ndisplay(Latex(r\"$$U = \\begin{bmatrix} 4 & -6 & 0 \\\\ 0 & 4 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\"))\n\ndisplay(Markdown(\"&lt;hr /&gt;\"))\ndisplay(Markdown(\"**Verification: $PA = LU$**\"))\n\n# Note: If P = I (identity), then our manual L and U should match\nif np.allclose(P, np.eye(3)):\n    display(Markdown(\"‚úì No row swaps needed! Our manual $L$ and $U$ match SciPy.\"))\nelse:\n    display(Markdown(\"‚ö† **Row swaps were performed** (pivot strategy for numerical stability).\"))\n    display(Markdown(\"SciPy chose the largest pivot to minimize rounding errors.\"))\n    display(Markdown(\"Our manual decomposition is valid but uses a different pivot order.\"))\n\n\nSciPy‚Äôs LU decomposition:\n\n\nSciPy returns \\(P\\), \\(L\\), \\(U\\) where \\(PA = LU\\) (\\(P\\) is a permutation matrix)\n\n\n\n\n\nPermutation matrix P:\n\n\n\\[P = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n(This swaps rows 1 and 2 for numerical stability)\n\n\n\n\n\nLower triangular L:\n\n\n\\[L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0.5 & 1 & 0 \\\\ -0.5 & 1 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nUpper triangular U:\n\n\n\\[U = \\begin{bmatrix} 4 & -6 & 0 \\\\ 0 & 4 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\n\n\n\n\n\nVerification: \\(PA = LU\\)\n\n\n‚ö† Row swaps were performed (pivot strategy for numerical stability).\n\n\nSciPy chose the largest pivot to minimize rounding errors.\n\n\nOur manual decomposition is valid but uses a different pivot order."
  },
  {
    "objectID": "ML/papers/lora.html",
    "href": "ML/papers/lora.html",
    "title": "LoRA: Low-Rank Adaptation of Large Language Models",
    "section": "",
    "text": "Paper: LoRA: Low-Rank Adaptation of Large Language Models\n\nProblem\nFull fine-tuning updates all parameters of a large model for each downstream task. That is expensive in compute and memory, and it is hard to store many fine‚Äëtuned copies in production.\n\n\nKey idea\nFreeze the pretrained weights and learn a low‚Äërank update to each selected weight matrix:\n\\[\nW = W_0 + \\Delta W, \\qquad \\Delta W = BA\n\\]\nIf \\(W_0\\in\\mathbb{R}^{d\\times k}\\), then \\(B\\in\\mathbb{R}^{d\\times r}\\) and \\(A\\in\\mathbb{R}^{r\\times k}\\) with \\(r\\ll\\min(d,k)\\). This replaces \\(dk\\) trainable parameters with \\(r(d+k)\\).\n\n\nWhere LoRA plugs in\nLoRA is typically applied to attention projections (e.g., \\(W_q, W_k, W_v, W_o\\)) and sometimes to MLP layers. You choose which matrices get low‚Äërank updates and the rank \\(r\\).\n\nA compact block view\nBelow is a small matrix block for one attention projection (same idea for \\(W_q, W_k, W_v, W_o\\)):\n\\[\n\\begin{aligned}\n\\text{proj: } & y = (W_0 + \\Delta W)x \\\\\n& \\Delta W = BA,\\quad B\\in\\mathbb{R}^{d\\times r},\\; A\\in\\mathbb{R}^{r\\times k}\n\\end{aligned}\n\\]\nYou can apply this to Q/K/V/O, and optionally to MLP layers (e.g., the up/down projections).\n\n\n\nLoRA placement in a Transformer block\n\n\n\n\n\nIntuition: why low rank is enough\nMany task‚Äëspecific updates live in a low‚Äëdimensional subspace. LoRA assumes the weight update has low intrinsic rank, so a small \\(r\\) can capture most of the task‚Äëspecific change.\n\n\nInitialization and scaling\n\nInit: \\(A=0\\), \\(B\\) random Gaussian ‚Üí start from \\(\\Delta W=0\\) so the model begins exactly at \\(W_0\\).\nScaling: use \\(\\alpha/r\\) to control update magnitude (similar in spirit to tuning learning rate).\n\nIn forward pass (conceptually):\n# x: [batch, k]\n# A: [r, k], B: [d, r]\nscale = alpha / r\nlora_out = (x @ A.T @ B.T) * scale\n\n\nInference cost\nAt inference, merge \\(W_0\\) and \\(\\Delta W\\):\n\\[\nW = W_0 + BA\n\\]\nThis removes extra latency‚Äîthe model runs as usual with the merged weight.\n\n\nBenefits\n\nLarge parameter savings: train only \\(r(d+k)\\) parameters per adapted matrix.\nEasy multi‚Äëtask deployment: store a small LoRA adapter per task.\nMinimal inference overhead after merging.\n\n\n\nComparison (high‚Äëlevel)\n\nAdapters: insert new layers ‚Üí extra depth and potential latency.\nPrefix/Prompt tuning: modify inputs; can work well but may be less expressive than weight updates.\nBitFit: tune only bias terms; extremely cheap but less flexible.\n\n\n\nLimitations\n\nIf \\(r\\) is too small, LoRA may underfit the task.\nLarge domain shifts may need higher rank or full tuning.\nYou must choose where to apply LoRA and what rank to use.\n\n\nTakeaway. LoRA keeps the base model frozen and learns a compact low‚Äërank update, giving strong performance with far fewer trainable parameters and lightweight deployment across many tasks."
  },
  {
    "objectID": "ML/papers/index.html",
    "href": "ML/papers/index.html",
    "title": "Papers in Deep Learning",
    "section": "",
    "text": "Research paper reading notes focused on key ideas, math intuition, and practical takeaways.\n\n\n\nLoRA: Low-Rank Adaptation of Large Language Models Freeze the base model and learn a low-rank update \\(\\Delta W=BA\\) for selected layers, achieving strong performance with far fewer trainable parameters and easy deployment."
  },
  {
    "objectID": "ML/deep-learning-book.html",
    "href": "ML/deep-learning-book.html",
    "title": "Goodfellow Deep Learning Book",
    "section": "",
    "text": "My notes and implementations while studying the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\n\n\n\nChapter 6.1: XOR Problem & ReLU Networks How ReLU solves problems that linear models cannot handle.\n\n\nChapter 6.2: Likelihood-Based Loss Functions The mathematical connection between probabilistic models and loss functions.\n\n\nChapter 6.3: Hidden Units and Activation Functions Exploring activation functions and their impact on neural network learning.\n\n\nChapter 6.4: Architecture Design - Depth vs Width How depth enables hierarchical feature reuse and exponential expressiveness.\n\n\nChapter 6.5: Back-Propagation and Other Differentiation Algorithms The algorithm that makes training deep networks computationally feasible.\n\n\n\n\n\n\n\n\nChapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature Essential second-order calculus concepts needed before Chapter 7.\n\n\nChapter 7.1.1: L2 Regularization How L2 regularization shrinks weights based on Hessian eigenvalues.\n\n\nChapter 7.1.2: L1 Regularization L1 regularization uses soft thresholding to create sparse solutions.\n\n\nChapter 7.2: Constrained Optimization View of Regularization Regularization as constrained optimization with KKT conditions.\n\n\nChapter 7.3: Regularization and Under-Constrained Problems Why regularization is mathematically necessary and ensures invertibility.\n\n\nChapter 7.4: Dataset Augmentation How transforming existing data improves generalization.\n\n\nChapter 7.5: Noise Robustness How adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\nChapter 7.6: Semi-Supervised Learning Leveraging unlabeled data to improve model performance when labeled data is scarce.\n\n\nChapter 7.7: Multi-Task Learning Training a single model on multiple related tasks to improve generalization.\n\n\nChapter 7.8: Early Stopping Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.\n\n\nChapter 7.9: Parameter Tying and Parameter Sharing Two strategies for reducing parameters: encouraging similarity vs.¬†enforcing identity.\n\n\nChapter 7.10: Sparse Representations Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.\n\n\nChapter 7.11: Bagging and Other Ensemble Methods How training multiple models on bootstrap samples and averaging their predictions reduces variance.\n\n\nChapter 7.12: Dropout Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.\n\n\nChapter 7.13: Adversarial Training How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.\n\n\nChapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.\n\n\n\n\n\n\n\n\nChapter 8.1: How Learning Differs from Pure Optimization Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.\n\n\nChapter 8.2: Challenges in Deep Learning Optimization Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.\n\n\nChapter 8.3: Basic Algorithms SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.\n\n\nChapter 8.4: Parameter Initialization Strategies Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.\n\n\nChapter 8.5: Algorithms with Adaptive Learning Rates From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.\n\n\nChapter 8.6: Second-Order Optimization Methods Newton‚Äôs method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.\n\n\nChapter 8.7: Optimization Strategies and Meta-Algorithms Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.\n\n\n\n\n\n\n\n\nChapter 9.1: Convolution Computation The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.\n\n\nChapter 9.2: Motivation for Convolutional Networks Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m¬∑n) to O(k¬∑n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere‚Äîachieving 30,000√ó speedup over dense layers.\n\n\nChapter 9.3: Pooling Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28√ó28 feature maps to 14√ó14 (4√ó fewer activations). Comparing three architectures‚Äîstrided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.\n\n\nChapter 9.4: Convolution and Pooling as an Infinitely Strong Prior Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off‚Äîstrong priors reduce sample complexity but only when assumptions match the data structure.\n\n\nChapter 9.5: Convolutional Functions Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula \\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\), stride for downsampling, three padding strategies (valid/same/full), and gradient computation‚Äîkernel gradients via correlation with input, input gradients via convolution with flipped kernel.\n\n\nChapter 9.6: Structured Outputs CNNs can generate high-dimensional structured objects through pixel-level predictions. Preserving spatial dimensions (no pooling, no stride &gt; 1, SAME padding) enables full-resolution outputs. Recurrent convolution refines predictions iteratively: \\(U*X + H(t-1)*W = H(t)\\), producing dense predictions for segmentation, depth estimation, and flow prediction.\n\n\nChapter 9.7: Data Types CNNs can operate on different data types: 1D (audio, time series), 2D (images), and 3D (videos, CT scans) with varying channel counts. Unlike fully connected networks, convolutional kernels handle variable-sized inputs by sliding across spatial dimensions, producing outputs that scale accordingly‚Äîa unique flexibility for diverse domains.\n\n\nChapter 9.8: Efficient Convolution Algorithms Separable convolution reduces computational cost from \\(O(HWk^2)\\) to \\(O(HWk)\\) by decomposing a 2D kernel into two 1D filters (vertical and horizontal). Parameter storage shrinks from \\(k^2\\) to \\(2k\\). This factorization enables faster, more memory-efficient models without sacrificing accuracy‚Äîfoundational for architectures like MobileNet.\n\n\nChapter 9.9: Unsupervised or Semi-Supervised Feature Learning Before CNNs, computer vision relied on hand-crafted kernels (Sobel, Laplacian, Gaussian) and unsupervised methods (sparse coding, autoencoders, k-means). While these captured simple patterns, they couldn‚Äôt match CNNs‚Äô hierarchical, end-to-end feature learning. Modern systems use CNNs to learn features from edges to semantic concepts‚Äîmaking hand-crafted filters largely obsolete.\n\n\nChapter 9.10: Neuroscientific Basis for Convolutional Networks V1 simple cells detect oriented edges (modeled by Gabor filters), complex cells pool over simple cells for translation invariance (like CNN pooling). But CNNs lack key biological features: saccadic attention, multisensory integration, top-down feedback, and dynamic receptive fields. While CNNs excel at feed-forward recognition, biological vision is holistic, context-aware, and adaptive.\n\n\n\n\n\n\n\n\nChapter 10.1: Unfold Computation Graph Unfolding computation graphs in RNNs enables parameter sharing across time steps. The same function with fixed parameters processes sequences of any length, compressing input history into fixed-size hidden states that retain only task-relevant information for predictions.\n\n\nChapter 10.2: Recurrent Neural Networks RNN architecture with hidden-to-hidden connections, teacher forcing for parallel training, back-propagation through time (BPTT), RNN as directed graphical models with O(œÑ) parameter efficiency, and context-based sequence-to-sequence models.\n\n\nChapter 10.3: Bidirectional RNN Bidirectional RNNs process sequences in both forward and backward directions, allowing predictions to use information from the entire input sequence. Essential for tasks like speech recognition and handwriting recognition where future context matters.\n\n\nChapter 10.4: Encoder-Decoder Sequence-to-Sequence Architecture The seq2seq architecture handles variable-length input and output sequences by compressing the input into a fixed context vector C, then decoding it step-by-step. This enables machine translation, summarization, and dialogue generation where input and output lengths differ.\n\n\nChapter 10.5: Deep Recurrent Networks Three architectural patterns for adding depth to RNNs: hierarchical hidden states (vertical stacking), deep transition RNNs (MLPs replace transformations), and deep transition with skip connections (residual paths for gradient flow).\n\n\nChapter 10.6: Recursive Neural Network Recursive neural networks compute over tree structures rather than linear chains, applying shared composition functions at internal nodes to build hierarchical representations bottom-up. This reduces computation depth from O(œÑ) to O(log œÑ), but requires external tree structure specification.\n\n\nChapter 10.7: The Challenge of Long-Term Dependencies The fundamental challenge of long-term dependencies in RNNs is training difficulty: gradients propagated across many time steps either vanish exponentially (common) or explode (rare but severe). Eigenvalue analysis shows how powers of the transition matrix govern this instability.\n\n\nChapter 10.8: Echo State Networks ESNs fix recurrent weights and train only output weights, viewing the network as a dynamical reservoir. Setting spectral radius near one enables long-term memory retention. Learning reduces to linear regression on hidden states, avoiding backpropagation through time‚Äîshowing that carefully designed dynamics can capture temporal structure.\n\n\nChapter 10.9: Leaky Units and Multiple Time Scales Leaky units separate instantaneous state from long-term integration using \\(u^t = \\alpha u^{t-1}+(1-\\alpha)v^t\\). Multiple time scale strategies include temporal skip connections (direct pathways across time steps) and removing short connections (forcing coarser time scales) to address long-term dependencies.\n\n\nChapter 10.10: LSTM and GRU LSTM uses learned gates (forget, input, output) to control information flow through explicit cell state paths, enabling adaptive long-term memory retention. GRU simplifies this design by merging forget and input into a single update gate, reducing parameters while maintaining the ability to capture long-range dependencies through gating mechanisms.\n\n\nChapter 10.11: Optimizing Long-Term Dependencies Gradient clipping prevents training instability by rescaling gradients when their norm exceeds a threshold, protecting against sudden jumps across steep gradient cliffs. Regularizing information flow aims to maintain \\(\\|\\partial h^t/\\partial h^{t-1}\\| \\approx 1\\), though this is rarely used in practice due to computational cost.\n\n\nChapter 10.12: Explicit Memory Explicit memory separates storage from computation by introducing addressable memory outside network parameters. Classic architectures (Memory Networks, Neural Turing Machines) are computationally expensive but inspired modern mechanisms‚Äîattention, Transformers, and retrieval-augmented models are successful, scalable realizations of this idea.\n\n\n\n\n\n\n\n\nChapter 11: Practical Methodology Define performance metrics aligned with application goals. Build baseline systems quickly using architecture patterns matched to data structure. Diagnose by comparing training and test error. Tune learning rate first, then other hyperparameters via random search. Debug systematically by isolating components, checking gradients, and monitoring numerical behavior.\n\n\n\n\n\n\n\n\nChapter 12.1: Large-Scale Deep Learning Scaling deep learning requires specialized hardware (GPUs, TPUs, ASICs), distributed training strategies (data/model parallelism, asynchronous SGD), and efficiency optimizations (model compression, quantization, pruning). Dynamic computation enables conditional execution for computational efficiency. Specialized accelerators exploit reduced precision and massive parallelism for deployment on resource-constrained devices.\n\n\nChapter 12.2: Image Preprocessing and Normalization Preprocessing images through normalization (scaling to [0,1] or [-1,1]) and data augmentation improves training stability and generalization. Global Contrast Normalization (GCN) removes global lighting variations by centering and L2-normalizing images. Local Contrast Normalization (LCN) enhances local structures by normalizing within spatial neighborhoods. Modern networks rely on batch normalization, but explicit contrast normalization remains valuable for challenging datasets.\n\n\nChapter 12.3: Automatic Speech Recognition ASR evolution from GMM-HMM (classical statistical approach) through DNN-HMM (~30% error reduction with deep feedforward networks) to end-to-end systems using RNNs/LSTMs with CTC. CNNs treat spectrograms as 2D structures for frequency-invariant modeling. Modern systems learn direct acoustic-to-text mappings without forced alignment, integrating joint acoustic-phonetic modeling and hierarchical representations.\n\n\nChapter 12.4: NLP Applications N-gram models compute conditional probabilities over fixed contexts but suffer from sparsity and exponential growth. Neural language models use word embeddings to map discrete tokens into continuous space, enabling generalization across semantically similar words. High-dimensional vocabulary outputs require optimization: short lists partition frequent/rare words, hierarchical softmax reduces complexity to O(log|V|), and importance sampling approximates gradients. Attention mechanisms dynamically focus on relevant input positions, forming weighted context vectors that alleviate fixed-size representation bottlenecks in seq2seq tasks.\n\n\nChapter 12.5: Other Applications Collaborative filtering uses matrix factorization to learn latent user and item embeddings, decomposing ratings into user bias, item bias, and personalized interaction. Cold-start problems require side information. Recommendation systems face exploration-exploitation tradeoffs modeled as contextual bandits. Knowledge graphs represent facts as (subject, relation, object) triples; deep learning maps entities and relations to continuous embeddings for link prediction and reasoning. Evaluation challenges arise from open-world assumptions where unseen facts may be missing rather than false.\n\n\n\n\n\n\n\n\nChapter 13: Linear Factor Models Linear factor models decompose observed data into latent factors: \\(x = Wh + b + \\text{noise}\\). PCA uses Gaussian priors for dimensionality reduction. ICA recovers statistically independent non-Gaussian sources for signal separation. SFA learns slowly-varying features via temporal coherence. Sparse coding enforces L1 sparsity for interpretable representations. These models form the foundation for modern unsupervised learning and generative models.\n\n\n\n\n\n\n\n\nChapter 14: Autoencoders Autoencoders learn compressed representations by training encoder \\(h=f(x)\\) and decoder \\(r=g(h)\\) to reconstruct inputs through a bottleneck. Undercomplete autoencoders constrain \\(\\dim(h)&lt;\\dim(x)\\) to learn meaningful features‚Äîlinear versions recover PCA subspace. Regularized variants include sparse autoencoders (L1 penalty interpreted as Laplace prior on latent codes), denoising autoencoders (learn manifold structure by reconstructing from corrupted inputs \\(\\tilde{x}\\)), and contractive autoencoders (penalize Jacobian \\(\\|\\partial f/\\partial x\\|_F^2\\) to encourage local invariance). Two competing forces‚Äîreconstruction accuracy vs regularization‚Äîdrive autoencoders to become sensitive along data manifolds while contracting orthogonally. Applications include dimensionality reduction, semantic hashing for fast retrieval, and manifold learning via parametric coordinate systems.\n\n\n\n\n\n\n\n\nChapter 15: Representation Learning Greedy layer-wise pretraining learns meaningful representations through unsupervised learning of hierarchical features, providing better initialization than random weights. Transfer learning enables knowledge sharing across tasks by reusing learned representations‚Äîgeneric early-layer features transfer well while late layers adapt to task-specific patterns. Semi-supervised learning leverages both labeled and unlabeled data to discover disentangled causal factors that generate observations. Distributed representations enable exponential gains in capacity through shared features rather than symbolic codes. Depth provides exponential advantages via compositional hierarchies that match natural data structure. Multiple inductive biases (smoothness, sparsity, temporal coherence, manifolds) guide networks toward discovering meaningful underlying causes.\n\n\n\n\n\n\n\n\nChapter 16: Structured Probabilistic Models for Deep Learning Structured probabilistic models use graphs to factorize high-dimensional distributions into tractable components by encoding conditional independence. Directed models (Bayesian networks) represent causal relationships via \\(p(x)=\\prod_i p(x_i|Pa(x_i))\\) and support efficient ancestral sampling. Undirected models (Markov random fields) capture symmetric dependencies through clique potentials \\(\\tilde{p}(x)=\\prod_C \\phi_C(x_C)\\), normalized by partition function Z. Energy-based models express potentials as \\(\\exp(-E(x))\\) via Boltzmann distribution. Converting between directed and undirected models requires moralization or triangulation, typically adding edges and losing independence. Factor graphs explicitly represent factorization structure for message-passing inference. Deep learning embraces approximate inference with large latent variable models, prioritizing scalability over exact computations through distributed representations and parameter sharing.\n\n\n\n\n\n\n\n\nChapter 17: Monte Carlo Methods Monte Carlo estimation approximates expectations with samples; importance sampling reweights proposals to reduce variance, and MCMC methods like Gibbs sampling generate dependent samples when direct sampling is impossible. Tempering improves mixing across multimodal landscapes.\n\n\n\n\n\n\n\n\nChapter 18: Confronting the Partition Function Energy-based models require a partition function for normalization. This chapter follows how \\(\\nabla_\\theta \\log Z(\\theta)\\) enters the log-likelihood gradient and surveys training strategies like contrastive divergence, pseudolikelihood, score matching, NCE, and AIS that avoid or estimate \\(Z\\).\n\n\n\n\n\n\n\n\nChapter 19: Approximate Inference Exact posterior inference in deep latent models is intractable, so we optimize the ELBO instead. The chapter covers EM as coordinate ascent, MAP inference via point-mass variational families, and mean-field variational updates.\n\n\n\n\n\n\n\n\nChapter 20: Deep Generative Models Deep generative modeling unifies energy-based models, directed latent-variable models, and implicit generators. The chapter surveys RBMs, DBNs/DBMs, VAEs, GANs, autoregressive models, and evaluation challenges."
  },
  {
    "objectID": "ML/deep-learning-book.html#all-chapters",
    "href": "ML/deep-learning-book.html#all-chapters",
    "title": "Goodfellow Deep Learning Book",
    "section": "",
    "text": "My notes and implementations while studying the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\n\n\n\nChapter 6.1: XOR Problem & ReLU Networks How ReLU solves problems that linear models cannot handle.\n\n\nChapter 6.2: Likelihood-Based Loss Functions The mathematical connection between probabilistic models and loss functions.\n\n\nChapter 6.3: Hidden Units and Activation Functions Exploring activation functions and their impact on neural network learning.\n\n\nChapter 6.4: Architecture Design - Depth vs Width How depth enables hierarchical feature reuse and exponential expressiveness.\n\n\nChapter 6.5: Back-Propagation and Other Differentiation Algorithms The algorithm that makes training deep networks computationally feasible.\n\n\n\n\n\n\n\n\nChapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature Essential second-order calculus concepts needed before Chapter 7.\n\n\nChapter 7.1.1: L2 Regularization How L2 regularization shrinks weights based on Hessian eigenvalues.\n\n\nChapter 7.1.2: L1 Regularization L1 regularization uses soft thresholding to create sparse solutions.\n\n\nChapter 7.2: Constrained Optimization View of Regularization Regularization as constrained optimization with KKT conditions.\n\n\nChapter 7.3: Regularization and Under-Constrained Problems Why regularization is mathematically necessary and ensures invertibility.\n\n\nChapter 7.4: Dataset Augmentation How transforming existing data improves generalization.\n\n\nChapter 7.5: Noise Robustness How adding Gaussian noise to weights is equivalent to penalizing large gradients.\n\n\nChapter 7.6: Semi-Supervised Learning Leveraging unlabeled data to improve model performance when labeled data is scarce.\n\n\nChapter 7.7: Multi-Task Learning Training a single model on multiple related tasks to improve generalization.\n\n\nChapter 7.8: Early Stopping Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.\n\n\nChapter 7.9: Parameter Tying and Parameter Sharing Two strategies for reducing parameters: encouraging similarity vs.¬†enforcing identity.\n\n\nChapter 7.10: Sparse Representations Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.\n\n\nChapter 7.11: Bagging and Other Ensemble Methods How training multiple models on bootstrap samples and averaging their predictions reduces variance.\n\n\nChapter 7.12: Dropout Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.\n\n\nChapter 7.13: Adversarial Training How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.\n\n\nChapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.\n\n\n\n\n\n\n\n\nChapter 8.1: How Learning Differs from Pure Optimization Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.\n\n\nChapter 8.2: Challenges in Deep Learning Optimization Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.\n\n\nChapter 8.3: Basic Algorithms SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.\n\n\nChapter 8.4: Parameter Initialization Strategies Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.\n\n\nChapter 8.5: Algorithms with Adaptive Learning Rates From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.\n\n\nChapter 8.6: Second-Order Optimization Methods Newton‚Äôs method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.\n\n\nChapter 8.7: Optimization Strategies and Meta-Algorithms Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.\n\n\n\n\n\n\n\n\nChapter 9.1: Convolution Computation The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.\n\n\nChapter 9.2: Motivation for Convolutional Networks Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m¬∑n) to O(k¬∑n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere‚Äîachieving 30,000√ó speedup over dense layers.\n\n\nChapter 9.3: Pooling Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28√ó28 feature maps to 14√ó14 (4√ó fewer activations). Comparing three architectures‚Äîstrided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.\n\n\nChapter 9.4: Convolution and Pooling as an Infinitely Strong Prior Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off‚Äîstrong priors reduce sample complexity but only when assumptions match the data structure.\n\n\nChapter 9.5: Convolutional Functions Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula \\(Z_{l,x,y} = \\sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\\), stride for downsampling, three padding strategies (valid/same/full), and gradient computation‚Äîkernel gradients via correlation with input, input gradients via convolution with flipped kernel.\n\n\nChapter 9.6: Structured Outputs CNNs can generate high-dimensional structured objects through pixel-level predictions. Preserving spatial dimensions (no pooling, no stride &gt; 1, SAME padding) enables full-resolution outputs. Recurrent convolution refines predictions iteratively: \\(U*X + H(t-1)*W = H(t)\\), producing dense predictions for segmentation, depth estimation, and flow prediction.\n\n\nChapter 9.7: Data Types CNNs can operate on different data types: 1D (audio, time series), 2D (images), and 3D (videos, CT scans) with varying channel counts. Unlike fully connected networks, convolutional kernels handle variable-sized inputs by sliding across spatial dimensions, producing outputs that scale accordingly‚Äîa unique flexibility for diverse domains.\n\n\nChapter 9.8: Efficient Convolution Algorithms Separable convolution reduces computational cost from \\(O(HWk^2)\\) to \\(O(HWk)\\) by decomposing a 2D kernel into two 1D filters (vertical and horizontal). Parameter storage shrinks from \\(k^2\\) to \\(2k\\). This factorization enables faster, more memory-efficient models without sacrificing accuracy‚Äîfoundational for architectures like MobileNet.\n\n\nChapter 9.9: Unsupervised or Semi-Supervised Feature Learning Before CNNs, computer vision relied on hand-crafted kernels (Sobel, Laplacian, Gaussian) and unsupervised methods (sparse coding, autoencoders, k-means). While these captured simple patterns, they couldn‚Äôt match CNNs‚Äô hierarchical, end-to-end feature learning. Modern systems use CNNs to learn features from edges to semantic concepts‚Äîmaking hand-crafted filters largely obsolete.\n\n\nChapter 9.10: Neuroscientific Basis for Convolutional Networks V1 simple cells detect oriented edges (modeled by Gabor filters), complex cells pool over simple cells for translation invariance (like CNN pooling). But CNNs lack key biological features: saccadic attention, multisensory integration, top-down feedback, and dynamic receptive fields. While CNNs excel at feed-forward recognition, biological vision is holistic, context-aware, and adaptive.\n\n\n\n\n\n\n\n\nChapter 10.1: Unfold Computation Graph Unfolding computation graphs in RNNs enables parameter sharing across time steps. The same function with fixed parameters processes sequences of any length, compressing input history into fixed-size hidden states that retain only task-relevant information for predictions.\n\n\nChapter 10.2: Recurrent Neural Networks RNN architecture with hidden-to-hidden connections, teacher forcing for parallel training, back-propagation through time (BPTT), RNN as directed graphical models with O(œÑ) parameter efficiency, and context-based sequence-to-sequence models.\n\n\nChapter 10.3: Bidirectional RNN Bidirectional RNNs process sequences in both forward and backward directions, allowing predictions to use information from the entire input sequence. Essential for tasks like speech recognition and handwriting recognition where future context matters.\n\n\nChapter 10.4: Encoder-Decoder Sequence-to-Sequence Architecture The seq2seq architecture handles variable-length input and output sequences by compressing the input into a fixed context vector C, then decoding it step-by-step. This enables machine translation, summarization, and dialogue generation where input and output lengths differ.\n\n\nChapter 10.5: Deep Recurrent Networks Three architectural patterns for adding depth to RNNs: hierarchical hidden states (vertical stacking), deep transition RNNs (MLPs replace transformations), and deep transition with skip connections (residual paths for gradient flow).\n\n\nChapter 10.6: Recursive Neural Network Recursive neural networks compute over tree structures rather than linear chains, applying shared composition functions at internal nodes to build hierarchical representations bottom-up. This reduces computation depth from O(œÑ) to O(log œÑ), but requires external tree structure specification.\n\n\nChapter 10.7: The Challenge of Long-Term Dependencies The fundamental challenge of long-term dependencies in RNNs is training difficulty: gradients propagated across many time steps either vanish exponentially (common) or explode (rare but severe). Eigenvalue analysis shows how powers of the transition matrix govern this instability.\n\n\nChapter 10.8: Echo State Networks ESNs fix recurrent weights and train only output weights, viewing the network as a dynamical reservoir. Setting spectral radius near one enables long-term memory retention. Learning reduces to linear regression on hidden states, avoiding backpropagation through time‚Äîshowing that carefully designed dynamics can capture temporal structure.\n\n\nChapter 10.9: Leaky Units and Multiple Time Scales Leaky units separate instantaneous state from long-term integration using \\(u^t = \\alpha u^{t-1}+(1-\\alpha)v^t\\). Multiple time scale strategies include temporal skip connections (direct pathways across time steps) and removing short connections (forcing coarser time scales) to address long-term dependencies.\n\n\nChapter 10.10: LSTM and GRU LSTM uses learned gates (forget, input, output) to control information flow through explicit cell state paths, enabling adaptive long-term memory retention. GRU simplifies this design by merging forget and input into a single update gate, reducing parameters while maintaining the ability to capture long-range dependencies through gating mechanisms.\n\n\nChapter 10.11: Optimizing Long-Term Dependencies Gradient clipping prevents training instability by rescaling gradients when their norm exceeds a threshold, protecting against sudden jumps across steep gradient cliffs. Regularizing information flow aims to maintain \\(\\|\\partial h^t/\\partial h^{t-1}\\| \\approx 1\\), though this is rarely used in practice due to computational cost.\n\n\nChapter 10.12: Explicit Memory Explicit memory separates storage from computation by introducing addressable memory outside network parameters. Classic architectures (Memory Networks, Neural Turing Machines) are computationally expensive but inspired modern mechanisms‚Äîattention, Transformers, and retrieval-augmented models are successful, scalable realizations of this idea.\n\n\n\n\n\n\n\n\nChapter 11: Practical Methodology Define performance metrics aligned with application goals. Build baseline systems quickly using architecture patterns matched to data structure. Diagnose by comparing training and test error. Tune learning rate first, then other hyperparameters via random search. Debug systematically by isolating components, checking gradients, and monitoring numerical behavior.\n\n\n\n\n\n\n\n\nChapter 12.1: Large-Scale Deep Learning Scaling deep learning requires specialized hardware (GPUs, TPUs, ASICs), distributed training strategies (data/model parallelism, asynchronous SGD), and efficiency optimizations (model compression, quantization, pruning). Dynamic computation enables conditional execution for computational efficiency. Specialized accelerators exploit reduced precision and massive parallelism for deployment on resource-constrained devices.\n\n\nChapter 12.2: Image Preprocessing and Normalization Preprocessing images through normalization (scaling to [0,1] or [-1,1]) and data augmentation improves training stability and generalization. Global Contrast Normalization (GCN) removes global lighting variations by centering and L2-normalizing images. Local Contrast Normalization (LCN) enhances local structures by normalizing within spatial neighborhoods. Modern networks rely on batch normalization, but explicit contrast normalization remains valuable for challenging datasets.\n\n\nChapter 12.3: Automatic Speech Recognition ASR evolution from GMM-HMM (classical statistical approach) through DNN-HMM (~30% error reduction with deep feedforward networks) to end-to-end systems using RNNs/LSTMs with CTC. CNNs treat spectrograms as 2D structures for frequency-invariant modeling. Modern systems learn direct acoustic-to-text mappings without forced alignment, integrating joint acoustic-phonetic modeling and hierarchical representations.\n\n\nChapter 12.4: NLP Applications N-gram models compute conditional probabilities over fixed contexts but suffer from sparsity and exponential growth. Neural language models use word embeddings to map discrete tokens into continuous space, enabling generalization across semantically similar words. High-dimensional vocabulary outputs require optimization: short lists partition frequent/rare words, hierarchical softmax reduces complexity to O(log|V|), and importance sampling approximates gradients. Attention mechanisms dynamically focus on relevant input positions, forming weighted context vectors that alleviate fixed-size representation bottlenecks in seq2seq tasks.\n\n\nChapter 12.5: Other Applications Collaborative filtering uses matrix factorization to learn latent user and item embeddings, decomposing ratings into user bias, item bias, and personalized interaction. Cold-start problems require side information. Recommendation systems face exploration-exploitation tradeoffs modeled as contextual bandits. Knowledge graphs represent facts as (subject, relation, object) triples; deep learning maps entities and relations to continuous embeddings for link prediction and reasoning. Evaluation challenges arise from open-world assumptions where unseen facts may be missing rather than false.\n\n\n\n\n\n\n\n\nChapter 13: Linear Factor Models Linear factor models decompose observed data into latent factors: \\(x = Wh + b + \\text{noise}\\). PCA uses Gaussian priors for dimensionality reduction. ICA recovers statistically independent non-Gaussian sources for signal separation. SFA learns slowly-varying features via temporal coherence. Sparse coding enforces L1 sparsity for interpretable representations. These models form the foundation for modern unsupervised learning and generative models.\n\n\n\n\n\n\n\n\nChapter 14: Autoencoders Autoencoders learn compressed representations by training encoder \\(h=f(x)\\) and decoder \\(r=g(h)\\) to reconstruct inputs through a bottleneck. Undercomplete autoencoders constrain \\(\\dim(h)&lt;\\dim(x)\\) to learn meaningful features‚Äîlinear versions recover PCA subspace. Regularized variants include sparse autoencoders (L1 penalty interpreted as Laplace prior on latent codes), denoising autoencoders (learn manifold structure by reconstructing from corrupted inputs \\(\\tilde{x}\\)), and contractive autoencoders (penalize Jacobian \\(\\|\\partial f/\\partial x\\|_F^2\\) to encourage local invariance). Two competing forces‚Äîreconstruction accuracy vs regularization‚Äîdrive autoencoders to become sensitive along data manifolds while contracting orthogonally. Applications include dimensionality reduction, semantic hashing for fast retrieval, and manifold learning via parametric coordinate systems.\n\n\n\n\n\n\n\n\nChapter 15: Representation Learning Greedy layer-wise pretraining learns meaningful representations through unsupervised learning of hierarchical features, providing better initialization than random weights. Transfer learning enables knowledge sharing across tasks by reusing learned representations‚Äîgeneric early-layer features transfer well while late layers adapt to task-specific patterns. Semi-supervised learning leverages both labeled and unlabeled data to discover disentangled causal factors that generate observations. Distributed representations enable exponential gains in capacity through shared features rather than symbolic codes. Depth provides exponential advantages via compositional hierarchies that match natural data structure. Multiple inductive biases (smoothness, sparsity, temporal coherence, manifolds) guide networks toward discovering meaningful underlying causes.\n\n\n\n\n\n\n\n\nChapter 16: Structured Probabilistic Models for Deep Learning Structured probabilistic models use graphs to factorize high-dimensional distributions into tractable components by encoding conditional independence. Directed models (Bayesian networks) represent causal relationships via \\(p(x)=\\prod_i p(x_i|Pa(x_i))\\) and support efficient ancestral sampling. Undirected models (Markov random fields) capture symmetric dependencies through clique potentials \\(\\tilde{p}(x)=\\prod_C \\phi_C(x_C)\\), normalized by partition function Z. Energy-based models express potentials as \\(\\exp(-E(x))\\) via Boltzmann distribution. Converting between directed and undirected models requires moralization or triangulation, typically adding edges and losing independence. Factor graphs explicitly represent factorization structure for message-passing inference. Deep learning embraces approximate inference with large latent variable models, prioritizing scalability over exact computations through distributed representations and parameter sharing.\n\n\n\n\n\n\n\n\nChapter 17: Monte Carlo Methods Monte Carlo estimation approximates expectations with samples; importance sampling reweights proposals to reduce variance, and MCMC methods like Gibbs sampling generate dependent samples when direct sampling is impossible. Tempering improves mixing across multimodal landscapes.\n\n\n\n\n\n\n\n\nChapter 18: Confronting the Partition Function Energy-based models require a partition function for normalization. This chapter follows how \\(\\nabla_\\theta \\log Z(\\theta)\\) enters the log-likelihood gradient and surveys training strategies like contrastive divergence, pseudolikelihood, score matching, NCE, and AIS that avoid or estimate \\(Z\\).\n\n\n\n\n\n\n\n\nChapter 19: Approximate Inference Exact posterior inference in deep latent models is intractable, so we optimize the ELBO instead. The chapter covers EM as coordinate ascent, MAP inference via point-mass variational families, and mean-field variational updates.\n\n\n\n\n\n\n\n\nChapter 20: Deep Generative Models Deep generative modeling unifies energy-based models, directed latent-variable models, and implicit generators. The chapter surveys RBMs, DBNs/DBMs, VAEs, GANs, autoregressive models, and evaluation challenges."
  }
]