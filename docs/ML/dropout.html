<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chao Ma">
<meta name="dcterms.date" content="2025-10-30">
<meta name="description" content="Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units">

<title>Goodfellow Deep Learning — Chapter 7.12: Dropout – ∇ ickma.dev</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.svg" rel="icon" type="image/svg+xml">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PK5N5KWZBF"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-PK5N5KWZBF', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">∇ ickma.dev</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-deep-learning" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Deep Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-deep-learning">    
        <li class="dropdown-header">Papers in Deep Learning</li>
        <li>
    <a class="dropdown-item" href="../ML/papers/index.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/papers/attention-origin-transformer.html">
 <span class="dropdown-text">Attention: The Origin of Transformer</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/papers/batch-normalization.html">
 <span class="dropdown-text">Batch Normalization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/papers/lora.html">
 <span class="dropdown-text">LoRA: Low-Rank Adaptation</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../ML/deep-learning-book.html">
 <span class="dropdown-text">Goodfellow Deep Learning Book (Overview)</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../ML/xor-deep-learning.html">
 <span class="dropdown-text">Chapter 6.1: XOR Problem</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/likelihood-loss-functions.html">
 <span class="dropdown-text">Chapter 6.2: Loss Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/activation-functions.html">
 <span class="dropdown-text">Chapter 6.3: Activation Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/architecture-design.html">
 <span class="dropdown-text">Chapter 6.4: Architecture Design</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/backpropagation.html">
 <span class="dropdown-text">Chapter 6.5: Back-Propagation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/hessian-prerequisites.html">
 <span class="dropdown-text">Chapter 7 Prerequisites: Hessian Matrix</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/l2-regularization.html">
 <span class="dropdown-text">Chapter 7.1.1: L2 Regularization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/l1-regularization.html">
 <span class="dropdown-text">Chapter 7.1.2: L1 Regularization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/constrained-optimization-regularization.html">
 <span class="dropdown-text">Chapter 7.2: Constrained Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/regularization-underconstrained.html">
 <span class="dropdown-text">Chapter 7.3: Under-Constrained Problems</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/dataset-augmentation.html">
 <span class="dropdown-text">Chapter 7.4: Dataset Augmentation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/noise-robustness.html">
 <span class="dropdown-text">Chapter 7.5: Noise Robustness</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/semi-supervised-learning.html">
 <span class="dropdown-text">Chapter 7.6: Semi-Supervised Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/multi-task-learning.html">
 <span class="dropdown-text">Chapter 7.7: Multi-Task Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/early-stopping.html">
 <span class="dropdown-text">Chapter 7.8: Early Stopping</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/parameter-tying-sharing.html">
 <span class="dropdown-text">Chapter 7.9: Parameter Tying &amp; Sharing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/representation-sparsity.html">
 <span class="dropdown-text">Chapter 7.10: Sparse Representations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/bagging-ensemble.html">
 <span class="dropdown-text">Chapter 7.11: Bagging &amp; Ensemble Methods</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/dropout.html">
 <span class="dropdown-text">Chapter 7.12: Dropout</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/adversarial-training.html">
 <span class="dropdown-text">Chapter 7.13: Adversarial Training</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/tangent-prop-manifold.html">
 <span class="dropdown-text">Chapter 7.14: Tangent Prop &amp; Manifolds</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/learning-vs-optimization.html">
 <span class="dropdown-text">Chapter 8.1: Learning vs Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/optimization-challenges.html">
 <span class="dropdown-text">Chapter 8.2: Optimization Challenges</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/basic-optimization-algorithms.html">
 <span class="dropdown-text">Chapter 8.3: Basic Algorithms</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/parameter-initialization.html">
 <span class="dropdown-text">Chapter 8.4: Parameter Initialization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/adaptive-learning-rates.html">
 <span class="dropdown-text">Chapter 8.5: Adaptive Learning Rates</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/second-order-methods.html">
 <span class="dropdown-text">Chapter 8.6: Second-Order Methods</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/optimization-strategies.html">
 <span class="dropdown-text">Chapter 8.7: Optimization Strategies</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/convolution-computation.html">
 <span class="dropdown-text">Chapter 9.1: Convolution Computation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-motivation.html">
 <span class="dropdown-text">Chapter 9.2: CNN Motivation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-pooling.html">
 <span class="dropdown-text">Chapter 9.3: Pooling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-infinitely-strong-prior.html">
 <span class="dropdown-text">Chapter 9.4: Infinitely Strong Prior</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/convolutional-functions.html">
 <span class="dropdown-text">Chapter 9.5: Convolutional Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-structured-outputs.html">
 <span class="dropdown-text">Chapter 9.6: Structured Outputs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-data-types.html">
 <span class="dropdown-text">Chapter 9.7: Data Types</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-efficient-convolution.html">
 <span class="dropdown-text">Chapter 9.8: Efficient Convolution</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-unsupervised-learning.html">
 <span class="dropdown-text">Chapter 9.9: Unsupervised Feature Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-neuroscience.html">
 <span class="dropdown-text">Chapter 9.10: Neuroscientific Basis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-unfold-computation-graph.html">
 <span class="dropdown-text">Chapter 10.1: Unfold Computation Graph</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-recurrent-neural-networks.html">
 <span class="dropdown-text">Chapter 10.2: Recurrent Neural Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-bidirectional.html">
 <span class="dropdown-text">Chapter 10.3: Bidirectional RNN</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-encoder-decoder.html">
 <span class="dropdown-text">Chapter 10.4: Encoder-Decoder Seq2Seq</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-deep.html">
 <span class="dropdown-text">Chapter 10.5: Deep RNN</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-recursive.html">
 <span class="dropdown-text">Chapter 10.6: Recursive Neural Network</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-long-term-dependency.html">
 <span class="dropdown-text">Chapter 10.7: Long-Term Dependencies</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-echo-state.html">
 <span class="dropdown-text">Chapter 10.8: Echo State Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-leaky-unit.html">
 <span class="dropdown-text">Chapter 10.9: Leaky Units</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-lstm-gru.html">
 <span class="dropdown-text">Chapter 10.10: LSTM and GRU</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-optimize-long-term.html">
 <span class="dropdown-text">Chapter 10.11: Optimizing Long-Term Dependencies</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-explicit-memory.html">
 <span class="dropdown-text">Chapter 10.12: Explicit Memory</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/practical-methodology.html">
 <span class="dropdown-text">Chapter 11: Practical Methodology</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/large-scale-deep-learning.html">
 <span class="dropdown-text">Chapter 12.1: Large-Scale Deep Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/image-preprocessing-normalization.html">
 <span class="dropdown-text">Chapter 12.2: Image Preprocessing and Normalization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/automatic-speech-recognition.html">
 <span class="dropdown-text">Chapter 12.3: Automatic Speech Recognition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/nlp-applications.html">
 <span class="dropdown-text">Chapter 12.4: NLP Applications</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/other-applications.html">
 <span class="dropdown-text">Chapter 12.5: Other Applications</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/linear-factor-models.html">
 <span class="dropdown-text">Chapter 13: Linear Factor Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/autoencoders.html">
 <span class="dropdown-text">Chapter 14: Autoencoders</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/representation-learning.html">
 <span class="dropdown-text">Chapter 15: Representation Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/structured-probabilistic-models.html">
 <span class="dropdown-text">Chapter 16: Structured Probabilistic Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/monte-carlo-methods.html">
 <span class="dropdown-text">Chapter 17: Monte Carlo Methods</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/confronting-partition-function.html">
 <span class="dropdown-text">Chapter 18: Confronting the Partition Function</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/approximate-inference.html">
 <span class="dropdown-text">Chapter 19: Approximate Inference</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/deep-generative-models.html">
 <span class="dropdown-text">Chapter 20: Deep Generative Models</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-linear-algebra" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Linear Algebra</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-linear-algebra">    
        <li class="dropdown-header">Reflections &amp; Synthesis</li>
        <li>
    <a class="dropdown-item" href="../Math/reflections/mit1806-invertibility-connections.html">
 <span class="dropdown-text">Invertibility Connections</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/reflections/taylor-euler-fourier.html">
 <span class="dropdown-text">Taylor, Euler, and Fourier</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">MIT 18.06SC Linear Algebra</li>
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture1-geometry.html">
 <span class="dropdown-text">Lecture 1: Geometry</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture2-elimination.html">
 <span class="dropdown-text">Lecture 2: Elimination</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture3-multiplication.html">
 <span class="dropdown-text">Lecture 3: Multiplication</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture4-lu-decomposition.html">
 <span class="dropdown-text">Lecture 4: LU Decomposition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-permutations.html">
 <span class="dropdown-text">Lecture 5.1: Permutations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-2-transpose.html">
 <span class="dropdown-text">Lecture 5.2: Transpose</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-3-spaces.html">
 <span class="dropdown-text">Lecture 5.3: Spaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture6-column-null-space.html">
 <span class="dropdown-text">Lecture 6: Column &amp; Null Space</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture7-solving-ax-0.html">
 <span class="dropdown-text">Lecture 7: Solving Ax=0</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture8-solving-ax-b.html">
 <span class="dropdown-text">Lecture 8: Solving Ax=b</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html">
 <span class="dropdown-text">Lecture 9: Independence, Basis, Dimension</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture10-four-subspaces.html">
 <span class="dropdown-text">Lecture 10: Four Fundamental Subspaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture11-matrix-spaces.html">
 <span class="dropdown-text">Lecture 11: Matrix Spaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture12-graphs-networks.html">
 <span class="dropdown-text">Lecture 12: Graphs and Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture13-quiz-review.html">
 <span class="dropdown-text">Lecture 13: Quiz 1 Review</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture14-orthogonality.html">
 <span class="dropdown-text">Lecture 14: Orthogonality</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture15-projections.html">
 <span class="dropdown-text">Lecture 15: Projections</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture16-least-squares.html">
 <span class="dropdown-text">Lecture 16: Least Squares</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture17-gram-schmidt.html">
 <span class="dropdown-text">Lecture 17: Gram-Schmidt</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture18-determinants.html">
 <span class="dropdown-text">Lecture 18: Determinants</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture19-determinant-formulas.html">
 <span class="dropdown-text">Lecture 19: Determinant Formulas</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture20-cramers-rule.html">
 <span class="dropdown-text">Lecture 20: Inverse &amp; Volume</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html">
 <span class="dropdown-text">Lecture 21: Eigenvalues &amp; Eigenvectors</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture22-diagonalization-powers.html">
 <span class="dropdown-text">Lecture 22: Diagonalization &amp; Powers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture23-differential-equations.html">
 <span class="dropdown-text">Lecture 23: Differential Equations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture24-markov-fourier.html">
 <span class="dropdown-text">Lecture 24: Markov &amp; Fourier</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html">
 <span class="dropdown-text">Lecture 25: Symmetric &amp; Positive Definite</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html">
 <span class="dropdown-text">Lecture 26: Complex Matrices &amp; FFT</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html">
 <span class="dropdown-text">Lecture 27: Positive Definite &amp; Minima</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html">
 <span class="dropdown-text">Lecture 28: Similar Matrices &amp; Jordan Form</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture29-svd.html">
 <span class="dropdown-text">Lecture 29: Singular Value Decomposition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture30-linear-transformations.html">
 <span class="dropdown-text">Lecture 30: Linear Transformations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture31-change-of-basis.html">
 <span class="dropdown-text">Lecture 31: Change of Basis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture33-left-right-inverse.html">
 <span class="dropdown-text">Lecture 33: Left &amp; Right Inverse</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">MIT 18.065: Linear Algebra Applications</li>
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture1-column-space.html">
 <span class="dropdown-text">Lecture 1: Column Space</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture2-multiplying-factoring.html">
 <span class="dropdown-text">Lecture 2: Multiplying and Factoring Matrices</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture3-orthonormal-columns.html">
 <span class="dropdown-text">Lecture 3: Orthonormal Columns</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture4-eigenvalues-eigenvectors.html">
 <span class="dropdown-text">Lecture 4: Eigenvalues and Eigenvectors</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture5-positive-definite.html">
 <span class="dropdown-text">Lecture 5: Positive Definite Matrices</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture6-svd.html">
 <span class="dropdown-text">Lecture 6: Singular Value Decomposition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture7-eckart-young.html">
 <span class="dropdown-text">Lecture 7: Eckart-Young Theorem</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture8-norms.html">
 <span class="dropdown-text">Lecture 8: Norms</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture9-least-squares.html">
 <span class="dropdown-text">Lecture 9: Least Squares</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture10-axb-difficulties.html">
 <span class="dropdown-text">Lecture 10: Survey of Ax=b Difficulties</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/lecture-11-minimize-norm-subject-to-constraint.html">
 <span class="dropdown-text">Lecture 11: Minimize Norm subject to Constraint</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture12-eigenvalue-algorithms.html">
 <span class="dropdown-text">Lecture 12: Computing Eigenvalues</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture13-randomized-matrix-multiplication.html">
 <span class="dropdown-text">Lecture 13: Randomized Matrix Multiplication</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture14-low-rank-changes-inverse.html">
 <span class="dropdown-text">Lecture 14: Low Rank Changes</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture15-matrix-derivatives.html">
 <span class="dropdown-text">Lecture 15: Matrix Derivatives</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture16-derivative-inverse-singular-values.html">
 <span class="dropdown-text">Lecture 16: Derivative of Inverse and Singular Values</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture17-rapidly-decreasing-singular-values.html">
 <span class="dropdown-text">Lecture 17: Rapidly Decreasing Singular Values</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture18-counting-parameters-svd-lu-qr-saddle-points.html">
 <span class="dropdown-text">Lecture 18: Counting Parameters in SVD, LU, QR, and Saddle Points</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture19-saddle-point-maxmin-principle.html">
 <span class="dropdown-text">Lecture 19: Saddle Point and the Max–Min Principle</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-calculus" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Calculus</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-calculus">    
        <li>
    <a class="dropdown-item" href="../Math/Calculus/index.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/Calculus/max-min-second-derivative.html">
 <span class="dropdown-text">Gilbert Strang’s Calculus: Max, Min, and Second Derivative</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/Calculus/big-picture-derivatives.html">
 <span class="dropdown-text">Gilbert Strang’s Calculus: Big Picture on Derivatives</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/Calculus/highlights-of-calculus.html">
 <span class="dropdown-text">Gilbert Strang’s Calculus: Highlights</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-numerical-optimization" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Numerical Optimization</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-numerical-optimization">    
        <li>
    <a class="dropdown-item" href="../Math/EE364A/lectures.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li class="dropdown-header">Stanford EE 364A: Convex Optimization</li>
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture1-intro.html">
 <span class="dropdown-text">Lecture 1: Introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture2-math-foundations.html">
 <span class="dropdown-text">Lecture 2: Convex Sets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture3-convex-functions.html">
 <span class="dropdown-text">Lecture 3: Convex Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html">
 <span class="dropdown-text">Lecture 4.1: Operations Preserving Convexity</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture4-part2-conjugate-quasiconvex.html">
 <span class="dropdown-text">Lecture 4.2: Conjugate &amp; Quasiconvex Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html">
 <span class="dropdown-text">Lecture 5.1: Log-Concave &amp; Log-Convex</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture5-part2-monotonicity.html">
 <span class="dropdown-text">Lecture 5.2: Monotonicity</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture6-optimization-problems.html">
 <span class="dropdown-text">Chapter 4.1: Optimization Problems</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture7-convex-optimization.html">
 <span class="dropdown-text">Chapter 4.2: Convex Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture8-linear-optimization.html">
 <span class="dropdown-text">Chapter 4.3: Linear Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture9-quadratic-optimization.html">
 <span class="dropdown-text">Chapter 4.4: Quadratic Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-chapter4-5-geometric-programming.html">
 <span class="dropdown-text">Chapter 4.5: Geometric Programming</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.html">
 <span class="dropdown-text">Chapter 4.6: Generalized Inequality Constraints</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-chapter4-7-vector-optimization.html">
 <span class="dropdown-text">Chapter 4.7: Vector Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-chapter5-1-lagrange-dual-function.html">
 <span class="dropdown-text">Chapter 5.1: The Lagrange Dual Function</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-theory-to-repro" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Theory-to-Repro</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-theory-to-repro">    
        <li>
    <a class="dropdown-item" href="../Theory-to-Repro/index.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Theory-to-Repro/linear-regression-three-ways.html">
 <span class="dropdown-text">Linear Regression via Three Solvers</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../ML/k_means_clustering.html">
 <span class="dropdown-text">K-Means Clustering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/logistic_regression.html">
 <span class="dropdown-text">Logistic Regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/axis.html">
 <span class="dropdown-text">Axis Operations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Algorithm/dp_regex.html">
 <span class="dropdown-text">DP Regex</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/ickma2311/ickma2311.github.io/discussions"> 
<span class="menu-text">Feedback</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#limitation-of-bagging" id="toc-limitation-of-bagging" class="nav-link active" data-scroll-target="#limitation-of-bagging">Limitation of Bagging</a></li>
  <li><a href="#dropout" id="toc-dropout" class="nav-link" data-scroll-target="#dropout">Dropout</a></li>
  <li><a href="#comparing-with-bagging" id="toc-comparing-with-bagging" class="nav-link" data-scroll-target="#comparing-with-bagging">Comparing with Bagging</a></li>
  <li><a href="#masks" id="toc-masks" class="nav-link" data-scroll-target="#masks">Masks</a></li>
  <li><a href="#feasibility-of-simple-forward-propagation-in-dropout-inference" id="toc-feasibility-of-simple-forward-propagation-in-dropout-inference" class="nav-link" data-scroll-target="#feasibility-of-simple-forward-propagation-in-dropout-inference">Feasibility of Simple Forward Propagation in Dropout Inference</a>
  <ul class="collapse">
  <li><a href="#deriving-the-weight-scaling-rule" id="toc-deriving-the-weight-scaling-rule" class="nav-link" data-scroll-target="#deriving-the-weight-scaling-rule">Deriving the Weight Scaling Rule</a></li>
  </ul></li>
  <li><a href="#computational-efficiency-of-dropout" id="toc-computational-efficiency-of-dropout" class="nav-link" data-scroll-target="#computational-efficiency-of-dropout">Computational Efficiency of Dropout</a></li>
  <li><a href="#limitations-of-dropout" id="toc-limitations-of-dropout" class="nav-link" data-scroll-target="#limitations-of-dropout">Limitations of Dropout</a></li>
  <li><a href="#intuition-and-insights-behind-dropout" id="toc-intuition-and-insights-behind-dropout" class="nav-link" data-scroll-target="#intuition-and-insights-behind-dropout">Intuition and Insights Behind Dropout</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Goodfellow Deep Learning — Chapter 7.12: Dropout</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">regularization</div>
    <div class="quarto-category">dropout</div>
    <div class="quarto-category">ensemble methods</div>
  </div>
  </div>

<div>
  <div class="description">
    Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Chao Ma </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 30, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="limitation-of-bagging" class="level2">
<h2 class="anchored" data-anchor-id="limitation-of-bagging">Limitation of Bagging</h2>
<p>When training a very large neural network, it is often impractical to train and average multiple models because the computational cost is too high.</p>
</section>
<section id="dropout" class="level2">
<h2 class="anchored" data-anchor-id="dropout">Dropout</h2>
<p>Dropout is a computationally efficient alternative to bagging that trains an ensemble of subnetworks by randomly dropping units during training.</p>
<p>If we have <span class="math inline">\(n\)</span> droppable units, each of them can be either kept or dropped independently, we have <span class="math inline">\(2^n\)</span> subnetworks.</p>
<p><span class="math display">\[
2\times2\times2\text{...}\times2=2^n
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../media/dropout.png" class="img-fluid figure-img"></p>
<figcaption>Dropout Training</figcaption>
</figure>
</div>
<hr>
</section>
<section id="comparing-with-bagging" class="level2">
<h2 class="anchored" data-anchor-id="comparing-with-bagging">Comparing with Bagging</h2>
<p>Assume our task is to output the probability.</p>
<p><strong>Bagging</strong> (Equation 7.52): Averages predictions from <span class="math inline">\(k\)</span> independently trained models.</p>
<p><span class="math display">\[
\frac{1}{k}\sum_{i=1}^kp^{(i)}(y|x)
\]</span></p>
<p><strong>Dropout</strong> (Equation 7.53): Takes a weighted sum over all possible mask configurations, where <span class="math inline">\(p(\mu)\)</span> is the probability to sample mask <span class="math inline">\(\mu\)</span>.</p>
<p><span class="math display">\[
\sum_\mu p(\mu)p(y|x,\mu)
\]</span></p>
<hr>
</section>
<section id="masks" class="level2">
<h2 class="anchored" data-anchor-id="masks">Masks</h2>
<p>We use a vector to represent masks for each unit:</p>
<p><span class="math display">\[
\mu=[\mu_1,\mu_2,...\mu_n]
\]</span></p>
<p>During training, we sample masks:</p>
<p><span class="math display">\[
\begin{aligned}
h &amp;= [h_1,h_2,...,h_n] \\
h' &amp;= h\odot\mu
\end{aligned}
\]</span></p>
<p>For example, if <span class="math inline">\(\mu=[1,0,1]\)</span>, then <span class="math inline">\(h'=[h_1,0,h_3]\)</span></p>
<hr>
</section>
<section id="feasibility-of-simple-forward-propagation-in-dropout-inference" class="level2">
<h2 class="anchored" data-anchor-id="feasibility-of-simple-forward-propagation-in-dropout-inference">Feasibility of Simple Forward Propagation in Dropout Inference</h2>
<p>Using geometric average (Equation 7.54):</p>
<p><span class="math display">\[
\tilde{p}_{\text{ensemble}}(y|x)=\sqrt[2^d]{\prod_{\mu}p(y|x,\mu)}
\]</span></p>
<p><strong>Normalization</strong> (Equation 7.55): Assuming the distribution is uniform:</p>
<p><span class="math display">\[
p_{\text{ensemble}}(y|x)=\frac{\tilde{p}(y|x)}{\sum_{y'}\tilde{p}(y'|x)}
\]</span></p>
<section id="deriving-the-weight-scaling-rule" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-weight-scaling-rule">Deriving the Weight Scaling Rule</h3>
<p>For model families without non-linear hidden units, we can derive an exact solution.</p>
<p><strong>Standard softmax output</strong> (Equation 7.56):</p>
<p><span class="math display">\[
P(y=y|v)=\text{softmax}(W^\top v+b)_y
\]</span></p>
<p><strong>With dropout mask</strong> (Equation 7.57):</p>
<p><span class="math display">\[
P(y=y|\mathbf{v};\mathbf{d})=\text{softmax}(\mathbf{W}^\top (\mathbf{d}\odot\mathbf{v})+b)_y
\]</span></p>
<p><strong>Ensemble prediction</strong> (Equation 7.58):</p>
<p><span class="math display">\[
P_{\text{ensemble}}(y=y|\mathbf{v})=\frac{\tilde{P}_{\text{ensemble}}(y=y|\mathbf{v})}{\sum_{y'}\tilde{P}_{\text{ensemble}}(y=y'|\mathbf{v})}
\]</span></p>
<p><strong>Geometric average</strong> (Equation 7.59):</p>
<p><span class="math display">\[
\tilde{P}_{\text{ensemble}}(y=y\mid\mathbf{v})=\sqrt[2^n]{\prod_{\mathbf{d}\in\{0,1\}^n}P(y=y\mid \mathbf{v};\mathbf{d})}
\]</span></p>
<p>Expanding the softmax (Equation 7.62):</p>
<p><span class="math display">\[
\tilde{P}_{\text{ensemble}}(y|\mathbf{v}) = \sqrt[2^n]{\prod_{\mathbf{d} \in \{0,1\}^n} \frac{\exp(\mathbf{W}_y^\top(\mathbf{d} \odot\mathbf{v})+b_y)}{\sum_{y'}\exp(\mathbf{W}_{y'}^\top(\mathbf{d}\odot\mathbf{v})+b_{y'})}}
\]</span></p>
<p>Separating numerator and denominator (Equation 7.63):</p>
<p><span class="math display">\[
\tilde{P}_{\text{ensemble}}(y \mid \mathbf{v}) = \frac{\sqrt[2^n]{\displaystyle \prod_{\mathbf{d} \in \{0,1\}^n} \exp\big(\mathbf{W}_{y}^{\top}(\mathbf{d}\odot \mathbf{v}) + b_y\big)}}{\sqrt[2^n]{\displaystyle \prod_{\mathbf{d} \in \{0,1\}^n} \sum_{y'} \exp\big(\mathbf{W}_{y'}^{\top}(\mathbf{d}\odot \mathbf{v}) + b_{y'}\big)}}
\]</span></p>
<p><strong>Key properties of exponentials</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
\prod_i e^{a_i} &amp;= e^{\sum_i a_i} \\
\sqrt[k]{\prod_i^k x_i} &amp;= \exp\left(\frac{1}{k}\sum_i\log x_i\right)
\end{aligned}
\]</span></p>
<p>Since the denominator is constant with respect to <span class="math inline">\(y\)</span> (Equation 7.64):</p>
<p><span class="math display">\[
\tilde{P}_{\text{ensemble}}(y=y|v) \propto \sqrt[2^n]{\prod_{\mathbf{d} \in \{0,1\}^n} \exp(\mathbf{W}_y^\top(\mathbf{d}\odot v)+b_y)}
\]</span></p>
<p>Applying the geometric average property (Equation 7.65):</p>
<p><span class="math display">\[
= \exp\left(\frac{1}{2^n}\sum_{\mathbf{d} \in \{0,1\}^n} (\mathbf{W}^\top(\mathbf{d}\odot v)+b_y)\right)
\]</span></p>
<p><strong>Final result</strong> (Equation 7.66):</p>
<p><span class="math display">\[
= \exp\left(\frac{1}{2}\mathbf{W}^\top v+b_y\right)
\]</span></p>
<p>This shows that at inference, we can simply <strong>scale weights by the keep probability</strong> (e.g., 0.5) instead of sampling multiple masks.</p>
<p><strong>Intuition</strong>: Each unit has probability 0.5 of being active, so the expected input is <span class="math inline">\(0.5 \times v\)</span>. Therefore, multiplying by 0.5 at inference approximates the ensemble average.</p>
<hr>
</section>
</section>
<section id="computational-efficiency-of-dropout" class="level2">
<h2 class="anchored" data-anchor-id="computational-efficiency-of-dropout">Computational Efficiency of Dropout</h2>
<p>Dropout acts as an <strong>implicit ensemble</strong>, where all subnetworks share the same parameters within one network.</p>
<p><strong>During training</strong>: Each unit has a probability (e.g., 0.5) of being active, so all <span class="math inline">\(2^n\)</span> subnetworks are trained efficiently within a single forward/backward pass.</p>
<p><strong>During inference</strong>: Only one forward pass is required — we simply multiply the activations (or equivalently the weights) by the keep probability (e.g., 0.5).</p>
<p><strong>Alternative approach</strong>: Gal and Ghahramani (2015) found that some models can achieve better classification accuracy by using Monte Carlo approximation with around 20 dropout samples. The optimal number of samples for inference approximation appears to be problem-dependent.</p>
<p>Dropout outperforms traditional low-cost regularization methods (e.g., weight decay, norm or sparsity constraints) and can be combined with them for additional gains.</p>
<hr>
</section>
<section id="limitations-of-dropout" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-dropout">Limitations of Dropout</h2>
<ol type="1">
<li><p><strong>Requires a sufficiently large model capacity</strong></p>
<p>Dropout is most effective when the network has enough parameters to compensate for the random removal of units. Small models may underfit when dropout is applied.</p></li>
<li><p><strong>May be less effective with small training datasets</strong></p>
<p>When the dataset is small, the stochastic noise introduced by dropout can overwhelm the learning signal, leading to unstable training or degraded performance.</p></li>
</ol>
<hr>
</section>
<section id="intuition-and-insights-behind-dropout" class="level2">
<h2 class="anchored" data-anchor-id="intuition-and-insights-behind-dropout">Intuition and Insights Behind Dropout</h2>
<p>Dropout forces each unit to perform well independently, without relying on the presence of specific other units. This encourages the network to learn <strong>redundant yet complementary representations</strong>, so that every subnetwork formed during dropout can perform reasonably well. As a result, combining many of these “good-enough” subnetworks produces a more powerful ensemble.</p>
<hr>
<p><strong>Biological inspiration</strong>: Hinton proposed that dropout resembles the process of gene exchange between organisms. Evolutionary pressure not only rewards strong genes but also favors genes that remain effective after recombination. Similarly, dropout encourages units to learn features that are robust to co-adaptation and can function well under many combinations.</p>
<hr>
<p><strong>Adaptive destruction</strong>: By randomly “corrupting” its own input during training, dropout teaches the network to adapt to noise and missing information. This adaptive destruction mechanism leads to features that are more stable and robust to input perturbations and unseen conditions.</p>
<hr>
<p><em>Source: Deep Learning (Ian Goodfellow, Yoshua Bengio, Aaron Courville), Chapter 7.12</em></p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="light">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "ickma2311/ickma2311.github.io";
    script.dataset.repoId = "R_kgDOOzbaAg";
    script.dataset.category = "Comments";
    script.dataset.categoryId = "DIC_kwDOOzbaAs4CxPFj";
    script.dataset.mapping = "pathname";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Goodfellow Deep Learning — Chapter 7.12: Dropout"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Chao Ma"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2025-10-30"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [deep learning, regularization, dropout, ensemble methods]</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units"</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">## Limitation of Bagging</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>When training a very large neural network, it is often impractical to train and average multiple models because the computational cost is too high.</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">## Dropout</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>Dropout is a computationally efficient alternative to bagging that trains an ensemble of subnetworks by randomly dropping units during training.</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>If we have $n$ droppable units, each of them can be either kept or dropped independently, we have $2^n$ subnetworks.</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>2\times2\times2\text{...}\times2=2^n</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="al">![Dropout Training](../media/dropout.png)</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="fu">## Comparing with Bagging</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>Assume our task is to output the probability.</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>**Bagging** (Equation 7.52): Averages predictions from $k$ independently trained models.</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>\frac{1}{k}\sum_{i=1}^kp^{(i)}(y|x)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>**Dropout** (Equation 7.53): Takes a weighted sum over all possible mask configurations, where $p(\mu)$ is the probability to sample mask $\mu$.</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>\sum_\mu p(\mu)p(y|x,\mu)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="fu">## Masks</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>We use a vector to represent masks for each unit:</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>\mu=<span class="co">[</span><span class="ot">\mu_1,\mu_2,...\mu_n</span><span class="co">]</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>During training, we sample masks:</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>h &amp;= <span class="co">[</span><span class="ot">h_1,h_2,...,h_n</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>h' &amp;= h\odot\mu</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>For example, if $\mu=<span class="co">[</span><span class="ot">1,0,1</span><span class="co">]</span>$, then $h'=<span class="co">[</span><span class="ot">h_1,0,h_3</span><span class="co">]</span>$</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="fu">## Feasibility of Simple Forward Propagation in Dropout Inference</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>Using geometric average (Equation 7.54):</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>\tilde{p}_{\text{ensemble}}(y|x)=\sqrt[2^d]{\prod_{\mu}p(y|x,\mu)}</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>**Normalization** (Equation 7.55): Assuming the distribution is uniform:</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>p_{\text{ensemble}}(y|x)=\frac{\tilde{p}(y|x)}{\sum_{y'}\tilde{p}(y'|x)}</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deriving the Weight Scaling Rule</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>For model families without non-linear hidden units, we can derive an exact solution.</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>**Standard softmax output** (Equation 7.56):</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>P(y=y|v)=\text{softmax}(W^\top v+b)_y</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>**With dropout mask** (Equation 7.57):</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>P(y=y|\mathbf{v};\mathbf{d})=\text{softmax}(\mathbf{W}^\top (\mathbf{d}\odot\mathbf{v})+b)_y</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>**Ensemble prediction** (Equation 7.58):</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>P_{\text{ensemble}}(y=y|\mathbf{v})=\frac{\tilde{P}_{\text{ensemble}}(y=y|\mathbf{v})}{\sum_{y'}\tilde{P}_{\text{ensemble}}(y=y'|\mathbf{v})}</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>**Geometric average** (Equation 7.59):</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>\tilde{P}_{\text{ensemble}}(y=y\mid\mathbf{v})=\sqrt[2^n]{\prod_{\mathbf{d}\in<span class="sc">\{</span>0,1<span class="sc">\}</span>^n}P(y=y\mid \mathbf{v};\mathbf{d})}</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>Expanding the softmax (Equation 7.62):</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>\tilde{P}_{\text{ensemble}}(y|\mathbf{v}) = \sqrt[2^n]{\prod_{\mathbf{d} \in \{0,1\}^n} \frac{\exp(\mathbf{W}_y^\top(\mathbf{d} \odot\mathbf{v})+b_y)}{\sum_{y'}\exp(\mathbf{W}_{y'}^\top(\mathbf{d}\odot\mathbf{v})+b_{y'})}}</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>Separating numerator and denominator (Equation 7.63):</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>\tilde{P}_{\text{ensemble}}(y \mid \mathbf{v}) = \frac{\sqrt[2^n]{\displaystyle \prod_{\mathbf{d} \in \{0,1\}^n} \exp\big(\mathbf{W}_{y}^{\top}(\mathbf{d}\odot \mathbf{v}) + b_y\big)}}{\sqrt[2^n]{\displaystyle \prod_{\mathbf{d} \in \{0,1\}^n} \sum_{y'} \exp\big(\mathbf{W}_{y'}^{\top}(\mathbf{d}\odot \mathbf{v}) + b_{y'}\big)}}</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>**Key properties of exponentials**:</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>\prod_i e^{a_i} &amp;= e^{\sum_i a_i} <span class="sc">\\</span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>\sqrt<span class="co">[</span><span class="ot">k</span><span class="co">]</span>{\prod_i^k x_i} &amp;= \exp\left(\frac{1}{k}\sum_i\log x_i\right)</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>Since the denominator is constant with respect to $y$ (Equation 7.64):</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>\tilde{P}_{\text{ensemble}}(y=y|v) \propto \sqrt[2^n]{\prod_{\mathbf{d} \in <span class="sc">\{</span>0,1<span class="sc">\}</span>^n} \exp(\mathbf{W}_y^\top(\mathbf{d}\odot v)+b_y)}</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>Applying the geometric average property (Equation 7.65):</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>= \exp\left(\frac{1}{2^n}\sum_{\mathbf{d} \in <span class="sc">\{</span>0,1<span class="sc">\}</span>^n} (\mathbf{W}^\top(\mathbf{d}\odot v)+b_y)\right)</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>**Final result** (Equation 7.66):</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>= \exp\left(\frac{1}{2}\mathbf{W}^\top v+b_y\right)</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>This shows that at inference, we can simply **scale weights by the keep probability** (e.g., 0.5) instead of sampling multiple masks.</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>**Intuition**: Each unit has probability 0.5 of being active, so the expected input is $0.5 \times v$. Therefore, multiplying by 0.5 at inference approximates the ensemble average.</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational Efficiency of Dropout</span></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>Dropout acts as an **implicit ensemble**, where all subnetworks share the same parameters within one network.</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>**During training**: Each unit has a probability (e.g., 0.5) of being active, so all $2^n$ subnetworks are trained efficiently within a single forward/backward pass.</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>**During inference**: Only one forward pass is required — we simply multiply the activations (or equivalently the weights) by the keep probability (e.g., 0.5).</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>**Alternative approach**: Gal and Ghahramani (2015) found that some models can achieve better classification accuracy by using Monte Carlo approximation with around 20 dropout samples. The optimal number of samples for inference approximation appears to be problem-dependent.</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>Dropout outperforms traditional low-cost regularization methods (e.g., weight decay, norm or sparsity constraints) and can be combined with them for additional gains.</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a><span class="fu">## Limitations of Dropout</span></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Requires a sufficiently large model capacity**</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>   Dropout is most effective when the network has enough parameters to compensate for the random removal of units. Small models may underfit when dropout is applied.</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**May be less effective with small training datasets**</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>   When the dataset is small, the stochastic noise introduced by dropout can overwhelm the learning signal, leading to unstable training or degraded performance.</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intuition and Insights Behind Dropout</span></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>Dropout forces each unit to perform well independently, without relying on the presence of specific other units. This encourages the network to learn **redundant yet complementary representations**, so that every subnetwork formed during dropout can perform reasonably well. As a result, combining many of these "good-enough" subnetworks produces a more powerful ensemble.</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>**Biological inspiration**: Hinton proposed that dropout resembles the process of gene exchange between organisms. Evolutionary pressure not only rewards strong genes but also favors genes that remain effective after recombination. Similarly, dropout encourages units to learn features that are robust to co-adaptation and can function well under many combinations.</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>**Adaptive destruction**: By randomly "corrupting" its own input during training, dropout teaches the network to adapt to noise and missing information. This adaptive destruction mechanism leads to features that are more stable and robust to input perturbations and unseen conditions.</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>*Source: Deep Learning (Ian Goodfellow, Yoshua Bengio, Aaron Courville), Chapter 7.12*</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>