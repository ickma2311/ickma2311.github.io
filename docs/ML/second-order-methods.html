<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chao Ma">
<meta name="dcterms.date" content="2025-11-15">
<meta name="description" content="Newton’s method, Conjugate Gradient, and BFGS: elegant second-order methods that use curvature information but are rarely used in deep learning due to computational cost and scalability issues">

<title>Chapter 8.6: Second-Order Optimization Methods – ∇ ickma.dev</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.svg" rel="icon" type="image/svg+xml">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">∇ ickma.dev</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-deep-learning-book" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Deep Learning Book</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-deep-learning-book">    
        <li>
    <a class="dropdown-item" href="../ML/xor-deep-learning.html">
 <span class="dropdown-text">Chapter 6.1: XOR Problem</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/likelihood-loss-functions.html">
 <span class="dropdown-text">Chapter 6.2: Loss Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/activation-functions.html">
 <span class="dropdown-text">Chapter 6.3: Activation Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/architecture-design.html">
 <span class="dropdown-text">Chapter 6.4: Architecture Design</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/backpropagation.html">
 <span class="dropdown-text">Chapter 6.5: Back-Propagation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/hessian-prerequisites.html">
 <span class="dropdown-text">Chapter 7 Prerequisites: Hessian Matrix</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/l2-regularization.html">
 <span class="dropdown-text">Chapter 7.1.1: L2 Regularization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/l1-regularization.html">
 <span class="dropdown-text">Chapter 7.1.2: L1 Regularization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/constrained-optimization-regularization.html">
 <span class="dropdown-text">Chapter 7.2: Constrained Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/regularization-underconstrained.html">
 <span class="dropdown-text">Chapter 7.3: Under-Constrained Problems</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/dataset-augmentation.html">
 <span class="dropdown-text">Chapter 7.4: Dataset Augmentation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/noise-robustness.html">
 <span class="dropdown-text">Chapter 7.5: Noise Robustness</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/semi-supervised-learning.html">
 <span class="dropdown-text">Chapter 7.6: Semi-Supervised Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/multi-task-learning.html">
 <span class="dropdown-text">Chapter 7.7: Multi-Task Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/early-stopping.html">
 <span class="dropdown-text">Chapter 7.8: Early Stopping</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/parameter-tying-sharing.html">
 <span class="dropdown-text">Chapter 7.9: Parameter Tying &amp; Sharing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/representation-sparsity.html">
 <span class="dropdown-text">Chapter 7.10: Sparse Representations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/bagging-ensemble.html">
 <span class="dropdown-text">Chapter 7.11: Bagging &amp; Ensemble Methods</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/dropout.html">
 <span class="dropdown-text">Chapter 7.12: Dropout</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/adversarial-training.html">
 <span class="dropdown-text">Chapter 7.13: Adversarial Training</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/tangent-prop-manifold.html">
 <span class="dropdown-text">Chapter 7.14: Tangent Prop &amp; Manifolds</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/learning-vs-optimization.html">
 <span class="dropdown-text">Chapter 8.1: Learning vs Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/optimization-challenges.html">
 <span class="dropdown-text">Chapter 8.2: Optimization Challenges</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/basic-optimization-algorithms.html">
 <span class="dropdown-text">Chapter 8.3: Basic Algorithms</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/parameter-initialization.html">
 <span class="dropdown-text">Chapter 8.4: Parameter Initialization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/adaptive-learning-rates.html">
 <span class="dropdown-text">Chapter 8.5: Adaptive Learning Rates</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/second-order-methods.html">
 <span class="dropdown-text">Chapter 8.6: Second-Order Methods</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/optimization-strategies.html">
 <span class="dropdown-text">Chapter 8.7: Optimization Strategies</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/convolution-computation.html">
 <span class="dropdown-text">Chapter 9.1: Convolution Computation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-motivation.html">
 <span class="dropdown-text">Chapter 9.2: CNN Motivation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-pooling.html">
 <span class="dropdown-text">Chapter 9.3: Pooling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-infinitely-strong-prior.html">
 <span class="dropdown-text">Chapter 9.4: Infinitely Strong Prior</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-math" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Math</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-math">    
        <li class="dropdown-header">Reflections &amp; Synthesis</li>
        <li>
    <a class="dropdown-item" href="../Math/reflections/mit1806-invertibility-connections.html">
 <span class="dropdown-text">Invertibility Connections</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/reflections/taylor-euler-fourier.html">
 <span class="dropdown-text">Taylor, Euler, and Fourier</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">MIT 18.06SC Linear Algebra</li>
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture1-geometry.html">
 <span class="dropdown-text">Lecture 1: Geometry</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture2-elimination.html">
 <span class="dropdown-text">Lecture 2: Elimination</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture3-multiplication.html">
 <span class="dropdown-text">Lecture 3: Multiplication</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture4-lu-decomposition.html">
 <span class="dropdown-text">Lecture 4: LU Decomposition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-permutations.html">
 <span class="dropdown-text">Lecture 5.1: Permutations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-2-transpose.html">
 <span class="dropdown-text">Lecture 5.2: Transpose</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-3-spaces.html">
 <span class="dropdown-text">Lecture 5.3: Spaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture6-column-null-space.html">
 <span class="dropdown-text">Lecture 6: Column &amp; Null Space</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture7-solving-ax-0.html">
 <span class="dropdown-text">Lecture 7: Solving Ax=0</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture8-solving-ax-b.html">
 <span class="dropdown-text">Lecture 8: Solving Ax=b</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html">
 <span class="dropdown-text">Lecture 9: Independence, Basis, Dimension</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture10-four-subspaces.html">
 <span class="dropdown-text">Lecture 10: Four Fundamental Subspaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture11-matrix-spaces.html">
 <span class="dropdown-text">Lecture 11: Matrix Spaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture12-graphs-networks.html">
 <span class="dropdown-text">Lecture 12: Graphs and Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture13-quiz-review.html">
 <span class="dropdown-text">Lecture 13: Quiz 1 Review</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture14-orthogonality.html">
 <span class="dropdown-text">Lecture 14: Orthogonality</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture15-projections.html">
 <span class="dropdown-text">Lecture 15: Projections</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture16-least-squares.html">
 <span class="dropdown-text">Lecture 16: Least Squares</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture17-gram-schmidt.html">
 <span class="dropdown-text">Lecture 17: Gram-Schmidt</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture18-determinants.html">
 <span class="dropdown-text">Lecture 18: Determinants</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture19-determinant-formulas.html">
 <span class="dropdown-text">Lecture 19: Determinant Formulas</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture20-cramers-rule.html">
 <span class="dropdown-text">Lecture 20: Inverse &amp; Volume</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html">
 <span class="dropdown-text">Lecture 21: Eigenvalues &amp; Eigenvectors</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture22-diagonalization-powers.html">
 <span class="dropdown-text">Lecture 22: Diagonalization &amp; Powers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture23-differential-equations.html">
 <span class="dropdown-text">Lecture 23: Differential Equations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture24-markov-fourier.html">
 <span class="dropdown-text">Lecture 24: Markov &amp; Fourier</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html">
 <span class="dropdown-text">Lecture 25: Symmetric &amp; Positive Definite</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html">
 <span class="dropdown-text">Lecture 26: Complex Matrices &amp; FFT</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html">
 <span class="dropdown-text">Lecture 27: Positive Definite &amp; Minima</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html">
 <span class="dropdown-text">Lecture 28: Similar Matrices &amp; Jordan Form</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture29-svd.html">
 <span class="dropdown-text">Lecture 29: Singular Value Decomposition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture30-linear-transformations.html">
 <span class="dropdown-text">Lecture 30: Linear Transformations</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-more" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">More</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-more">    
        <li>
    <a class="dropdown-item" href="../ML/kmeans.html">
 <span class="dropdown-text">K-Means Clustering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/logistic_regression.html">
 <span class="dropdown-text">Logistic Regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/axis.html">
 <span class="dropdown-text">Axis Operations</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../Algorithm/dp_regex.html">
 <span class="dropdown-text">DP Regex</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/ickma2311/ickma2311.github.io/discussions"> 
<span class="menu-text">Feedback</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#newtons-method" id="toc-newtons-method" class="nav-link active" data-scroll-target="#newtons-method">Newton’s Method</a>
  <ul class="collapse">
  <li><a href="#second-order-taylor-approximation" id="toc-second-order-taylor-approximation" class="nav-link" data-scroll-target="#second-order-taylor-approximation">Second-order Taylor Approximation</a></li>
  <li><a href="#newtons-update-rule" id="toc-newtons-update-rule" class="nav-link" data-scroll-target="#newtons-update-rule">Newton’s Update Rule</a></li>
  <li><a href="#deriving-the-newton-step" id="toc-deriving-the-newton-step" class="nav-link" data-scroll-target="#deriving-the-newton-step">Deriving the Newton Step</a></li>
  <li><a href="#key-property-one-step-convergence-for-quadratics" id="toc-key-property-one-step-convergence-for-quadratics" class="nav-link" data-scroll-target="#key-property-one-step-convergence-for-quadratics">Key Property: One-step Convergence for Quadratics</a></li>
  <li><a href="#newtons-method-algorithm" id="toc-newtons-method-algorithm" class="nav-link" data-scroll-target="#newtons-method-algorithm">Newton’s Method Algorithm</a></li>
  <li><a href="#regularization-handling-non-positive-definite-hessians" id="toc-regularization-handling-non-positive-definite-hessians" class="nav-link" data-scroll-target="#regularization-handling-non-positive-definite-hessians">Regularization: Handling Non-positive-definite Hessians</a></li>
  <li><a href="#computational-challenge" id="toc-computational-challenge" class="nav-link" data-scroll-target="#computational-challenge">Computational Challenge</a></li>
  </ul></li>
  <li><a href="#conjugate-gradient" id="toc-conjugate-gradient" class="nav-link" data-scroll-target="#conjugate-gradient">Conjugate Gradient</a>
  <ul class="collapse">
  <li><a href="#the-zig-zag-problem-in-steepest-descent" id="toc-the-zig-zag-problem-in-steepest-descent" class="nav-link" data-scroll-target="#the-zig-zag-problem-in-steepest-descent">The Zig-zag Problem in Steepest Descent</a></li>
  <li><a href="#a-orthogonality-the-key-concept" id="toc-a-orthogonality-the-key-concept" class="nav-link" data-scroll-target="#a-orthogonality-the-key-concept">A-orthogonality: The Key Concept</a></li>
  <li><a href="#conjugate-gradient-update-rule" id="toc-conjugate-gradient-update-rule" class="nav-link" data-scroll-target="#conjugate-gradient-update-rule">Conjugate Gradient Update Rule</a></li>
  <li><a href="#computing-beta_t-fletcher-reeves" id="toc-computing-beta_t-fletcher-reeves" class="nav-link" data-scroll-target="#computing-beta_t-fletcher-reeves">Computing <span class="math inline">\(\beta_t\)</span>: Fletcher-Reeves</a></li>
  <li><a href="#computing-beta_t-polak-ribière" id="toc-computing-beta_t-polak-ribière" class="nav-link" data-scroll-target="#computing-beta_t-polak-ribière">Computing <span class="math inline">\(\beta_t\)</span>: Polak-Ribière</a></li>
  <li><a href="#line-search-choosing-the-step-size-epsilon" id="toc-line-search-choosing-the-step-size-epsilon" class="nav-link" data-scroll-target="#line-search-choosing-the-step-size-epsilon">Line Search: Choosing the Step Size <span class="math inline">\(\epsilon^*\)</span></a></li>
  <li><a href="#conjugate-gradient-in-practice" id="toc-conjugate-gradient-in-practice" class="nav-link" data-scroll-target="#conjugate-gradient-in-practice">Conjugate Gradient in Practice</a></li>
  </ul></li>
  <li><a href="#bfgs" id="toc-bfgs" class="nav-link" data-scroll-target="#bfgs">BFGS</a>
  <ul class="collapse">
  <li><a href="#core-idea" id="toc-core-idea" class="nav-link" data-scroll-target="#core-idea">Core Idea</a></li>
  <li><a href="#how-m_t-is-updated" id="toc-how-m_t-is-updated" class="nav-link" data-scroll-target="#how-m_t-is-updated">How <span class="math inline">\(M_t\)</span> is Updated</a></li>
  <li><a href="#limited-memory-bfgs-l-bfgs" id="toc-limited-memory-bfgs-l-bfgs" class="nav-link" data-scroll-target="#limited-memory-bfgs-l-bfgs">Limited-Memory BFGS (L-BFGS)</a></li>
  </ul></li>
  <li><a href="#why-second-order-methods-are-rarely-used-in-deep-learning" id="toc-why-second-order-methods-are-rarely-used-in-deep-learning" class="nav-link" data-scroll-target="#why-second-order-methods-are-rarely-used-in-deep-learning">Why Second-order Methods Are Rarely Used in Deep Learning</a>
  <ul class="collapse">
  <li><a href="#computational-cost" id="toc-computational-cost" class="nav-link" data-scroll-target="#computational-cost">1. Computational Cost</a></li>
  <li><a href="#stochastic-gradients" id="toc-stochastic-gradients" class="nav-link" data-scroll-target="#stochastic-gradients">2. Stochastic Gradients</a></li>
  <li><a href="#non-convexity" id="toc-non-convexity" class="nav-link" data-scroll-target="#non-convexity">3. Non-convexity</a></li>
  <li><a href="#scalability" id="toc-scalability" class="nav-link" data-scroll-target="#scalability">4. Scalability</a></li>
  <li><a href="#practical-consequence" id="toc-practical-consequence" class="nav-link" data-scroll-target="#practical-consequence">Practical Consequence</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Chapter 8.6: Second-Order Optimization Methods</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">Optimization</div>
    <div class="quarto-category">Newton's Method</div>
    <div class="quarto-category">Conjugate Gradient</div>
    <div class="quarto-category">BFGS</div>
    <div class="quarto-category">Hessian</div>
  </div>
  </div>

<div>
  <div class="description">
    Newton’s method, Conjugate Gradient, and BFGS: elegant second-order methods that use curvature information but are rarely used in deep learning due to computational cost and scalability issues
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Chao Ma </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 15, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>While first-order methods like gradient descent rely only on the gradient, second-order methods incorporate curvature information through the Hessian matrix. This section explores classical second-order optimization techniques: Newton’s method, Conjugate Gradient, and BFGS.</p>
<p><strong>Important context</strong>: Although these methods are theoretically elegant and powerful, they are rarely used in modern deep learning due to computational cost and scalability issues. Practical deep learning almost exclusively relies on first-order methods (SGD, Momentum, RMSProp, Adam).</p>
<section id="newtons-method" class="level2">
<h2 class="anchored" data-anchor-id="newtons-method">Newton’s Method</h2>
<p>Newton’s method uses a second-order Taylor approximation to find the optimal step direction.</p>
<section id="second-order-taylor-approximation" class="level3">
<h3 class="anchored" data-anchor-id="second-order-taylor-approximation">Second-order Taylor Approximation</h3>
<p>Around a point <span class="math inline">\(\theta_0\)</span>, we approximate the objective function using both first and second derivatives:</p>
<p><span class="math display">\[
J(\theta) \approx J(\theta_0) + (\theta - \theta_0)^{\top}\nabla_{\theta}J(\theta_0) + \frac{1}{2}(\theta - \theta_0)^{\top}H(\theta_0)(\theta - \theta_0) \tag{8.26}
\]</span></p>
<p>where <span class="math inline">\(H(\theta_0)\)</span> is the <strong>Hessian matrix</strong> at <span class="math inline">\(\theta_0\)</span>:</p>
<p><span class="math display">\[
H_{ij} = \frac{\partial^2 J}{\partial \theta_i \partial \theta_j}
\]</span></p>
<p>This approximation is exact for quadratic functions and captures local curvature for general functions.</p>
</section>
<section id="newtons-update-rule" class="level3">
<h3 class="anchored" data-anchor-id="newtons-update-rule">Newton’s Update Rule</h3>
<p>To minimize the quadratic approximation, we find where its gradient equals zero:</p>
<p><span class="math display">\[
\theta^* = \theta_0 - H^{-1}\nabla_{\theta_0}J(\theta_0) \tag{8.27}
\]</span></p>
<p>This is the <strong>Newton step</strong>: instead of moving in the direction of the gradient, we move in the direction <span class="math inline">\(H^{-1}g\)</span>, which accounts for the curvature of the loss surface.</p>
</section>
<section id="deriving-the-newton-step" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-newton-step">Deriving the Newton Step</h3>
<p>Let’s derive equation 8.27 from equation 8.26.</p>
<p><strong>Step 1</strong>: Define simplified notation:</p>
<ul>
<li><span class="math inline">\(\Delta = \theta - \theta_0\)</span> (the step we want to find)</li>
<li><span class="math inline">\(g = \nabla_{\theta}J(\theta_0)\)</span> (gradient at current point)</li>
<li><span class="math inline">\(H = H(\theta_0)\)</span> (Hessian at current point)</li>
</ul>
<p><strong>Step 2</strong>: Rewrite the approximation:</p>
<p><span class="math display">\[
\tilde{J}(\theta) \approx J(\theta_0) + \Delta^{\top}g + \frac{1}{2}\Delta^{\top}H\Delta
\]</span></p>
<p><strong>Step 3</strong>: Find the minimum by taking the derivative with respect to <span class="math inline">\(\Delta\)</span>:</p>
<p><span class="math display">\[
\frac{\partial \tilde{J}}{\partial \Delta} = g + H\Delta
\]</span></p>
<p><strong>Explanation of the derivative</strong>:</p>
<ul>
<li>Derivative of <span class="math inline">\(\Delta^{\top}g\)</span> is <span class="math inline">\(g\)</span> (linear term)</li>
<li>Derivative of <span class="math inline">\(\frac{1}{2}\Delta^{\top}H\Delta\)</span> is <span class="math inline">\(H\Delta\)</span> (quadratic term, using symmetry of <span class="math inline">\(H\)</span>)</li>
</ul>
<p><strong>Step 4</strong>: Set the derivative to zero:</p>
<p><span class="math display">\[
g + H\Delta = 0
\]</span></p>
<p><span class="math display">\[
H\Delta = -g
\]</span></p>
<p><span class="math display">\[
\Delta = -H^{-1}g
\]</span></p>
<p><strong>Step 5</strong>: Substitute back <span class="math inline">\(\theta^* = \theta_0 + \Delta\)</span>:</p>
<p><span class="math display">\[
\theta^* = \theta_0 - H^{-1}\nabla_{\theta_0}J(\theta_0)
\]</span></p>
</section>
<section id="key-property-one-step-convergence-for-quadratics" class="level3">
<h3 class="anchored" data-anchor-id="key-property-one-step-convergence-for-quadratics">Key Property: One-step Convergence for Quadratics</h3>
<p><strong>If the Hessian is positive definite and the objective is quadratic, Newton’s method jumps directly to the minimum in one step.</strong></p>
<p>For non-quadratic objectives or non-positive-definite Hessians, we must iterate.</p>
</section>
<section id="newtons-method-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="newtons-method-algorithm">Newton’s Method Algorithm</h3>
<p><strong>Hyperparameters</strong>:</p>
<ul>
<li>Initial parameters <span class="math inline">\(\theta_0\)</span></li>
<li>Convergence threshold</li>
</ul>
<p><strong>Training procedure</strong>:</p>
<p>While stopping criterion not met:</p>
<ol type="1">
<li><strong>Compute gradient</strong>:</li>
</ol>
<p><span class="math display">\[
g = \frac{1}{m}\sum_{i=1}^m \nabla_\theta L(f(x^{(i)};\theta), y^{(i)})
\]</span></p>
<ol start="2" type="1">
<li><strong>Compute Hessian</strong>:</li>
</ol>
<p><span class="math display">\[
H = \frac{1}{m}\sum_{i=1}^m \nabla_\theta^2 L(f(x^{(i)};\theta), y^{(i)})
\]</span></p>
<ol start="3" type="1">
<li><p><strong>Compute Hessian inverse</strong>: <span class="math inline">\(H^{-1}\)</span></p></li>
<li><p><strong>Compute update step</strong>:</p></li>
</ol>
<p><span class="math display">\[
\Delta\theta = -H^{-1}g
\]</span></p>
<ol start="5" type="1">
<li><strong>Update parameters</strong>:</li>
</ol>
<p><span class="math display">\[
\theta \leftarrow \theta + \Delta\theta
\]</span></p>
</section>
<section id="regularization-handling-non-positive-definite-hessians" class="level3">
<h3 class="anchored" data-anchor-id="regularization-handling-non-positive-definite-hessians">Regularization: Handling Non-positive-definite Hessians</h3>
<p>When the Hessian is not positive definite (which happens at saddle points or near maxima), the Newton update may move in an ascent direction.</p>
<p>To stabilize the update, we use a <strong>damped</strong> or <strong>regularized</strong> Newton step:</p>
<p><span class="math display">\[
\theta^* = \theta_0 - [H(f(\theta_0)) + \alpha I]^{-1}\nabla_{\theta_0}f(\theta_0) \tag{8.28}
\]</span></p>
<p><strong>How regularization works</strong>:</p>
<ul>
<li>The term <span class="math inline">\(\alpha I\)</span> shifts all eigenvalues of <span class="math inline">\(H\)</span> by <span class="math inline">\(+\alpha\)</span></li>
<li>Negative eigenvalues (indicating concave directions) become closer to zero or positive</li>
<li>This ensures the regularized matrix is positive definite for sufficiently large <span class="math inline">\(\alpha\)</span></li>
</ul>
<p><strong>Behavior as <span class="math inline">\(\alpha\)</span> varies</strong>:</p>
<ul>
<li><strong>Small <span class="math inline">\(\alpha\)</span></strong>: Nearly pure Newton step, fast convergence near minima</li>
<li><strong>Large <span class="math inline">\(\alpha\)</span></strong>: Update approaches <span class="math inline">\(-\frac{1}{\alpha}\nabla_\theta f(\theta_0)\)</span>, which is essentially a scaled gradient descent step</li>
<li>This creates a smooth interpolation between Newton’s method and gradient descent</li>
</ul>
</section>
<section id="computational-challenge" class="level3">
<h3 class="anchored" data-anchor-id="computational-challenge">Computational Challenge</h3>
<p>Newton’s method faces severe scalability issues:</p>
<p><strong>For a model with <span class="math inline">\(k\)</span> parameters</strong>:</p>
<ul>
<li>Hessian is a <span class="math inline">\(k \times k\)</span> matrix with <span class="math inline">\(k^2\)</span> elements</li>
<li>Computing the Hessian requires <span class="math inline">\(O(k^2)\)</span> operations</li>
<li>Inverting the Hessian requires <span class="math inline">\(O(k^3)\)</span> operations</li>
</ul>
<p><strong>In modern deep learning</strong>:</p>
<ul>
<li>Networks easily have millions of parameters (<span class="math inline">\(k \approx 10^6\)</span> or more)</li>
<li>Storing the Hessian would require terabytes of memory</li>
<li>Computing <span class="math inline">\(H^{-1}\)</span> would be prohibitively expensive</li>
</ul>
<p><strong>Conclusion</strong>: Newton’s method is practical only for very small models with at most a few thousand parameters.</p>
</section>
</section>
<section id="conjugate-gradient" class="level2">
<h2 class="anchored" data-anchor-id="conjugate-gradient">Conjugate Gradient</h2>
<p>Conjugate Gradient (CG) is a sophisticated algorithm that achieves near-Newton performance without computing or storing the full Hessian.</p>
<section id="the-zig-zag-problem-in-steepest-descent" class="level3">
<h3 class="anchored" data-anchor-id="the-zig-zag-problem-in-steepest-descent">The Zig-zag Problem in Steepest Descent</h3>
<p>Standard gradient descent on ill-conditioned problems exhibits <strong>zig-zag behavior</strong>: each step is orthogonal to the previous step, causing the algorithm to repeatedly undo its own progress.</p>
<p><strong>Why this happens</strong>:</p>
<ul>
<li>At each step, gradient descent moves along the steepest direction</li>
<li>After one step, the new gradient is orthogonal to the previous direction (for quadratic functions with exact line search)</li>
<li>This orthogonality causes oscillation in narrow valleys</li>
</ul>
<p><strong>Reference</strong>: <a href="https://www.youtube.com/watch?v=ZlcQ8xHwx0Y">Visualization of ill-conditioned optimization and zig-zag</a></p>
</section>
<section id="a-orthogonality-the-key-concept" class="level3">
<h3 class="anchored" data-anchor-id="a-orthogonality-the-key-concept">A-orthogonality: The Key Concept</h3>
<p>Conjugate Gradient eliminates zig-zag by using <strong>conjugate directions</strong> instead of orthogonal directions.</p>
<p><strong>Definition</strong>: Two directions <span class="math inline">\(d_i\)</span> and <span class="math inline">\(d_j\)</span> are <strong>conjugate</strong> with respect to matrix <span class="math inline">\(A\)</span> if:</p>
<p><span class="math display">\[
d_i^{\top}Ad_j = 0
\]</span></p>
<p>This is also called <strong>A-orthogonality</strong> because the directions are orthogonal under the metric defined by <span class="math inline">\(A\)</span> (the Hessian).</p>
<p><strong>Why this matters</strong>: A-orthogonal directions have a crucial property: <strong>a line search along a new direction does not undo progress made along any previous direction.</strong></p>
<p>In the context of minimizing a quadratic function <span class="math inline">\(J(\theta) = \frac{1}{2}\theta^{\top}A\theta + b^{\top}\theta + c\)</span>:</p>
<ul>
<li>Orthogonal directions: <span class="math inline">\(d_i^{\top}d_j = 0\)</span> (geometric orthogonality)</li>
<li>Conjugate directions: <span class="math inline">\(d_i^{\top}Ad_j = 0\)</span> (orthogonality in curvature space)</li>
</ul>
<p>Conjugate directions preserve all previous progress, eliminating zig-zag.</p>
</section>
<section id="conjugate-gradient-update-rule" class="level3">
<h3 class="anchored" data-anchor-id="conjugate-gradient-update-rule">Conjugate Gradient Update Rule</h3>
<p>At each iteration, we construct a search direction that is conjugate to all previous directions:</p>
<p><span class="math display">\[
d_t = -\nabla_{\theta}J(\theta) + \beta_t d_{t-1} \tag{8.29}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(-\nabla_{\theta}J(\theta)\)</span> is the current gradient</li>
<li><span class="math inline">\(\beta_t\)</span> controls how much we add from the previous direction</li>
<li>The combination ensures <span class="math inline">\(d_t\)</span> is conjugate to <span class="math inline">\(d_{t-1}\)</span></li>
</ul>
</section>
<section id="computing-beta_t-fletcher-reeves" class="level3">
<h3 class="anchored" data-anchor-id="computing-beta_t-fletcher-reeves">Computing <span class="math inline">\(\beta_t\)</span>: Fletcher-Reeves</h3>
<p>The <strong>Fletcher-Reeves</strong> formula computes <span class="math inline">\(\beta_t\)</span> based on gradient magnitudes:</p>
<p><span class="math display">\[
\beta_t = \frac{\nabla_{\theta}J(\theta_t)^{\top}\nabla_{\theta}J(\theta_t)}{\nabla_{\theta}J(\theta_{t-1})^{\top}\nabla_{\theta}J(\theta_{t-1})} \tag{8.30}
\]</span></p>
<p><strong>Derivation</strong> (for quadratic objectives with exact line search):</p>
<p><strong>Step 1</strong>: Impose the conjugacy condition:</p>
<p><span class="math display">\[
d_t^{\top}Ad_{t-1} = 0 \tag{1}
\]</span></p>
<p><strong>Step 2</strong>: Substitute the update rule <span class="math inline">\(d_t = -g_t + \beta_t d_{t-1}\)</span>:</p>
<p><span class="math display">\[
(-g_t + \beta_t d_{t-1})^{\top}Ad_{t-1} = 0
\]</span></p>
<p><span class="math display">\[
-g_t^{\top}Ad_{t-1} + \beta_t d_{t-1}^{\top}Ad_{t-1} = 0
\]</span></p>
<p><strong>Step 3</strong>: Solve for <span class="math inline">\(\beta_t\)</span>:</p>
<p><span class="math display">\[
\beta_t = \frac{g_t^{\top}\cancel{Ad_{t-1}}}{d_{t-1}^{\top}\cancel{Ad_{t-1}}} \tag{3}
\]</span></p>
<p><strong>Step 4</strong>: Replace <span class="math inline">\(Ad_{t-1}\)</span> using gradient update properties:</p>
<p>Under a quadratic objective with exact line search, the gradient evolves as:</p>
<p><span class="math display">\[
g_t = g_{t-1} + \alpha_{t-1}Ad_{t-1}
\]</span></p>
<p>where <span class="math inline">\(\alpha_{t-1}\)</span> is the step size from the previous iteration.</p>
<p>This gives us:</p>
<p><span class="math display">\[
Ad_{t-1} = \frac{1}{\alpha_{t-1}}(g_t - g_{t-1})
\]</span></p>
<p>Additionally, exact line search ensures:</p>
<p><span class="math display">\[
g_t^{\top}d_{t-1} = 0
\]</span></p>
<p>(the new gradient is orthogonal to the previous search direction).</p>
<p><strong>Step 5</strong>: Substitute <span class="math inline">\(Ad_{t-1}\)</span> in equation (3):</p>
<p><span class="math display">\[
\beta_t = \frac{g_t^{\top}\left(\frac{1}{\alpha_{t-1}}(g_t - g_{t-1})\right)}{d_{t-1}^{\top}\left(\frac{1}{\alpha_{t-1}}(g_t - g_{t-1})\right)}
\]</span></p>
<p>The factor <span class="math inline">\(\frac{1}{\alpha_{t-1}}\)</span> cancels:</p>
<p><span class="math display">\[
\beta_t = \frac{g_t^{\top}(g_t - g_{t-1})}{d_{t-1}^{\top}(g_t - g_{t-1})}
\]</span></p>
<p><strong>Step 6</strong>: Apply the orthogonality condition <span class="math inline">\(g_t^{\top}d_{t-1} = 0\)</span>:</p>
<p>In the numerator:</p>
<p><span class="math display">\[
g_t^{\top}(g_t - g_{t-1}) = g_t^{\top}g_t - g_t^{\top}g_{t-1}
\]</span></p>
<p>In the denominator, using <span class="math inline">\(g_t^{\top}d_{t-1} = 0\)</span>:</p>
<p><span class="math display">\[
d_{t-1}^{\top}(g_t - g_{t-1}) = d_{t-1}^{\top}g_t - d_{t-1}^{\top}g_{t-1} = 0 - d_{t-1}^{\top}g_{t-1} = -d_{t-1}^{\top}g_{t-1}
\]</span></p>
<p>Since <span class="math inline">\(d_{t-1} = -g_{t-1} + \beta_{t-1}d_{t-2}\)</span> and by repeated application of orthogonality, we have <span class="math inline">\(d_{t-1}^{\top}g_{t-1} \propto -g_{t-1}^{\top}g_{t-1}\)</span>.</p>
<p>This yields:</p>
<p><span class="math display">\[
\beta_t = \frac{g_t^{\top}g_t}{g_{t-1}^{\top}g_{t-1}}
\]</span></p>
<p>This is the Fletcher-Reeves formula. The derivation shows how the curvature term <span class="math inline">\(Ad_{t-1}\)</span> is replaced by the change in gradients <span class="math inline">\((g_t - g_{t-1})\)</span>, which then simplifies using the orthogonality property <span class="math inline">\(g_t^{\top}d_{t-1} = 0\)</span> from exact line search.</p>
</section>
<section id="computing-beta_t-polak-ribière" class="level3">
<h3 class="anchored" data-anchor-id="computing-beta_t-polak-ribière">Computing <span class="math inline">\(\beta_t\)</span>: Polak-Ribière</h3>
<p>The <strong>Polak-Ribière</strong> formula uses the change in gradients to incorporate curvature information:</p>
<p><span class="math display">\[
\beta_t = \frac{(\nabla J(\theta_t) - \nabla J(\theta_{t-1}))^{\top}\nabla J(\theta_t)}{\nabla J(\theta_{t-1})^{\top}\nabla J(\theta_{t-1})} \tag{8.31}
\]</span></p>
<p><strong>Key difference from Fletcher-Reeves</strong>:</p>
<ul>
<li><strong>Fletcher-Reeves</strong>: Depends only on gradient norms</li>
<li><strong>Polak-Ribière</strong>: Uses <span class="math inline">\((g_t - g_{t-1})\)</span>, which approximates <span class="math inline">\(Ad_{t-1}\)</span> and captures curvature</li>
</ul>
<p><strong>Advantage</strong>: Polak-Ribière is more adaptive and often more effective on nonlinear problems, as it adjusts based on how gradients change rather than just their magnitude.</p>
</section>
<section id="line-search-choosing-the-step-size-epsilon" class="level3">
<h3 class="anchored" data-anchor-id="line-search-choosing-the-step-size-epsilon">Line Search: Choosing the Step Size <span class="math inline">\(\epsilon^*\)</span></h3>
<p>Unlike gradient descent with a fixed learning rate, Conjugate Gradient uses <strong>line search</strong> to find the optimal step size at each iteration.</p>
<p>Given a search direction <span class="math inline">\(\rho_t\)</span>, we solve:</p>
<p><span class="math display">\[
\rho_t = -g_t + \beta_t\rho_{t-1}
\]</span></p>
<p><span class="math display">\[
\epsilon^* = \arg\min_{\epsilon} \frac{1}{m}\sum_{i=1}^m L(f(x^{(i)};\theta_t + \epsilon\rho_t), y^{(i)})
\]</span></p>
<p><strong>How line search works</strong>:</p>
<ul>
<li>Start with an initial bracket or step size</li>
<li>Iteratively shrink or adjust the interval</li>
<li>Stop when sufficient decrease conditions are met (e.g., Armijo or Wolfe conditions)</li>
</ul>
<p>This dynamic search ensures each step is optimal along <span class="math inline">\(\rho_t\)</span> and preserves the conjugacy properties needed to avoid zig-zag behavior.</p>
</section>
<section id="conjugate-gradient-in-practice" class="level3">
<h3 class="anchored" data-anchor-id="conjugate-gradient-in-practice">Conjugate Gradient in Practice</h3>
<p><strong>Ideal case</strong>: For quadratic objectives with exact line search, CG converges in at most <span class="math inline">\(n\)</span> iterations (where <span class="math inline">\(n\)</span> is the number of parameters).</p>
<p><strong>Practical case</strong>: For general nonlinear functions:</p>
<ul>
<li>Conjugacy degrades over iterations</li>
<li>Gradient orthogonality <span class="math inline">\(g_t^{\top}Ad_{t-1} = 0\)</span> no longer holds exactly</li>
<li>Line search is approximate (using mini-batches introduces noise)</li>
</ul>
<p><strong>Result</strong>: CG is effective for medium-sized problems but struggles with stochastic gradients and high dimensionality in deep learning.</p>
</section>
</section>
<section id="bfgs" class="level2">
<h2 class="anchored" data-anchor-id="bfgs">BFGS</h2>
<p><strong>BFGS</strong> (Broyden-Fletcher-Goldfarb-Shanno) is a quasi-Newton method that approximates the inverse Hessian without computing it directly.</p>
<section id="core-idea" class="level3">
<h3 class="anchored" data-anchor-id="core-idea">Core Idea</h3>
<p>Instead of computing <span class="math inline">\(H^{-1}\)</span> explicitly, BFGS maintains an approximation <span class="math inline">\(M_t \approx H^{-1}\)</span> and updates it iteratively using gradient information.</p>
<p><strong>Update rule</strong>:</p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t + \epsilon^*\rho_t \tag{8.33}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\rho_t = -M_tg_t\)</span> is the search direction (quasi-Newton step)</li>
<li><span class="math inline">\(\epsilon^*\)</span> is found via line search</li>
<li><span class="math inline">\(M_t\)</span> is updated using curvature information from successive gradients</li>
</ul>
</section>
<section id="how-m_t-is-updated" class="level3">
<h3 class="anchored" data-anchor-id="how-m_t-is-updated">How <span class="math inline">\(M_t\)</span> is Updated</h3>
<p>BFGS uses the <strong>secant condition</strong> to update <span class="math inline">\(M_t\)</span>:</p>
<p><span class="math display">\[
M_{t+1}(g_{t+1} - g_t) = \theta_{t+1} - \theta_t
\]</span></p>
<p>This ensures <span class="math inline">\(M_{t+1}\)</span> approximates <span class="math inline">\(H^{-1}\)</span> by matching the observed change in gradients to the change in parameters.</p>
<p>The update formula is:</p>
<p><span class="math display">\[
M_{t+1} = M_t + \text{correction terms based on } (g_{t+1} - g_t) \text{ and } (\theta_{t+1} - \theta_t)
\]</span></p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>No need to compute or store the full Hessian</li>
<li><span class="math inline">\(M_t\)</span> is always symmetric and positive definite (with proper initialization)</li>
<li>Converges superlinearly near the optimum</li>
</ul>
<p><strong>Storage cost</strong>: Still requires <span class="math inline">\(O(k^2)\)</span> memory to store <span class="math inline">\(M_t\)</span>, making it impractical for networks with millions of parameters.</p>
</section>
<section id="limited-memory-bfgs-l-bfgs" class="level3">
<h3 class="anchored" data-anchor-id="limited-memory-bfgs-l-bfgs">Limited-Memory BFGS (L-BFGS)</h3>
<p>To reduce memory requirements, <strong>L-BFGS</strong> stores only the last <span class="math inline">\(m\)</span> updates (typically <span class="math inline">\(m = 10\)</span>) instead of the full matrix <span class="math inline">\(M_t\)</span>.</p>
<p><strong>Memory cost</strong>: Reduced from <span class="math inline">\(O(k^2)\)</span> to <span class="math inline">\(O(mk)\)</span>, making it feasible for moderate-sized problems.</p>
<p><strong>Trade-off</strong>: L-BFGS converges slower than full BFGS but is much more memory-efficient.</p>
</section>
</section>
<section id="why-second-order-methods-are-rarely-used-in-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="why-second-order-methods-are-rarely-used-in-deep-learning">Why Second-order Methods Are Rarely Used in Deep Learning</h2>
<p>Despite their theoretical elegance, second-order methods face fundamental challenges in modern deep learning:</p>
<section id="computational-cost" class="level3">
<h3 class="anchored" data-anchor-id="computational-cost">1. Computational Cost</h3>
<ul>
<li><strong>Hessian computation</strong>: <span class="math inline">\(O(k^2)\)</span> for storage, <span class="math inline">\(O(k^3)\)</span> for inversion</li>
<li>Modern networks have <span class="math inline">\(k \approx 10^6\)</span> to <span class="math inline">\(10^9\)</span> parameters</li>
<li>Even approximate methods like BFGS require <span class="math inline">\(O(k^2)\)</span> memory</li>
</ul>
</section>
<section id="stochastic-gradients" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-gradients">2. Stochastic Gradients</h3>
<ul>
<li>Second-order methods assume accurate gradients and curvature</li>
<li>Mini-batch training introduces noise, breaking line search and conjugacy</li>
<li>Hessian estimates from mini-batches are unreliable</li>
</ul>
</section>
<section id="non-convexity" class="level3">
<h3 class="anchored" data-anchor-id="non-convexity">3. Non-convexity</h3>
<ul>
<li>Neural networks are highly non-convex with many saddle points</li>
<li>Hessian may have negative eigenvalues, requiring expensive regularization</li>
<li>Conjugacy assumptions (quadratic approximation) break down far from optima</li>
</ul>
</section>
<section id="scalability" class="level3">
<h3 class="anchored" data-anchor-id="scalability">4. Scalability</h3>
<ul>
<li>First-order methods scale to billions of parameters and massive datasets</li>
<li>Second-order methods require full-batch or large-batch training</li>
<li>Distributed training is much simpler with gradient-based methods</li>
</ul>
</section>
<section id="practical-consequence" class="level3">
<h3 class="anchored" data-anchor-id="practical-consequence">Practical Consequence</h3>
<p><strong>Modern deep learning almost exclusively uses first-order methods</strong>: SGD, Momentum, RMSProp, and Adam. These methods:</p>
<ul>
<li>Require only gradient computation (<span class="math inline">\(O(k)\)</span> per iteration)</li>
<li>Work well with mini-batch stochastic gradients</li>
<li>Scale to massive models and datasets</li>
<li>Are robust to non-convexity and noise</li>
</ul>
<p>Second-order methods remain important in small-scale optimization, classical machine learning (e.g., logistic regression), and as theoretical tools for understanding optimization landscapes.</p>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>This section covered three classical second-order optimization methods:</p>
<ol type="1">
<li><p><strong>Newton’s Method</strong>: Uses <span class="math inline">\(H^{-1}g\)</span> for optimal quadratic convergence, but requires <span class="math inline">\(O(k^3)\)</span> computation</p></li>
<li><p><strong>Conjugate Gradient</strong>: Achieves near-Newton performance through conjugate directions and line search, avoiding Hessian computation</p></li>
<li><p><strong>BFGS</strong>: Approximates <span class="math inline">\(H^{-1}\)</span> iteratively using gradient information, with L-BFGS reducing memory requirements</p></li>
</ol>
<p>While theoretically powerful, these methods are rarely used in deep learning due to computational cost, incompatibility with stochastic gradients, and scalability challenges. First-order adaptive methods (Chapter 8.5) dominate in practice.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="light">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "ickma2311/ickma2311.github.io";
    script.dataset.repoId = "R_kgDOOzbaAg";
    script.dataset.category = "Comments";
    script.dataset.categoryId = "DIC_kwDOOzbaAs4CxPFj";
    script.dataset.mapping = "pathname";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Chapter 8.6: Second-Order Optimization Methods"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Chao Ma"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2025-11-15"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Deep Learning, Optimization, Newton's Method, Conjugate Gradient, BFGS, Hessian]</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Newton's method, Conjugate Gradient, and BFGS: elegant second-order methods that use curvature information but are rarely used in deep learning due to computational cost and scalability issues"</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>While first-order methods like gradient descent rely only on the gradient, second-order methods incorporate curvature information through the Hessian matrix. This section explores classical second-order optimization techniques: Newton's method, Conjugate Gradient, and BFGS.</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>**Important context**: Although these methods are theoretically elegant and powerful, they are rarely used in modern deep learning due to computational cost and scalability issues. Practical deep learning almost exclusively relies on first-order methods (SGD, Momentum, RMSProp, Adam).</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">## Newton's Method</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>Newton's method uses a second-order Taylor approximation to find the optimal step direction.</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="fu">### Second-order Taylor Approximation</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>Around a point $\theta_0$, we approximate the objective function using both first and second derivatives:</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>J(\theta) \approx J(\theta_0) + (\theta - \theta_0)^{\top}\nabla_{\theta}J(\theta_0) + \frac{1}{2}(\theta - \theta_0)^{\top}H(\theta_0)(\theta - \theta_0) \tag{8.26}</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>where $H(\theta_0)$ is the **Hessian matrix** at $\theta_0$:</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>H_{ij} = \frac{\partial^2 J}{\partial \theta_i \partial \theta_j}</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>This approximation is exact for quadratic functions and captures local curvature for general functions.</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="fu">### Newton's Update Rule</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>To minimize the quadratic approximation, we find where its gradient equals zero:</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>\theta^* = \theta_0 - H^{-1}\nabla_{\theta_0}J(\theta_0) \tag{8.27}</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>This is the **Newton step**: instead of moving in the direction of the gradient, we move in the direction $H^{-1}g$, which accounts for the curvature of the loss surface.</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deriving the Newton Step</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>Let's derive equation 8.27 from equation 8.26.</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>**Step 1**: Define simplified notation:</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Delta = \theta - \theta_0$ (the step we want to find)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$g = \nabla_{\theta}J(\theta_0)$ (gradient at current point)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$H = H(\theta_0)$ (Hessian at current point)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>**Step 2**: Rewrite the approximation:</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>\tilde{J}(\theta) \approx J(\theta_0) + \Delta^{\top}g + \frac{1}{2}\Delta^{\top}H\Delta</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>**Step 3**: Find the minimum by taking the derivative with respect to $\Delta$:</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>\frac{\partial \tilde{J}}{\partial \Delta} = g + H\Delta</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>**Explanation of the derivative**:</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Derivative of $\Delta^{\top}g$ is $g$ (linear term)</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Derivative of $\frac{1}{2}\Delta^{\top}H\Delta$ is $H\Delta$ (quadratic term, using symmetry of $H$)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>**Step 4**: Set the derivative to zero:</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>g + H\Delta = 0</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>H\Delta = -g</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>\Delta = -H^{-1}g</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>**Step 5**: Substitute back $\theta^* = \theta_0 + \Delta$:</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>\theta^* = \theta_0 - H^{-1}\nabla_{\theta_0}J(\theta_0)</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Property: One-step Convergence for Quadratics</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>**If the Hessian is positive definite and the objective is quadratic, Newton's method jumps directly to the minimum in one step.**</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>For non-quadratic objectives or non-positive-definite Hessians, we must iterate.</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a><span class="fu">### Newton's Method Algorithm</span></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>**Hyperparameters**:</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Initial parameters $\theta_0$</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Convergence threshold</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>**Training procedure**:</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>While stopping criterion not met:</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Compute gradient**:</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>g = \frac{1}{m}\sum_{i=1}^m \nabla_\theta L(f(x^{(i)};\theta), y^{(i)})</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Compute Hessian**:</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>H = \frac{1}{m}\sum_{i=1}^m \nabla_\theta^2 L(f(x^{(i)};\theta), y^{(i)})</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Compute Hessian inverse**: $H^{-1}$</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Compute update step**:</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>\Delta\theta = -H^{-1}g</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Update parameters**:</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>\theta \leftarrow \theta + \Delta\theta</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regularization: Handling Non-positive-definite Hessians</span></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>When the Hessian is not positive definite (which happens at saddle points or near maxima), the Newton update may move in an ascent direction.</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>To stabilize the update, we use a **damped** or **regularized** Newton step:</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>\theta^* = \theta_0 - <span class="co">[</span><span class="ot">H(f(\theta_0)) + \alpha I</span><span class="co">]</span>^{-1}\nabla_{\theta_0}f(\theta_0) \tag{8.28}</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>**How regularization works**:</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The term $\alpha I$ shifts all eigenvalues of $H$ by $+\alpha$</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Negative eigenvalues (indicating concave directions) become closer to zero or positive</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This ensures the regularized matrix is positive definite for sufficiently large $\alpha$</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>**Behavior as $\alpha$ varies**:</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Small $\alpha$**: Nearly pure Newton step, fast convergence near minima</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Large $\alpha$**: Update approaches $-\frac{1}{\alpha}\nabla_\theta f(\theta_0)$, which is essentially a scaled gradient descent step</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This creates a smooth interpolation between Newton's method and gradient descent</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computational Challenge</span></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>Newton's method faces severe scalability issues:</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>**For a model with $k$ parameters**:</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Hessian is a $k \times k$ matrix with $k^2$ elements</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computing the Hessian requires $O(k^2)$ operations</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Inverting the Hessian requires $O(k^3)$ operations</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>**In modern deep learning**:</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Networks easily have millions of parameters ($k \approx 10^6$ or more)</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Storing the Hessian would require terabytes of memory</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computing $H^{-1}$ would be prohibitively expensive</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>**Conclusion**: Newton's method is practical only for very small models with at most a few thousand parameters.</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conjugate Gradient</span></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>Conjugate Gradient (CG) is a sophisticated algorithm that achieves near-Newton performance without computing or storing the full Hessian.</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Zig-zag Problem in Steepest Descent</span></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>Standard gradient descent on ill-conditioned problems exhibits **zig-zag behavior**: each step is orthogonal to the previous step, causing the algorithm to repeatedly undo its own progress.</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>**Why this happens**:</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>At each step, gradient descent moves along the steepest direction</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>After one step, the new gradient is orthogonal to the previous direction (for quadratic functions with exact line search)</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This orthogonality causes oscillation in narrow valleys</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>**Reference**: <span class="co">[</span><span class="ot">Visualization of ill-conditioned optimization and zig-zag</span><span class="co">](https://www.youtube.com/watch?v=ZlcQ8xHwx0Y)</span></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a><span class="fu">### A-orthogonality: The Key Concept</span></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>Conjugate Gradient eliminates zig-zag by using **conjugate directions** instead of orthogonal directions.</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>**Definition**: Two directions $d_i$ and $d_j$ are **conjugate** with respect to matrix $A$ if:</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>d_i^{\top}Ad_j = 0</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>This is also called **A-orthogonality** because the directions are orthogonal under the metric defined by $A$ (the Hessian).</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>**Why this matters**: A-orthogonal directions have a crucial property: **a line search along a new direction does not undo progress made along any previous direction.**</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>In the context of minimizing a quadratic function $J(\theta) = \frac{1}{2}\theta^{\top}A\theta + b^{\top}\theta + c$:</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Orthogonal directions: $d_i^{\top}d_j = 0$ (geometric orthogonality)</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conjugate directions: $d_i^{\top}Ad_j = 0$ (orthogonality in curvature space)</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>Conjugate directions preserve all previous progress, eliminating zig-zag.</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conjugate Gradient Update Rule</span></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>At each iteration, we construct a search direction that is conjugate to all previous directions:</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>d_t = -\nabla_{\theta}J(\theta) + \beta_t d_{t-1} \tag{8.29}</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$-\nabla_{\theta}J(\theta)$ is the current gradient</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\beta_t$ controls how much we add from the previous direction</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The combination ensures $d_t$ is conjugate to $d_{t-1}$</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computing $\beta_t$: Fletcher-Reeves</span></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>The **Fletcher-Reeves** formula computes $\beta_t$ based on gradient magnitudes:</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>\beta_t = \frac{\nabla_{\theta}J(\theta_t)^{\top}\nabla_{\theta}J(\theta_t)}{\nabla_{\theta}J(\theta_{t-1})^{\top}\nabla_{\theta}J(\theta_{t-1})} \tag{8.30}</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>**Derivation** (for quadratic objectives with exact line search):</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>**Step 1**: Impose the conjugacy condition:</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>d_t^{\top}Ad_{t-1} = 0 \tag{1}</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>**Step 2**: Substitute the update rule $d_t = -g_t + \beta_t d_{t-1}$:</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>(-g_t + \beta_t d_{t-1})^{\top}Ad_{t-1} = 0</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>-g_t^{\top}Ad_{t-1} + \beta_t d_{t-1}^{\top}Ad_{t-1} = 0</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>**Step 3**: Solve for $\beta_t$:</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>\beta_t = \frac{g_t^{\top}\cancel{Ad_{t-1}}}{d_{t-1}^{\top}\cancel{Ad_{t-1}}} \tag{3}</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>**Step 4**: Replace $Ad_{t-1}$ using gradient update properties:</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>Under a quadratic objective with exact line search, the gradient evolves as:</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>g_t = g_{t-1} + \alpha_{t-1}Ad_{t-1}</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>where $\alpha_{t-1}$ is the step size from the previous iteration.</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>This gives us:</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>Ad_{t-1} = \frac{1}{\alpha_{t-1}}(g_t - g_{t-1})</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>Additionally, exact line search ensures:</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>g_t^{\top}d_{t-1} = 0</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>(the new gradient is orthogonal to the previous search direction).</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>**Step 5**: Substitute $Ad_{t-1}$ in equation (3):</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>\beta_t = \frac{g_t^{\top}\left(\frac{1}{\alpha_{t-1}}(g_t - g_{t-1})\right)}{d_{t-1}^{\top}\left(\frac{1}{\alpha_{t-1}}(g_t - g_{t-1})\right)}</span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>The factor $\frac{1}{\alpha_{t-1}}$ cancels:</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>\beta_t = \frac{g_t^{\top}(g_t - g_{t-1})}{d_{t-1}^{\top}(g_t - g_{t-1})}</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>**Step 6**: Apply the orthogonality condition $g_t^{\top}d_{t-1} = 0$:</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>In the numerator:</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>g_t^{\top}(g_t - g_{t-1}) = g_t^{\top}g_t - g_t^{\top}g_{t-1}</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>In the denominator, using $g_t^{\top}d_{t-1} = 0$:</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>d_{t-1}^{\top}(g_t - g_{t-1}) = d_{t-1}^{\top}g_t - d_{t-1}^{\top}g_{t-1} = 0 - d_{t-1}^{\top}g_{t-1} = -d_{t-1}^{\top}g_{t-1}</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>Since $d_{t-1} = -g_{t-1} + \beta_{t-1}d_{t-2}$ and by repeated application of orthogonality, we have $d_{t-1}^{\top}g_{t-1} \propto -g_{t-1}^{\top}g_{t-1}$.</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>This yields:</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>\beta_t = \frac{g_t^{\top}g_t}{g_{t-1}^{\top}g_{t-1}}</span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>This is the Fletcher-Reeves formula. The derivation shows how the curvature term $Ad_{t-1}$ is replaced by the change in gradients $(g_t - g_{t-1})$, which then simplifies using the orthogonality property $g_t^{\top}d_{t-1} = 0$ from exact line search.</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computing $\beta_t$: Polak-Ribière</span></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a>The **Polak-Ribière** formula uses the change in gradients to incorporate curvature information:</span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a>\beta_t = \frac{(\nabla J(\theta_t) - \nabla J(\theta_{t-1}))^{\top}\nabla J(\theta_t)}{\nabla J(\theta_{t-1})^{\top}\nabla J(\theta_{t-1})} \tag{8.31}</span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>**Key difference from Fletcher-Reeves**:</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fletcher-Reeves**: Depends only on gradient norms</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Polak-Ribière**: Uses $(g_t - g_{t-1})$, which approximates $Ad_{t-1}$ and captures curvature</span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a>**Advantage**: Polak-Ribière is more adaptive and often more effective on nonlinear problems, as it adjusts based on how gradients change rather than just their magnitude.</span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a><span class="fu">### Line Search: Choosing the Step Size $\epsilon^*$</span></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>Unlike gradient descent with a fixed learning rate, Conjugate Gradient uses **line search** to find the optimal step size at each iteration.</span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>Given a search direction $\rho_t$, we solve:</span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>\rho_t = -g_t + \beta_t\rho_{t-1}</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a>\epsilon^* = \arg\min_{\epsilon} \frac{1}{m}\sum_{i=1}^m L(f(x^{(i)};\theta_t + \epsilon\rho_t), y^{(i)})</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>**How line search works**:</span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Start with an initial bracket or step size</span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Iteratively shrink or adjust the interval</span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stop when sufficient decrease conditions are met (e.g., Armijo or Wolfe conditions)</span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a>This dynamic search ensures each step is optimal along $\rho_t$ and preserves the conjugacy properties needed to avoid zig-zag behavior.</span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conjugate Gradient in Practice</span></span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a>**Ideal case**: For quadratic objectives with exact line search, CG converges in at most $n$ iterations (where $n$ is the number of parameters).</span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>**Practical case**: For general nonlinear functions:</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conjugacy degrades over iterations</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Gradient orthogonality $g_t^{\top}Ad_{t-1} = 0$ no longer holds exactly</span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Line search is approximate (using mini-batches introduces noise)</span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a>**Result**: CG is effective for medium-sized problems but struggles with stochastic gradients and high dimensionality in deep learning.</span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a><span class="fu">## BFGS</span></span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a>**BFGS** (Broyden-Fletcher-Goldfarb-Shanno) is a quasi-Newton method that approximates the inverse Hessian without computing it directly.</span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a><span class="fu">### Core Idea</span></span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a>Instead of computing $H^{-1}$ explicitly, BFGS maintains an approximation $M_t \approx H^{-1}$ and updates it iteratively using gradient information.</span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a>**Update rule**:</span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a>\theta_{t+1} = \theta_t + \epsilon^*\rho_t \tag{8.33}</span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\rho_t = -M_tg_t$ is the search direction (quasi-Newton step)</span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\epsilon^*$ is found via line search</span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$M_t$ is updated using curvature information from successive gradients</span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a><span class="fu">### How $M_t$ is Updated</span></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a>BFGS uses the **secant condition** to update $M_t$:</span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a>M_{t+1}(g_{t+1} - g_t) = \theta_{t+1} - \theta_t</span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a>This ensures $M_{t+1}$ approximates $H^{-1}$ by matching the observed change in gradients to the change in parameters.</span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a>The update formula is:</span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a>M_{t+1} = M_t + \text{correction terms based on } (g_{t+1} - g_t) \text{ and } (\theta_{t+1} - \theta_t)</span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a>**Advantages**:</span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No need to compute or store the full Hessian</span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$M_t$ is always symmetric and positive definite (with proper initialization)</span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Converges superlinearly near the optimum</span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a>**Storage cost**: Still requires $O(k^2)$ memory to store $M_t$, making it impractical for networks with millions of parameters.</span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a><span class="fu">### Limited-Memory BFGS (L-BFGS)</span></span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a>To reduce memory requirements, **L-BFGS** stores only the last $m$ updates (typically $m = 10$) instead of the full matrix $M_t$.</span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a>**Memory cost**: Reduced from $O(k^2)$ to $O(mk)$, making it feasible for moderate-sized problems.</span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a>**Trade-off**: L-BFGS converges slower than full BFGS but is much more memory-efficient.</span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Second-order Methods Are Rarely Used in Deep Learning</span></span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a>Despite their theoretical elegance, second-order methods face fundamental challenges in modern deep learning:</span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1. Computational Cost</span></span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hessian computation**: $O(k^2)$ for storage, $O(k^3)$ for inversion</span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Modern networks have $k \approx 10^6$ to $10^9$ parameters</span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Even approximate methods like BFGS require $O(k^2)$ memory</span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2. Stochastic Gradients</span></span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Second-order methods assume accurate gradients and curvature</span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mini-batch training introduces noise, breaking line search and conjugacy</span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Hessian estimates from mini-batches are unreliable</span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3. Non-convexity</span></span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Neural networks are highly non-convex with many saddle points</span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Hessian may have negative eigenvalues, requiring expensive regularization</span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conjugacy assumptions (quadratic approximation) break down far from optima</span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4. Scalability</span></span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>First-order methods scale to billions of parameters and massive datasets</span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Second-order methods require full-batch or large-batch training</span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Distributed training is much simpler with gradient-based methods</span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a><span class="fu">### Practical Consequence</span></span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a>**Modern deep learning almost exclusively uses first-order methods**: SGD, Momentum, RMSProp, and Adam. These methods:</span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Require only gradient computation ($O(k)$ per iteration)</span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Work well with mini-batch stochastic gradients</span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scale to massive models and datasets</span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Are robust to non-convexity and noise</span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a>Second-order methods remain important in small-scale optimization, classical machine learning (e.g., logistic regression), and as theoretical tools for understanding optimization landscapes.</span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a>This section covered three classical second-order optimization methods:</span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Newton's Method**: Uses $H^{-1}g$ for optimal quadratic convergence, but requires $O(k^3)$ computation</span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Conjugate Gradient**: Achieves near-Newton performance through conjugate directions and line search, avoiding Hessian computation</span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**BFGS**: Approximates $H^{-1}$ iteratively using gradient information, with L-BFGS reducing memory requirements</span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a>While theoretically powerful, these methods are rarely used in deep learning due to computational cost, incompatibility with stochastic gradients, and scalability challenges. First-order adaptive methods (Chapter 8.5) dominate in practice.</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>