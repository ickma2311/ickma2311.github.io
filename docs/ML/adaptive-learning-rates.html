<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chao Ma">
<meta name="dcterms.date" content="2025-11-12">
<meta name="description" content="From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter">

<title>Goodfellow Deep Learning — Chapter 8.5: Algorithms with Adaptive Learning Rates – ∇ ickma.dev</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.svg" rel="icon" type="image/svg+xml">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PK5N5KWZBF"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-PK5N5KWZBF', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">∇ ickma.dev</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-deep-learning" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Deep Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-deep-learning">    
        <li class="dropdown-header">Papers in Deep Learning</li>
        <li>
    <a class="dropdown-item" href="../ML/papers/index.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/papers/lora.html">
 <span class="dropdown-text">LoRA: Low-Rank Adaptation</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../ML/deep-learning-book.html">
 <span class="dropdown-text">Goodfellow Deep Learning Book (Overview)</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../ML/xor-deep-learning.html">
 <span class="dropdown-text">Chapter 6.1: XOR Problem</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/likelihood-loss-functions.html">
 <span class="dropdown-text">Chapter 6.2: Loss Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/activation-functions.html">
 <span class="dropdown-text">Chapter 6.3: Activation Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/architecture-design.html">
 <span class="dropdown-text">Chapter 6.4: Architecture Design</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/backpropagation.html">
 <span class="dropdown-text">Chapter 6.5: Back-Propagation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/hessian-prerequisites.html">
 <span class="dropdown-text">Chapter 7 Prerequisites: Hessian Matrix</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/l2-regularization.html">
 <span class="dropdown-text">Chapter 7.1.1: L2 Regularization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/l1-regularization.html">
 <span class="dropdown-text">Chapter 7.1.2: L1 Regularization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/constrained-optimization-regularization.html">
 <span class="dropdown-text">Chapter 7.2: Constrained Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/regularization-underconstrained.html">
 <span class="dropdown-text">Chapter 7.3: Under-Constrained Problems</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/dataset-augmentation.html">
 <span class="dropdown-text">Chapter 7.4: Dataset Augmentation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/noise-robustness.html">
 <span class="dropdown-text">Chapter 7.5: Noise Robustness</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/semi-supervised-learning.html">
 <span class="dropdown-text">Chapter 7.6: Semi-Supervised Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/multi-task-learning.html">
 <span class="dropdown-text">Chapter 7.7: Multi-Task Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/early-stopping.html">
 <span class="dropdown-text">Chapter 7.8: Early Stopping</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/parameter-tying-sharing.html">
 <span class="dropdown-text">Chapter 7.9: Parameter Tying &amp; Sharing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/representation-sparsity.html">
 <span class="dropdown-text">Chapter 7.10: Sparse Representations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/bagging-ensemble.html">
 <span class="dropdown-text">Chapter 7.11: Bagging &amp; Ensemble Methods</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/dropout.html">
 <span class="dropdown-text">Chapter 7.12: Dropout</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/adversarial-training.html">
 <span class="dropdown-text">Chapter 7.13: Adversarial Training</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/tangent-prop-manifold.html">
 <span class="dropdown-text">Chapter 7.14: Tangent Prop &amp; Manifolds</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/learning-vs-optimization.html">
 <span class="dropdown-text">Chapter 8.1: Learning vs Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/optimization-challenges.html">
 <span class="dropdown-text">Chapter 8.2: Optimization Challenges</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/basic-optimization-algorithms.html">
 <span class="dropdown-text">Chapter 8.3: Basic Algorithms</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/parameter-initialization.html">
 <span class="dropdown-text">Chapter 8.4: Parameter Initialization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/adaptive-learning-rates.html">
 <span class="dropdown-text">Chapter 8.5: Adaptive Learning Rates</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/second-order-methods.html">
 <span class="dropdown-text">Chapter 8.6: Second-Order Methods</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/optimization-strategies.html">
 <span class="dropdown-text">Chapter 8.7: Optimization Strategies</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/convolution-computation.html">
 <span class="dropdown-text">Chapter 9.1: Convolution Computation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-motivation.html">
 <span class="dropdown-text">Chapter 9.2: CNN Motivation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-pooling.html">
 <span class="dropdown-text">Chapter 9.3: Pooling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-infinitely-strong-prior.html">
 <span class="dropdown-text">Chapter 9.4: Infinitely Strong Prior</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/convolutional-functions.html">
 <span class="dropdown-text">Chapter 9.5: Convolutional Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-structured-outputs.html">
 <span class="dropdown-text">Chapter 9.6: Structured Outputs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-data-types.html">
 <span class="dropdown-text">Chapter 9.7: Data Types</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-efficient-convolution.html">
 <span class="dropdown-text">Chapter 9.8: Efficient Convolution</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-unsupervised-learning.html">
 <span class="dropdown-text">Chapter 9.9: Unsupervised Feature Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-neuroscience.html">
 <span class="dropdown-text">Chapter 9.10: Neuroscientific Basis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-unfold-computation-graph.html">
 <span class="dropdown-text">Chapter 10.1: Unfold Computation Graph</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-recurrent-neural-networks.html">
 <span class="dropdown-text">Chapter 10.2: Recurrent Neural Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-bidirectional.html">
 <span class="dropdown-text">Chapter 10.3: Bidirectional RNN</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-encoder-decoder.html">
 <span class="dropdown-text">Chapter 10.4: Encoder-Decoder Seq2Seq</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-deep.html">
 <span class="dropdown-text">Chapter 10.5: Deep RNN</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-recursive.html">
 <span class="dropdown-text">Chapter 10.6: Recursive Neural Network</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-long-term-dependency.html">
 <span class="dropdown-text">Chapter 10.7: Long-Term Dependencies</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-echo-state.html">
 <span class="dropdown-text">Chapter 10.8: Echo State Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-leaky-unit.html">
 <span class="dropdown-text">Chapter 10.9: Leaky Units</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-lstm-gru.html">
 <span class="dropdown-text">Chapter 10.10: LSTM and GRU</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-optimize-long-term.html">
 <span class="dropdown-text">Chapter 10.11: Optimizing Long-Term Dependencies</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-explicit-memory.html">
 <span class="dropdown-text">Chapter 10.12: Explicit Memory</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/practical-methodology.html">
 <span class="dropdown-text">Chapter 11: Practical Methodology</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/large-scale-deep-learning.html">
 <span class="dropdown-text">Chapter 12.1: Large-Scale Deep Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/image-preprocessing-normalization.html">
 <span class="dropdown-text">Chapter 12.2: Image Preprocessing and Normalization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/automatic-speech-recognition.html">
 <span class="dropdown-text">Chapter 12.3: Automatic Speech Recognition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/nlp-applications.html">
 <span class="dropdown-text">Chapter 12.4: NLP Applications</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/other-applications.html">
 <span class="dropdown-text">Chapter 12.5: Other Applications</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/linear-factor-models.html">
 <span class="dropdown-text">Chapter 13: Linear Factor Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/autoencoders.html">
 <span class="dropdown-text">Chapter 14: Autoencoders</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/representation-learning.html">
 <span class="dropdown-text">Chapter 15: Representation Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/structured-probabilistic-models.html">
 <span class="dropdown-text">Chapter 16: Structured Probabilistic Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/monte-carlo-methods.html">
 <span class="dropdown-text">Chapter 17: Monte Carlo Methods</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/confronting-partition-function.html">
 <span class="dropdown-text">Chapter 18: Confronting the Partition Function</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/approximate-inference.html">
 <span class="dropdown-text">Chapter 19: Approximate Inference</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/deep-generative-models.html">
 <span class="dropdown-text">Chapter 20: Deep Generative Models</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-linear-algebra" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Linear Algebra</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-linear-algebra">    
        <li class="dropdown-header">Reflections &amp; Synthesis</li>
        <li>
    <a class="dropdown-item" href="../Math/reflections/mit1806-invertibility-connections.html">
 <span class="dropdown-text">Invertibility Connections</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/reflections/taylor-euler-fourier.html">
 <span class="dropdown-text">Taylor, Euler, and Fourier</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">MIT 18.06SC Linear Algebra</li>
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture1-geometry.html">
 <span class="dropdown-text">Lecture 1: Geometry</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture2-elimination.html">
 <span class="dropdown-text">Lecture 2: Elimination</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture3-multiplication.html">
 <span class="dropdown-text">Lecture 3: Multiplication</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture4-lu-decomposition.html">
 <span class="dropdown-text">Lecture 4: LU Decomposition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-permutations.html">
 <span class="dropdown-text">Lecture 5.1: Permutations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-2-transpose.html">
 <span class="dropdown-text">Lecture 5.2: Transpose</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-3-spaces.html">
 <span class="dropdown-text">Lecture 5.3: Spaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture6-column-null-space.html">
 <span class="dropdown-text">Lecture 6: Column &amp; Null Space</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture7-solving-ax-0.html">
 <span class="dropdown-text">Lecture 7: Solving Ax=0</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture8-solving-ax-b.html">
 <span class="dropdown-text">Lecture 8: Solving Ax=b</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html">
 <span class="dropdown-text">Lecture 9: Independence, Basis, Dimension</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture10-four-subspaces.html">
 <span class="dropdown-text">Lecture 10: Four Fundamental Subspaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture11-matrix-spaces.html">
 <span class="dropdown-text">Lecture 11: Matrix Spaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture12-graphs-networks.html">
 <span class="dropdown-text">Lecture 12: Graphs and Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture13-quiz-review.html">
 <span class="dropdown-text">Lecture 13: Quiz 1 Review</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture14-orthogonality.html">
 <span class="dropdown-text">Lecture 14: Orthogonality</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture15-projections.html">
 <span class="dropdown-text">Lecture 15: Projections</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture16-least-squares.html">
 <span class="dropdown-text">Lecture 16: Least Squares</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture17-gram-schmidt.html">
 <span class="dropdown-text">Lecture 17: Gram-Schmidt</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture18-determinants.html">
 <span class="dropdown-text">Lecture 18: Determinants</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture19-determinant-formulas.html">
 <span class="dropdown-text">Lecture 19: Determinant Formulas</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture20-cramers-rule.html">
 <span class="dropdown-text">Lecture 20: Inverse &amp; Volume</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html">
 <span class="dropdown-text">Lecture 21: Eigenvalues &amp; Eigenvectors</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture22-diagonalization-powers.html">
 <span class="dropdown-text">Lecture 22: Diagonalization &amp; Powers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture23-differential-equations.html">
 <span class="dropdown-text">Lecture 23: Differential Equations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture24-markov-fourier.html">
 <span class="dropdown-text">Lecture 24: Markov &amp; Fourier</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html">
 <span class="dropdown-text">Lecture 25: Symmetric &amp; Positive Definite</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html">
 <span class="dropdown-text">Lecture 26: Complex Matrices &amp; FFT</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html">
 <span class="dropdown-text">Lecture 27: Positive Definite &amp; Minima</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html">
 <span class="dropdown-text">Lecture 28: Similar Matrices &amp; Jordan Form</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture29-svd.html">
 <span class="dropdown-text">Lecture 29: Singular Value Decomposition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture30-linear-transformations.html">
 <span class="dropdown-text">Lecture 30: Linear Transformations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture31-change-of-basis.html">
 <span class="dropdown-text">Lecture 31: Change of Basis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture33-left-right-inverse.html">
 <span class="dropdown-text">Lecture 33: Left &amp; Right Inverse</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">MIT 18.065: Linear Algebra Applications</li>
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture1-column-space.html">
 <span class="dropdown-text">Lecture 1: Column Space</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture2-multiplying-factoring.html">
 <span class="dropdown-text">Lecture 2: Multiplying and Factoring Matrices</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture3-orthonormal-columns.html">
 <span class="dropdown-text">Lecture 3: Orthonormal Columns</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture4-eigenvalues-eigenvectors.html">
 <span class="dropdown-text">Lecture 4: Eigenvalues and Eigenvectors</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture5-positive-definite.html">
 <span class="dropdown-text">Lecture 5: Positive Definite Matrices</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture6-svd.html">
 <span class="dropdown-text">Lecture 6: Singular Value Decomposition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture7-eckart-young.html">
 <span class="dropdown-text">Lecture 7: Eckart-Young Theorem</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture8-norms.html">
 <span class="dropdown-text">Lecture 8: Norms</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture9-least-squares.html">
 <span class="dropdown-text">Lecture 9: Least Squares</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture10-axb-difficulties.html">
 <span class="dropdown-text">Lecture 10: Survey of Ax=b Difficulties</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/lecture-11-minimize-norm-subject-to-constraint.html">
 <span class="dropdown-text">Lecture 11: Minimize Norm subject to Constraint</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture12-eigenvalue-algorithms.html">
 <span class="dropdown-text">Lecture 12: Computing Eigenvalues</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture13-randomized-matrix-multiplication.html">
 <span class="dropdown-text">Lecture 13: Randomized Matrix Multiplication</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture14-low-rank-changes-inverse.html">
 <span class="dropdown-text">Lecture 14: Low Rank Changes</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture15-matrix-derivatives.html">
 <span class="dropdown-text">Lecture 15: Matrix Derivatives</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture16-derivative-inverse-singular-values.html">
 <span class="dropdown-text">Lecture 16: Derivative of Inverse and Singular Values</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture17-rapidly-decreasing-singular-values.html">
 <span class="dropdown-text">Lecture 17: Rapidly Decreasing Singular Values</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture18-counting-parameters-svd-lu-qr-saddle-points.html">
 <span class="dropdown-text">Lecture 18: Counting Parameters in SVD, LU, QR, and Saddle Points</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-calculus" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Calculus</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-calculus">    
        <li>
    <a class="dropdown-item" href="../Math/Calculus/index.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/Calculus/highlights-of-calculus.html">
 <span class="dropdown-text">Gilbert Strang’s Calculus: Highlights</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-numerical-optimization" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Numerical Optimization</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-numerical-optimization">    
        <li>
    <a class="dropdown-item" href="../Math/EE364A/lectures.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li class="dropdown-header">Stanford EE 364A: Convex Optimization</li>
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture1-intro.html">
 <span class="dropdown-text">Lecture 1: Introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture2-math-foundations.html">
 <span class="dropdown-text">Lecture 2: Convex Sets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture3-convex-functions.html">
 <span class="dropdown-text">Lecture 3: Convex Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html">
 <span class="dropdown-text">Lecture 4.1: Operations Preserving Convexity</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture4-part2-conjugate-quasiconvex.html">
 <span class="dropdown-text">Lecture 4.2: Conjugate &amp; Quasiconvex Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html">
 <span class="dropdown-text">Lecture 5.1: Log-Concave &amp; Log-Convex</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture5-part2-monotonicity.html">
 <span class="dropdown-text">Lecture 5.2: Monotonicity</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture6-optimization-problems.html">
 <span class="dropdown-text">Chapter 4.1: Optimization Problems</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture7-convex-optimization.html">
 <span class="dropdown-text">Chapter 4.2: Convex Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture8-linear-optimization.html">
 <span class="dropdown-text">Chapter 4.3: Linear Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture9-quadratic-optimization.html">
 <span class="dropdown-text">Chapter 4.4: Quadratic Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-chapter4-5-geometric-programming.html">
 <span class="dropdown-text">Chapter 4.5: Geometric Programming</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.html">
 <span class="dropdown-text">Chapter 4.6: Generalized Inequality Constraints</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-chapter4-7-vector-optimization.html">
 <span class="dropdown-text">Chapter 4.7: Vector Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-chapter5-1-lagrange-dual-function.html">
 <span class="dropdown-text">Chapter 5.1: The Lagrange Dual Function</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-theory-to-repro" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Theory-to-Repro</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-theory-to-repro">    
        <li>
    <a class="dropdown-item" href="../Theory-to-Repro/index.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Theory-to-Repro/linear-regression-three-ways.html">
 <span class="dropdown-text">Linear Regression via Three Solvers</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../ML/k_means_clustering.html">
 <span class="dropdown-text">K-Means Clustering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/logistic_regression.html">
 <span class="dropdown-text">Logistic Regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/axis.html">
 <span class="dropdown-text">Axis Operations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Algorithm/dp_regex.html">
 <span class="dropdown-text">DP Regex</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/ickma2311/ickma2311.github.io/discussions"> 
<span class="menu-text">Feedback</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#adagrad" id="toc-adagrad" class="nav-link active" data-scroll-target="#adagrad">AdaGrad</a>
  <ul class="collapse">
  <li><a href="#algorithm" id="toc-algorithm" class="nav-link" data-scroll-target="#algorithm">Algorithm</a></li>
  <li><a href="#key-properties" id="toc-key-properties" class="nav-link" data-scroll-target="#key-properties">Key Properties</a></li>
  </ul></li>
  <li><a href="#rmsprop" id="toc-rmsprop" class="nav-link" data-scroll-target="#rmsprop">RMSProp</a>
  <ul class="collapse">
  <li><a href="#algorithm-1" id="toc-algorithm-1" class="nav-link" data-scroll-target="#algorithm-1">Algorithm</a></li>
  <li><a href="#key-properties-1" id="toc-key-properties-1" class="nav-link" data-scroll-target="#key-properties-1">Key Properties</a></li>
  </ul></li>
  <li><a href="#rmsprop-with-nesterov-momentum" id="toc-rmsprop-with-nesterov-momentum" class="nav-link" data-scroll-target="#rmsprop-with-nesterov-momentum">RMSProp with Nesterov Momentum</a>
  <ul class="collapse">
  <li><a href="#algorithm-2" id="toc-algorithm-2" class="nav-link" data-scroll-target="#algorithm-2">Algorithm</a></li>
  </ul></li>
  <li><a href="#adam" id="toc-adam" class="nav-link" data-scroll-target="#adam">Adam</a>
  <ul class="collapse">
  <li><a href="#algorithm-3" id="toc-algorithm-3" class="nav-link" data-scroll-target="#algorithm-3">Algorithm</a></li>
  <li><a href="#understanding-bias-correction" id="toc-understanding-bias-correction" class="nav-link" data-scroll-target="#understanding-bias-correction">Understanding Bias Correction</a></li>
  <li><a href="#interpreting-the-update-rule" id="toc-interpreting-the-update-rule" class="nav-link" data-scroll-target="#interpreting-the-update-rule">Interpreting the Update Rule</a></li>
  <li><a href="#key-properties-2" id="toc-key-properties-2" class="nav-link" data-scroll-target="#key-properties-2">Key Properties</a></li>
  </ul></li>
  <li><a href="#summary-comparison-of-adaptive-methods" id="toc-summary-comparison-of-adaptive-methods" class="nav-link" data-scroll-target="#summary-comparison-of-adaptive-methods">Summary: Comparison of Adaptive Methods</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Goodfellow Deep Learning — Chapter 8.5: Algorithms with Adaptive Learning Rates</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">Optimization</div>
    <div class="quarto-category">AdaGrad</div>
    <div class="quarto-category">RMSProp</div>
    <div class="quarto-category">Adam</div>
  </div>
  </div>

<div>
  <div class="description">
    From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Chao Ma </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 12, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>A fundamental challenge in optimization is choosing the right learning rate. Too large, and training diverges; too small, and progress is painfully slow. Moreover, different parameters may benefit from different learning rates—some require large steps while others need fine-tuning.</p>
<p><strong>Adaptive learning rate algorithms</strong> address this challenge by automatically adjusting the learning rate for each parameter based on the history of gradients. This section covers three major algorithms: AdaGrad, RMSProp, and Adam.</p>
<section id="adagrad" class="level2">
<h2 class="anchored" data-anchor-id="adagrad">AdaGrad</h2>
<p><strong>Core idea</strong>: AdaGrad scales the learning rate for each parameter inversely proportional to the square root of the cumulative sum of its past squared gradients.</p>
<p><strong>Intuition</strong>: Parameters with large gradients have received large updates in the past and should now take smaller steps. Parameters with small gradients have moved little and can afford larger steps.</p>
<section id="algorithm" class="level3">
<h3 class="anchored" data-anchor-id="algorithm">Algorithm</h3>
<p><strong>Hyperparameters</strong>:</p>
<ul>
<li>Learning rate <span class="math inline">\(\epsilon\)</span></li>
<li>Small constant <span class="math inline">\(\delta\)</span> (typically <span class="math inline">\(10^{-7}\)</span> for numerical stability)</li>
<li>Initial gradient accumulator <span class="math inline">\(r = 0\)</span></li>
</ul>
<p><strong>Training procedure</strong>:</p>
<p>While stopping criterion not met:</p>
<ol type="1">
<li><p>Sample <span class="math inline">\(m\)</span> examples from the training set</p></li>
<li><p>Compute the gradient:</p></li>
</ol>
<p><span class="math display">\[
g \leftarrow \frac{1}{m}\nabla_{\theta}\sum_{i=1}^m L(f(x^{(i)};\theta), y^{(i)})
\]</span></p>
<ol start="3" type="1">
<li>Accumulate squared gradients:</li>
</ol>
<p><span class="math display">\[
r \leftarrow r + g \odot g
\]</span></p>
<p>where <span class="math inline">\(\odot\)</span> denotes element-wise multiplication.</p>
<ol start="4" type="1">
<li>Compute the parameter update:</li>
</ol>
<p><span class="math display">\[
\Delta\theta \leftarrow -\frac{\epsilon}{\delta + \sqrt{r}} \odot g
\]</span></p>
<ol start="5" type="1">
<li>Update parameters:</li>
</ol>
<p><span class="math display">\[
\theta \leftarrow \theta + \Delta\theta
\]</span></p>
</section>
<section id="key-properties" class="level3">
<h3 class="anchored" data-anchor-id="key-properties">Key Properties</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Automatically adapts learning rates for each parameter</li>
<li>No manual learning rate tuning required for each parameter</li>
<li>Works well for sparse features (e.g., in NLP tasks)</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>The accumulator <span class="math inline">\(r\)</span> grows monotonically, causing learning rates to shrink continuously</li>
<li>Eventually, learning rates become infinitesimally small, and learning stops</li>
<li>This makes AdaGrad unsuitable for training deep neural networks</li>
</ul>
</section>
</section>
<section id="rmsprop" class="level2">
<h2 class="anchored" data-anchor-id="rmsprop">RMSProp</h2>
<p><strong>Core idea</strong>: RMSProp (Root Mean Square Propagation) uses an exponential decay to discount very old gradients, allowing the algorithm to forget distant history and achieve faster convergence once it reaches a convex bowl.</p>
<p><strong>Intuition</strong>: Unlike AdaGrad, which accumulates all past gradients forever, RMSProp uses an exponentially weighted moving average. This allows the learning rate to increase again if recent gradients are small, even if very old gradients were large.</p>
<section id="algorithm-1" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-1">Algorithm</h3>
<p><strong>Hyperparameters</strong>:</p>
<ul>
<li>Learning rate <span class="math inline">\(\epsilon\)</span> (typically 0.001)</li>
<li>Decay rate <span class="math inline">\(\rho\)</span> (typically 0.9)</li>
<li>Small constant <span class="math inline">\(\delta\)</span> (typically <span class="math inline">\(10^{-6}\)</span>)</li>
<li>Initial gradient accumulator <span class="math inline">\(r = 0\)</span></li>
</ul>
<p><strong>Training procedure</strong>:</p>
<p>While stopping criterion not met:</p>
<ol type="1">
<li><p>Sample <span class="math inline">\(m\)</span> examples from the training set</p></li>
<li><p>Compute the gradient:</p></li>
</ol>
<p><span class="math display">\[
g \leftarrow \frac{1}{m}\nabla_{\theta}\sum_{i=1}^m L(f(x^{(i)};\theta), y^{(i)})
\]</span></p>
<ol start="3" type="1">
<li>Accumulate squared gradients with exponential decay:</li>
</ol>
<p><span class="math display">\[
r \leftarrow \rho r + (1 - \rho) g \odot g
\]</span></p>
<p>This is the key difference from AdaGrad: instead of <span class="math inline">\(r \leftarrow r + g \odot g\)</span>, we use a weighted average.</p>
<ol start="4" type="1">
<li>Compute the parameter update:</li>
</ol>
<p><span class="math display">\[
\Delta\theta \leftarrow -\frac{\epsilon}{\delta + \sqrt{r}} \odot g
\]</span></p>
<ol start="5" type="1">
<li>Update parameters:</li>
</ol>
<p><span class="math display">\[
\theta \leftarrow \theta + \Delta\theta
\]</span></p>
</section>
<section id="key-properties-1" class="level3">
<h3 class="anchored" data-anchor-id="key-properties-1">Key Properties</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Overcomes AdaGrad’s aggressive learning rate decay</li>
<li>Learning rates can increase when recent gradients are smaller than historical averages</li>
<li>Generally more robust than AdaGrad for non-convex optimization</li>
</ul>
<p><strong>Comparison to AdaGrad</strong>:</p>
<ul>
<li>AdaGrad: <span class="math inline">\(r_t = r_{t-1} + g_t^2\)</span> (monotonically increasing)</li>
<li>RMSProp: <span class="math inline">\(r_t = \rho r_{t-1} + (1-\rho)g_t^2\)</span> (can increase or decrease)</li>
</ul>
</section>
</section>
<section id="rmsprop-with-nesterov-momentum" class="level2">
<h2 class="anchored" data-anchor-id="rmsprop-with-nesterov-momentum">RMSProp with Nesterov Momentum</h2>
<p>Combining RMSProp’s adaptive learning rates with Nesterov momentum’s lookahead gradient computation often yields better performance.</p>
<section id="algorithm-2" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-2">Algorithm</h3>
<p><strong>Hyperparameters</strong>:</p>
<ul>
<li>Learning rate <span class="math inline">\(\epsilon\)</span></li>
<li>Decay rate <span class="math inline">\(\rho\)</span></li>
<li>Small constant <span class="math inline">\(\delta\)</span></li>
<li>Momentum coefficient <span class="math inline">\(\alpha\)</span> (typically 0.9)</li>
<li>Initial gradient accumulator <span class="math inline">\(r = 0\)</span></li>
<li>Initial velocity <span class="math inline">\(v = 0\)</span></li>
</ul>
<p><strong>Training procedure</strong>:</p>
<p>While stopping criterion not met:</p>
<ol type="1">
<li><p>Sample <span class="math inline">\(m\)</span> examples from the training set</p></li>
<li><p>Compute the lookahead parameters:</p></li>
</ol>
<p><span class="math display">\[
\tilde{\theta} \leftarrow \theta + \alpha v
\]</span></p>
<ol start="3" type="1">
<li>Compute the gradient at the lookahead position:</li>
</ol>
<p><span class="math display">\[
g \leftarrow \frac{1}{m}\nabla_{\tilde{\theta}}\sum_{i=1}^m L(f(x^{(i)};\tilde{\theta}), y^{(i)})
\]</span></p>
<ol start="4" type="1">
<li>Accumulate squared gradients with exponential decay:</li>
</ol>
<p><span class="math display">\[
r \leftarrow \rho r + (1 - \rho) g \odot g
\]</span></p>
<ol start="5" type="1">
<li>Update velocity:</li>
</ol>
<p><span class="math display">\[
v \leftarrow \alpha v - \frac{\epsilon}{\delta + \sqrt{r}} \odot g
\]</span></p>
<ol start="6" type="1">
<li>Update parameters:</li>
</ol>
<p><span class="math display">\[
\theta \leftarrow \theta + v
\]</span></p>
<p>This combines the best of both worlds: Nesterov’s anticipatory gradient and RMSProp’s adaptive learning rates.</p>
</section>
</section>
<section id="adam" class="level2">
<h2 class="anchored" data-anchor-id="adam">Adam</h2>
<p><strong>Core idea</strong>: Adam (Adaptive Moment Estimation) combines the benefits of RMSProp and momentum by keeping track of both first-order moments (mean) and second-order moments (variance) of the gradients, along with bias correction for initialization.</p>
<p><strong>Intuition</strong>: Adam maintains two moving averages:</p>
<ul>
<li><span class="math inline">\(s\)</span>: The first moment (mean) of gradients, providing momentum-like behavior</li>
<li><span class="math inline">\(r\)</span>: The second moment (uncentered variance) of gradients, providing adaptive learning rates like RMSProp</li>
</ul>
<section id="algorithm-3" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-3">Algorithm</h3>
<p><strong>Hyperparameters</strong>:</p>
<ul>
<li>Learning rate <span class="math inline">\(\epsilon\)</span> (default: 0.001)</li>
<li>Decay rates:
<ul>
<li><span class="math inline">\(\rho_1\)</span> for first moment (default: 0.9)</li>
<li><span class="math inline">\(\rho_2\)</span> for second moment (default: 0.999)</li>
</ul></li>
<li>Small constant <span class="math inline">\(\delta\)</span> (typically <span class="math inline">\(10^{-8}\)</span>)</li>
<li>Initial first moment <span class="math inline">\(s = 0\)</span></li>
<li>Initial second moment <span class="math inline">\(r = 0\)</span></li>
<li>Time step <span class="math inline">\(t = 0\)</span></li>
</ul>
<p><strong>Training procedure</strong>:</p>
<p>While stopping criterion not met:</p>
<ol type="1">
<li><p>Increment time step: <span class="math inline">\(t \leftarrow t + 1\)</span></p></li>
<li><p>Sample <span class="math inline">\(m\)</span> examples from the training set</p></li>
<li><p>Compute the gradient:</p></li>
</ol>
<p><span class="math display">\[
g \leftarrow \frac{1}{m}\nabla_{\theta}\sum_{i=1}^m L(f(x^{(i)};\theta), y^{(i)})
\]</span></p>
<ol start="4" type="1">
<li>Update first moment estimate (momentum-like term):</li>
</ol>
<p><span class="math display">\[
s \leftarrow \rho_1 s + (1 - \rho_1) g
\]</span></p>
<ol start="5" type="1">
<li>Apply bias correction to first moment:</li>
</ol>
<p><span class="math display">\[
\hat{s} \leftarrow \frac{s}{1 - \rho_1^t}
\]</span></p>
<ol start="6" type="1">
<li>Update second moment estimate (adaptive learning rate term):</li>
</ol>
<p><span class="math display">\[
r \leftarrow \rho_2 r + (1 - \rho_2) g \odot g
\]</span></p>
<ol start="7" type="1">
<li>Apply bias correction to second moment:</li>
</ol>
<p><span class="math display">\[
\hat{r} \leftarrow \frac{r}{1 - \rho_2^t}
\]</span></p>
<ol start="8" type="1">
<li>Compute the parameter update:</li>
</ol>
<p><span class="math display">\[
\Delta\theta \leftarrow -\epsilon \frac{\hat{s}}{\sqrt{\hat{r}} + \delta}
\]</span></p>
<ol start="9" type="1">
<li>Update parameters:</li>
</ol>
<p><span class="math display">\[
\theta \leftarrow \theta + \Delta\theta
\]</span></p>
</section>
<section id="understanding-bias-correction" class="level3">
<h3 class="anchored" data-anchor-id="understanding-bias-correction">Understanding Bias Correction</h3>
<p>At the beginning of training, Adam initializes its moving averages to zero:</p>
<p><span class="math display">\[
s_0 = 0, \quad r_0 = 0
\]</span></p>
<p>This causes the first few estimates of the mean (<span class="math inline">\(s_t\)</span>) and variance (<span class="math inline">\(r_t\)</span>) of the gradients to be <strong>biased toward zero</strong>, simply because there is not enough historical data yet.</p>
<p><strong>Mathematical analysis</strong>:</p>
<p>The expected value of the uncorrected first moment is:</p>
<p><span class="math display">\[
\mathbb{E}[s_t] = (1 - \rho_1^t)\mathbb{E}[g_t]
\]</span></p>
<p>So <span class="math inline">\(s_t\)</span> underestimates the true mean by a factor of <span class="math inline">\(1 - \rho_1^t\)</span>.</p>
<p><strong>Bias correction</strong>: To correct this “cold start” bias, Adam divides each estimate by that same factor:</p>
<p><span class="math display">\[
\hat{s}_t = \frac{s_t}{1 - \rho_1^t}, \quad \hat{r}_t = \frac{r_t}{1 - \rho_2^t}
\]</span></p>
<p>After correction, the expected values become unbiased:</p>
<p><span class="math display">\[
\mathbb{E}[\hat{s}_t] = \mathbb{E}[g_t]
\]</span></p>
<p><strong>Note</strong>: As <span class="math inline">\(t\)</span> increases, <span class="math inline">\(\rho_1^t \to 0\)</span> and <span class="math inline">\(\rho_2^t \to 0\)</span>, so the bias correction becomes negligible after the initial training phase. The correction is most important in the first few iterations.</p>
</section>
<section id="interpreting-the-update-rule" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-the-update-rule">Interpreting the Update Rule</h3>
<p>Think of <span class="math inline">\(\hat{s}_t\)</span> as the <strong>average direction we want to go</strong>, and <span class="math inline">\(\sqrt{\hat{r}_t}\)</span> as the <strong>estimated magnitude or volatility</strong> of that direction.</p>
<p>The update rule <span class="math inline">\(\Delta\theta = -\epsilon \frac{\hat{s}}{\sqrt{\hat{r}} + \delta}\)</span> means:</p>
<p><strong>Parameters with large, noisy gradients</strong> → <strong>smaller updates</strong></p>
<ul>
<li>High gradient variance (<span class="math inline">\(\hat{r}\)</span> is large) → larger denominator → smaller step size</li>
<li>This prevents overshooting in directions with high uncertainty</li>
</ul>
<p><strong>Parameters with small, stable gradients</strong> → <strong>larger updates</strong></p>
<ul>
<li>Low gradient variance (<span class="math inline">\(\hat{r}\)</span> is small) → smaller denominator → larger step size</li>
<li>This accelerates progress in directions with consistent signal</li>
</ul>
<p>This adaptive behavior is why Adam works well across a wide range of problems without extensive hyperparameter tuning.</p>
</section>
<section id="key-properties-2" class="level3">
<h3 class="anchored" data-anchor-id="key-properties-2">Key Properties</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Combines benefits of momentum and adaptive learning rates</li>
<li>Bias correction prevents underestimation in early training</li>
<li>Generally robust default hyperparameters (often works with <span class="math inline">\(\epsilon = 0.001\)</span>, <span class="math inline">\(\rho_1 = 0.9\)</span>, <span class="math inline">\(\rho_2 = 0.999\)</span>)</li>
<li>Widely used in practice for training deep neural networks</li>
</ul>
<p><strong>When to use each algorithm</strong>:</p>
<ul>
<li><strong>AdaGrad</strong>: Sparse data, convex problems (but not deep learning)</li>
<li><strong>RMSProp</strong>: Good general-purpose optimizer, especially for RNNs</li>
<li><strong>Adam</strong>: Most popular default choice for deep learning</li>
</ul>
</section>
</section>
<section id="summary-comparison-of-adaptive-methods" class="level2">
<h2 class="anchored" data-anchor-id="summary-comparison-of-adaptive-methods">Summary: Comparison of Adaptive Methods</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 19%">
<col style="width: 20%">
<col style="width: 23%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>First Moment</th>
<th>Second Moment</th>
<th>Bias Correction</th>
<th>Best Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>SGD + Momentum</strong></td>
<td>✓ (momentum)</td>
<td>✗</td>
<td>✗</td>
<td>Simple, well-understood problems</td>
</tr>
<tr class="even">
<td><strong>AdaGrad</strong></td>
<td>✗</td>
<td>✓ (cumulative)</td>
<td>✗</td>
<td>Sparse features, convex optimization</td>
</tr>
<tr class="odd">
<td><strong>RMSProp</strong></td>
<td>✗</td>
<td>✓ (exponential avg)</td>
<td>✗</td>
<td>RNNs, non-convex problems</td>
</tr>
<tr class="even">
<td><strong>Adam</strong></td>
<td>✓ (exponential avg)</td>
<td>✓ (exponential avg)</td>
<td>✓</td>
<td>General deep learning (most popular)</td>
</tr>
</tbody>
</table>
<p>The evolution from SGD to Adam represents a progression toward algorithms that require less manual tuning while adapting automatically to the optimization landscape.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="light">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "ickma2311/ickma2311.github.io";
    script.dataset.repoId = "R_kgDOOzbaAg";
    script.dataset.category = "Comments";
    script.dataset.categoryId = "DIC_kwDOOzbaAs4CxPFj";
    script.dataset.mapping = "pathname";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Goodfellow Deep Learning — Chapter 8.5: Algorithms with Adaptive Learning Rates"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Chao Ma"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2025-11-12"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Deep Learning, Optimization, AdaGrad, RMSProp, Adam]</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter"</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>A fundamental challenge in optimization is choosing the right learning rate. Too large, and training diverges; too small, and progress is painfully slow. Moreover, different parameters may benefit from different learning rates—some require large steps while others need fine-tuning.</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>**Adaptive learning rate algorithms** address this challenge by automatically adjusting the learning rate for each parameter based on the history of gradients. This section covers three major algorithms: AdaGrad, RMSProp, and Adam.</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">## AdaGrad</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>**Core idea**: AdaGrad scales the learning rate for each parameter inversely proportional to the square root of the cumulative sum of its past squared gradients.</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>**Intuition**: Parameters with large gradients have received large updates in the past and should now take smaller steps. Parameters with small gradients have moved little and can afford larger steps.</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="fu">### Algorithm</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>**Hyperparameters**:</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learning rate $\epsilon$</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small constant $\delta$ (typically $10^{-7}$ for numerical stability)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Initial gradient accumulator $r = 0$</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>**Training procedure**:</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>While stopping criterion not met:</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sample $m$ examples from the training set</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Compute the gradient:</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>g \leftarrow \frac{1}{m}\nabla_{\theta}\sum_{i=1}^m L(f(x^{(i)};\theta), y^{(i)})</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Accumulate squared gradients:</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>r \leftarrow r + g \odot g</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>where $\odot$ denotes element-wise multiplication.</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Compute the parameter update:</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>\Delta\theta \leftarrow -\frac{\epsilon}{\delta + \sqrt{r}} \odot g</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Update parameters:</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>\theta \leftarrow \theta + \Delta\theta</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Properties</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>**Advantages**:</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Automatically adapts learning rates for each parameter</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No manual learning rate tuning required for each parameter</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Works well for sparse features (e.g., in NLP tasks)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>**Disadvantages**:</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The accumulator $r$ grows monotonically, causing learning rates to shrink continuously</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Eventually, learning rates become infinitesimally small, and learning stops</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This makes AdaGrad unsuitable for training deep neural networks</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="fu">## RMSProp</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>**Core idea**: RMSProp (Root Mean Square Propagation) uses an exponential decay to discount very old gradients, allowing the algorithm to forget distant history and achieve faster convergence once it reaches a convex bowl.</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>**Intuition**: Unlike AdaGrad, which accumulates all past gradients forever, RMSProp uses an exponentially weighted moving average. This allows the learning rate to increase again if recent gradients are small, even if very old gradients were large.</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="fu">### Algorithm</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>**Hyperparameters**:</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learning rate $\epsilon$ (typically 0.001)</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Decay rate $\rho$ (typically 0.9)</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small constant $\delta$ (typically $10^{-6}$)</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Initial gradient accumulator $r = 0$</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>**Training procedure**:</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>While stopping criterion not met:</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sample $m$ examples from the training set</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Compute the gradient:</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>g \leftarrow \frac{1}{m}\nabla_{\theta}\sum_{i=1}^m L(f(x^{(i)};\theta), y^{(i)})</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Accumulate squared gradients with exponential decay:</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>r \leftarrow \rho r + (1 - \rho) g \odot g</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>This is the key difference from AdaGrad: instead of $r \leftarrow r + g \odot g$, we use a weighted average.</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Compute the parameter update:</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>\Delta\theta \leftarrow -\frac{\epsilon}{\delta + \sqrt{r}} \odot g</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Update parameters:</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>\theta \leftarrow \theta + \Delta\theta</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Properties</span></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>**Advantages**:</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Overcomes AdaGrad's aggressive learning rate decay</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learning rates can increase when recent gradients are smaller than historical averages</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Generally more robust than AdaGrad for non-convex optimization</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>**Comparison to AdaGrad**:</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>AdaGrad: $r_t = r_{t-1} + g_t^2$ (monotonically increasing)</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>RMSProp: $r_t = \rho r_{t-1} + (1-\rho)g_t^2$ (can increase or decrease)</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a><span class="fu">## RMSProp with Nesterov Momentum</span></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>Combining RMSProp's adaptive learning rates with Nesterov momentum's lookahead gradient computation often yields better performance.</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a><span class="fu">### Algorithm</span></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>**Hyperparameters**:</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learning rate $\epsilon$</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Decay rate $\rho$</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small constant $\delta$</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Momentum coefficient $\alpha$ (typically 0.9)</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Initial gradient accumulator $r = 0$</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Initial velocity $v = 0$</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>**Training procedure**:</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>While stopping criterion not met:</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sample $m$ examples from the training set</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Compute the lookahead parameters:</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>\tilde{\theta} \leftarrow \theta + \alpha v</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Compute the gradient at the lookahead position:</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>g \leftarrow \frac{1}{m}\nabla_{\tilde{\theta}}\sum_{i=1}^m L(f(x^{(i)};\tilde{\theta}), y^{(i)})</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Accumulate squared gradients with exponential decay:</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>r \leftarrow \rho r + (1 - \rho) g \odot g</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Update velocity:</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>v \leftarrow \alpha v - \frac{\epsilon}{\delta + \sqrt{r}} \odot g</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Update parameters:</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>\theta \leftarrow \theta + v</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>This combines the best of both worlds: Nesterov's anticipatory gradient and RMSProp's adaptive learning rates.</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a><span class="fu">## Adam</span></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>**Core idea**: Adam (Adaptive Moment Estimation) combines the benefits of RMSProp and momentum by keeping track of both first-order moments (mean) and second-order moments (variance) of the gradients, along with bias correction for initialization.</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>**Intuition**: Adam maintains two moving averages:</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$s$: The first moment (mean) of gradients, providing momentum-like behavior</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$r$: The second moment (uncentered variance) of gradients, providing adaptive learning rates like RMSProp</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a><span class="fu">### Algorithm</span></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>**Hyperparameters**:</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learning rate $\epsilon$ (default: 0.001)</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Decay rates:</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\rho_1$ for first moment (default: 0.9)</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\rho_2$ for second moment (default: 0.999)</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small constant $\delta$ (typically $10^{-8}$)</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Initial first moment $s = 0$</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Initial second moment $r = 0$</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Time step $t = 0$</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>**Training procedure**:</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>While stopping criterion not met:</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Increment time step: $t \leftarrow t + 1$</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Sample $m$ examples from the training set</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Compute the gradient:</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>g \leftarrow \frac{1}{m}\nabla_{\theta}\sum_{i=1}^m L(f(x^{(i)};\theta), y^{(i)})</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Update first moment estimate (momentum-like term):</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>s \leftarrow \rho_1 s + (1 - \rho_1) g</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Apply bias correction to first moment:</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>\hat{s} \leftarrow \frac{s}{1 - \rho_1^t}</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Update second moment estimate (adaptive learning rate term):</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>r \leftarrow \rho_2 r + (1 - \rho_2) g \odot g</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>Apply bias correction to second moment:</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>\hat{r} \leftarrow \frac{r}{1 - \rho_2^t}</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>Compute the parameter update:</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>\Delta\theta \leftarrow -\epsilon \frac{\hat{s}}{\sqrt{\hat{r}} + \delta}</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span>Update parameters:</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>\theta \leftarrow \theta + \Delta\theta</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a><span class="fu">### Understanding Bias Correction</span></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>At the beginning of training, Adam initializes its moving averages to zero:</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>s_0 = 0, \quad r_0 = 0</span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>This causes the first few estimates of the mean ($s_t$) and variance ($r_t$) of the gradients to be **biased toward zero**, simply because there is not enough historical data yet.</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>**Mathematical analysis**:</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>The expected value of the uncorrected first moment is:</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">s_t</span><span class="co">]</span> = (1 - \rho_1^t)\mathbb{E}<span class="co">[</span><span class="ot">g_t</span><span class="co">]</span></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a>So $s_t$ underestimates the true mean by a factor of $1 - \rho_1^t$.</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>**Bias correction**: To correct this "cold start" bias, Adam divides each estimate by that same factor:</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>\hat{s}_t = \frac{s_t}{1 - \rho_1^t}, \quad \hat{r}_t = \frac{r_t}{1 - \rho_2^t}</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>After correction, the expected values become unbiased:</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">\hat{s}_t</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">g_t</span><span class="co">]</span></span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>**Note**: As $t$ increases, $\rho_1^t \to 0$ and $\rho_2^t \to 0$, so the bias correction becomes negligible after the initial training phase. The correction is most important in the first few iterations.</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpreting the Update Rule</span></span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>Think of $\hat{s}_t$ as the **average direction we want to go**, and $\sqrt{\hat{r}_t}$ as the **estimated magnitude or volatility** of that direction.</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>The update rule $\Delta\theta = -\epsilon \frac{\hat{s}}{\sqrt{\hat{r}} + \delta}$ means:</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>**Parameters with large, noisy gradients** → **smaller updates**</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High gradient variance ($\hat{r}$ is large) → larger denominator → smaller step size</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This prevents overshooting in directions with high uncertainty</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>**Parameters with small, stable gradients** → **larger updates**</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Low gradient variance ($\hat{r}$ is small) → smaller denominator → larger step size</span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This accelerates progress in directions with consistent signal</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>This adaptive behavior is why Adam works well across a wide range of problems without extensive hyperparameter tuning.</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Properties</span></span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>**Advantages**:</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Combines benefits of momentum and adaptive learning rates</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bias correction prevents underestimation in early training</span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Generally robust default hyperparameters (often works with $\epsilon = 0.001$, $\rho_1 = 0.9$, $\rho_2 = 0.999$)</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Widely used in practice for training deep neural networks</span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a>**When to use each algorithm**:</span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**AdaGrad**: Sparse data, convex problems (but not deep learning)</span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**RMSProp**: Good general-purpose optimizer, especially for RNNs</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Adam**: Most popular default choice for deep learning</span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary: Comparison of Adaptive Methods</span></span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Algorithm <span class="pp">|</span> First Moment <span class="pp">|</span> Second Moment <span class="pp">|</span> Bias Correction <span class="pp">|</span> Best Use Case <span class="pp">|</span></span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----------|--------------|---------------|-----------------|---------------|</span></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **SGD + Momentum** <span class="pp">|</span> ✓ (momentum) <span class="pp">|</span> ✗ <span class="pp">|</span> ✗ <span class="pp">|</span> Simple, well-understood problems <span class="pp">|</span></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **AdaGrad** <span class="pp">|</span> ✗ <span class="pp">|</span> ✓ (cumulative) <span class="pp">|</span> ✗ <span class="pp">|</span> Sparse features, convex optimization <span class="pp">|</span></span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **RMSProp** <span class="pp">|</span> ✗ <span class="pp">|</span> ✓ (exponential avg) <span class="pp">|</span> ✗ <span class="pp">|</span> RNNs, non-convex problems <span class="pp">|</span></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Adam** <span class="pp">|</span> ✓ (exponential avg) <span class="pp">|</span> ✓ (exponential avg) <span class="pp">|</span> ✓ <span class="pp">|</span> General deep learning (most popular) <span class="pp">|</span></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a>The evolution from SGD to Adam represents a progression toward algorithms that require less manual tuning while adapting automatically to the optimization landscape.</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>