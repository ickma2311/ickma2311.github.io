<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chao Ma">
<meta name="dcterms.date" content="2025-11-29">

<title>Deep Learning Book – ∇ ickma.dev</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.svg" rel="icon" type="image/svg+xml">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PK5N5KWZBF"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-PK5N5KWZBF', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">∇ ickma.dev</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-deep-learning-book" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Deep Learning Book</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-deep-learning-book">    
        <li>
    <a class="dropdown-item" href="../ML/xor-deep-learning.html">
 <span class="dropdown-text">Chapter 6.1: XOR Problem</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/likelihood-loss-functions.html">
 <span class="dropdown-text">Chapter 6.2: Loss Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/activation-functions.html">
 <span class="dropdown-text">Chapter 6.3: Activation Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/architecture-design.html">
 <span class="dropdown-text">Chapter 6.4: Architecture Design</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/backpropagation.html">
 <span class="dropdown-text">Chapter 6.5: Back-Propagation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/hessian-prerequisites.html">
 <span class="dropdown-text">Chapter 7 Prerequisites: Hessian Matrix</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/l2-regularization.html">
 <span class="dropdown-text">Chapter 7.1.1: L2 Regularization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/l1-regularization.html">
 <span class="dropdown-text">Chapter 7.1.2: L1 Regularization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/constrained-optimization-regularization.html">
 <span class="dropdown-text">Chapter 7.2: Constrained Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/regularization-underconstrained.html">
 <span class="dropdown-text">Chapter 7.3: Under-Constrained Problems</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/dataset-augmentation.html">
 <span class="dropdown-text">Chapter 7.4: Dataset Augmentation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/noise-robustness.html">
 <span class="dropdown-text">Chapter 7.5: Noise Robustness</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/semi-supervised-learning.html">
 <span class="dropdown-text">Chapter 7.6: Semi-Supervised Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/multi-task-learning.html">
 <span class="dropdown-text">Chapter 7.7: Multi-Task Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/early-stopping.html">
 <span class="dropdown-text">Chapter 7.8: Early Stopping</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/parameter-tying-sharing.html">
 <span class="dropdown-text">Chapter 7.9: Parameter Tying &amp; Sharing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/representation-sparsity.html">
 <span class="dropdown-text">Chapter 7.10: Sparse Representations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/bagging-ensemble.html">
 <span class="dropdown-text">Chapter 7.11: Bagging &amp; Ensemble Methods</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/dropout.html">
 <span class="dropdown-text">Chapter 7.12: Dropout</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/adversarial-training.html">
 <span class="dropdown-text">Chapter 7.13: Adversarial Training</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/tangent-prop-manifold.html">
 <span class="dropdown-text">Chapter 7.14: Tangent Prop &amp; Manifolds</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/learning-vs-optimization.html">
 <span class="dropdown-text">Chapter 8.1: Learning vs Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/optimization-challenges.html">
 <span class="dropdown-text">Chapter 8.2: Optimization Challenges</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/basic-optimization-algorithms.html">
 <span class="dropdown-text">Chapter 8.3: Basic Algorithms</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/parameter-initialization.html">
 <span class="dropdown-text">Chapter 8.4: Parameter Initialization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/adaptive-learning-rates.html">
 <span class="dropdown-text">Chapter 8.5: Adaptive Learning Rates</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/second-order-methods.html">
 <span class="dropdown-text">Chapter 8.6: Second-Order Methods</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/optimization-strategies.html">
 <span class="dropdown-text">Chapter 8.7: Optimization Strategies</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/convolution-computation.html">
 <span class="dropdown-text">Chapter 9.1: Convolution Computation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-motivation.html">
 <span class="dropdown-text">Chapter 9.2: CNN Motivation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-pooling.html">
 <span class="dropdown-text">Chapter 9.3: Pooling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-infinitely-strong-prior.html">
 <span class="dropdown-text">Chapter 9.4: Infinitely Strong Prior</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/convolutional-functions.html">
 <span class="dropdown-text">Chapter 9.5: Convolutional Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-structured-outputs.html">
 <span class="dropdown-text">Chapter 9.6: Structured Outputs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-data-types.html">
 <span class="dropdown-text">Chapter 9.7: Data Types</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-efficient-convolution.html">
 <span class="dropdown-text">Chapter 9.8: Efficient Convolution</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-unsupervised-learning.html">
 <span class="dropdown-text">Chapter 9.9: Unsupervised Feature Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/cnn-neuroscience.html">
 <span class="dropdown-text">Chapter 9.10: Neuroscientific Basis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-unfold-computation-graph.html">
 <span class="dropdown-text">Chapter 10.1: Unfold Computation Graph</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-recurrent-neural-networks.html">
 <span class="dropdown-text">Chapter 10.2: Recurrent Neural Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-bidirectional.html">
 <span class="dropdown-text">Chapter 10.3: Bidirectional RNN</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-encoder-decoder.html">
 <span class="dropdown-text">Chapter 10.4: Encoder-Decoder Seq2Seq</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-deep.html">
 <span class="dropdown-text">Chapter 10.5: Deep RNN</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-recursive.html">
 <span class="dropdown-text">Chapter 10.6: Recursive Neural Network</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-long-term-dependency.html">
 <span class="dropdown-text">Chapter 10.7: Long-Term Dependencies</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-echo-state.html">
 <span class="dropdown-text">Chapter 10.8: Echo State Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-leaky-unit.html">
 <span class="dropdown-text">Chapter 10.9: Leaky Units</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-lstm-gru.html">
 <span class="dropdown-text">Chapter 10.10: LSTM and GRU</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-optimize-long-term.html">
 <span class="dropdown-text">Chapter 10.11: Optimizing Long-Term Dependencies</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/rnn-explicit-memory.html">
 <span class="dropdown-text">Chapter 10.12: Explicit Memory</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/practical-methodology.html">
 <span class="dropdown-text">Chapter 11: Practical Methodology</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/large-scale-deep-learning.html">
 <span class="dropdown-text">Chapter 12.1: Large-Scale Deep Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/image-preprocessing-normalization.html">
 <span class="dropdown-text">Chapter 12.2: Image Preprocessing and Normalization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/automatic-speech-recognition.html">
 <span class="dropdown-text">Chapter 12.3: Automatic Speech Recognition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/nlp-applications.html">
 <span class="dropdown-text">Chapter 12.4: NLP Applications</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/other-applications.html">
 <span class="dropdown-text">Chapter 12.5: Other Applications</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-math" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Math</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-math">    
        <li class="dropdown-header">Reflections &amp; Synthesis</li>
        <li>
    <a class="dropdown-item" href="../Math/reflections/mit1806-invertibility-connections.html">
 <span class="dropdown-text">Invertibility Connections</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/reflections/taylor-euler-fourier.html">
 <span class="dropdown-text">Taylor, Euler, and Fourier</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">MIT 18.06SC Linear Algebra</li>
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture1-geometry.html">
 <span class="dropdown-text">Lecture 1: Geometry</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture2-elimination.html">
 <span class="dropdown-text">Lecture 2: Elimination</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture3-multiplication.html">
 <span class="dropdown-text">Lecture 3: Multiplication</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture4-lu-decomposition.html">
 <span class="dropdown-text">Lecture 4: LU Decomposition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-permutations.html">
 <span class="dropdown-text">Lecture 5.1: Permutations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-2-transpose.html">
 <span class="dropdown-text">Lecture 5.2: Transpose</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-3-spaces.html">
 <span class="dropdown-text">Lecture 5.3: Spaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture6-column-null-space.html">
 <span class="dropdown-text">Lecture 6: Column &amp; Null Space</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture7-solving-ax-0.html">
 <span class="dropdown-text">Lecture 7: Solving Ax=0</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture8-solving-ax-b.html">
 <span class="dropdown-text">Lecture 8: Solving Ax=b</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html">
 <span class="dropdown-text">Lecture 9: Independence, Basis, Dimension</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture10-four-subspaces.html">
 <span class="dropdown-text">Lecture 10: Four Fundamental Subspaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture11-matrix-spaces.html">
 <span class="dropdown-text">Lecture 11: Matrix Spaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture12-graphs-networks.html">
 <span class="dropdown-text">Lecture 12: Graphs and Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture13-quiz-review.html">
 <span class="dropdown-text">Lecture 13: Quiz 1 Review</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture14-orthogonality.html">
 <span class="dropdown-text">Lecture 14: Orthogonality</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture15-projections.html">
 <span class="dropdown-text">Lecture 15: Projections</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture16-least-squares.html">
 <span class="dropdown-text">Lecture 16: Least Squares</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture17-gram-schmidt.html">
 <span class="dropdown-text">Lecture 17: Gram-Schmidt</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture18-determinants.html">
 <span class="dropdown-text">Lecture 18: Determinants</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture19-determinant-formulas.html">
 <span class="dropdown-text">Lecture 19: Determinant Formulas</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture20-cramers-rule.html">
 <span class="dropdown-text">Lecture 20: Inverse &amp; Volume</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.html">
 <span class="dropdown-text">Lecture 21: Eigenvalues &amp; Eigenvectors</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture22-diagonalization-powers.html">
 <span class="dropdown-text">Lecture 22: Diagonalization &amp; Powers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture23-differential-equations.html">
 <span class="dropdown-text">Lecture 23: Differential Equations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture24-markov-fourier.html">
 <span class="dropdown-text">Lecture 24: Markov &amp; Fourier</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.html">
 <span class="dropdown-text">Lecture 25: Symmetric &amp; Positive Definite</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.html">
 <span class="dropdown-text">Lecture 26: Complex Matrices &amp; FFT</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture27-positive-definite-minima.html">
 <span class="dropdown-text">Lecture 27: Positive Definite &amp; Minima</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.html">
 <span class="dropdown-text">Lecture 28: Similar Matrices &amp; Jordan Form</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture29-svd.html">
 <span class="dropdown-text">Lecture 29: Singular Value Decomposition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture30-linear-transformations.html">
 <span class="dropdown-text">Lecture 30: Linear Transformations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture31-change-of-basis.html">
 <span class="dropdown-text">Lecture 31: Change of Basis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture33-left-right-inverse.html">
 <span class="dropdown-text">Lecture 33: Left &amp; Right Inverse</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">MIT 18.065: Linear Algebra Applications</li>
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture1-column-space.html">
 <span class="dropdown-text">Lecture 1: Column Space</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture2-multiplying-factoring.html">
 <span class="dropdown-text">Lecture 2: Multiplying and Factoring Matrices</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture3-orthonormal-columns.html">
 <span class="dropdown-text">Lecture 3: Orthonormal Columns</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture8-norms.html">
 <span class="dropdown-text">Lecture 8: Norms</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.065/mit18065-lecture9-least-squares.html">
 <span class="dropdown-text">Lecture 9: Least Squares</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Stanford EE 364A: Convex Optimization</li>
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture1-intro.html">
 <span class="dropdown-text">Lecture 1: Introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture2-math-foundations.html">
 <span class="dropdown-text">Lecture 2: Convex Sets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture3-convex-functions.html">
 <span class="dropdown-text">Lecture 3: Convex Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.html">
 <span class="dropdown-text">Lecture 4.1: Operations Preserving Convexity</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture4-part2-conjugate-quasiconvex.html">
 <span class="dropdown-text">Lecture 4.2: Conjugate &amp; Quasiconvex Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture5-part1-log-concave-convex.html">
 <span class="dropdown-text">Lecture 5.1: Log-Concave &amp; Log-Convex</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture5-part2-monotonicity.html">
 <span class="dropdown-text">Lecture 5.2: Monotonicity</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/EE364A/ee364a-lecture6-optimization-problems.html">
 <span class="dropdown-text">Chapter 4.1: Optimization Problems</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-more" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">More</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-more">    
        <li>
    <a class="dropdown-item" href="../ML/k_means_clustering.html">
 <span class="dropdown-text">K-Means Clustering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/logistic_regression.html">
 <span class="dropdown-text">Logistic Regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/axis.html">
 <span class="dropdown-text">Axis Operations</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../Algorithm/dp_regex.html">
 <span class="dropdown-text">DP Regex</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/ickma2311/ickma2311.github.io/discussions"> 
<span class="menu-text">Feedback</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#all-chapters" id="toc-all-chapters" class="nav-link active" data-scroll-target="#all-chapters">All Chapters</a>
  <ul class="collapse">
  <li><a href="#chapter-6-deep-feedforward-networks" id="toc-chapter-6-deep-feedforward-networks" class="nav-link" data-scroll-target="#chapter-6-deep-feedforward-networks">Chapter 6: Deep Feedforward Networks</a></li>
  <li><a href="#chapter-7-regularization-for-deep-learning" id="toc-chapter-7-regularization-for-deep-learning" class="nav-link" data-scroll-target="#chapter-7-regularization-for-deep-learning">Chapter 7: Regularization for Deep Learning</a></li>
  <li><a href="#chapter-8-optimization-for-training-deep-models" id="toc-chapter-8-optimization-for-training-deep-models" class="nav-link" data-scroll-target="#chapter-8-optimization-for-training-deep-models">Chapter 8: Optimization for Training Deep Models</a></li>
  <li><a href="#chapter-9-convolutional-networks" id="toc-chapter-9-convolutional-networks" class="nav-link" data-scroll-target="#chapter-9-convolutional-networks">Chapter 9: Convolutional Networks</a></li>
  <li><a href="#chapter-10-sequence-modeling" id="toc-chapter-10-sequence-modeling" class="nav-link" data-scroll-target="#chapter-10-sequence-modeling">Chapter 10: Sequence Modeling</a></li>
  <li><a href="#chapter-11-practical-methodology" id="toc-chapter-11-practical-methodology" class="nav-link" data-scroll-target="#chapter-11-practical-methodology">Chapter 11: Practical Methodology</a></li>
  <li><a href="#chapter-12-applications" id="toc-chapter-12-applications" class="nav-link" data-scroll-target="#chapter-12-applications">Chapter 12: Applications</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Deep Learning Book</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Chao Ma </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 29, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="all-chapters" class="level2">
<h2 class="anchored" data-anchor-id="all-chapters">All Chapters</h2>
<p>My notes and implementations while studying the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.</p>
<hr>
<section id="chapter-6-deep-feedforward-networks" class="level3">
<h3 class="anchored" data-anchor-id="chapter-6-deep-feedforward-networks">Chapter 6: Deep Feedforward Networks</h3>
<div class="content-grid">
<div class="content-card">
<p><strong><a href="../ML/xor-deep-learning.html">Chapter 6.1: XOR Problem &amp; ReLU Networks</a></strong> How ReLU solves problems that linear models cannot handle.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/likelihood-loss-functions.html">Chapter 6.2: Likelihood-Based Loss Functions</a></strong> The mathematical connection between probabilistic models and loss functions.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/activation-functions.html">Chapter 6.3: Hidden Units and Activation Functions</a></strong> Exploring activation functions and their impact on neural network learning.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/architecture-design.html">Chapter 6.4: Architecture Design - Depth vs Width</a></strong> How depth enables hierarchical feature reuse and exponential expressiveness.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/backpropagation.html">Chapter 6.5: Back-Propagation and Other Differentiation Algorithms</a></strong> The algorithm that makes training deep networks computationally feasible.</p>
</div>
</div>
<hr>
</section>
<section id="chapter-7-regularization-for-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="chapter-7-regularization-for-deep-learning">Chapter 7: Regularization for Deep Learning</h3>
<div class="content-grid">
<div class="content-card">
<p><strong><a href="../ML/hessian-prerequisites.html">Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature</a></strong> Essential second-order calculus concepts needed before Chapter 7.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/l2-regularization.html">Chapter 7.1.1: L2 Regularization</a></strong> How L2 regularization shrinks weights based on Hessian eigenvalues.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/l1-regularization.html">Chapter 7.1.2: L1 Regularization</a></strong> L1 regularization uses soft thresholding to create sparse solutions.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/constrained-optimization-regularization.html">Chapter 7.2: Constrained Optimization View of Regularization</a></strong> Regularization as constrained optimization with KKT conditions.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/regularization-underconstrained.html">Chapter 7.3: Regularization and Under-Constrained Problems</a></strong> Why regularization is mathematically necessary and ensures invertibility.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/dataset-augmentation.html">Chapter 7.4: Dataset Augmentation</a></strong> How transforming existing data improves generalization.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/noise-robustness.html">Chapter 7.5: Noise Robustness</a></strong> How adding Gaussian noise to weights is equivalent to penalizing large gradients.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/semi-supervised-learning.html">Chapter 7.6: Semi-Supervised Learning</a></strong> Leveraging unlabeled data to improve model performance when labeled data is scarce.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/multi-task-learning.html">Chapter 7.7: Multi-Task Learning</a></strong> Training a single model on multiple related tasks to improve generalization.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/early-stopping.html">Chapter 7.8: Early Stopping</a></strong> Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/parameter-tying-sharing.html">Chapter 7.9: Parameter Tying and Parameter Sharing</a></strong> Two strategies for reducing parameters: encouraging similarity vs.&nbsp;enforcing identity.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/representation-sparsity.html">Chapter 7.10: Sparse Representations</a></strong> Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/bagging-ensemble.html">Chapter 7.11: Bagging and Other Ensemble Methods</a></strong> How training multiple models on bootstrap samples and averaging their predictions reduces variance.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/dropout.html">Chapter 7.12: Dropout</a></strong> Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/adversarial-training.html">Chapter 7.13: Adversarial Training</a></strong> How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/tangent-prop-manifold.html">Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier</a></strong> Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.</p>
</div>
</div>
<hr>
</section>
<section id="chapter-8-optimization-for-training-deep-models" class="level3">
<h3 class="anchored" data-anchor-id="chapter-8-optimization-for-training-deep-models">Chapter 8: Optimization for Training Deep Models</h3>
<div class="content-grid">
<div class="content-card">
<p><strong><a href="../ML/learning-vs-optimization.html">Chapter 8.1: How Learning Differs from Pure Optimization</a></strong> Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/optimization-challenges.html">Chapter 8.2: Challenges in Deep Learning Optimization</a></strong> Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/basic-optimization-algorithms.html">Chapter 8.3: Basic Algorithms</a></strong> SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/parameter-initialization.html">Chapter 8.4: Parameter Initialization Strategies</a></strong> Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/adaptive-learning-rates.html">Chapter 8.5: Algorithms with Adaptive Learning Rates</a></strong> From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/second-order-methods.html">Chapter 8.6: Second-Order Optimization Methods</a></strong> Newton’s method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/optimization-strategies.html">Chapter 8.7: Optimization Strategies and Meta-Algorithms</a></strong> Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.</p>
</div>
</div>
<hr>
</section>
<section id="chapter-9-convolutional-networks" class="level3">
<h3 class="anchored" data-anchor-id="chapter-9-convolutional-networks">Chapter 9: Convolutional Networks</h3>
<div class="content-grid">
<div class="content-card">
<p><strong><a href="../ML/convolution-computation.html">Chapter 9.1: Convolution Computation</a></strong> The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/cnn-motivation.html">Chapter 9.2: Motivation for Convolutional Networks</a></strong> Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m·n) to O(k·n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere—achieving 30,000× speedup over dense layers.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/cnn-pooling.html">Chapter 9.3: Pooling</a></strong> Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28×28 feature maps to 14×14 (4× fewer activations). Comparing three architectures—strided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/cnn-infinitely-strong-prior.html">Chapter 9.4: Convolution and Pooling as an Infinitely Strong Prior</a></strong> Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off—strong priors reduce sample complexity but only when assumptions match the data structure.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/convolutional-functions.html">Chapter 9.5: Convolutional Functions</a></strong> Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula <span class="math inline">\(Z_{l,x,y} = \sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}\)</span>, stride for downsampling, three padding strategies (valid/same/full), and gradient computation—kernel gradients via correlation with input, input gradients via convolution with flipped kernel.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/cnn-structured-outputs.html">Chapter 9.6: Structured Outputs</a></strong> CNNs can generate high-dimensional structured objects through pixel-level predictions. Preserving spatial dimensions (no pooling, no stride &gt; 1, SAME padding) enables full-resolution outputs. Recurrent convolution refines predictions iteratively: <span class="math inline">\(U*X + H(t-1)*W = H(t)\)</span>, producing dense predictions for segmentation, depth estimation, and flow prediction.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/cnn-data-types.html">Chapter 9.7: Data Types</a></strong> CNNs can operate on different data types: 1D (audio, time series), 2D (images), and 3D (videos, CT scans) with varying channel counts. Unlike fully connected networks, convolutional kernels handle variable-sized inputs by sliding across spatial dimensions, producing outputs that scale accordingly—a unique flexibility for diverse domains.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/cnn-efficient-convolution.html">Chapter 9.8: Efficient Convolution Algorithms</a></strong> Separable convolution reduces computational cost from <span class="math inline">\(O(HWk^2)\)</span> to <span class="math inline">\(O(HWk)\)</span> by decomposing a 2D kernel into two 1D filters (vertical and horizontal). Parameter storage shrinks from <span class="math inline">\(k^2\)</span> to <span class="math inline">\(2k\)</span>. This factorization enables faster, more memory-efficient models without sacrificing accuracy—foundational for architectures like MobileNet.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/cnn-unsupervised-learning.html">Chapter 9.9: Unsupervised or Semi-Supervised Feature Learning</a></strong> Before CNNs, computer vision relied on hand-crafted kernels (Sobel, Laplacian, Gaussian) and unsupervised methods (sparse coding, autoencoders, k-means). While these captured simple patterns, they couldn’t match CNNs’ hierarchical, end-to-end feature learning. Modern systems use CNNs to learn features from edges to semantic concepts—making hand-crafted filters largely obsolete.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/cnn-neuroscience.html">Chapter 9.10: Neuroscientific Basis for Convolutional Networks</a></strong> V1 simple cells detect oriented edges (modeled by Gabor filters), complex cells pool over simple cells for translation invariance (like CNN pooling). But CNNs lack key biological features: saccadic attention, multisensory integration, top-down feedback, and dynamic receptive fields. While CNNs excel at feed-forward recognition, biological vision is holistic, context-aware, and adaptive.</p>
</div>
</div>
<hr>
</section>
<section id="chapter-10-sequence-modeling" class="level3">
<h3 class="anchored" data-anchor-id="chapter-10-sequence-modeling">Chapter 10: Sequence Modeling</h3>
<div class="content-grid">
<div class="content-card">
<p><strong><a href="../ML/rnn-unfold-computation-graph.html">Chapter 10.1: Unfold Computation Graph</a></strong> Unfolding computation graphs in RNNs enables parameter sharing across time steps. The same function with fixed parameters processes sequences of any length, compressing input history into fixed-size hidden states that retain only task-relevant information for predictions.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/rnn-recurrent-neural-networks.html">Chapter 10.2: Recurrent Neural Networks</a></strong> RNN architecture with hidden-to-hidden connections, teacher forcing for parallel training, back-propagation through time (BPTT), RNN as directed graphical models with O(τ) parameter efficiency, and context-based sequence-to-sequence models.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/rnn-bidirectional.html">Chapter 10.3: Bidirectional RNN</a></strong> Bidirectional RNNs process sequences in both forward and backward directions, allowing predictions to use information from the entire input sequence. Essential for tasks like speech recognition and handwriting recognition where future context matters.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/rnn-encoder-decoder.html">Chapter 10.4: Encoder-Decoder Sequence-to-Sequence Architecture</a></strong> The seq2seq architecture handles variable-length input and output sequences by compressing the input into a fixed context vector C, then decoding it step-by-step. This enables machine translation, summarization, and dialogue generation where input and output lengths differ.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/rnn-deep.html">Chapter 10.5: Deep Recurrent Networks</a></strong> Three architectural patterns for adding depth to RNNs: hierarchical hidden states (vertical stacking), deep transition RNNs (MLPs replace transformations), and deep transition with skip connections (residual paths for gradient flow).</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/rnn-recursive.html">Chapter 10.6: Recursive Neural Network</a></strong> Recursive neural networks compute over tree structures rather than linear chains, applying shared composition functions at internal nodes to build hierarchical representations bottom-up. This reduces computation depth from O(τ) to O(log τ), but requires external tree structure specification.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/rnn-long-term-dependency.html">Chapter 10.7: The Challenge of Long-Term Dependencies</a></strong> The fundamental challenge of long-term dependencies in RNNs is training difficulty: gradients propagated across many time steps either vanish exponentially (common) or explode (rare but severe). Eigenvalue analysis shows how powers of the transition matrix govern this instability.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/rnn-echo-state.html">Chapter 10.8: Echo State Networks</a></strong> ESNs fix recurrent weights and train only output weights, viewing the network as a dynamical reservoir. Setting spectral radius near one enables long-term memory retention. Learning reduces to linear regression on hidden states, avoiding backpropagation through time—showing that carefully designed dynamics can capture temporal structure.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/rnn-leaky-unit.html">Chapter 10.9: Leaky Units and Multiple Time Scales</a></strong> Leaky units separate instantaneous state from long-term integration using <span class="math inline">\(u^t = \alpha u^{t-1}+(1-\alpha)v^t\)</span>. Multiple time scale strategies include temporal skip connections (direct pathways across time steps) and removing short connections (forcing coarser time scales) to address long-term dependencies.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/rnn-lstm-gru.html">Chapter 10.10: LSTM and GRU</a></strong> LSTM uses learned gates (forget, input, output) to control information flow through explicit cell state paths, enabling adaptive long-term memory retention. GRU simplifies this design by merging forget and input into a single update gate, reducing parameters while maintaining the ability to capture long-range dependencies through gating mechanisms.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/rnn-optimize-long-term.html">Chapter 10.11: Optimizing Long-Term Dependencies</a></strong> Gradient clipping prevents training instability by rescaling gradients when their norm exceeds a threshold, protecting against sudden jumps across steep gradient cliffs. Regularizing information flow aims to maintain <span class="math inline">\(\|\partial h^t/\partial h^{t-1}\| \approx 1\)</span>, though this is rarely used in practice due to computational cost.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/rnn-explicit-memory.html">Chapter 10.12: Explicit Memory</a></strong> Explicit memory separates storage from computation by introducing addressable memory outside network parameters. Classic architectures (Memory Networks, Neural Turing Machines) are computationally expensive but inspired modern mechanisms—attention, Transformers, and retrieval-augmented models are successful, scalable realizations of this idea.</p>
</div>
</div>
<hr>
</section>
<section id="chapter-11-practical-methodology" class="level3">
<h3 class="anchored" data-anchor-id="chapter-11-practical-methodology">Chapter 11: Practical Methodology</h3>
<div class="content-grid">
<div class="content-card">
<p><strong><a href="../ML/practical-methodology.html">Chapter 11: Practical Methodology</a></strong> Define performance metrics aligned with application goals. Build baseline systems quickly using architecture patterns matched to data structure. Diagnose by comparing training and test error. Tune learning rate first, then other hyperparameters via random search. Debug systematically by isolating components, checking gradients, and monitoring numerical behavior.</p>
</div>
</div>
<hr>
</section>
<section id="chapter-12-applications" class="level3">
<h3 class="anchored" data-anchor-id="chapter-12-applications">Chapter 12: Applications</h3>
<div class="content-grid">
<div class="content-card">
<p><strong><a href="../ML/large-scale-deep-learning.html">Chapter 12.1: Large-Scale Deep Learning</a></strong> Scaling deep learning requires specialized hardware (GPUs, TPUs, ASICs), distributed training strategies (data/model parallelism, asynchronous SGD), and efficiency optimizations (model compression, quantization, pruning). Dynamic computation enables conditional execution for computational efficiency. Specialized accelerators exploit reduced precision and massive parallelism for deployment on resource-constrained devices.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/image-preprocessing-normalization.html">Chapter 12.2: Image Preprocessing and Normalization</a></strong> Preprocessing images through normalization (scaling to [0,1] or [-1,1]) and data augmentation improves training stability and generalization. Global Contrast Normalization (GCN) removes global lighting variations by centering and L2-normalizing images. Local Contrast Normalization (LCN) enhances local structures by normalizing within spatial neighborhoods. Modern networks rely on batch normalization, but explicit contrast normalization remains valuable for challenging datasets.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/automatic-speech-recognition.html">Chapter 12.3: Automatic Speech Recognition</a></strong> ASR evolution from GMM-HMM (classical statistical approach) through DNN-HMM (~30% error reduction with deep feedforward networks) to end-to-end systems using RNNs/LSTMs with CTC. CNNs treat spectrograms as 2D structures for frequency-invariant modeling. Modern systems learn direct acoustic-to-text mappings without forced alignment, integrating joint acoustic-phonetic modeling and hierarchical representations.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/nlp-applications.html">Chapter 12.4: NLP Applications</a></strong> N-gram models compute conditional probabilities over fixed contexts but suffer from sparsity and exponential growth. Neural language models use word embeddings to map discrete tokens into continuous space, enabling generalization across semantically similar words. High-dimensional vocabulary outputs require optimization: short lists partition frequent/rare words, hierarchical softmax reduces complexity to O(log|V|), and importance sampling approximates gradients. Attention mechanisms dynamically focus on relevant input positions, forming weighted context vectors that alleviate fixed-size representation bottlenecks in seq2seq tasks.</p>
</div>
<div class="content-card">
<p><strong><a href="../ML/other-applications.html">Chapter 12.5: Other Applications</a></strong> Collaborative filtering uses matrix factorization to learn latent user and item embeddings, decomposing ratings into user bias, item bias, and personalized interaction. Cold-start problems require side information. Recommendation systems face exploration-exploitation tradeoffs modeled as contextual bandits. Knowledge graphs represent facts as (subject, relation, object) triples; deep learning maps entities and relations to continuous embeddings for link prediction and reasoning. Evaluation challenges arise from open-world assumptions where unseen facts may be missing rather than false.</p>
</div>
</div>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="light">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "ickma2311/ickma2311.github.io";
    script.dataset.repoId = "R_kgDOOzbaAg";
    script.dataset.category = "Comments";
    script.dataset.categoryId = "DIC_kwDOOzbaAs4CxPFj";
    script.dataset.mapping = "pathname";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Deep Learning Book"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Chao Ma"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2025-11-29"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">## All Chapters</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>My notes and implementations while studying the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 6: Deep Feedforward Networks</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>::: {.content-grid}</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>**[Chapter 6.1: XOR Problem &amp; ReLU Networks](xor-deep-learning.qmd)**</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>How ReLU solves problems that linear models cannot handle.</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>**[Chapter 6.2: Likelihood-Based Loss Functions](likelihood-loss-functions.qmd)**</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>The mathematical connection between probabilistic models and loss functions.</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>**[Chapter 6.3: Hidden Units and Activation Functions](activation-functions.qmd)**</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>Exploring activation functions and their impact on neural network learning.</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>**[Chapter 6.4: Architecture Design - Depth vs Width](architecture-design.qmd)**</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>How depth enables hierarchical feature reuse and exponential expressiveness.</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>**[Chapter 6.5: Back-Propagation and Other Differentiation Algorithms](backpropagation.qmd)**</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>The algorithm that makes training deep networks computationally feasible.</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 7: Regularization for Deep Learning</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>::: {.content-grid}</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>**[Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature](hessian-prerequisites.ipynb)**</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>Essential second-order calculus concepts needed before Chapter 7.</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.1.1: L2 Regularization](l2-regularization.qmd)**</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>How L2 regularization shrinks weights based on Hessian eigenvalues.</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.1.2: L1 Regularization](l1-regularization.qmd)**</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>L1 regularization uses soft thresholding to create sparse solutions.</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.2: Constrained Optimization View of Regularization](constrained-optimization-regularization.qmd)**</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>Regularization as constrained optimization with KKT conditions.</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.3: Regularization and Under-Constrained Problems](regularization-underconstrained.qmd)**</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>Why regularization is mathematically necessary and ensures invertibility.</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.4: Dataset Augmentation](dataset-augmentation.qmd)**</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>How transforming existing data improves generalization.</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.5: Noise Robustness](noise-robustness.qmd)**</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>How adding Gaussian noise to weights is equivalent to penalizing large gradients.</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.6: Semi-Supervised Learning](semi-supervised-learning.qmd)**</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>Leveraging unlabeled data to improve model performance when labeled data is scarce.</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.7: Multi-Task Learning](multi-task-learning.qmd)**</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>Training a single model on multiple related tasks to improve generalization.</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.8: Early Stopping](early-stopping.qmd)**</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.9: Parameter Tying and Parameter Sharing](parameter-tying-sharing.qmd)**</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>Two strategies for reducing parameters: encouraging similarity vs. enforcing identity.</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.10: Sparse Representations](representation-sparsity.qmd)**</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.11: Bagging and Other Ensemble Methods](bagging-ensemble.qmd)**</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>How training multiple models on bootstrap samples and averaging their predictions reduces variance.</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.12: Dropout](dropout.qmd)**</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.13: Adversarial Training](adversarial-training.qmd)**</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>**[Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier](tangent-prop-manifold.qmd)**</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 8: Optimization for Training Deep Models</span></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>::: {.content-grid}</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>**[Chapter 8.1: How Learning Differs from Pure Optimization](learning-vs-optimization.qmd)**</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>**[Chapter 8.2: Challenges in Deep Learning Optimization](optimization-challenges.qmd)**</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>**[Chapter 8.3: Basic Algorithms](basic-optimization-algorithms.qmd)**</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>**[Chapter 8.4: Parameter Initialization Strategies](parameter-initialization.qmd)**</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>**[Chapter 8.5: Algorithms with Adaptive Learning Rates](adaptive-learning-rates.qmd)**</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>**[Chapter 8.6: Second-Order Optimization Methods](second-order-methods.qmd)**</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>Newton's method, Conjugate Gradient, and BFGS: elegant methods using curvature information, but rarely used in deep learning due to computational cost.</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>**[Chapter 8.7: Optimization Strategies and Meta-Algorithms](optimization-strategies.qmd)**</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>Advanced optimization strategies: batch normalization, coordinate descent, Polyak averaging, supervised pretraining, and continuation methods that enhance training efficiency and stability.</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 9: Convolutional Networks</span></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>::: {.content-grid}</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>**[Chapter 9.1: Convolution Computation](convolution-computation.qmd)**</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>The mathematical foundation of CNNs: from continuous convolution to discrete 2D operations. Understand why deep learning uses cross-correlation (not true convolution), and how parameter sharing and translation equivariance make CNNs powerful for spatial data.</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>**[Chapter 9.2: Motivation for Convolutional Networks](cnn-motivation.qmd)**</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>Why CNNs dominate computer vision: sparse interactions reduce parameters from O(m·n) to O(k·n), parameter sharing enforces translation invariance, and equivariance ensures patterns are detected anywhere—achieving 30,000× speedup over dense layers.</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>**[Chapter 9.3: Pooling](cnn-pooling.qmd)**</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>Downsampling through local aggregation: max pooling provides translation invariance by selecting strongest activations, reducing 28×28 feature maps to 14×14 (4× fewer activations). Comparing three architectures—strided convolutions, max pooling networks, and global average pooling that eliminates fully connected layers.</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>**[Chapter 9.4: Convolution and Pooling as an Infinitely Strong Prior](cnn-infinitely-strong-prior.qmd)**</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>Why CNNs work on images but not everywhere: architectural constraints (local connectivity + weight sharing) act as infinitely strong Bayesian priors, assigning probability 1 to translation-equivariant functions and 0 to all others. The bias-variance trade-off—strong priors reduce sample complexity but only when assumptions match the data structure.</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>**[Chapter 9.5: Convolutional Functions](convolutional-functions.qmd)**</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>Mathematical details of convolution operations: deep learning uses cross-correlation (not true convolution), multi-channel formula $Z_{l,x,y} = \sum_{i,j,k} V_{i,x+j-1,y+k-1} K_{l,i,j,k}$, stride for downsampling, three padding strategies (valid/same/full), and gradient computation—kernel gradients via correlation with input, input gradients via convolution with flipped kernel.</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>**[Chapter 9.6: Structured Outputs](cnn-structured-outputs.qmd)**</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>CNNs can generate high-dimensional structured objects through pixel-level predictions. Preserving spatial dimensions (no pooling, no stride &gt; 1, SAME padding) enables full-resolution outputs. Recurrent convolution refines predictions iteratively: $U*X + H(t-1)*W = H(t)$, producing dense predictions for segmentation, depth estimation, and flow prediction.</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>**[Chapter 9.7: Data Types](cnn-data-types.qmd)**</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>CNNs can operate on different data types: 1D (audio, time series), 2D (images), and 3D (videos, CT scans) with varying channel counts. Unlike fully connected networks, convolutional kernels handle variable-sized inputs by sliding across spatial dimensions, producing outputs that scale accordingly—a unique flexibility for diverse domains.</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>**[Chapter 9.8: Efficient Convolution Algorithms](cnn-efficient-convolution.qmd)**</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>Separable convolution reduces computational cost from $O(HWk^2)$ to $O(HWk)$ by decomposing a 2D kernel into two 1D filters (vertical and horizontal). Parameter storage shrinks from $k^2$ to $2k$. This factorization enables faster, more memory-efficient models without sacrificing accuracy—foundational for architectures like MobileNet.</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>**[Chapter 9.9: Unsupervised or Semi-Supervised Feature Learning](cnn-unsupervised-learning.qmd)**</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>Before CNNs, computer vision relied on hand-crafted kernels (Sobel, Laplacian, Gaussian) and unsupervised methods (sparse coding, autoencoders, k-means). While these captured simple patterns, they couldn't match CNNs' hierarchical, end-to-end feature learning. Modern systems use CNNs to learn features from edges to semantic concepts—making hand-crafted filters largely obsolete.</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>**[Chapter 9.10: Neuroscientific Basis for Convolutional Networks](cnn-neuroscience.qmd)**</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>V1 simple cells detect oriented edges (modeled by Gabor filters), complex cells pool over simple cells for translation invariance (like CNN pooling). But CNNs lack key biological features: saccadic attention, multisensory integration, top-down feedback, and dynamic receptive fields. While CNNs excel at feed-forward recognition, biological vision is holistic, context-aware, and adaptive.</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 10: Sequence Modeling</span></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>::: {.content-grid}</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>**[Chapter 10.1: Unfold Computation Graph](rnn-unfold-computation-graph.qmd)**</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>Unfolding computation graphs in RNNs enables parameter sharing across time steps. The same function with fixed parameters processes sequences of any length, compressing input history into fixed-size hidden states that retain only task-relevant information for predictions.</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>**[Chapter 10.2: Recurrent Neural Networks](rnn-recurrent-neural-networks.qmd)**</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>RNN architecture with hidden-to-hidden connections, teacher forcing for parallel training, back-propagation through time (BPTT), RNN as directed graphical models with O(τ) parameter efficiency, and context-based sequence-to-sequence models.</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>**[Chapter 10.3: Bidirectional RNN](rnn-bidirectional.qmd)**</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>Bidirectional RNNs process sequences in both forward and backward directions, allowing predictions to use information from the entire input sequence. Essential for tasks like speech recognition and handwriting recognition where future context matters.</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>**[Chapter 10.4: Encoder-Decoder Sequence-to-Sequence Architecture](rnn-encoder-decoder.qmd)**</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>The seq2seq architecture handles variable-length input and output sequences by compressing the input into a fixed context vector C, then decoding it step-by-step. This enables machine translation, summarization, and dialogue generation where input and output lengths differ.</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>**[Chapter 10.5: Deep Recurrent Networks](rnn-deep.qmd)**</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>Three architectural patterns for adding depth to RNNs: hierarchical hidden states (vertical stacking), deep transition RNNs (MLPs replace transformations), and deep transition with skip connections (residual paths for gradient flow).</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>**[Chapter 10.6: Recursive Neural Network](rnn-recursive.qmd)**</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>Recursive neural networks compute over tree structures rather than linear chains, applying shared composition functions at internal nodes to build hierarchical representations bottom-up. This reduces computation depth from O(τ) to O(log τ), but requires external tree structure specification.</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>**[Chapter 10.7: The Challenge of Long-Term Dependencies](rnn-long-term-dependency.qmd)**</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>The fundamental challenge of long-term dependencies in RNNs is training difficulty: gradients propagated across many time steps either vanish exponentially (common) or explode (rare but severe). Eigenvalue analysis shows how powers of the transition matrix govern this instability.</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>**[Chapter 10.8: Echo State Networks](rnn-echo-state.qmd)**</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a>ESNs fix recurrent weights and train only output weights, viewing the network as a dynamical reservoir. Setting spectral radius near one enables long-term memory retention. Learning reduces to linear regression on hidden states, avoiding backpropagation through time—showing that carefully designed dynamics can capture temporal structure.</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>**[Chapter 10.9: Leaky Units and Multiple Time Scales](rnn-leaky-unit.qmd)**</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>Leaky units separate instantaneous state from long-term integration using $u^t = \alpha u^{t-1}+(1-\alpha)v^t$. Multiple time scale strategies include temporal skip connections (direct pathways across time steps) and removing short connections (forcing coarser time scales) to address long-term dependencies.</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>**[Chapter 10.10: LSTM and GRU](rnn-lstm-gru.qmd)**</span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>LSTM uses learned gates (forget, input, output) to control information flow through explicit cell state paths, enabling adaptive long-term memory retention. GRU simplifies this design by merging forget and input into a single update gate, reducing parameters while maintaining the ability to capture long-range dependencies through gating mechanisms.</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>**[Chapter 10.11: Optimizing Long-Term Dependencies](rnn-optimize-long-term.qmd)**</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>Gradient clipping prevents training instability by rescaling gradients when their norm exceeds a threshold, protecting against sudden jumps across steep gradient cliffs. Regularizing information flow aims to maintain $<span class="sc">\|</span>\partial h^t/\partial h^{t-1}<span class="sc">\|</span> \approx 1$, though this is rarely used in practice due to computational cost.</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>**[Chapter 10.12: Explicit Memory](rnn-explicit-memory.qmd)**</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>Explicit memory separates storage from computation by introducing addressable memory outside network parameters. Classic architectures (Memory Networks, Neural Turing Machines) are computationally expensive but inspired modern mechanisms—attention, Transformers, and retrieval-augmented models are successful, scalable realizations of this idea.</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 11: Practical Methodology</span></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>::: {.content-grid}</span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>**[Chapter 11: Practical Methodology](practical-methodology.qmd)**</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>Define performance metrics aligned with application goals. Build baseline systems quickly using architecture patterns matched to data structure. Diagnose by comparing training and test error. Tune learning rate first, then other hyperparameters via random search. Debug systematically by isolating components, checking gradients, and monitoring numerical behavior.</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 12: Applications</span></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a>::: {.content-grid}</span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a>**[Chapter 12.1: Large-Scale Deep Learning](large-scale-deep-learning.qmd)**</span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>Scaling deep learning requires specialized hardware (GPUs, TPUs, ASICs), distributed training strategies (data/model parallelism, asynchronous SGD), and efficiency optimizations (model compression, quantization, pruning). Dynamic computation enables conditional execution for computational efficiency. Specialized accelerators exploit reduced precision and massive parallelism for deployment on resource-constrained devices.</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a>**[Chapter 12.2: Image Preprocessing and Normalization](image-preprocessing-normalization.qmd)**</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>Preprocessing images through normalization (scaling to <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span> or <span class="co">[</span><span class="ot">-1,1</span><span class="co">]</span>) and data augmentation improves training stability and generalization. Global Contrast Normalization (GCN) removes global lighting variations by centering and L2-normalizing images. Local Contrast Normalization (LCN) enhances local structures by normalizing within spatial neighborhoods. Modern networks rely on batch normalization, but explicit contrast normalization remains valuable for challenging datasets.</span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>**[Chapter 12.3: Automatic Speech Recognition](automatic-speech-recognition.qmd)**</span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a>ASR evolution from GMM-HMM (classical statistical approach) through DNN-HMM (~30% error reduction with deep feedforward networks) to end-to-end systems using RNNs/LSTMs with CTC. CNNs treat spectrograms as 2D structures for frequency-invariant modeling. Modern systems learn direct acoustic-to-text mappings without forced alignment, integrating joint acoustic-phonetic modeling and hierarchical representations.</span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a>**[Chapter 12.4: NLP Applications](nlp-applications.qmd)**</span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a>N-gram models compute conditional probabilities over fixed contexts but suffer from sparsity and exponential growth. Neural language models use word embeddings to map discrete tokens into continuous space, enabling generalization across semantically similar words. High-dimensional vocabulary outputs require optimization: short lists partition frequent/rare words, hierarchical softmax reduces complexity to O(log|V|), and importance sampling approximates gradients. Attention mechanisms dynamically focus on relevant input positions, forming weighted context vectors that alleviate fixed-size representation bottlenecks in seq2seq tasks.</span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a>::: {.content-card}</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>**[Chapter 12.5: Other Applications](other-applications.qmd)**</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a>Collaborative filtering uses matrix factorization to learn latent user and item embeddings, decomposing ratings into user bias, item bias, and personalized interaction. Cold-start problems require side information. Recommendation systems face exploration-exploitation tradeoffs modeled as contextual bandits. Knowledge graphs represent facts as (subject, relation, object) triples; deep learning maps entities and relations to continuous embeddings for link prediction and reasoning. Evaluation challenges arise from open-world assumptions where unseen facts may be missing rather than false.</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>:::</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>