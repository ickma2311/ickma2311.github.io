<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chao Ma">
<meta name="dcterms.date" content="2025-09-29">

<title>Deep Learning Book 6.3: Hidden Units and Activation Functions – ∇ ickma.dev</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.svg" rel="icon" type="image/svg+xml">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">∇ ickma.dev</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-deep-learning-book" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Deep Learning Book</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-deep-learning-book">    
        <li>
    <a class="dropdown-item" href="../ML/xor-deep-learning.html">
 <span class="dropdown-text">Chapter 6.1: XOR Problem</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/likelihood-loss-functions.html">
 <span class="dropdown-text">Chapter 6.2: Loss Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/activation-functions.html">
 <span class="dropdown-text">Chapter 6.3: Activation Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/architecture-design.html">
 <span class="dropdown-text">Chapter 6.4: Architecture Design</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/backpropagation.html">
 <span class="dropdown-text">Chapter 6.5: Back-Propagation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/hessian-prerequisites.html">
 <span class="dropdown-text">Chapter 7 Prerequisites: Hessian Matrix</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/l2-regularization.html">
 <span class="dropdown-text">Chapter 7.1.1: L2 Regularization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/l1-regularization.html">
 <span class="dropdown-text">Chapter 7.1.2: L1 Regularization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/constrained-optimization-regularization.html">
 <span class="dropdown-text">Chapter 7.2: Constrained Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/regularization-underconstrained.html">
 <span class="dropdown-text">Chapter 7.3: Under-Constrained Problems</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/dataset-augmentation.html">
 <span class="dropdown-text">Chapter 7.4: Dataset Augmentation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/noise-robustness.html">
 <span class="dropdown-text">Chapter 7.5: Noise Robustness</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/semi-supervised-learning.html">
 <span class="dropdown-text">Chapter 7.6: Semi-Supervised Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/multi-task-learning.html">
 <span class="dropdown-text">Chapter 7.7: Multi-Task Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/early-stopping.html">
 <span class="dropdown-text">Chapter 7.8: Early Stopping</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/parameter-tying-sharing.html">
 <span class="dropdown-text">Chapter 7.9: Parameter Tying &amp; Sharing</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-math" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Math</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-math">    
        <li class="dropdown-header">Reflections &amp; Synthesis</li>
        <li>
    <a class="dropdown-item" href="../Math/reflections/mit1806-invertibility-connections.html">
 <span class="dropdown-text">Invertibility Connections</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">MIT 18.06SC Linear Algebra</li>
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture1-geometry.html">
 <span class="dropdown-text">Lecture 1: Geometry</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture2-elimination.html">
 <span class="dropdown-text">Lecture 2: Elimination</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture3-multiplication.html">
 <span class="dropdown-text">Lecture 3: Multiplication</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture4-lu-decomposition.html">
 <span class="dropdown-text">Lecture 4: LU Decomposition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-permutations.html">
 <span class="dropdown-text">Lecture 5.1: Permutations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-2-transpose.html">
 <span class="dropdown-text">Lecture 5.2: Transpose</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture5-3-spaces.html">
 <span class="dropdown-text">Lecture 5.3: Spaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture6-column-null-space.html">
 <span class="dropdown-text">Lecture 6: Column &amp; Null Space</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture7-solving-ax-0.html">
 <span class="dropdown-text">Lecture 7: Solving Ax=0</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture8-solving-ax-b.html">
 <span class="dropdown-text">Lecture 8: Solving Ax=b</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.html">
 <span class="dropdown-text">Lecture 9: Independence, Basis, Dimension</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture10-four-subspaces.html">
 <span class="dropdown-text">Lecture 10: Four Fundamental Subspaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture11-matrix-spaces.html">
 <span class="dropdown-text">Lecture 11: Matrix Spaces</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture12-graphs-networks.html">
 <span class="dropdown-text">Lecture 12: Graphs and Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Math/MIT18.06/mit1806-lecture13-quiz-review.html">
 <span class="dropdown-text">Lecture 13: Quiz 1 Review</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-more" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">More</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-more">    
        <li>
    <a class="dropdown-item" href="../ML/kmeans.html">
 <span class="dropdown-text">K-Means Clustering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/logistic_regression.html">
 <span class="dropdown-text">Logistic Regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/axis.html">
 <span class="dropdown-text">Axis Operations</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../Algorithm/dp_regex.html">
 <span class="dropdown-text">DP Regex</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#why-activation-functions-matter" id="toc-why-activation-functions-matter" class="nav-link active" data-scroll-target="#why-activation-functions-matter">Why Activation Functions Matter</a></li>
  <li><a href="#exploring-activation-functions-shape-and-derivatives" id="toc-exploring-activation-functions-shape-and-derivatives" class="nav-link" data-scroll-target="#exploring-activation-functions-shape-and-derivatives">🎯 Exploring Activation Functions: Shape and Derivatives</a>
  <ul class="collapse">
  <li><a href="#define-activation-functions" id="toc-define-activation-functions" class="nav-link" data-scroll-target="#define-activation-functions">Define Activation Functions</a></li>
  <li><a href="#plot-functions-and-derivatives" id="toc-plot-functions-and-derivatives" class="nav-link" data-scroll-target="#plot-functions-and-derivatives">Plot Functions and Derivatives</a></li>
  </ul></li>
  <li><a href="#the-dead-relu-problem-when-neurons-stop-learning" id="toc-the-dead-relu-problem-when-neurons-stop-learning" class="nav-link" data-scroll-target="#the-dead-relu-problem-when-neurons-stop-learning">The Dead ReLU Problem: When Neurons Stop Learning</a></li>
  <li><a href="#experiment-do-different-activations-make-a-difference" id="toc-experiment-do-different-activations-make-a-difference" class="nav-link" data-scroll-target="#experiment-do-different-activations-make-a-difference">Experiment: Do Different Activations Make a Difference?</a>
  <ul class="collapse">
  <li><a href="#generate-data" id="toc-generate-data" class="nav-link" data-scroll-target="#generate-data">Generate Data</a></li>
  <li><a href="#create-and-train-models" id="toc-create-and-train-models" class="nav-link" data-scroll-target="#create-and-train-models">Create and Train Models</a></li>
  <li><a href="#compare-learning-curves" id="toc-compare-learning-curves" class="nav-link" data-scroll-target="#compare-learning-curves">Compare Learning Curves</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Deep Learning Book 6.3: Hidden Units and Activation Functions</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">Activation Functions</div>
    <div class="quarto-category">Neural Networks</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Chao Ma </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 29, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><em>This exploration of Deep Learning Chapter 6.3 reveals how activation functions shape the behavior of hidden units in neural networks - and why choosing the right one matters.</em></p>
<p>📓 <strong>For the complete implementation with additional exercises</strong>, see the <a href="https://github.com/ickma2311/foundations/blob/main/deep_learning/chapter6/6.3/exercises.ipynb">notebook on GitHub</a>.</p>
<p>📚 <strong>For theoretical background and summary</strong>, see the <a href="https://github.com/ickma2311/foundations/blob/main/deep_learning/chapter6/6.3/hidden_units_summary.md">chapter summary</a>.</p>
<section id="why-activation-functions-matter" class="level2">
<h2 class="anchored" data-anchor-id="why-activation-functions-matter">Why Activation Functions Matter</h2>
<p>Linear transformations alone can only represent linear relationships. No matter how many layers you stack, <span class="math inline">\(W_3(W_2(W_1x))\)</span> is still just a linear function. Activation functions introduce the non-linearity that makes deep learning powerful.</p>
<p>But <strong>which activation function should you use?</strong> The answer depends on understanding their mathematical properties and how they affect gradient flow during training.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 27%">
<col style="width: 27%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Activation</strong></th>
<th><strong>Output Range</strong></th>
<th><strong>Key Property</strong></th>
<th><strong>Best For</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ReLU</td>
<td><span class="math inline">\([0, \infty)\)</span></td>
<td>Zero for negatives</td>
<td>Hidden layers (default choice)</td>
</tr>
<tr class="even">
<td>Sigmoid</td>
<td><span class="math inline">\((0, 1)\)</span></td>
<td>Squashing, smooth</td>
<td>Binary classification output</td>
</tr>
<tr class="odd">
<td>Tanh</td>
<td><span class="math inline">\((-1, 1)\)</span></td>
<td>Zero-centered</td>
<td>Hidden layers (when centering helps)</td>
</tr>
</tbody>
</table>
</section>
<section id="exploring-activation-functions-shape-and-derivatives" class="level2">
<h2 class="anchored" data-anchor-id="exploring-activation-functions-shape-and-derivatives">🎯 Exploring Activation Functions: Shape and Derivatives</h2>
<div id="82a91355" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Show code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure plotting</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.facecolor'</span>] <span class="op">=</span> <span class="st">'white'</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'axes.facecolor'</span>] <span class="op">=</span> <span class="st">'white'</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'axes.grid'</span>] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'grid.alpha'</span>] <span class="op">=</span> <span class="fl">0.3</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<p>The behavior of an activation function is determined by two things: 1. <strong>Its shape</strong> - how it transforms inputs 2. <strong>Its derivative</strong> - how gradients flow backward during training</p>
<section id="define-activation-functions" class="level3">
<h3 class="anchored" data-anchor-id="define-activation-functions">Define Activation Functions</h3>
<div id="0bf8914f" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.clip(x, <span class="dv">0</span>, np.inf)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu_derivative(x):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.where(x <span class="op">&gt;</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid_derivative(x):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sigmoid(x) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> sigmoid(x))</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh(x):</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.tanh(x)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh_derivative(x):</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">-</span> np.tanh(x)<span class="op">**</span><span class="dv">2</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
</section>
<section id="plot-functions-and-derivatives" class="level3">
<h3 class="anchored" data-anchor-id="plot-functions-and-derivatives">Plot Functions and Derivatives</h3>
<div id="ea7ae157" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Show code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">1000</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">8</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Common Activation Functions and Their Derivatives'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># ReLU</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].plot(x, relu(x), linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'ReLU'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'f(x)'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].plot(x, relu_derivative(x), linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_title(<span class="st">'ReLU Derivative'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_ylabel(<span class="st">"f'(x)"</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Sigmoid</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].plot(x, sigmoid(x), linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">'Sigmoid'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].plot(x, sigmoid_derivative(x), linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">'Sigmoid Derivative'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Tanh</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">2</span>].plot(x, tanh(x), linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">2</span>].set_title(<span class="st">'Tanh'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">2</span>].plot(x, tanh_derivative(x), linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">2</span>].set_title(<span class="st">'Tanh Derivative'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">2</span>].set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="activation-functions_files/figure-html/cell-4-output-1.png" width="1525" height="756" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Key observations:</strong></p>
<ul>
<li><strong>ReLU</strong>: <span class="math inline">\(f(x) = \max(0, x)\)</span> - Zero for negative inputs, identity for positive. Derivative is 0 or 1 (simple!).</li>
<li><strong>Sigmoid</strong>: <span class="math inline">\(f(x) = \frac{1}{1+e^{-x}}\)</span> - Squashes inputs to <span class="math inline">\((0, 1)\)</span>. Derivative peaks at 0, vanishes at extremes (gradient vanishing problem).</li>
<li><strong>Tanh</strong>: <span class="math inline">\(f(x) = \tanh(x)\)</span> - Similar to sigmoid but outputs in <span class="math inline">\((-1, 1)\)</span>. Zero-centered with stronger gradients than sigmoid.</li>
</ul>
</section>
</section>
<section id="the-dead-relu-problem-when-neurons-stop-learning" class="level2">
<h2 class="anchored" data-anchor-id="the-dead-relu-problem-when-neurons-stop-learning">The Dead ReLU Problem: When Neurons Stop Learning</h2>
<p>ReLU’s simplicity is its strength, but also its weakness. A ReLU neuron can “die” - permanently outputting zero and never learning again.</p>
<p><strong>Why does this happen?</strong></p>
<p>When a neuron’s pre-activation values are consistently negative (due to poor initialization, high learning rate, or bad gradients), ReLU outputs zero. Since the derivative is also zero for negative inputs, no gradient flows backward. The neuron is stuck forever.</p>
<div id="2d8c0ff4" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate input data</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1000</span>, <span class="dv">10</span>)  <span class="co"># 1000 samples, 10 features</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> nn.Linear(<span class="dv">10</span>, <span class="dv">5</span>)   <span class="co"># 5 hidden units</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set bias to large negative values to "kill" neurons</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    linear.bias.fill_(<span class="op">-</span><span class="fl">10.0</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>pre_activation <span class="op">=</span> linear(x)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>post_activation <span class="op">=</span> torch.relu(pre_activation)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate statistics</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>dead_percentage <span class="op">=</span> (post_activation <span class="op">==</span> <span class="dv">0</span>).<span class="bu">float</span>().mean() <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Percentage of dead neurons: </span><span class="sc">{</span>dead_percentage<span class="sc">:.2f}</span><span class="ss">%</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Display table showing ReLU input vs output</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ReLU Input vs Output (first 10 samples, neuron 0):"</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Sample'</span><span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Pre-Activation'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Post-Activation'</span><span class="sc">:&lt;20}</span><span class="ss">"</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    pre_val <span class="op">=</span> pre_activation[i, <span class="dv">0</span>].item()</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    post_val <span class="op">=</span> post_activation[i, <span class="dv">0</span>].item()</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span>pre_val<span class="sc">:&lt;20.4f}</span><span class="ss"> </span><span class="sc">{</span>post_val<span class="sc">:&lt;20.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Observation: All negative inputs become 0 after ReLU → Dead neuron!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Percentage of dead neurons: 100.00%

ReLU Input vs Output (first 10 samples, neuron 0):
--------------------------------------------------
Sample     Pre-Activation       Post-Activation     
--------------------------------------------------
0          -9.7837              0.0000              
1          -10.0322             0.0000              
2          -10.4466             0.0000              
3          -10.3243             0.0000              
4          -10.5448             0.0000              
5          -9.7712              0.0000              
6          -10.8104             0.0000              
7          -11.3418             0.0000              
8          -9.8559              0.0000              
9          -8.6873              0.0000              

Observation: All negative inputs become 0 after ReLU → Dead neuron!</code></pre>
</div>
</div>
<p>With a large negative bias, every input becomes negative after the linear transformation. ReLU zeros them all out. The gradient is zero everywhere. The neuron never updates. It’s dead.</p>
</section>
<section id="experiment-do-different-activations-make-a-difference" class="level2">
<h2 class="anchored" data-anchor-id="experiment-do-different-activations-make-a-difference">Experiment: Do Different Activations Make a Difference?</h2>
<p>Theory is nice, but let’s see activation functions in action. We’ll train three identical networks with different activations on a simple regression task: <span class="math inline">\(y = \sin(x) + x^2 + 1\)</span>.</p>
<section id="generate-data" class="level3">
<h3 class="anchored" data-anchor-id="generate-data">Generate Data</h3>
<div id="5a85d083" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Show code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training data</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> np.random.rand(<span class="dv">200</span>, <span class="dv">1</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> np.sin(x_train) <span class="op">+</span> np.power(x_train, <span class="dv">2</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Test data</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> np.random.rand(<span class="dv">50</span>, <span class="dv">1</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> np.sin(x_test) <span class="op">+</span> np.power(x_test, <span class="dv">2</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to PyTorch tensors</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>x_train_tensor <span class="op">=</span> torch.FloatTensor(x_train)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>y_train_tensor <span class="op">=</span> torch.FloatTensor(y_train)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>x_test_tensor <span class="op">=</span> torch.FloatTensor(x_test)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>y_test_tensor <span class="op">=</span> torch.FloatTensor(y_test)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
</section>
<section id="create-and-train-models" class="level3">
<h3 class="anchored" data-anchor-id="create-and-train-models">Create and Train Models</h3>
<div id="3c40c580" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Show code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_regression_model(activation_fn):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create a 2-layer network with specified activation"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">1</span>, <span class="dv">20</span>),</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        activation_fn,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">20</span>, <span class="dv">1</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create 3 models with different activations</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> {</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ReLU'</span>: create_regression_model(nn.ReLU()),</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sigmoid'</span>: create_regression_model(nn.Sigmoid()),</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Tanh'</span>: create_regression_model(nn.Tanh())</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Training configuration</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.MSELoss()</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Track metrics</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>loss_history <span class="op">=</span> {name: [] <span class="cf">for</span> name <span class="kw">in</span> models.keys()}</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>test_mse_history <span class="op">=</span> {name: [] <span class="cf">for</span> name <span class="kw">in</span> models.keys()}</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Train each model</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, model <span class="kw">in</span> models.items():</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(x_train_tensor)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(y_pred, y_train_tensor)</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        loss_history[name].append(loss.item())</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Evaluation on test set</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>            y_test_pred <span class="op">=</span> model(x_test_tensor)</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>            test_mse <span class="op">=</span> loss_fn(y_test_pred, y_test_tensor).item()</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>            test_mse_history[name].append(test_mse)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
</section>
<section id="compare-learning-curves" class="level3">
<h3 class="anchored" data-anchor-id="compare-learning-curves">Compare Learning Curves</h3>
<div id="d769ae6d" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Show code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> {<span class="st">'ReLU'</span>: <span class="st">'blue'</span>, <span class="st">'Sigmoid'</span>: <span class="st">'red'</span>, <span class="st">'Tanh'</span>: <span class="st">'green'</span>}</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training loss</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, losses <span class="kw">in</span> loss_history.items():</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].plot(losses, label<span class="op">=</span>name, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span>colors[name])</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Epoch'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Training Loss (MSE)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Training Loss Over Time'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yscale(<span class="st">'log'</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot test MSE</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, test_mse <span class="kw">in</span> test_mse_history.items():</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].plot(test_mse, label<span class="op">=</span>name, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span>colors[name])</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Epoch'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Test Loss (MSE)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Test Loss Over Time'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_yscale(<span class="st">'log'</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Print final metrics</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Final Metrics after </span><span class="sc">{}</span><span class="st"> epochs:"</span>.<span class="bu">format</span>(n_epochs))</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Activation'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Train Loss'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Test Loss'</span><span class="sc">:&lt;15}</span><span class="ss">"</span>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name <span class="kw">in</span> models.keys():</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> loss_history[name][<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> test_mse_history[name][<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>train_loss<span class="sc">:&lt;15.6f}</span><span class="ss"> </span><span class="sc">{</span>test_loss<span class="sc">:&lt;15.6f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="activation-functions_files/figure-html/cell-8-output-1.png" width="1334" height="470" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Final Metrics after 100 epochs:
------------------------------------------------------------
Activation      Train Loss      Test Loss      
------------------------------------------------------------
ReLU            0.007420        0.008211       
Sigmoid         0.227441        0.247947       
Tanh            0.035384        0.038743       </code></pre>
</div>
</div>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Deep Learning Book 6.3: Hidden Units and Activation Functions"</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Chao Ma"</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2025-09-29"</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> ["Deep Learning", "Activation Functions", "Neural Networks"]</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="an">code-summary:</span><span class="co"> "Show code"</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>*This exploration of Deep Learning Chapter 6.3 reveals how activation functions shape the behavior of hidden units in neural networks - and why choosing the right one matters.*</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>📓 **For the complete implementation with additional exercises**, see the <span class="co">[</span><span class="ot">notebook on GitHub</span><span class="co">](https://github.com/ickma2311/foundations/blob/main/deep_learning/chapter6/6.3/exercises.ipynb)</span>.</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>📚 **For theoretical background and summary**, see the <span class="co">[</span><span class="ot">chapter summary</span><span class="co">](https://github.com/ickma2311/foundations/blob/main/deep_learning/chapter6/6.3/hidden_units_summary.md)</span>.</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Activation Functions Matter</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>Linear transformations alone can only represent linear relationships. No matter how many layers you stack, $W_3(W_2(W_1x))$ is still just a linear function. Activation functions introduce the non-linearity that makes deep learning powerful.</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>But **which activation function should you use?** The answer depends on understanding their mathematical properties and how they affect gradient flow during training.</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Activation** | **Output Range** | **Key Property** | **Best For** <span class="pp">|</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="pp">|----------------|------------------|------------------|--------------|</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ReLU <span class="pp">|</span> $[0, \infty)$ <span class="pp">|</span> Zero for negatives <span class="pp">|</span> Hidden layers (default choice) <span class="pp">|</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Sigmoid <span class="pp">|</span> $(0, 1)$ <span class="pp">|</span> Squashing, smooth <span class="pp">|</span> Binary classification output <span class="pp">|</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Tanh <span class="pp">|</span> $(-1, 1)$ <span class="pp">|</span> Zero-centered <span class="pp">|</span> Hidden layers (when centering helps) <span class="pp">|</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="fu">## 🎯 Exploring Activation Functions: Shape and Derivatives</span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure plotting</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.facecolor'</span>] <span class="op">=</span> <span class="st">'white'</span></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'axes.facecolor'</span>] <span class="op">=</span> <span class="st">'white'</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'axes.grid'</span>] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'grid.alpha'</span>] <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>The behavior of an activation function is determined by two things:</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Its shape** - how it transforms inputs</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Its derivative** - how gradients flow backward during training</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a><span class="fu">### Define Activation Functions</span></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x):</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.clip(x, <span class="dv">0</span>, np.inf)</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu_derivative(x):</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.where(x <span class="op">&gt;</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid_derivative(x):</span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sigmoid(x) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> sigmoid(x))</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh(x):</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.tanh(x)</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh_derivative(x):</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">-</span> np.tanh(x)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a><span class="fu">### Plot Functions and Derivatives</span></span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">1000</span>)</span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">8</span>))</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Common Activation Functions and Their Derivatives'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a><span class="co"># ReLU</span></span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].plot(x, relu(x), linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'ReLU'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'f(x)'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].plot(x, relu_derivative(x), linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_title(<span class="st">'ReLU Derivative'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_ylabel(<span class="st">"f'(x)"</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a><span class="co"># Sigmoid</span></span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].plot(x, sigmoid(x), linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">'Sigmoid'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].plot(x, sigmoid_derivative(x), linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">'Sigmoid Derivative'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a><span class="co"># Tanh</span></span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">2</span>].plot(x, tanh(x), linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">2</span>].set_title(<span class="st">'Tanh'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">2</span>].plot(x, tanh_derivative(x), linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">2</span>].set_title(<span class="st">'Tanh Derivative'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">2</span>].set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a>**Key observations:**</span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**ReLU**: $f(x) = \max(0, x)$ - Zero for negative inputs, identity for positive. Derivative is 0 or 1 (simple!).</span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sigmoid**: $f(x) = \frac{1}{1+e^{-x}}$ - Squashes inputs to $(0, 1)$. Derivative peaks at 0, vanishes at extremes (gradient vanishing problem).</span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Tanh**: $f(x) = \tanh(x)$ - Similar to sigmoid but outputs in $(-1, 1)$. Zero-centered with stronger gradients than sigmoid.</span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Dead ReLU Problem: When Neurons Stop Learning</span></span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a>ReLU's simplicity is its strength, but also its weakness. A ReLU neuron can "die" - permanently outputting zero and never learning again.</span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a>**Why does this happen?**</span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a>When a neuron's pre-activation values are consistently negative (due to poor initialization, high learning rate, or bad gradients), ReLU outputs zero. Since the derivative is also zero for negative inputs, no gradient flows backward. The neuron is stuck forever.</span>
<span id="cb10-127"><a href="#cb10-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-130"><a href="#cb10-130" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate input data</span></span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1000</span>, <span class="dv">10</span>)  <span class="co"># 1000 samples, 10 features</span></span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> nn.Linear(<span class="dv">10</span>, <span class="dv">5</span>)   <span class="co"># 5 hidden units</span></span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a><span class="co"># Set bias to large negative values to "kill" neurons</span></span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-137"><a href="#cb10-137" aria-hidden="true" tabindex="-1"></a>    linear.bias.fill_(<span class="op">-</span><span class="fl">10.0</span>)</span>
<span id="cb10-138"><a href="#cb10-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-139"><a href="#cb10-139" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass</span></span>
<span id="cb10-140"><a href="#cb10-140" aria-hidden="true" tabindex="-1"></a>pre_activation <span class="op">=</span> linear(x)</span>
<span id="cb10-141"><a href="#cb10-141" aria-hidden="true" tabindex="-1"></a>post_activation <span class="op">=</span> torch.relu(pre_activation)</span>
<span id="cb10-142"><a href="#cb10-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-143"><a href="#cb10-143" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate statistics</span></span>
<span id="cb10-144"><a href="#cb10-144" aria-hidden="true" tabindex="-1"></a>dead_percentage <span class="op">=</span> (post_activation <span class="op">==</span> <span class="dv">0</span>).<span class="bu">float</span>().mean() <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb10-145"><a href="#cb10-145" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Percentage of dead neurons: </span><span class="sc">{</span>dead_percentage<span class="sc">:.2f}</span><span class="ss">%</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb10-146"><a href="#cb10-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-147"><a href="#cb10-147" aria-hidden="true" tabindex="-1"></a><span class="co"># Display table showing ReLU input vs output</span></span>
<span id="cb10-148"><a href="#cb10-148" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ReLU Input vs Output (first 10 samples, neuron 0):"</span>)</span>
<span id="cb10-149"><a href="#cb10-149" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb10-150"><a href="#cb10-150" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Sample'</span><span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Pre-Activation'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Post-Activation'</span><span class="sc">:&lt;20}</span><span class="ss">"</span>)</span>
<span id="cb10-151"><a href="#cb10-151" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb10-152"><a href="#cb10-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-153"><a href="#cb10-153" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb10-154"><a href="#cb10-154" aria-hidden="true" tabindex="-1"></a>    pre_val <span class="op">=</span> pre_activation[i, <span class="dv">0</span>].item()</span>
<span id="cb10-155"><a href="#cb10-155" aria-hidden="true" tabindex="-1"></a>    post_val <span class="op">=</span> post_activation[i, <span class="dv">0</span>].item()</span>
<span id="cb10-156"><a href="#cb10-156" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span>pre_val<span class="sc">:&lt;20.4f}</span><span class="ss"> </span><span class="sc">{</span>post_val<span class="sc">:&lt;20.4f}</span><span class="ss">"</span>)</span>
<span id="cb10-157"><a href="#cb10-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-158"><a href="#cb10-158" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Observation: All negative inputs become 0 after ReLU → Dead neuron!"</span>)</span>
<span id="cb10-159"><a href="#cb10-159" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-160"><a href="#cb10-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-161"><a href="#cb10-161" aria-hidden="true" tabindex="-1"></a>With a large negative bias, every input becomes negative after the linear transformation. ReLU zeros them all out. The gradient is zero everywhere. The neuron never updates. It's dead.</span>
<span id="cb10-162"><a href="#cb10-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-163"><a href="#cb10-163" aria-hidden="true" tabindex="-1"></a><span class="fu">## Experiment: Do Different Activations Make a Difference?</span></span>
<span id="cb10-164"><a href="#cb10-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-165"><a href="#cb10-165" aria-hidden="true" tabindex="-1"></a>Theory is nice, but let's see activation functions in action. We'll train three identical networks with different activations on a simple regression task: $y = \sin(x) + x^2 + 1$.</span>
<span id="cb10-166"><a href="#cb10-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-167"><a href="#cb10-167" aria-hidden="true" tabindex="-1"></a><span class="fu">### Generate Data</span></span>
<span id="cb10-168"><a href="#cb10-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-171"><a href="#cb10-171" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-172"><a href="#cb10-172" aria-hidden="true" tabindex="-1"></a><span class="co"># Training data</span></span>
<span id="cb10-173"><a href="#cb10-173" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> np.random.rand(<span class="dv">200</span>, <span class="dv">1</span>)</span>
<span id="cb10-174"><a href="#cb10-174" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> np.sin(x_train) <span class="op">+</span> np.power(x_train, <span class="dv">2</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb10-175"><a href="#cb10-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-176"><a href="#cb10-176" aria-hidden="true" tabindex="-1"></a><span class="co"># Test data</span></span>
<span id="cb10-177"><a href="#cb10-177" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> np.random.rand(<span class="dv">50</span>, <span class="dv">1</span>)</span>
<span id="cb10-178"><a href="#cb10-178" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> np.sin(x_test) <span class="op">+</span> np.power(x_test, <span class="dv">2</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb10-179"><a href="#cb10-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-180"><a href="#cb10-180" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to PyTorch tensors</span></span>
<span id="cb10-181"><a href="#cb10-181" aria-hidden="true" tabindex="-1"></a>x_train_tensor <span class="op">=</span> torch.FloatTensor(x_train)</span>
<span id="cb10-182"><a href="#cb10-182" aria-hidden="true" tabindex="-1"></a>y_train_tensor <span class="op">=</span> torch.FloatTensor(y_train)</span>
<span id="cb10-183"><a href="#cb10-183" aria-hidden="true" tabindex="-1"></a>x_test_tensor <span class="op">=</span> torch.FloatTensor(x_test)</span>
<span id="cb10-184"><a href="#cb10-184" aria-hidden="true" tabindex="-1"></a>y_test_tensor <span class="op">=</span> torch.FloatTensor(y_test)</span>
<span id="cb10-185"><a href="#cb10-185" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-186"><a href="#cb10-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-187"><a href="#cb10-187" aria-hidden="true" tabindex="-1"></a><span class="fu">### Create and Train Models</span></span>
<span id="cb10-188"><a href="#cb10-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-191"><a href="#cb10-191" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-192"><a href="#cb10-192" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_regression_model(activation_fn):</span>
<span id="cb10-193"><a href="#cb10-193" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create a 2-layer network with specified activation"""</span></span>
<span id="cb10-194"><a href="#cb10-194" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(</span>
<span id="cb10-195"><a href="#cb10-195" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">1</span>, <span class="dv">20</span>),</span>
<span id="cb10-196"><a href="#cb10-196" aria-hidden="true" tabindex="-1"></a>        activation_fn,</span>
<span id="cb10-197"><a href="#cb10-197" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">20</span>, <span class="dv">1</span>)</span>
<span id="cb10-198"><a href="#cb10-198" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-199"><a href="#cb10-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-200"><a href="#cb10-200" aria-hidden="true" tabindex="-1"></a><span class="co"># Create 3 models with different activations</span></span>
<span id="cb10-201"><a href="#cb10-201" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> {</span>
<span id="cb10-202"><a href="#cb10-202" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ReLU'</span>: create_regression_model(nn.ReLU()),</span>
<span id="cb10-203"><a href="#cb10-203" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sigmoid'</span>: create_regression_model(nn.Sigmoid()),</span>
<span id="cb10-204"><a href="#cb10-204" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Tanh'</span>: create_regression_model(nn.Tanh())</span>
<span id="cb10-205"><a href="#cb10-205" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-206"><a href="#cb10-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-207"><a href="#cb10-207" aria-hidden="true" tabindex="-1"></a><span class="co"># Training configuration</span></span>
<span id="cb10-208"><a href="#cb10-208" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb10-209"><a href="#cb10-209" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb10-210"><a href="#cb10-210" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.MSELoss()</span>
<span id="cb10-211"><a href="#cb10-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-212"><a href="#cb10-212" aria-hidden="true" tabindex="-1"></a><span class="co"># Track metrics</span></span>
<span id="cb10-213"><a href="#cb10-213" aria-hidden="true" tabindex="-1"></a>loss_history <span class="op">=</span> {name: [] <span class="cf">for</span> name <span class="kw">in</span> models.keys()}</span>
<span id="cb10-214"><a href="#cb10-214" aria-hidden="true" tabindex="-1"></a>test_mse_history <span class="op">=</span> {name: [] <span class="cf">for</span> name <span class="kw">in</span> models.keys()}</span>
<span id="cb10-215"><a href="#cb10-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-216"><a href="#cb10-216" aria-hidden="true" tabindex="-1"></a><span class="co"># Train each model</span></span>
<span id="cb10-217"><a href="#cb10-217" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, model <span class="kw">in</span> models.items():</span>
<span id="cb10-218"><a href="#cb10-218" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb10-219"><a href="#cb10-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-220"><a href="#cb10-220" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb10-221"><a href="#cb10-221" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training</span></span>
<span id="cb10-222"><a href="#cb10-222" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb10-223"><a href="#cb10-223" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(x_train_tensor)</span>
<span id="cb10-224"><a href="#cb10-224" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(y_pred, y_train_tensor)</span>
<span id="cb10-225"><a href="#cb10-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-226"><a href="#cb10-226" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb10-227"><a href="#cb10-227" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb10-228"><a href="#cb10-228" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb10-229"><a href="#cb10-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-230"><a href="#cb10-230" aria-hidden="true" tabindex="-1"></a>        loss_history[name].append(loss.item())</span>
<span id="cb10-231"><a href="#cb10-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-232"><a href="#cb10-232" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Evaluation on test set</span></span>
<span id="cb10-233"><a href="#cb10-233" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb10-234"><a href="#cb10-234" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-235"><a href="#cb10-235" aria-hidden="true" tabindex="-1"></a>            y_test_pred <span class="op">=</span> model(x_test_tensor)</span>
<span id="cb10-236"><a href="#cb10-236" aria-hidden="true" tabindex="-1"></a>            test_mse <span class="op">=</span> loss_fn(y_test_pred, y_test_tensor).item()</span>
<span id="cb10-237"><a href="#cb10-237" aria-hidden="true" tabindex="-1"></a>            test_mse_history[name].append(test_mse)</span>
<span id="cb10-238"><a href="#cb10-238" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-239"><a href="#cb10-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-240"><a href="#cb10-240" aria-hidden="true" tabindex="-1"></a><span class="fu">### Compare Learning Curves</span></span>
<span id="cb10-241"><a href="#cb10-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-244"><a href="#cb10-244" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-245"><a href="#cb10-245" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb10-246"><a href="#cb10-246" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> {<span class="st">'ReLU'</span>: <span class="st">'blue'</span>, <span class="st">'Sigmoid'</span>: <span class="st">'red'</span>, <span class="st">'Tanh'</span>: <span class="st">'green'</span>}</span>
<span id="cb10-247"><a href="#cb10-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-248"><a href="#cb10-248" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training loss</span></span>
<span id="cb10-249"><a href="#cb10-249" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, losses <span class="kw">in</span> loss_history.items():</span>
<span id="cb10-250"><a href="#cb10-250" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].plot(losses, label<span class="op">=</span>name, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span>colors[name])</span>
<span id="cb10-251"><a href="#cb10-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-252"><a href="#cb10-252" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Epoch'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb10-253"><a href="#cb10-253" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Training Loss (MSE)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb10-254"><a href="#cb10-254" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Training Loss Over Time'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb10-255"><a href="#cb10-255" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb10-256"><a href="#cb10-256" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-257"><a href="#cb10-257" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yscale(<span class="st">'log'</span>)</span>
<span id="cb10-258"><a href="#cb10-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-259"><a href="#cb10-259" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot test MSE</span></span>
<span id="cb10-260"><a href="#cb10-260" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, test_mse <span class="kw">in</span> test_mse_history.items():</span>
<span id="cb10-261"><a href="#cb10-261" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].plot(test_mse, label<span class="op">=</span>name, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span>colors[name])</span>
<span id="cb10-262"><a href="#cb10-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-263"><a href="#cb10-263" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Epoch'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb10-264"><a href="#cb10-264" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Test Loss (MSE)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb10-265"><a href="#cb10-265" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Test Loss Over Time'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb10-266"><a href="#cb10-266" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb10-267"><a href="#cb10-267" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-268"><a href="#cb10-268" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_yscale(<span class="st">'log'</span>)</span>
<span id="cb10-269"><a href="#cb10-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-270"><a href="#cb10-270" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-271"><a href="#cb10-271" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-272"><a href="#cb10-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-273"><a href="#cb10-273" aria-hidden="true" tabindex="-1"></a><span class="co"># Print final metrics</span></span>
<span id="cb10-274"><a href="#cb10-274" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Final Metrics after </span><span class="sc">{}</span><span class="st"> epochs:"</span>.<span class="bu">format</span>(n_epochs))</span>
<span id="cb10-275"><a href="#cb10-275" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb10-276"><a href="#cb10-276" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Activation'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Train Loss'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Test Loss'</span><span class="sc">:&lt;15}</span><span class="ss">"</span>)</span>
<span id="cb10-277"><a href="#cb10-277" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb10-278"><a href="#cb10-278" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name <span class="kw">in</span> models.keys():</span>
<span id="cb10-279"><a href="#cb10-279" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> loss_history[name][<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb10-280"><a href="#cb10-280" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> test_mse_history[name][<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb10-281"><a href="#cb10-281" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>train_loss<span class="sc">:&lt;15.6f}</span><span class="ss"> </span><span class="sc">{</span>test_loss<span class="sc">:&lt;15.6f}</span><span class="ss">"</span>)</span>
<span id="cb10-282"><a href="#cb10-282" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>