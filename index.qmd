---
title: "ickma.dev"
---

::: {.hero-banner}
# ickma.dev ‚Äî Notes on Deep Learning and Math

A growing collection of structured study notes and visual explanations ‚Äî
written for clarity, reproducibility, and long-term memory.
:::

## Latest Updates

::: {.section-block}
### ‚àá Deep Learning Book <span class="section-count">56 chapters</span>

My notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.

::: {.content-grid}

::: {.content-card}
**[Chapter 12.5: Other Applications](ML/other-applications.qmd)**
Collaborative filtering uses matrix factorization to learn latent user and item embeddings, decomposing ratings into user bias, item bias, and personalized interaction. Cold-start problems require side information. Recommendation systems face exploration-exploitation tradeoffs modeled as contextual bandits. Knowledge graphs represent facts as (subject, relation, object) triples; deep learning maps entities and relations to continuous embeddings for link prediction and reasoning. Evaluation challenges arise from open-world assumptions where unseen facts may be missing rather than false.
:::

::: {.content-card}
**[Chapter 12.4: NLP Applications](ML/nlp-applications.qmd)**
N-gram models compute conditional probabilities over fixed contexts but suffer from sparsity and exponential growth. Neural language models use word embeddings to map discrete tokens into continuous space, enabling generalization across semantically similar words. High-dimensional vocabulary outputs require optimization: short lists partition frequent/rare words, hierarchical softmax reduces complexity to O(log|V|), and importance sampling approximates gradients. Attention mechanisms dynamically focus on relevant input positions, forming weighted context vectors that alleviate fixed-size representation bottlenecks in seq2seq tasks.
:::

::: {.content-card}
**[Chapter 12.3: Automatic Speech Recognition](ML/automatic-speech-recognition.qmd)**
ASR evolution from GMM-HMM (classical statistical approach) through DNN-HMM (~30% error reduction with deep feedforward networks) to end-to-end systems using RNNs/LSTMs with CTC. CNNs treat spectrograms as 2D structures for frequency-invariant modeling. Modern systems learn direct acoustic-to-text mappings without forced alignment, integrating joint acoustic-phonetic modeling and hierarchical representations.
:::

::: {.content-card}
**[Chapter 12.2: Image Preprocessing and Normalization](ML/image-preprocessing-normalization.qmd)**
Preprocessing images through normalization (scaling to [0,1] or [-1,1]) and data augmentation improves training stability and generalization. Global Contrast Normalization (GCN) removes global lighting variations by centering and L2-normalizing images. Local Contrast Normalization (LCN) enhances local structures by normalizing within spatial neighborhoods. Modern networks rely on batch normalization, but explicit contrast normalization remains valuable for challenging datasets.
:::

:::

::: {.see-all-button}
[See all Deep Learning chapters ‚Üí](ML/deep-learning-book.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê MIT 18.06SC Linear Algebra <span class="section-count">36 lectures</span>

My journey through MIT's Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.

::: {.content-grid}

::: {.content-card}
**[Lecture 27: Positive Definite Matrices and Minima](Math/MIT18.06/mit1806-lecture27-positive-definite-minima.qmd)**
Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.
:::

::: {.content-card}
**[Lecture 26: Complex Matrices and Fast Fourier Transform](Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.qmd)**
Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).
:::

::: {.content-card}
**[Lecture 28: Similar Matrices and Jordan Form](Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.qmd)**
When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.
:::

::: {.content-card}
**[Lecture 25: Symmetric Matrices and Positive Definiteness](Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.qmd)**
The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.
:::

:::

::: {.see-all-button}
[See all MIT 18.06SC lectures ‚Üí](Math/MIT18.06/lectures.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê MIT 18.065: Linear Algebra Applications <span class="section-count">4 lectures</span>

My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning‚Äîexploring how linear algebra powers modern applications.

::: {.content-grid}

::: {.content-card}
**[Lecture 2: Multiplying and Factoring Matrices](Math/MIT18.065/mit18065-lecture2-multiplying-factoring.qmd)**
Elimination as rank-1 updates, $LU$ and $QR$ factorizations, spectral decomposition $S=Q\Lambda Q^{\top}$, diagonalization $A=X\Lambda X^{-1}$, and SVD‚Äîplus the four fundamental subspaces.
:::

::: {.content-card}
**[Lecture 1: The Column Space of A Contains All Vectors Ax](Math/MIT18.065/mit18065-lecture1-column-space.qmd)**
Every vector $Ax$ is a linear combination of the columns of $A$, so the column space $C(A)$ contains all possible outputs. Rank counts independent columns, and CR factorization reveals a basis for the column and row spaces.
:::

::: {.content-card}
**[Lecture 9: Four Ways to Solve Least Squares Problems](Math/MIT18.065/mit18065-lecture9-least-squares.qmd)**
Four equivalent methods for solving $Ax = b$ when $A$ has no inverse: pseudo-inverse, normal equations, algebraic minimization, and geometric projection‚Äîall converging to the same optimal solution.
:::

::: {.content-card}
**[Lecture 8: Norms of Vectors and Matrices](Math/MIT18.065/mit18065-lecture8-norms.qmd)**
Understanding vector p-norms ($\|v\|_p$) and when they satisfy the triangle inequality (only for $p \geq 1$). The "$\frac{1}{2}$-norm" creates non-convex unit balls for strong sparsity.
:::

:::

::: {.see-all-button}
[See all MIT 18.065 lectures ‚Üí](Math/MIT18.065/lectures.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê Stanford EE 364A: Convex Optimization <span class="section-count">7 lectures</span>

My notes from Stanford EE 364A: Convex Optimization‚Äîtheory and applications of optimization problems.

::: {.content-grid}

::: {.content-card}
**[Chapter 4.1: Optimization Problems](Math/EE364A/ee364a-lecture6-optimization-problems.qmd)**
Basic terminology (decision variables, objective, constraints, domain, feasibility, optimal values, local optimality), standard form conversion, equivalent problems (change of variables, slack variables, constraint elimination, epigraph form), and parameter vs oracle problem descriptions.
:::

::: {.content-card}
**[Lecture 5.2: Monotonicity with Generalized Inequalities](Math/EE364A/ee364a-lecture5-part2-monotonicity.qmd)**
K-nondecreasing functions satisfy $x \preceq_K y \Rightarrow f(x)\le f(y)$. Gradient condition: $\nabla f(x) \succeq_{K^*} 0$ (dual inequality). Matrix monotone examples: $\mathrm{tr}(WX)$, $\mathrm{det}X$. K-convexity extends convexity to generalized inequalities with dual characterization and composition theorems.
:::

::: {.content-card}
**[Lecture 5.1: Log-Concave and Log-Convex Functions](Math/EE364A/ee364a-lecture5-part1-log-concave-convex.qmd)**
Log-concave functions satisfy $f(\theta x+(1-\theta)y)\ge f(x)^\theta f(y)^{1-\theta}$. Powers $x^a$ are log-concave for $a \ge 0$ and log-convex for $a \le 0$. Key properties: products preserve log-concavity, but sums do not. Integration of log-concave functions preserves log-concavity.
:::

::: {.content-card}
**[Lecture 4 Part 2: Conjugate and Quasiconvex Functions](Math/EE364A/ee364a-lecture4-part2-conjugate-quasiconvex.qmd)**
Conjugate functions $f^*(y) = \sup_x (y^\top x - f(x))$ are always convex, with examples including negative logarithm and quadratic functions. Quasiconvex functions have convex sublevel sets with modified Jensen inequality $f(\theta x + (1-\theta)y) \leq \max\{f(x), f(y)\}$. Examples include linear-fractional functions and distance ratios.
:::

:::

::: {.see-all-button}
[See all EE 364A lectures ‚Üí](Math/EE364A/lectures.qmd){.btn .btn-primary}
:::

:::

---

::: {.footer-section}
## More Topics

::: {.footer-grid}

::: {.footer-card}
### Machine Learning
- [K-Means Clustering](ML/k_means_clustering.qmd)
- [Logistic Regression](ML/logistic_regression.qmd)
- [Axis Operations](ML/axis.qmd)
:::

::: {.footer-card}
### Algorithms
- [DP Regex](Algorithm/dp_regex.qmd)
:::

:::
:::
