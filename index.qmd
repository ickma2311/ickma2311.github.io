---
title: "ickma.dev"
---

# Welcome to ickma.dev

My learning notes and thoughts on math and machine learning.

Currently reading the **Deep Learning book**.

## Deep Learning Book

### [Chapter 6.1: XOR Problem & ReLU Networks](ML/xor-deep-learning.ipynb)
How ReLU solves problems that linear models cannot handle.

### [Chapter 6.2: Likelihood-Based Loss Functions](ML/likelihood-loss-functions.ipynb)
The mathematical connection between probabilistic models and loss functions.

### [Chapter 6.3: Hidden Units and Activation Functions](ML/activation-functions.qmd)
Exploring activation functions and their impact on neural network learning.

### [Chapter 6.4: Architecture Design - Depth vs Width](ML/architecture-design.qmd)
How depth enables hierarchical feature reuse and exponential expressiveness with fewer parameters.

### [Chapter 6.5: Back-Propagation and Other Differentiation Algorithms](ML/backpropagation.qmd)
The algorithm that makes training deep networks computationally feasible through efficient gradient computation.

### [Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature](ML/hessian-prerequisites.ipynb)
Essential second-order calculus concepts needed before Chapter 7 on optimization algorithms.

### [Chapter 7.1.1: L2 Regularization (Parameter Norm Penalty)](ML/l2-regularization.qmd)
How L2 regularization shrinks weights based on Hessian eigenvalues, preserving important directions while penalizing less sensitive ones.

## Mathematics

### [MIT 18.06SC Lecture 1: The Geometry of Linear Equations](Math/mit1806-lecture1-geometry.ipynb)
Two powerful perspectives that reveal the hidden beauty of linear systems: row picture vs column picture.

### [MIT 18.06SC Lecture 2: Elimination with Matrices](Math/mit1806-lecture2-elimination.qmd)
The systematic algorithm that transforms linear systems into upper triangular form for easy solution.

### [MIT 18.06SC Lecture 3: Matrix Multiplication and Inverse](Math/mit1806-lecture3-multiplication.qmd)
Five different perspectives on matrix multiplication, from element-wise computation to rank-1 decomposition, plus understanding when matrices can't be inverted.

### [MIT 18.06SC Lecture 4: LU Decomposition](Math/mit1806-lecture4-lu-decomposition.ipynb)
Factoring matrices into Lower Ã— Upper triangular form: the foundation of efficient numerical linear algebra and solving multiple systems with the same matrix.

### [MIT 18.06SC Lecture 5.1: Permutation Matrices](Math/mit1806-lecture5-permutations.qmd)
Permutation matrices reorder rows and columns using a simple structure of 0s and 1s.

### [MIT 18.06SC Lecture 5.2: Transpose](Math/mit1806-lecture5-2-transpose.qmd)
The transpose operation switches rows to columns, creating symmetric matrices.

### [MIT 18.06SC Lecture 5.3: Vector Spaces](Math/mit1806-lecture5-3-spaces.qmd)
Vector spaces and subspaces: closed under addition and scalar multiplication.

## More

### Machine Learning
- [K-Means Clustering](ML/kmeans.ipynb)
- [Logistic Regression](ML/logistic_regression.qmd)
- [Axis Operations](ML/axis.qmd)

### Algorithms
- [DP Regex](Algorithm/dp_regex.qmd)

---

