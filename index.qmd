---
title: "ickma.dev"
---

::: {.hero-banner}
# ickma.dev — Notes on Deep Learning and Math

A growing collection of structured study notes and visual explanations —
written for clarity, reproducibility, and long-term memory.
:::

## Deep Learning Book

::: {.content-grid}

::: {.content-card}
**[Chapter 6.1: XOR Problem & ReLU Networks](ML/xor-deep-learning.ipynb)**
How ReLU solves problems that linear models cannot handle.

:::

::: {.content-card}
**[Chapter 6.2: Likelihood-Based Loss Functions](ML/likelihood-loss-functions.ipynb)**
The mathematical connection between probabilistic models and loss functions.

:::

::: {.content-card}
**[Chapter 6.3: Hidden Units and Activation Functions](ML/activation-functions.qmd)**
Exploring activation functions and their impact on neural network learning.

:::

::: {.content-card}
**[Chapter 6.4: Architecture Design - Depth vs Width](ML/architecture-design.qmd)**
How depth enables hierarchical feature reuse and exponential expressiveness.

:::

::: {.content-card}
**[Chapter 6.5: Back-Propagation and Other Differentiation Algorithms](ML/backpropagation.qmd)**
The algorithm that makes training deep networks computationally feasible.

:::

::: {.content-card}
**[Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature](ML/hessian-prerequisites.ipynb)**
Essential second-order calculus concepts needed before Chapter 7.

:::

::: {.content-card}
**[Chapter 7.1.1: L2 Regularization](ML/l2-regularization.qmd)**
How L2 regularization shrinks weights based on Hessian eigenvalues.

:::

::: {.content-card}
**[Chapter 7.1.2: L1 Regularization](ML/l1-regularization.qmd)**
L1 regularization uses soft thresholding to create sparse solutions.

:::

::: {.content-card}
**[Chapter 7.2: Constrained Optimization View of Regularization](ML/constrained-optimization-regularization.qmd)**
Regularization as constrained optimization with KKT conditions.

:::

::: {.content-card}
**[Chapter 7.3: Regularization and Under-Constrained Problems](ML/regularization-underconstrained.qmd)**
Why regularization is mathematically necessary and ensures invertibility.

:::

::: {.content-card}
**[Chapter 7.4: Dataset Augmentation](ML/dataset-augmentation.qmd)**
How transforming existing data improves generalization.

:::

::: {.content-card}
**[Chapter 7.5: Noise Robustness](ML/noise-robustness.qmd)**
How adding Gaussian noise to weights is equivalent to penalizing large gradients.

:::

::: {.content-card}
**[Chapter 7.6: Semi-Supervised Learning](ML/semi-supervised-learning.qmd)**
Leveraging unlabeled data to improve model performance when labeled data is scarce.

:::

::: {.content-card}
**[Chapter 7.7: Multi-Task Learning](ML/multi-task-learning.qmd)**
Training a single model on multiple related tasks to improve generalization.

:::

:::

## Mathematics

::: {.math-preface}
My journey through MIT's Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.
:::

### Reflections & Synthesis

::: {.content-card}
**[Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots](Math/reflections/mit1806-invertibility-connections.qmd)**
Exploring how these fundamental concepts are different perspectives on information preservation.

:::

### MIT 18.06SC Linear Algebra

::: {.content-grid}

::: {.content-card}
**[Lecture 1: The Geometry of Linear Equations](Math/MIT18.06/mit1806-lecture1-geometry.ipynb)**
Two powerful perspectives: row picture vs column picture.

:::

::: {.content-card}
**[Lecture 2: Elimination with Matrices](Math/MIT18.06/mit1806-lecture2-elimination.qmd)**
The systematic algorithm that transforms linear systems into upper triangular form.

:::

::: {.content-card}
**[Lecture 3: Matrix Multiplication and Inverse](Math/MIT18.06/mit1806-lecture3-multiplication.qmd)**
Five different perspectives on matrix multiplication.

:::

::: {.content-card}
**[Lecture 4: LU Decomposition](Math/MIT18.06/mit1806-lecture4-lu-decomposition.ipynb)**
Factoring matrices into Lower × Upper triangular form.

:::

::: {.content-card}
**[Lecture 5.1: Permutation Matrices](Math/MIT18.06/mit1806-lecture5-permutations.qmd)**
Permutation matrices reorder rows and columns.

:::

::: {.content-card}
**[Lecture 5.2: Transpose](Math/MIT18.06/mit1806-lecture5-2-transpose.qmd)**
The transpose operation switches rows to columns.

:::

::: {.content-card}
**[Lecture 5.3: Vector Spaces](Math/MIT18.06/mit1806-lecture5-3-spaces.qmd)**
Vector spaces and subspaces: closed under addition and scalar multiplication.

:::

::: {.content-card}
**[Lecture 6: Column Space and Null Space](Math/MIT18.06/mit1806-lecture6-column-null-space.qmd)**
Column space determines which $b$ make $Ax = b$ solvable.

:::

::: {.content-card}
**[Lecture 7: Solving Ax=0](Math/MIT18.06/mit1806-lecture7-solving-ax-0.qmd)**
Systematic algorithm to find null space using pivot/free variables and RREF.

:::

::: {.content-card}
**[Lecture 8: Solving Ax=b](Math/MIT18.06/mit1806-lecture8-solving-ax-b.qmd)**
Complete solution is particular solution plus null space.

:::

::: {.content-card}
**[Lecture 9: Independence, Basis, and Dimension](Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.qmd)**
Linear independence, basis, and dimension. Rank-nullity theorem.

:::

::: {.content-card}
**[Lecture 10: Four Fundamental Subspaces](Math/MIT18.06/mit1806-lecture10-four-subspaces.qmd)**
The four fundamental subspaces completely characterize any matrix.

:::

::: {.content-card}
**[Lecture 11: Matrix Spaces, Rank-1, and Graphs](Math/MIT18.06/mit1806-lecture11-matrix-spaces.qmd)**
Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.

:::

:::

---

::: {.footer-section}
## More Topics

::: {.footer-grid}

::: {.footer-card}
### Machine Learning
- [K-Means Clustering](ML/kmeans.ipynb)
- [Logistic Regression](ML/logistic_regression.qmd)
- [Axis Operations](ML/axis.qmd)
:::

::: {.footer-card}
### Algorithms
- [DP Regex](Algorithm/dp_regex.qmd)
:::

:::
:::
