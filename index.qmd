---
title: "ickma.dev"
---

::: {.hero-banner}
# ickma.dev ‚Äî Notes on Deep Learning and Math

A growing collection of structured study notes and visual explanations ‚Äî
written for clarity, reproducibility, and long-term memory.
:::

## Latest Updates

::: {.section-block}
### ‚àá Goodfellow Deep Learning Book <span class="section-count">64 chapters</span>

My notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.

::: {.content-grid}

::: {.content-card}
**[Chapter 20: Deep Generative Models](ML/deep-generative-models.qmd)**
Deep generative modeling spans energy-based models, directed latent-variable models, and implicit generators. This chapter surveys RBMs, DBNs/DBMs, VAEs, GANs, autoregressive models, and evaluation pitfalls.
:::

::: {.content-card}
**[Chapter 19: Approximate Inference](ML/approximate-inference.qmd)**
Exact posterior inference is intractable in deep latent models, so we optimize the ELBO instead. EM, MAP inference, and mean-field variational updates provide scalable approximations.
:::

::: {.content-card}
**[Chapter 18: Confronting the Partition Function](ML/confronting-partition-function.qmd)**
Energy-based models require a partition function for normalization. This chapter follows how $\nabla_\theta \log Z(\theta)$ enters the log-likelihood gradient and surveys training strategies like contrastive divergence, pseudolikelihood, score matching, NCE, and AIS that avoid or estimate $Z$.
:::

::: {.content-card}
**[Chapter 17: Monte Carlo Methods](ML/monte-carlo-methods.qmd)**
Monte Carlo estimation replaces intractable expectations with sample averages. Importance sampling reweights proposal draws to reduce variance, while Markov chain methods like Gibbs sampling generate dependent samples when direct sampling is infeasible. Tempering improves mixing across multimodal landscapes.
:::

:::

::: {.see-all-button}
[See all Deep Learning chapters ‚Üí](ML/deep-learning-book.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìÑ Papers in Deep Learning <span class="section-count">2 notes</span>

Paper reading notes that focus on key ideas, math intuition, and practical takeaways.

::: {.content-grid}

::: {.content-card}
**[Batch Normalization: Accelerating Deep Network Training](ML/papers/batch-normalization.qmd)**
Normalize mini-batch activations with learnable scale and shift to stabilize training, improve conditioning, and speed convergence.
:::

::: {.content-card}
**[LoRA: Low-Rank Adaptation of Large Language Models](ML/papers/lora.qmd)**
Freeze the base model and learn a low‚Äërank update $\\Delta W=BA$ for selected layers, enabling efficient fine‚Äëtuning with minimal storage.
:::

:::

::: {.see-all-button}
[See all Deep Learning papers ‚Üí](ML/papers/index.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üß™ Theory-to-Repro <span class="section-count">1 note</span>

Low-level ML understanding and paper reproduction through derivations and code.

::: {.content-grid}

::: {.content-card}
**[Linear Regression via Three Solvers](Theory-to-Repro/linear-regression-three-ways.qmd)**
Solve the same least-squares objective with pseudo-inverse, convex optimization, and SGD, then compare assumptions and scalability.
:::

:::

::: {.see-all-button}
[See all Theory-to-Repro notes ‚Üí](Theory-to-Repro/index.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### ‚à´ Calculus <span class="section-count">1 note</span>

Foundational notes on calculus, centered on rates of change, accumulation, and geometric intuition.

::: {.content-grid}

::: {.content-card}
**[Gilbert Strang's Calculus: Highlights](Math/Calculus/highlights-of-calculus.qmd)**
A concise tour of derivatives, slopes, second derivatives, exponential growth, extrema, and the integral as accumulation‚Äîguided by graphs and intuition.
:::

:::

::: {.see-all-button}
[See all Calculus notes ‚Üí](Math/Calculus/index.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê MIT 18.06SC Linear Algebra <span class="section-count">36 lectures</span>

My journey through MIT's Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.

::: {.content-grid}

::: {.content-card}
**[Lecture 27: Positive Definite Matrices and Minima](Math/MIT18.06/mit1806-lecture27-positive-definite-minima.qmd)**
Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.
:::

::: {.content-card}
**[Lecture 26: Complex Matrices and Fast Fourier Transform](Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.qmd)**
Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).
:::

::: {.content-card}
**[Lecture 28: Similar Matrices and Jordan Form](Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.qmd)**
When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.
:::

::: {.content-card}
**[Lecture 25: Symmetric Matrices and Positive Definiteness](Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.qmd)**
The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.
:::

:::

::: {.see-all-button}
[See all MIT 18.06SC lectures ‚Üí](Math/MIT18.06/lectures.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê MIT 18.065: Linear Algebra Applications <span class="section-count">18 lectures</span>

My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning‚Äîexploring how linear algebra powers modern applications.

::: {.content-grid}

::: {.content-card}
**[Lecture 18: Counting Parameters in SVD, LU, QR, and Saddle Points](Math/MIT18.065/mit18065-lecture18-counting-parameters-svd-lu-qr-saddle-points.qmd)**
How LU/QR/eigendecomposition/SVD split degrees of freedom, with parameter checks for each case and saddle points from KKT systems and Rayleigh quotients.
:::

::: {.content-card}
**[Lecture 17: Rapidly Decreasing Singular Values](Math/MIT18.065/mit18065-lecture17-rapidly-decreasing-singular-values.qmd)**
Rapid singular-value decay makes matrices effectively low rank, enabling compression. Numerical rank captures this decay, and Sylvester equations explain why spectral separation drives it.
:::

::: {.content-card}
**[Lecture 16: Derivative of Inverse and Singular Values](Math/MIT18.065/mit18065-lecture16-derivative-inverse-singular-values.qmd)**
Order-sensitive derivatives give $\frac{d}{dt}A^2 = \dot A A + A \dot A$. Singular value motion follows $\dot\sigma=u^\top \dot A v$, Weyl‚Äôs inequality bounds eigenvalue shifts, and nuclear norm minimization enables matrix completion.
:::

::: {.content-card}
**[Lecture 15: Matrix Derivatives and Eigenvalue Changes](Math/MIT18.065/mit18065-lecture15-matrix-derivatives.qmd)**
Differentiate the inverse with $\dot A^{-1}=-A^{-1}\dot A A^{-1}$ and track eigenvalue motion via $\dot\lambda=y^\top \dot A x$. Low-rank PSD updates interlace eigenvalues and bound spectral shifts.
:::

:::

::: {.see-all-button}
[See all MIT 18.065 lectures ‚Üí](Math/MIT18.065/lectures.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê Stanford EE 364A: Convex Optimization <span class="section-count">15 lectures</span>

My notes from Stanford EE 364A: Convex Optimization‚Äîtheory and applications of optimization problems.

::: {.content-grid}

::: {.content-card}
**[Chapter 5.1: The Lagrange Dual Function](Math/EE364A/ee364a-chapter5-1-lagrange-dual-function.qmd)**
The dual function takes the infimum of the Lagrangian over $x$, producing a concave lower bound on the primal optimum and revealing a direct connection to conjugate functions.
:::

::: {.content-card}
**[Chapter 4.7: Vector Optimization](Math/EE364A/ee364a-chapter4-7-vector-optimization.qmd)**
Vector objectives are only partially ordered, so a global optimum may not exist. Pareto optimality captures efficient trade-offs, and scalarization recovers Pareto points via weighted sums.
:::

::: {.content-card}
**[Chapter 4.6: Generalized Inequality Constraints](Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.qmd)**
Generalized inequalities replace componentwise order with cone-induced order. Cone programs unify LP and SOCP, while SDPs use positive semidefinite cones to express linear matrix inequalities.
:::

::: {.content-card}
**[Chapter 4.5: Geometric Programming](Math/EE364A/ee364a-chapter4-5-geometric-programming.qmd)**
Geometric programming uses monomials and posynomials over positive variables; a log change of variables turns constraints into convex log-sum-exp and affine forms, enabling global optimization with applications like cantilever beam design.
:::

:::

::: {.see-all-button}
[See all EE 364A lectures ‚Üí](Math/EE364A/lectures.qmd){.btn .btn-primary}
:::

:::

---

::: {.footer-section}
## More Topics

::: {.footer-grid}

::: {.footer-card}
### Machine Learning
- [K-Means Clustering](ML/k_means_clustering.qmd)
- [Logistic Regression](ML/logistic_regression.qmd)
- [Axis Operations](ML/axis.qmd)
:::

::: {.footer-card}
### Algorithms
- [DP Regex](Algorithm/dp_regex.qmd)
:::

:::

:::
