---
title: "ickma.dev"
---

::: {.hero-banner}
# ickma.dev ‚Äî Notes on Deep Learning and Math

A growing collection of structured study notes and visual explanations ‚Äî
written for clarity, reproducibility, and long-term memory.
:::

## Latest Updates

::: {.section-block}
### ‚àá Deep Learning Book <span class="section-count">60 chapters</span>

My notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.

::: {.content-grid}

::: {.content-card}
**[Chapter 16: Structured Probabilistic Models for Deep Learning](ML/structured-probabilistic-models.qmd)**
Structured probabilistic models use graphs to factorize high-dimensional distributions into tractable components by encoding conditional independence. Directed models represent causal relationships and support efficient ancestral sampling. Undirected models capture symmetric dependencies through clique potentials normalized by partition function. Energy-based models express potentials via Boltzmann distribution. Deep learning embraces approximate inference with large latent variable models, prioritizing scalability over exact computations through distributed representations.
:::

::: {.content-card}
**[Chapter 15: Representation Learning](ML/representation-learning.qmd)**
Greedy layer-wise pretraining learns meaningful representations through unsupervised learning of hierarchical features, providing better initialization than random weights. Transfer learning enables knowledge sharing across tasks by reusing learned representations‚Äîgeneric early-layer features transfer well while late layers adapt to task-specific patterns. Semi-supervised learning leverages both labeled and unlabeled data to discover disentangled causal factors that generate observations. Distributed representations enable exponential gains in capacity through shared features rather than symbolic codes.
:::

::: {.content-card}
**[Chapter 14: Autoencoders](ML/autoencoders.qmd)**
Autoencoders learn compressed representations by training encoder $h=f(x)$ and decoder $r=g(h)$ to reconstruct inputs through a bottleneck. Undercomplete autoencoders constrain $\dim(h)<\dim(x)$ to learn meaningful features. Regularized variants include sparse autoencoders (L1 penalty as Laplace prior), denoising autoencoders (learn manifold structure from corrupted inputs), and contractive autoencoders (penalize Jacobian for local invariance). Applications: dimensionality reduction, semantic hashing, and manifold learning.
:::

::: {.content-card}
**[Chapter 13: Linear Factor Models](ML/linear-factor-models.qmd)**
Linear factor models decompose observed data into latent factors: $x = Wh + b + \text{noise}$. PCA uses Gaussian priors for dimensionality reduction. ICA recovers statistically independent non-Gaussian sources for signal separation. SFA learns slowly-varying features via temporal coherence. Sparse coding enforces L1 sparsity for interpretable representations. These models form the foundation for modern unsupervised learning and generative models.
:::

:::

::: {.see-all-button}
[See all Deep Learning chapters ‚Üí](ML/deep-learning-book.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê MIT 18.06SC Linear Algebra <span class="section-count">36 lectures</span>

My journey through MIT's Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.

::: {.content-grid}

::: {.content-card}
**[Lecture 27: Positive Definite Matrices and Minima](Math/MIT18.06/mit1806-lecture27-positive-definite-minima.qmd)**
Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.
:::

::: {.content-card}
**[Lecture 26: Complex Matrices and Fast Fourier Transform](Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.qmd)**
Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).
:::

::: {.content-card}
**[Lecture 28: Similar Matrices and Jordan Form](Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.qmd)**
When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.
:::

::: {.content-card}
**[Lecture 25: Symmetric Matrices and Positive Definiteness](Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.qmd)**
The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.
:::

:::

::: {.see-all-button}
[See all MIT 18.06SC lectures ‚Üí](Math/MIT18.06/lectures.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê MIT 18.065: Linear Algebra Applications <span class="section-count">11 lectures</span>

My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning‚Äîexploring how linear algebra powers modern applications.

::: {.content-grid}

::: {.content-card}
**[Lecture 11: Minimize ||x|| subject to Ax=b](Math/MIT18.065/lecture-11-minimize-norm-subject-to-constraint.qmd)**
Three norms produce different solution geometries: L1 promotes sparsity (diamond touches constraint at corner), L2 yields smooth isotropic solutions (circle tangency), and L‚àû balances components (square equalization). QR factorization via Gram‚ÄìSchmidt computes minimum-norm solutions; column pivoting improves numerical stability. Krylov subspace methods solve large sparse systems via matrix‚Äìvector products.
:::

::: {.content-card}
**[Lecture 10: Survey of Difficulties of Ax=b](Math/MIT18.065/mit18065-lecture10-axb-difficulties.qmd)**
Comprehensive survey of challenges in solving $Ax=b$: overdetermined systems require least squares, underdetermined systems select minimum-norm solutions via $A^+b$, ill-conditioned problems add ridge penalty $\min ||Ax-b||^2+\delta^2||x||^2$, large systems use Krylov methods, and massive-scale problems employ randomized linear algebra. Moore-Penrose pseudo-inverse unifies all regimes: exact inverse when square, least-squares when overdetermined, minimum-norm when underdetermined, and the zero-regularization limit of ridge regression. Deep learning operates in underdetermined regime where gradient descent implicitly biases toward low-norm solutions.
:::

::: {.content-card}
**[Lecture 7: Eckart-Young Theorem - The Closest Rank k Matrix](Math/MIT18.065/mit18065-lecture7-eckart-young.qmd)**
The Eckart-Young theorem proves that truncated SVD $A_k = \sum_{i=1}^k \sigma_i u_i v_i^\top$ gives the best rank-$k$ approximation. Applications: Netflix collaborative filtering predicts movie ratings using low-rank matrices, MRI reconstruction exploits natural image structure, and PCA finds optimal low-dimensional subspaces by computing dominant singular vectors of centered data.
:::

::: {.content-card}
**[Lecture 6: Singular Value Decomposition (SVD)](Math/MIT18.065/mit18065-lecture6-svd.qmd)**
SVD factorizes any $m \times n$ matrix as $A = U\Sigma V^\top$: rotate-scale-rotate. Compute U from eigenvectors of $AA^\top$, V from eigenvectors of $A^\top A$, and $\Sigma$ from square roots of shared eigenvalues. Singular values relate to eigenvalues: $\det A = \prod \sigma_i = \prod \lambda_i$ for square matrices. Polar decomposition $A = (U\Sigma U^\top)(UV^\top)$ splits any matrix into symmetric positive semidefinite √ó orthogonal.
:::

::: {.content-card}
**[Lecture 5: Positive Definite and Semidefinite Matrices](Math/MIT18.065/mit18065-lecture5-positive-definite.qmd)**
Positive definite matrices have all $\lambda_i > 0$ and energy $x^\top Sx > 0$ for all $x \ne 0$. Factorize as $S = AA^\top$ with full rank A. The quadratic form creates an energy bowl‚Äîconvex and minimizable via gradient descent. This connects linear algebra to optimization: deep learning minimizes loss landscapes just as we minimize $x^\top Sx$. Positive semidefinite relaxes to $\lambda_i \ge 0$.
:::

:::

::: {.see-all-button}
[See all MIT 18.065 lectures ‚Üí](Math/MIT18.065/lectures.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê Stanford EE 364A: Convex Optimization <span class="section-count">9 lectures</span>

My notes from Stanford EE 364A: Convex Optimization‚Äîtheory and applications of optimization problems.

::: {.content-grid}

::: {.content-card}
**[Chapter 4.2: Convex Optimization](Math/EE364A/ee364a-lecture7-convex-optimization.qmd)**
Convex problems in standard form (convex objective/inequalities, affine equalities), local vs global optimality, first-order conditions, equivalent reformulations (change of variables, slack variables), and quasiconvex problems via bisection.
:::

::: {.content-card}
**[Chapter 4.1: Optimization Problems](Math/EE364A/ee364a-lecture6-optimization-problems.qmd)**
Basic terminology (decision variables, objective, constraints, domain, feasibility, optimal values, local optimality), standard form conversion, equivalent problems (change of variables, slack variables, constraint elimination, epigraph form), and parameter vs oracle problem descriptions.
:::

::: {.content-card}
**[Lecture 5.2: Monotonicity with Generalized Inequalities](Math/EE364A/ee364a-lecture5-part2-monotonicity.qmd)**
K-nondecreasing functions satisfy $x \preceq_K y \Rightarrow f(x)\le f(y)$. Gradient condition: $\nabla f(x) \succeq_{K^*} 0$ (dual inequality). Matrix monotone examples: $\mathrm{tr}(WX)$, $\mathrm{det}X$. K-convexity extends convexity to generalized inequalities with dual characterization and composition theorems.
:::

::: {.content-card}
**[Lecture 5.1: Log-Concave and Log-Convex Functions](Math/EE364A/ee364a-lecture5-part1-log-concave-convex.qmd)**
Log-concave functions satisfy $f(\theta x+(1-\theta)y)\ge f(x)^\theta f(y)^{1-\theta}$. Powers $x^a$ are log-concave for $a \ge 0$ and log-convex for $a \le 0$. Key properties: products preserve log-concavity, but sums do not. Integration of log-concave functions preserves log-concavity.
:::

::: {.content-card}
**[Lecture 4 Part 2: Conjugate and Quasiconvex Functions](Math/EE364A/ee364a-lecture4-part2-conjugate-quasiconvex.qmd)**
Conjugate functions $f^*(y) = \sup_x (y^\top x - f(x))$ are always convex, with examples including negative logarithm and quadratic functions. Quasiconvex functions have convex sublevel sets with modified Jensen inequality $f(\theta x + (1-\theta)y) \leq \max\{f(x), f(y)\}$. Examples include linear-fractional functions and distance ratios.
:::

:::

::: {.see-all-button}
[See all EE 364A lectures ‚Üí](Math/EE364A/lectures.qmd){.btn .btn-primary}
:::

:::

---

::: {.footer-section}
## More Topics

::: {.footer-grid}

::: {.footer-card}
### Machine Learning
- [K-Means Clustering](ML/k_means_clustering.qmd)
- [Logistic Regression](ML/logistic_regression.qmd)
- [Axis Operations](ML/axis.qmd)
:::

::: {.footer-card}
### Algorithms
- [DP Regex](Algorithm/dp_regex.qmd)
:::

:::
:::
