---
title: "ickma.dev"
---

::: {.hero-banner}
# ickma.dev — Notes on Deep Learning and Math

A growing collection of structured study notes and visual explanations —
written for clarity, reproducibility, and long-term memory.
:::

## Deep Learning Book

::: {.content-grid}

::: {.content-card}
**[Chapter 6.1: XOR Problem & ReLU Networks](ML/xor-deep-learning.ipynb)**
How ReLU solves problems that linear models cannot handle.

:::

::: {.content-card}
**[Chapter 6.2: Likelihood-Based Loss Functions](ML/likelihood-loss-functions.ipynb)**
The mathematical connection between probabilistic models and loss functions.

:::

::: {.content-card}
**[Chapter 6.3: Hidden Units and Activation Functions](ML/activation-functions.qmd)**
Exploring activation functions and their impact on neural network learning.

:::

::: {.content-card}
**[Chapter 6.4: Architecture Design - Depth vs Width](ML/architecture-design.qmd)**
How depth enables hierarchical feature reuse and exponential expressiveness.

:::

::: {.content-card}
**[Chapter 6.5: Back-Propagation and Other Differentiation Algorithms](ML/backpropagation.qmd)**
The algorithm that makes training deep networks computationally feasible.

:::

::: {.content-card}
**[Chapter 7 Prerequisites: Hessian Matrix, Definiteness, and Curvature](ML/hessian-prerequisites.ipynb)**
Essential second-order calculus concepts needed before Chapter 7.

:::

::: {.content-card}
**[Chapter 7.1.1: L2 Regularization](ML/l2-regularization.qmd)**
How L2 regularization shrinks weights based on Hessian eigenvalues.

:::

::: {.content-card}
**[Chapter 7.1.2: L1 Regularization](ML/l1-regularization.qmd)**
L1 regularization uses soft thresholding to create sparse solutions.

:::

::: {.content-card}
**[Chapter 7.2: Constrained Optimization View of Regularization](ML/constrained-optimization-regularization.qmd)**
Regularization as constrained optimization with KKT conditions.

:::

::: {.content-card}
**[Chapter 7.3: Regularization and Under-Constrained Problems](ML/regularization-underconstrained.qmd)**
Why regularization is mathematically necessary and ensures invertibility.

:::

::: {.content-card}
**[Chapter 7.4: Dataset Augmentation](ML/dataset-augmentation.qmd)**
How transforming existing data improves generalization.

:::

::: {.content-card}
**[Chapter 7.5: Noise Robustness](ML/noise-robustness.qmd)**
How adding Gaussian noise to weights is equivalent to penalizing large gradients.

:::

::: {.content-card}
**[Chapter 7.6: Semi-Supervised Learning](ML/semi-supervised-learning.qmd)**
Leveraging unlabeled data to improve model performance when labeled data is scarce.

:::

::: {.content-card}
**[Chapter 7.7: Multi-Task Learning](ML/multi-task-learning.qmd)**
Training a single model on multiple related tasks to improve generalization.

:::

::: {.content-card}
**[Chapter 7.8: Early Stopping](ML/early-stopping.qmd)**
Early stopping as implicit L2 regularization: fewer training steps equals stronger regularization.

:::

::: {.content-card}
**[Chapter 7.9: Parameter Tying and Parameter Sharing](ML/parameter-tying-sharing.qmd)**
Two strategies for reducing parameters: encouraging similarity vs. enforcing identity.

:::

::: {.content-card}
**[Chapter 7.10: Sparse Representations](ML/representation-sparsity.qmd)**
Regularizing learned representations rather than model parameters: the difference between parameter sparsity and representation sparsity.

:::

::: {.content-card}
**[Chapter 7.11: Bagging and Other Ensemble Methods](ML/bagging-ensemble.qmd)**
How training multiple models on bootstrap samples and averaging their predictions reduces variance.

:::

::: {.content-card}
**[Chapter 7.12: Dropout](ML/dropout.qmd)**
Dropout as a computationally efficient alternative to bagging - training an ensemble of subnetworks by randomly dropping units.

:::

::: {.content-card}
**[Chapter 7.13: Adversarial Training](ML/adversarial-training.qmd)**
How training on adversarial examples improves model robustness by reducing sensitivity to imperceptible perturbations.

:::

::: {.content-card}
**[Chapter 7.14: Tangent Distance, Tangent Prop and Manifold Tangent Classifier](ML/tangent-prop-manifold.qmd)**
Enforcing invariance along manifold tangent directions to regularize models against meaningful transformations.

:::

::: {.content-card}
**[Chapter 8.1: How Learning Differs from Pure Optimization](ML/learning-vs-optimization.qmd)**
Why machine learning optimization is fundamentally different from pure optimization and why mini-batch methods work.

:::

::: {.content-card}
**[Chapter 8.2: Challenges in Deep Learning Optimization](ML/optimization-challenges.qmd)**
Understanding why deep learning optimization is hard: ill-conditioning, local minima, saddle points, exploding gradients, and the theoretical limits of optimization.

:::

::: {.content-card}
**[Chapter 8.3: Basic Algorithms](ML/basic-optimization-algorithms.qmd)**
SGD, momentum, and Nesterov momentum: the foundational algorithms for training deep neural networks.

:::

::: {.content-card}
**[Chapter 8.4: Parameter Initialization Strategies](ML/parameter-initialization.qmd)**
Why initialization matters: breaking symmetry, avoiding null spaces, and finding the right balance for convergence and generalization.

:::

::: {.content-card}
**[Chapter 8.5: Algorithms with Adaptive Learning Rates](ML/adaptive-learning-rates.qmd)**
From AdaGrad to Adam: how adaptive learning rates automatically tune optimization for each parameter.

:::

:::

## Mathematics

::: {.math-preface}
My journey through MIT's Linear Algebra course (18.06SC), focusing on building intuition and making connections between fundamental concepts. Each lecture note emphasizes geometric interpretation and practical applications.
:::

### Reflections & Synthesis

::: {.content-card}
**[Deep Connections: Invertibility, Null Space, Independence, Rank, and Pivots](Math/reflections/mit1806-invertibility-connections.qmd)**
Exploring how these fundamental concepts are different perspectives on information preservation.

:::

::: {.content-card}
**[From Taylor Expansion to Euler's Formula: The Mathematical Foundation of Fourier Series](Math/reflections/taylor-euler-fourier.qmd)**
The beautiful mathematical journey from Taylor expansions to Euler's formula and ultimately to Fourier series, revealing deep connections between polynomials, exponentials, and trigonometric functions.

:::

### MIT 18.06SC Linear Algebra

::: {.content-grid}

::: {.content-card}
**[Lecture 1: The Geometry of Linear Equations](Math/MIT18.06/mit1806-lecture1-geometry.ipynb)**
Two powerful perspectives: row picture vs column picture.

:::

::: {.content-card}
**[Lecture 2: Elimination with Matrices](Math/MIT18.06/mit1806-lecture2-elimination.qmd)**
The systematic algorithm that transforms linear systems into upper triangular form.

:::

::: {.content-card}
**[Lecture 3: Matrix Multiplication and Inverse](Math/MIT18.06/mit1806-lecture3-multiplication.qmd)**
Five different perspectives on matrix multiplication.

:::

::: {.content-card}
**[Lecture 4: LU Decomposition](Math/MIT18.06/mit1806-lecture4-lu-decomposition.ipynb)**
Factoring matrices into Lower × Upper triangular form.

:::

::: {.content-card}
**[Lecture 5.1: Permutation Matrices](Math/MIT18.06/mit1806-lecture5-permutations.qmd)**
Permutation matrices reorder rows and columns.

:::

::: {.content-card}
**[Lecture 5.2: Transpose](Math/MIT18.06/mit1806-lecture5-2-transpose.qmd)**
The transpose operation switches rows to columns.

:::

::: {.content-card}
**[Lecture 5.3: Vector Spaces](Math/MIT18.06/mit1806-lecture5-3-spaces.qmd)**
Vector spaces and subspaces: closed under addition and scalar multiplication.

:::

::: {.content-card}
**[Lecture 6: Column Space and Null Space](Math/MIT18.06/mit1806-lecture6-column-null-space.qmd)**
Column space determines which $b$ make $Ax = b$ solvable.

:::

::: {.content-card}
**[Lecture 7: Solving Ax=0](Math/MIT18.06/mit1806-lecture7-solving-ax-0.qmd)**
Systematic algorithm to find null space using pivot/free variables and RREF.

:::

::: {.content-card}
**[Lecture 8: Solving Ax=b](Math/MIT18.06/mit1806-lecture8-solving-ax-b.qmd)**
Complete solution is particular solution plus null space.

:::

::: {.content-card}
**[Lecture 9: Independence, Basis, and Dimension](Math/MIT18.06/mit1806-lecture9-independence-basis-dimension.qmd)**
Linear independence, basis, and dimension. Rank-nullity theorem.

:::

::: {.content-card}
**[Lecture 10: Four Fundamental Subspaces](Math/MIT18.06/mit1806-lecture10-four-subspaces.qmd)**
The four fundamental subspaces completely characterize any matrix.

:::

::: {.content-card}
**[Lecture 11: Matrix Spaces, Rank-1, and Graphs](Math/MIT18.06/mit1806-lecture11-matrix-spaces.qmd)**
Matrix spaces as vector spaces, rank-1 matrices, dimension formulas.

:::

::: {.content-card}
**[Lecture 12: Graphs, Networks, and Incidence Matrices](Math/MIT18.06/mit1806-lecture12-graphs-networks.qmd)**
Applying linear algebra to graph theory: incidence matrices, Kirchhoff's laws, and Euler's formula.

:::

::: {.content-card}
**[Lecture 13: Quiz 1 Review](Math/MIT18.06/mit1806-lecture13-quiz-review.qmd)**
Practice problems reviewing key concepts: subspaces, rank, null spaces, and the four fundamental subspaces.

:::

::: {.content-card}
**[Lecture 14: Orthogonal Vectors and Subspaces](Math/MIT18.06/mit1806-lecture14-orthogonality.qmd)**
Orthogonal vectors, orthogonal subspaces, and the fundamental relationships between the four subspaces.

:::

::: {.content-card}
**[Lecture 15: Projection onto Subspaces](Math/MIT18.06/mit1806-lecture15-projections.qmd)**
Projecting onto lines and subspaces, the geometry of least squares, and deriving normal equations.

:::

::: {.content-card}
**[Lecture 16: Projection Matrices and Least Squares](Math/MIT18.06/mit1806-lecture16-least-squares.qmd)**
Projection matrices, least squares fitting, and the connection between linear algebra and calculus optimization.

:::

::: {.content-card}
**[Lecture 17: Orthogonal Matrices and Gram-Schmidt](Math/MIT18.06/mit1806-lecture17-gram-schmidt.qmd)**
Orthonormal matrices, the Gram-Schmidt process for orthogonalization, and QR factorization.

:::

::: {.content-card}
**[Lecture 18: Properties of Determinants](Math/MIT18.06/mit1806-lecture18-determinants.qmd)**
Ten fundamental properties that completely define the determinant and reveal when matrices are invertible.

:::

::: {.content-card}
**[Lecture 19: Determinant Formulas and Cofactors](Math/MIT18.06/mit1806-lecture19-determinant-formulas.qmd)**
Three computational methods for determinants: pivots, the big formula, and cofactor expansion.

:::

::: {.content-card}
**[Lecture 20: Inverse & Volume](Math/MIT18.06/mit1806-lecture20-cramers-rule.qmd)**
The inverse matrix formula using cofactors, Cramer's rule for solving linear systems, and the geometric interpretation of determinants as volume.

:::

::: {.content-card}
**[Lecture 21: Eigenvalues and Eigenvectors](Math/MIT18.06/mit1806-lecture21-eigenvalues-eigenvectors.qmd)**
The directions that matrices can only scale, not rotate: $Ax = \lambda x$.

:::

::: {.content-card}
**[Lecture 22: Diagonalization and Powers of A](Math/MIT18.06/mit1806-lecture22-diagonalization-powers.qmd)**
Computing matrix powers efficiently and solving Fibonacci with eigenvalues.

:::

::: {.content-card}
**[Lecture 23: Differential Equations and exp(At)](Math/MIT18.06/mit1806-lecture23-differential-equations.qmd)**
Connecting linear algebra to differential equations: how eigenvalues determine stability and matrix exponentials solve systems.

:::

:::

---

::: {.footer-section}
## More Topics

::: {.footer-grid}

::: {.footer-card}
### Machine Learning
- [K-Means Clustering](ML/kmeans.ipynb)
- [Logistic Regression](ML/logistic_regression.qmd)
- [Axis Operations](ML/axis.qmd)
:::

::: {.footer-card}
### Algorithms
- [DP Regex](Algorithm/dp_regex.qmd)
:::

:::
:::
