---
title: "ickma.dev"
---

::: {.hero-banner}
# ickma.dev ‚Äî Notes on Deep Learning and Math

A growing collection of structured study notes and visual explanations ‚Äî
written for clarity, reproducibility, and long-term memory.
:::

## Latest Updates

::: {.section-block}
### ‚àá Deep Learning Book <span class="section-count">62 chapters</span>

My notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.

::: {.content-grid}

::: {.content-card}
**[Chapter 18: Confronting the Partition Function](ML/confronting-partition-function.qmd)**
Energy-based models require a partition function for normalization. This chapter follows how $\nabla_\theta \log Z(\theta)$ enters the log-likelihood gradient and surveys training strategies like contrastive divergence, pseudolikelihood, score matching, NCE, and AIS that avoid or estimate $Z$.
:::

::: {.content-card}
**[Chapter 17: Monte Carlo Methods](ML/monte-carlo-methods.qmd)**
Monte Carlo estimation replaces intractable expectations with sample averages. Importance sampling reweights proposal draws to reduce variance, while Markov chain methods like Gibbs sampling generate dependent samples when direct sampling is infeasible. Tempering improves mixing across multimodal landscapes.
:::

::: {.content-card}
**[Chapter 16: Structured Probabilistic Models for Deep Learning](ML/structured-probabilistic-models.qmd)**
Structured probabilistic models use graphs to factorize high-dimensional distributions into tractable components by encoding conditional independence. Directed models represent causal relationships and support efficient ancestral sampling. Undirected models capture symmetric dependencies through clique potentials normalized by partition function. Energy-based models express potentials via Boltzmann distribution. Deep learning embraces approximate inference with large latent variable models, prioritizing scalability over exact computations through distributed representations.
:::

::: {.content-card}
**[Chapter 15: Representation Learning](ML/representation-learning.qmd)**
Greedy layer-wise pretraining learns meaningful representations through unsupervised learning of hierarchical features, providing better initialization than random weights. Transfer learning enables knowledge sharing across tasks by reusing learned representations‚Äîgeneric early-layer features transfer well while late layers adapt to task-specific patterns. Semi-supervised learning leverages both labeled and unlabeled data to discover disentangled causal factors that generate observations. Distributed representations enable exponential gains in capacity through shared features rather than symbolic codes.
:::

:::

::: {.see-all-button}
[See all Deep Learning chapters ‚Üí](ML/deep-learning-book.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê MIT 18.06SC Linear Algebra <span class="section-count">36 lectures</span>

My journey through MIT's Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.

::: {.content-grid}

::: {.content-card}
**[Lecture 27: Positive Definite Matrices and Minima](Math/MIT18.06/mit1806-lecture27-positive-definite-minima.qmd)**
Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.
:::

::: {.content-card}
**[Lecture 26: Complex Matrices and Fast Fourier Transform](Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.qmd)**
Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).
:::

::: {.content-card}
**[Lecture 28: Similar Matrices and Jordan Form](Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.qmd)**
When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.
:::

::: {.content-card}
**[Lecture 25: Symmetric Matrices and Positive Definiteness](Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.qmd)**
The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.
:::

:::

::: {.see-all-button}
[See all MIT 18.06SC lectures ‚Üí](Math/MIT18.06/lectures.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê MIT 18.065: Linear Algebra Applications <span class="section-count">14 lectures</span>

My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning‚Äîexploring how linear algebra powers modern applications.

::: {.content-grid}

::: {.content-card}
**[Lecture 14: Low Rank Changes and Their Inverse](Math/MIT18.065/mit18065-lecture14-low-rank-changes-inverse.qmd)**
Low-rank updates reuse an existing inverse via the Sherman‚ÄìMorrison‚ÄìWoodbury identity. Rank-1 updates power recursive least squares; the full SMW formula updates $(A-UV^\top)^{-1}$ in terms of $A^{-1}$, avoiding a full recomputation.
:::

::: {.content-card}
**[Lecture 13: Randomized Matrix Multiplication](Math/MIT18.065/mit18065-lecture13-randomized-matrix-multiplication.qmd)**
Randomized matrix multiplication approximates $AB$ by sampling rank-1 outer products. The estimator remains unbiased, and the variance is minimized by sampling indices proportional to $\|a_j\|\,\|b_j\|$. This yields a low-rank approximation with predictable error decay in $1/\sqrt{s}$.
:::

::: {.content-card}
**[Lecture 12: Computing Eigenvalues and Eigenvectors](Math/MIT18.065/mit18065-lecture12-eigenvalue-algorithms.qmd)**
Eigenvalues are defined by $\det(A-\lambda I)=0$ but computed by stable similarity transformations. QR iteration transforms matrices toward triangular form where eigenvalues appear on the diagonal; shifted QR with Wilkinson shifts accelerates convergence. Reduction to Hessenberg (or tridiagonal for symmetric matrices) enables efficient iteration. For large sparse problems, Krylov subspace methods (Arnoldi/Lanczos) approximate eigenvalues using only matrix-vector products.
:::

::: {.content-card}
**[Lecture 11: Minimize ||x|| subject to Ax=b](Math/MIT18.065/lecture-11-minimize-norm-subject-to-constraint.qmd)**
Three norms produce different solution geometries: L1 promotes sparsity (diamond touches constraint at corner), L2 yields smooth isotropic solutions (circle tangency), and L‚àû balances components (square equalization). QR factorization via Gram‚ÄìSchmidt computes minimum-norm solutions; column pivoting improves numerical stability. Krylov subspace methods solve large sparse systems via matrix‚Äìvector products.
:::

:::

::: {.see-all-button}
[See all MIT 18.065 lectures ‚Üí](Math/MIT18.065/lectures.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê Stanford EE 364A: Convex Optimization <span class="section-count">14 lectures</span>

My notes from Stanford EE 364A: Convex Optimization‚Äîtheory and applications of optimization problems.

::: {.content-grid}

::: {.content-card}
**[Chapter 4.7: Vector Optimization](Math/EE364A/ee364a-chapter4-7-vector-optimization.qmd)**
Vector objectives are only partially ordered, so a global optimum may not exist. Pareto optimality captures efficient trade-offs, and scalarization recovers Pareto points via weighted sums.
:::

::: {.content-card}
**[Chapter 4.6: Generalized Inequality Constraints](Math/EE364A/ee364a-chapter4-6-generalized-inequality-constraints.qmd)**
Generalized inequalities replace componentwise order with cone-induced order. Cone programs unify LP and SOCP, while SDPs use positive semidefinite cones to express linear matrix inequalities.
:::

::: {.content-card}
**[Chapter 4.5: Geometric Programming](Math/EE364A/ee364a-chapter4-5-geometric-programming.qmd)**
Geometric programming uses monomials and posynomials over positive variables; a log change of variables turns constraints into convex log-sum-exp and affine forms, enabling global optimization with applications like cantilever beam design.
:::

::: {.content-card}
**[Chapter 4.4: Quadratic Optimization Problems](Math/EE364A/ee364a-lecture9-quadratic-optimization.qmd)**
Quadratic programs minimize convex quadratic objectives with affine constraints; QCQPs allow quadratic inequalities. SOCPs encode norm constraints and unify LP/QP/QCQP formulations. Robust LP handles uncertainty using ellipsoidal sets or Gaussian chance constraints, producing SOCP reformulations.
:::

:::

::: {.see-all-button}
[See all EE 364A lectures ‚Üí](Math/EE364A/lectures.qmd){.btn .btn-primary}
:::

:::

---

::: {.footer-section}
## More Topics

::: {.footer-grid}

::: {.footer-card}
### Machine Learning
- [K-Means Clustering](ML/k_means_clustering.qmd)
- [Logistic Regression](ML/logistic_regression.qmd)
- [Axis Operations](ML/axis.qmd)
:::

::: {.footer-card}
### Algorithms
- [DP Regex](Algorithm/dp_regex.qmd)
:::

:::

:::
