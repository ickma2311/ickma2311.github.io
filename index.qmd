---
title: "ickma.dev"
---

::: {.hero-banner}
# ickma.dev ‚Äî Notes on Deep Learning and Math

A growing collection of structured study notes and visual explanations ‚Äî
written for clarity, reproducibility, and long-term memory.
:::

## Latest Updates

::: {.section-block}
### ‚àá Deep Learning Book <span class="section-count">37 chapters</span>

My notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.

::: {.content-grid}

::: {.content-card}
**[Chapter 9.9: Unsupervised or Semi-Supervised Feature Learning](ML/cnn-unsupervised-learning.qmd)**
Before CNNs, computer vision relied on hand-crafted kernels (Sobel, Laplacian) and unsupervised methods (sparse coding, autoencoders, k-means). Modern CNNs surpass both by learning hierarchical features end-to-end‚Äîfrom edges to semantic concepts‚Äîoptimized for the task, making hand-crafted filters largely obsolete.
:::

::: {.content-card}
**[Chapter 9.8: Efficient Convolution Algorithms](ML/cnn-efficient-convolution.qmd)**
Separable convolution decomposes a 2D kernel into two 1D filters, reducing computational cost from $O(HWk^2)$ to $O(HWk)$ and parameter storage from $k^2$ to $2k$. This factorization enables faster, more memory-efficient models without sacrificing accuracy‚Äîfoundational for architectures like MobileNet.
:::

::: {.content-card}
**[Chapter 9.7: Data Types](ML/cnn-data-types.qmd)**
CNNs can operate on different data types: 1D (audio, time series), 2D (images), and 3D (videos, CT scans) with varying channel counts. Unlike fully connected networks, convolutional kernels handle variable-sized inputs‚Äîa unique flexibility for diverse domains.
:::

::: {.content-card}
**[Chapter 9.6: Structured Outputs](ML/cnn-structured-outputs.qmd)**
CNNs can generate high-dimensional structured objects through pixel-level predictions. Recurrent convolution refines predictions iteratively, producing dense outputs for segmentation, depth estimation, and flow prediction.
:::

:::

::: {.see-all-button}
[See all Deep Learning chapters ‚Üí](ML/deep-learning-book.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê MIT 18.06SC Linear Algebra <span class="section-count">36 lectures</span>

My journey through MIT's Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.

::: {.content-grid}

::: {.content-card}
**[Lecture 27: Positive Definite Matrices and Minima](Math/MIT18.06/mit1806-lecture27-positive-definite-minima.qmd)**
Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.
:::

::: {.content-card}
**[Lecture 26: Complex Matrices and Fast Fourier Transform](Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.qmd)**
Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).
:::

::: {.content-card}
**[Lecture 28: Similar Matrices and Jordan Form](Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.qmd)**
When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.
:::

::: {.content-card}
**[Lecture 25: Symmetric Matrices and Positive Definiteness](Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.qmd)**
The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.
:::

:::

::: {.see-all-button}
[See all MIT 18.06SC lectures ‚Üí](Math/MIT18.06/lectures.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê MIT 18.065: Linear Algebra Applications <span class="section-count">2 lectures</span>

My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning‚Äîexploring how linear algebra powers modern applications.

::: {.content-grid}

::: {.content-card}
**[Lecture 9: Four Ways to Solve Least Squares Problems](Math/MIT18.065/mit18065-lecture9-least-squares.qmd)**
Four equivalent methods for solving $Ax = b$ when $A$ has no inverse: pseudo-inverse, normal equations, algebraic minimization, and geometric projection‚Äîall converging to the same optimal solution.
:::

::: {.content-card}
**[Lecture 8: Norms of Vectors and Matrices](Math/MIT18.065/mit18065-lecture8-norms.qmd)**
Understanding vector p-norms ($\|v\|_p$) and when they satisfy the triangle inequality (only for $p \geq 1$). The "$\frac{1}{2}$-norm" creates non-convex unit balls for strong sparsity.
:::

:::

::: {.see-all-button}
[See all MIT 18.065 lectures ‚Üí](Math/MIT18.065/lectures.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê Stanford EE 364A: Convex Optimization <span class="section-count">1 lectures</span>

My notes from Stanford EE 364A: Convex Optimization‚Äîtheory and applications of optimization problems.

::: {.content-grid}

::: {.content-card}
**[Lecture 1: Introduction to Convex Optimization](Math/EE364A/ee364a-lecture1-intro.qmd)**
Introduction to constraint optimization: least squares ($\|Ax-b\|_2^2$), linear programming, and convex optimization. Convex problems generalize both while maintaining polynomial-time solvability through interior-point methods‚Äîbridging tractable special cases and general nonlinear programming.
:::

:::

::: {.see-all-button}
[See all EE 364A lectures ‚Üí](Math/EE364A/lectures.qmd){.btn .btn-primary}
:::

:::

---

::: {.footer-section}
## More Topics

::: {.footer-grid}

::: {.footer-card}
### Machine Learning
- [K-Means Clustering](ML/kmeans.ipynb)
- [Logistic Regression](ML/logistic_regression.qmd)
- [Axis Operations](ML/axis.qmd)
:::

::: {.footer-card}
### Algorithms
- [DP Regex](Algorithm/dp_regex.qmd)
:::

:::
:::
