---
title: "ickma.dev"
---

::: {.hero-banner}
# ickma.dev ‚Äî Notes on Deep Learning and Math

A growing collection of structured study notes and visual explanations ‚Äî
written for clarity, reproducibility, and long-term memory.
:::

## Latest Updates

::: {.section-block}
### ‚àá Deep Learning Book <span class="section-count">42 chapters</span>

My notes on the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.

::: {.content-grid}

::: {.content-card}
**[Chapter 10.4: Encoder-Decoder Sequence-to-Sequence Architecture](ML/rnn-encoder-decoder.qmd)**
The seq2seq architecture handles variable-length input and output sequences by compressing the input into a fixed context vector C, then decoding it step-by-step. This enables machine translation, summarization, and dialogue generation where input and output lengths differ.
:::

::: {.content-card}
**[Chapter 10.3: Bidirectional RNN](ML/rnn-bidirectional.qmd)**
Bidirectional RNNs process sequences in both forward and backward directions, allowing predictions to use information from the entire input sequence. Essential for tasks like speech recognition and handwriting recognition where future context matters.
:::

::: {.content-card}
**[Chapter 10.2: Recurrent Neural Networks](ML/rnn-recurrent-neural-networks.qmd)**
RNN architecture with hidden-to-hidden connections, teacher forcing for parallel training, back-propagation through time (BPTT), RNN as directed graphical models with O(œÑ) parameter efficiency, and context-based sequence-to-sequence models.
:::

::: {.content-card}
**[Chapter 10.1: Unfold Computation Graph](ML/rnn-unfold-computation-graph.qmd)**
Unfolding computation graphs in RNNs enables parameter sharing across time steps. The same function with fixed parameters processes sequences of any length, compressing input history into fixed-size hidden states that retain only task-relevant information for predictions.
:::

:::

::: {.see-all-button}
[See all Deep Learning chapters ‚Üí](ML/deep-learning-book.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê MIT 18.06SC Linear Algebra <span class="section-count">36 lectures</span>

My journey through MIT's Linear Algebra course, focusing on building intuition and making connections between fundamental concepts.

::: {.content-grid}

::: {.content-card}
**[Lecture 27: Positive Definite Matrices and Minima](Math/MIT18.06/mit1806-lecture27-positive-definite-minima.qmd)**
Connecting positive definite matrices to multivariable calculus and optimization: the Hessian matrix, second derivative tests, and the geometric interpretation of quadratic forms as ellipsoids.
:::

::: {.content-card}
**[Lecture 26: Complex Matrices and Fast Fourier Transform](Math/MIT18.06/mit1806-lecture26-complex-matrices-fft.qmd)**
Extending linear algebra to complex vectors: Hermitian matrices, unitary matrices, and the Fast Fourier Transform algorithm that reduces DFT complexity from O(N¬≤) to O(N log N).
:::

::: {.content-card}
**[Lecture 28: Similar Matrices and Jordan Form](Math/MIT18.06/mit1806-lecture28-similar-matrices-jordan.qmd)**
When matrices share eigenvalues but differ in structure: similar matrices represent the same transformation in different bases, and Jordan form reveals the canonical structure when diagonalization fails.
:::

::: {.content-card}
**[Lecture 25: Symmetric Matrices and Positive Definiteness](Math/MIT18.06/mit1806-lecture25-symmetric-positive-definite.qmd)**
The beautiful structure of symmetric matrices: real eigenvalues, orthogonal eigenvectors, spectral decomposition, and the important concept of positive definiteness.
:::

:::

::: {.see-all-button}
[See all MIT 18.06SC lectures ‚Üí](Math/MIT18.06/lectures.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê MIT 18.065: Linear Algebra Applications <span class="section-count">2 lectures</span>

My notes from MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning‚Äîexploring how linear algebra powers modern applications.

::: {.content-grid}

::: {.content-card}
**[Lecture 9: Four Ways to Solve Least Squares Problems](Math/MIT18.065/mit18065-lecture9-least-squares.qmd)**
Four equivalent methods for solving $Ax = b$ when $A$ has no inverse: pseudo-inverse, normal equations, algebraic minimization, and geometric projection‚Äîall converging to the same optimal solution.
:::

::: {.content-card}
**[Lecture 8: Norms of Vectors and Matrices](Math/MIT18.065/mit18065-lecture8-norms.qmd)**
Understanding vector p-norms ($\|v\|_p$) and when they satisfy the triangle inequality (only for $p \geq 1$). The "$\frac{1}{2}$-norm" creates non-convex unit balls for strong sparsity.
:::

:::

::: {.see-all-button}
[See all MIT 18.065 lectures ‚Üí](Math/MIT18.065/lectures.qmd){.btn .btn-primary}
:::

:::

::: {.section-block}
### üìê Stanford EE 364A: Convex Optimization <span class="section-count">4 lectures</span>

My notes from Stanford EE 364A: Convex Optimization‚Äîtheory and applications of optimization problems.

::: {.content-grid}

::: {.content-card}
**[Lecture 4 Part 1: Operations preserving Convexity](Math/EE364A/ee364a-lecture4-part1-operations-preserving-convexity.qmd)**
Pointwise maximum and supremum (support function, distance to farthest point, maximum eigenvalue), composition with scalar functions (exponential, reciprocal), vector composition (log-sum-exp), minimization over convex sets (Schur complement, distance to set), and perspective functions with examples.
:::

::: {.content-card}
**[Lecture 3: Convex Functions](Math/EE364A/ee364a-lecture3-convex-functions.qmd)**
Separating and supporting hyperplane theorems, dual cones, convex function definition and examples, first-order condition (tangent underestimates), second-order condition (Hessian PSD), epigraph and sublevel sets, Jensen's inequality, and operations preserving convexity.
:::

::: {.content-card}
**[Lecture 2: Convex Sets](Math/EE364A/ee364a-lecture2-math-foundations.qmd)**
Convex sets (affine combinations, convex hull, convex cone), hyperplanes and halfspaces, Euclidean balls and ellipsoids, norm balls and norm cones, polyhedra, positive semidefinite cone. Operations that preserve convexity: intersection, affine functions, perspective, linear-fractional.
:::

::: {.content-card}
**[Lecture 1: Introduction to Convex Optimization](Math/EE364A/ee364a-lecture1-intro.qmd)**
Introduction to constraint optimization: least squares ($\|Ax-b\|_2^2$), linear programming, and convex optimization. Convex problems generalize both while maintaining polynomial-time solvability through interior-point methods.
:::

:::

::: {.see-all-button}
[See all EE 364A lectures ‚Üí](Math/EE364A/lectures.qmd){.btn .btn-primary}
:::

:::

---

::: {.footer-section}
## More Topics

::: {.footer-grid}

::: {.footer-card}
### Machine Learning
- [K-Means Clustering](ML/k_means_clustering.ipynb)
- [Logistic Regression](ML/logistic_regression.qmd)
- [Axis Operations](ML/axis.qmd)
:::

::: {.footer-card}
### Algorithms
- [DP Regex](Algorithm/dp_regex.qmd)
:::

:::
:::
